<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.1" />
<title>mdsine2.posterior API documentation</title>
<meta name="description" content="Posterior objects used for inference of MDSINE2 model" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>mdsine2.posterior</code></h1>
</header>
<section id="section-intro">
<p>Posterior objects used for inference of MDSINE2 model</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#39;&#39;&#39;Posterior objects used for inference of MDSINE2 model
&#39;&#39;&#39;
import logging
import time
import itertools
import psutil
import os
import pandas as pd

import numpy as np
import numba
import scipy.sparse
import numpy.random as npr
import scipy.stats
import scipy.sparse
import scipy
import math
import random
from orderedset import OrderedSet

from typing import Union, Dict, Iterator, Tuple, List, Any, IO

from .names import STRNAMES
from . import pylab as pl
from .pylab import Graph, Variable, variables
from .util import generate_cluster_assignments_posthoc, generate_taxonomic_distribution_over_clusters_posthoc

from . import visualization
import matplotlib.pyplot as plt
import seaborn as sns
_LOG_INV_SQRT_2PI = np.log(1/np.sqrt(2*math.pi))

# Helper functions
#-----------------
def _normal_logpdf(value: float, loc: float, scale: float) -&gt; float:
    &#39;&#39;&#39;We use this function if `pylab.random.normal.logpdf` fails to compile,
    which can happen when running jobs on the cluster.
    &#39;&#39;&#39;
    return _LOG_INV_SQRT_2PI + (-0.5*((value-loc)/scale)**2) - np.log(scale)

def negbin_loglikelihood(k: Union[float, int], m: Union[float, int], dispersion: Union[float, int]) -&gt; float:
    &#39;&#39;&#39;Loglikelihood - with parameterization in [1]

    Parameters
    ----------
    k : int
        Observed counts
    m : int
        Mean
    phi : float
        Dispersion

    Returns
    -------
    float
        Negative Binomial Log Likelihood

    References
    ----------
    [1] TE Gibson, GK Gerber. Robust and Scalable Models of Microbiome Dynamics. ICML (2018)
    &#39;&#39;&#39;
    r = 1/dispersion
    return math.lgamma(k+r) - math.lgamma(k+1) - math.lgamma(r) \
            + r * (math.log(r) - math.log(r+m)) + k * (math.log(m) - math.log(r+m))

@numba.jit(nopython=True, fastmath=True, cache=False)
def negbin_loglikelihood_MH_condensed(k: Union[float, int], m: Union[float, int], dispersion: Union[float, int]) -&gt; float:
        &#39;&#39;&#39;
        Loglikelihood - with parameterization in [1] - but condensed (do not calculate stuff
        we do not have to)

        Parameters
        ----------
        k : int
            Observed counts
        m : int
            Mean
        phi : float
            Dispersion

        Returns
        -------
        float
            Negative Binomial Log Likelihood

        References
        ----------
        [1] TE Gibson, GK Gerber. Robust and Scalable Models of Microbiome Dynamics. ICML (2018)
        &#39;&#39;&#39;
        r = 1/dispersion
        rm = r+m
        return math.lgamma(k+r) - math.lgamma(r) \
            + r * (math.log(r) - math.log(rm)) + k * (math.log(m) - math.log(rm))

def negbin_loglikelihood_MH_condensed_not_fast(k: Union[float, int], m: Union[float, int], dispersion: Union[float, int]) -&gt; float:
        &#39;&#39;&#39;
        Loglikelihood - with parameterization in [1] - but condensed (do not calculate stuff
        we do not have to). We use this function if `negbin_loglikelihood_MH_condensed` fails to
        compile, which can happen when doing jobs on the cluster

        Parameters
        ----------
        k : int
            Observed counts
        m : int
            Mean
        phi : float
            Dispersion

        Returns
        -------
        float
            Negative Binomial Log Likelihood

        References
        ----------
        [1] TE Gibson, GK Gerber. Robust and Scalable Models of Microbiome Dynamics. ICML (2018)
        &#39;&#39;&#39;
        r = 1/dispersion
        rm = r+m
        return math.lgamma(k+r) - math.lgamma(r) \
            + r * (math.log(r) - math.log(rm)) + k * (math.log(m) - math.log(rm))

def expected_n_clusters(G: Graph) -&gt; int:
    &#39;&#39;&#39;Calculate the expected number of clusters given the number of Taxa

    Parameters
    ----------
    G : pl.Graph
        Graph object

    Returns
    -------
    int
        Expected number of clusters
    &#39;&#39;&#39;
    conc = G[STRNAMES.CONCENTRATION].prior.mean()
    return conc * np.log((G.data.n_taxa + conc) / conc)

def build_prior_covariance(G: Graph, cov: bool, order: List[str], sparse: bool=True, 
    diag: bool=False) -&gt; Union[scipy.sparse.spmatrix, np.ndarray]:
    &#39;&#39;&#39;Build basic prior covariance or precision for the variables
    specified in `order`

    Parameters
    ----------
    G : pylab.graph.Graph
        Graph to get the variables from
    cov : bool
        If True, build the covariance. If False, build the precision
    order : list(str)
        Which parameters to get the priors of
    sparse : bool
        If True, return as a sparse matrix
    diag : bool
        If True, returns the diagonal of the matrix. If this is True, it
        overwhelms the flag `sparse`

    Returns
    -------
    arr : np.ndarray, scipy.sparse.dia_matrix, torch.DoubleTensor
        Prior covariance or precision matrix in either dense (np.ndarray) or
        sparse (scipy.sparse.dia_matrix) form
    &#39;&#39;&#39;
    n_taxa = G.data.n_taxa
    a = []
    for reprname in order:
        if reprname == STRNAMES.GROWTH_VALUE:
            a.append(np.full(n_taxa, G[STRNAMES.PRIOR_VAR_GROWTH].value))

        elif reprname == STRNAMES.SELF_INTERACTION_VALUE:
            a.append(np.full(n_taxa, G[STRNAMES.PRIOR_VAR_SELF_INTERACTIONS].value))

        elif reprname == STRNAMES.CLUSTER_INTERACTION_VALUE:
            n_interactions = G[STRNAMES.CLUSTER_INTERACTION_INDICATOR].num_pos_indicators
            a.append(np.full(n_interactions, G[STRNAMES.PRIOR_VAR_INTERACTIONS].value))

        elif reprname == STRNAMES.PERT_VALUE:
            for perturbation in G.perturbations:
                num_on = perturbation.indicator.num_on_clusters()
                a.append(np.full(
                    num_on,
                    perturbation.magnitude.prior.scale2.value))

        else:
            raise ValueError(&#39;reprname ({}) not recognized&#39;.format(reprname))

    if len(a) == 1:
        arr = np.asarray(a[0])
    else:
        arr = np.asarray(list(itertools.chain.from_iterable(a)))
    if not cov:
        arr = 1/arr
    if diag:
        return arr
    if sparse:
        return scipy.sparse.dia_matrix((arr,[0]), shape=(len(arr),len(arr))).tocsc()
    else:
        return np.diag(arr)

def build_prior_mean(G: Graph, order: List[str], shape: Tuple=None) -&gt; np.ndarray:
    &#39;&#39;&#39;Builds the prior mean vector for all the variables in `order`.

    Parameters
    ----------
    G : pylab.grapg.Graph
        Graph to index the objects
    order : list
        list of objects to add the priors of. If the variable is the
        cluster interactions or cluster perturbations, then we assume the
        prior mean is a scalar and we set that value for every single value.
    shape : tuple, None
        Shape to cast the array into

    Returns
    -------
    np.ndarray, torch.DoubleTensor
    &#39;&#39;&#39;
    a = []
    for name in order:
        v = G[name]
        if v.name == STRNAMES.GROWTH_VALUE:
            a.append(v.prior.loc.value * np.ones(G.data.n_taxa))
        elif v.name == STRNAMES.SELF_INTERACTION_VALUE:
            a.append(v.prior.loc.value * np.ones(G.data.n_taxa))
        elif v.name == STRNAMES.CLUSTER_INTERACTION_VALUE:
            a.append(
                np.full(
                    G[STRNAMES.CLUSTER_INTERACTION_INDICATOR].num_pos_indicators,
                    v.prior.loc.value))
        elif v.name == STRNAMES.PERT_VALUE:
            for perturbation in G.perturbations:
                a.append(np.full(
                    perturbation.indicator.num_on_clusters(),
                    perturbation.magnitude.prior.loc.value))
        else:
            raise ValueError(&#39;`name` ({}) not recognized&#39;.format(name))
    if len(a) == 1:
        a = np.asarray(a[0])
    else:
        a = np.asarray(list(itertools.chain.from_iterable(a)))
    if shape is not None:
        a = a.reshape(*shape)
    return a

def sample_categorical_log(log_p: Iterator[float]) -&gt; int:
    &#39;&#39;&#39;Generate one sample from a categorical distribution with event
    probabilities provided in unnormalized log-space.

    Parameters
    ----------
    log_p : array_like
        logarithms of event probabilities, ***which need not be normalized***

    Returns
    -------
    int
        One sample from the categorical distribution, given as the index of that
        event from log_p.
    &#39;&#39;&#39;
    try:
        exp_sample = math.log(random.random())
        events = np.logaddexp.accumulate(np.hstack([[-np.inf], log_p]))
        events -= events[-1]
        return next(x[0]-1 for x in enumerate(events) if x[1] &gt;= exp_sample)
    except:
        logging.critical(&#39;CRASHED IN `sample_categorical_log`:\nlog_p{}&#39;.format(
            log_p))
        raise

def log_det(M: np.ndarray, var: Variable) -&gt; float:
    &#39;&#39;&#39;Computes pl.math.log_det but also saves the array if it crashes

    Parameters
    ----------
    M : nxn matrix (np.ndarray, scipy.sparse)
        Matrix to calculate the log determinant
    var : pl.variable.Variable subclass
        This is the variable that `log_det` was called from

    Returns
    -------
    float
        Log determinant of matrix
    &#39;&#39;&#39;
    if scipy.sparse.issparse(M):
        M_ = np.zeros(shape=M.shape)
        M.toarray(out=M_)
        M = M_
    try:
        # if type(M) == torch.Tensor:
        #     return torch.inverse(M)
        # else:
        return pl.math.log_det(M)
    except:
        try:
            sample_iter = var.sample_iter
        except:
            sample_iter = None
        filename = &#39;crashes/logdet_error_iter{}_var{}pinv_{}.npy&#39;.format(
            sample_iter, var.name, var.G.name)
        logging.critical(&#39;\n\n\n\n\n\n\n\nSaved array at &#34;{}&#34; - now crashing\n\n\n&#39;.format(
                filename))
        os.makedirs(&#39;crashes/&#39;, exist_ok=True)
        np.save(filename, M)
        raise

def pinv(M: np.ndarray, var: Variable) -&gt; np.ndarray:
    &#39;&#39;&#39;Computes np.linalg.pinv but it also saves the array that crashed it if
    it crashes.

    Parameters
    ----------
    M : nxn matrix (np.ndarray, scipy.sparse)
        Matrix to invert
    var : pl.variable.Variable subclass
        This is the variable that `pinv` was called from

    Returns
    -------
    np.ndarray
        Inverse of the matrix
    &#39;&#39;&#39;
    if scipy.sparse.issparse(M):
        M_ = np.zeros(shape=M.shape)
        M.toarray(out=M_)
        M = M_
    try:
        try:
            return np.linalg.pinv(M)
        except:
            try:
                return scipy.linalg.pinv(M)
            except:
                return scipy.linalg.inv(M)
    except:
        try:
            sample_iter = var.sample_iter
        except:
            sample_iter = None
        filename = &#39;crashes/pinv_error_iter{}_var{}pinv_{}.npy&#39;.format(
            sample_iter, var.name, var.G.name)
        logging.critical(&#39;\n\n\n\n\n\n\n\nSaved array at &#34;{}&#34; - now crashing\n\n\n&#39;.format(
                filename))
        os.makedirs(&#39;crashes/&#39;, exist_ok=True)
        np.save(filename, M)
        raise

def _scalar_visualize(obj: Variable, path: str, f: IO, section: str=&#39;posterior&#39;, 
    log_scale: bool=True) -&gt; IO:
    &#39;&#39;&#39;Render the traces in the folder `basepath` and write the 
    learned values to the file `f`. This works for scalar variables

    Parameters
    ----------
    obj : mdsine2.Variable
    path : str
        This is the path to write the files to
    f : _io.TextIOWrapper
        File that we are writing the values to
    section : str
        Section of the trace to compute on. Options:
            &#39;posterior&#39; : posterior samples
            &#39;burnin&#39; : burn-in samples
            &#39;entire&#39; : both burn-in and posterior samples

    Returns
    -------
    _io.TextIOWrapper
    &#39;&#39;&#39;
    if f is not None:
        f.write(&#39;\n\n###################################\n{}&#39;.format(obj.name))
        f.write(&#39;\n###################################\n&#39;)
        if not obj.G.inference.tracer.is_being_traced(obj):
            f.write(&#39;`{}` not learned\n\tValue: {}\n&#39;.format(obj.name, obj.value))
            return f

    summ = pl.summary(obj, section=section)
    if f is not None:
        for k,v in summ.items():
            f.write(&#39;\t{}: {}\n&#39;.format(k,v))
    ax1, _ = visualization.render_trace(var=obj, plt_type=&#39;both&#39;, 
        section=section, include_burnin=True, log_scale=log_scale, rasterized=True)

    if pl.hasprior(obj):
        l,h = ax1.get_xlim()
        try:
            xs = np.arange(l,h,step=(h-l)/1000) 
            ys = []
            for x in xs:
                ys.append(obj.prior.pdf(value=x))
            ax1.plot(xs, ys, label=&#39;prior&#39;, alpha=0.5, color=&#39;red&#39;)
            ax1.legend()
        except OverflowError:
            logging.critical(&#39;OverflowError while plotting prior&#39;)
        except Exception as e:
            logging.critical(&#39;Failed plotting prior of {}: {}&#39;.format(
                obj.name, e))

    fig = plt.gcf()
    fig.suptitle(obj.name)
    plt.savefig(path)
    plt.close()
    return f

def _make_cluster_order(graph: Graph, section: str) -&gt; np.ndarray:
    &#39;&#39;&#39;Make a cluster ordering that is consistent

    If clustering was being learned, then we return the agglomerative
    clustering set with the mode number of clusters learned.

    If clustering was not learned, then we add the clusters in cluster
    order, and set the Taxa indexes sorted within each cluster.

    Parameters
    ----------
    graph : md2.Graph
        Graph object where all of the variables are stored
    section : str
        Section of the trace to calculate

    Returns
    -------
    np.ndarray
    &#39;&#39;&#39;
    order = []
    if graph.inference.is_in_inference_order(STRNAMES.CLUSTERING):
        ret = generate_cluster_assignments_posthoc(clustering=graph[STRNAMES.CLUSTERING_OBJ], 
            n_clusters=&#39;mode&#39;, section=section, set_as_value=False)
        
        for cidx in range(np.max(ret)+1):
            idxs = np.where(ret == cidx)[0]
            for idx in idxs:
                order.append(idx)
    else:
        for cluster in graph[STRNAMES.CLUSTERING_OBJ]:
            idxs = np.asarray(list(cluster.members))
            idxs = np.sort(idxs)
            for idx in idxs:
                order.append(idx)

    return order

# @numba.jit(nopython=True, fastmath=True, cache=True)
def prod_gaussians(means: np.ndarray, variances: np.ndarray) -&gt; Tuple[float, float]:
    &#39;&#39;&#39;Product of Gaussians

    $\mu = [\mu_1, \mu_2, ..., \mu_n]$
    $\var = [\var_1, \var_2, ..., \var_3]$

    Means and variances must be in the same order.

    Parameters
    ----------
    means : np.ndarray
        All of the means
    variances : np.ndarray
        All of the means

    Returns
    -------
    float, float
    &#39;&#39;&#39;
    def _calc_params(mu1, mu2, var1, var2):
        v = var1+var2
        mu = ((var1*mu2) + (var2*mu1))/(v)
        var = (var1*var2)/v
        return mu,var
    mu = means[0]
    var = variances[0]
    for i in range(1,len(means)):
        mu, var = _calc_params(mu1=mu, mu2=means[i], var1=var, var2=variances[i])
    return mu, var

class _Loess(object):
    &#39;&#39;&#39;LOESS - Locally Estimated Scatterplot Smoothing
    This module was created by João Paulo Figueira and copied from the 
    repository: https://github.com/joaofig/pyloess.git
    There are a few modifications, mostly to handle edge cases, i.e., what 
    happens when the entire thing is zero
    &#39;&#39;&#39;
    def __init__(self, xx, yy, degree=1):
        self.n_xx, self.min_xx, self.max_xx = self.normalize_array(xx)
        self.n_yy, self.min_yy, self.max_yy = self.normalize_array(yy)
        self.degree = degree

        self.input_yy = yy

    @staticmethod
    def _tricubic(x):
        y = np.zeros_like(x)
        idx = (x &gt;= -1) &amp; (x &lt;= 1)
        y[idx] = np.power(1.0 - np.power(np.abs(x[idx]), 3), 3)
        return y

    @staticmethod
    def normalize_array(array):
        min_val = np.min(array)
        max_val = np.max(array)
        return (array - min_val) / (max_val - min_val), min_val, max_val

    @staticmethod
    def get_min_range(distances, window):
        min_idx = np.argmin(distances)
        n = len(distances)
        if min_idx == 0:
            return np.arange(0, window)
        if min_idx == n-1:
            return np.arange(n - window, n)

        min_range = [min_idx]
        while len(min_range) &lt; window:
            i0 = min_range[0]
            i1 = min_range[-1]
            if i0 == 0:
                min_range.append(i1 + 1)
            elif i1 == n-1:
                min_range.insert(0, i0 - 1)
            elif distances[i0-1] &lt; distances[i1+1]:
                min_range.insert(0, i0 - 1)
            else:
                min_range.append(i1 + 1)
        return np.array(min_range)

    @staticmethod
    def get_weights(distances, min_range):
        max_distance = np.max(distances[min_range])
        weights = _Loess._tricubic(distances[min_range] / max_distance)
        return weights

    def normalize_x(self, value):
        return (value - self.min_xx) / (self.max_xx - self.min_xx)

    def denormalize_y(self, value):
        return value * (self.max_yy - self.min_yy) + self.min_yy

    def estimate(self, x, window, use_matrix=False, degree=1):
        n_x = self.normalize_x(x)
        distances = np.abs(self.n_xx - n_x)
        min_range = self.get_min_range(distances, window)
        weights = self.get_weights(distances, min_range)

        if use_matrix or degree &gt; 1:
            wm = np.multiply(np.eye(window), weights)
            xm = np.ones((window, degree + 1))

            xp = np.array([[math.pow(n_x, p)] for p in range(degree + 1)])
            for i in range(1, degree + 1):
                xm[:, i] = np.power(self.n_xx[min_range], i)

            ym = self.n_yy[min_range]
            xmt_wm = np.transpose(xm) @ wm
            beta = np.linalg.pinv(xmt_wm @ xm) @ xmt_wm @ ym
            y = (beta @ xp)[0]
        else:
            xx = self.n_xx[min_range]
            yy = self.n_yy[min_range]
            sum_weight = np.sum(weights)
            sum_weight_x = np.dot(xx, weights)
            sum_weight_y = np.dot(yy, weights)
            sum_weight_x2 = np.dot(np.multiply(xx, xx), weights)
            sum_weight_xy = np.dot(np.multiply(xx, yy), weights)

            mean_x = sum_weight_x / sum_weight
            mean_y = sum_weight_y / sum_weight

            b = (sum_weight_xy - mean_x * mean_y * sum_weight) / \
                (sum_weight_x2 - mean_x * mean_x * sum_weight)
            a = mean_y - b * mean_x
            y = a + b * n_x
        
        ret = self.denormalize_y(y)
        if np.isnan(ret):
            if np.all(self.input_yy == 0):
                return 0
            else:
                raise ValueError(&#39;`Returning `np.nan`&#39;)
        return ret


# Process Variance
# ----------------
class ProcessVarGlobal(pl.variables.SICS):
    &#39;&#39;&#39;Learn a Process variance where we learn th same process variance
    for each Taxa. This assumes that the model we&#39;re using uses the logscale
    of the data.
    &#39;&#39;&#39;
    def __init__(self, prior: variables.SICS, **kwargs):
        &#39;&#39;&#39;
        Parameters
        ----------
        prior : pl.variables.SICS
            This is the prior of the distribution
        kwargs : dict
            These are the extra parameters for the Variable class
        &#39;&#39;&#39;
        kwargs[&#39;name&#39;] = STRNAMES.PROCESSVAR
        pl.variables.SICS.__init__(self, dtype=float, **kwargs)
        self.add_prior(prior)
        self.global_variance = True
        self._strr = &#39;NA&#39;

    def __str__(self) -&gt; str:
        return self._strr

    def initialize(self, dof_option: str, scale_option: str, value_option: str, 
        dof: Union[float, int]=None, scale: Union[float, int]=None, 
        value: Union[float, int]=None, variance_scaling: Union[float, int]=1,
        delay: int=0):
        &#39;&#39;&#39;Initialize the value and hyperparameter.

        Parameters
        ----------
        dof_option : str
            How to initialize the `dof` parameter. Options:
                &#39;manual&#39;
                    Manually specify the dof with the parameter `dof`
                &#39;half&#39;
                    Set the degrees of freedom to the number of data points
                &#39;auto&#39;, &#39;diffuse&#39;
                    Set the degrees of freedom to a sparse number (2.5)
        scale_option : str
            How to initialize the scale of the parameter. Options:
                &#39;manual&#39;
                    Need to also specify `scale` parameter
                &#39;med&#39;, &#39;auto&#39;
                    Set the scale such that mean of the distribution
                    has medium noise (20%)
                &#39;low&#39;
                    Set the scale such that mean of the distribution
                    has low noise (10%)
                &#39;high&#39;
                    Set the scale such that mean of the distribution
                    has high noise (30%)
        value_option : str
            How to initialize the value
                &#39;manual&#39;
                    Set the value with the `value` parameter
                &#39;prior-mean&#39;, &#39;auto&#39;
                    Set the value to the mean of the prior
        variance_scaling : float, None
            How much to inflate the variance
        &#39;&#39;&#39;
        if not pl.isint(delay):
            raise TypeError(&#39;`delay` ({}) must be an int&#39;.format(type(delay)))
        if delay &lt; 0:
            raise ValueError(&#39;`delay` ({}) must be &gt;= 0&#39;.format(delay))
        self.delay = delay

        # Set the dof
        if not pl.isstr(dof_option):
            raise TypeError(&#39;`dof_option` ({}) must be a str&#39;.format(type(dof_option)))
        if dof_option == &#39;manual&#39;:
            if not pl.isnumeric(dof):
                raise TypeError(&#39;`dof` ({}) must be a numeric&#39;.format(type(dof)))
            if dof &lt;= 0:
                raise ValueError(&#39;`dof` ({}) must be &gt; 0&#39;.format(dof))
            if dof &lt;= 2:
                logging.critical(&#39;Process Variance dof ({}) is set unproper&#39;.format(dof))
        elif dof_option == &#39;half&#39;:
            dof = len(self.G.data.lhs)
        elif dof_option in [&#39;auto&#39;, &#39;diffuse&#39;]:
            dof = 2.5
        else:
            raise ValueError(&#39;`dof_option` ({}) not recognized&#39;.format(dof_option))
        self.prior.dof.override_value(dof)

        # Set the scale
        if not pl.isstr(scale_option):
            raise TypeError(&#39;`scale_option` ({}) must be a str&#39;.format(type(scale_option)))
        if scale_option == &#39;manual&#39;:
            if not pl.isnumeric(scale):
                raise TypeError(&#39;`scale` ({}) must be a numeric&#39;.format(type(scale)))
            if scale &lt;= 0:
                raise ValueError(&#39;`scale` ({}) must be &gt; 0&#39;.format(scale))
        elif scale_option in [&#39;auto&#39;, &#39;med&#39;]:
            scale = (0.2 ** 2) * (self.prior.dof.value - 2) / self.prior.dof.value
        elif scale_option ==  &#39;low&#39;:
            scale = (0.1 ** 2) * (self.prior.dof.value - 2) / self.prior.dof.value
        elif scale_option ==  &#39;high&#39;:
            scale = (0.3 ** 2) * (self.prior.dof.value - 2) / self.prior.dof.value
        else:
            raise ValueError(&#39;`scale_option` ({}) not recognized&#39;.format(scale_option))
        self.prior.scale.override_value(scale)

        # Set the value
        if not pl.isstr(value_option):
            raise TypeError(&#39;`value_option` ({}) must be a str&#39;.format(type(value_option)))
        if value_option == &#39;manual&#39;:
            if not pl.isnumeric(dof):
                raise TypeError(&#39;`value` ({}) must be a numeric&#39;.format(type(value)))
            if value &lt;= 0:
                raise ValueError(&#39;`value` ({}) must be &gt; 0&#39;.format(value))
        elif value_option in [&#39;prior-mean&#39;, &#39;auto&#39;]:
            value = self.prior.mean()
        else:
            raise ValueError(&#39;`value_option` ({}) not recognized&#39;.format(value_option))
        self.value = value
        
        self.rebuild_diag()
        self._there_are_perturbations = self.G.perturbations is not None

    def build_matrix(self, cov: bool, sparse: bool=True) -&gt; Union[scipy.sparse.spmatrix, np.ndarray]:
        &#39;&#39;&#39;Builds the process variance as a covariance or precision
        matrix.

        Parameters
        ----------
        cov : bool
            If True, make the covariance matrix
        sparse : bool
            If True, return the matrix as a sparse matrix

        Returns
        -------
        np.ndarray or scipy.sparse
            Either sparse or dense covariance/precision matrix
        &#39;&#39;&#39;
        a = self.diag
        if not cov:
            a = 1/a
        if sparse:
            return scipy.sparse.dia_matrix((a,[0]), shape=(len(a),len(a))).tocsc()
        else:
            return np.diag(a)

    def rebuild_diag(self):
        &#39;&#39;&#39;Builds up the process variance diagonal that we use to make the matrix
        &#39;&#39;&#39;
        a = self.value / self.G.data.dt_vec
        if self.G.data.zero_inflation_transition_policy is not None:
            a = a[self.G.data.rows_to_include_zero_inflation]
        
        self.diag = a
        self.prec = 1/a

    def update(self):
        &#39;&#39;&#39;Update the process variance

        ..math:: y = (log(x_{k+1}) - log(x_k))/dt
        ..math:: Xb = a_1 (1 + \\gamma) + A x

        % These are our dynamics with the process variance
        ..math:: y ~ Normal(Xb , \\sigma^2_w / dt)

        % Subtract the mean
        ..math:: y - Xb ~ Normal( 0, \sigma^2_w / dt)
        
        % Substitute
        ..math:: z = y - Xb 

        ..math:: z ~ Normal (0, \\sigma^2_w / dt)
        ..math:: z * \\sqrt{dt} ~ Normal (0, \\sigma^2_w)

        This is now in a form we can use to calculate the posterior
        &#39;&#39;&#39;
        if self._there_are_perturbations:
            lhs = [
                STRNAMES.GROWTH_VALUE, 
                STRNAMES.SELF_INTERACTION_VALUE,
                STRNAMES.CLUSTER_INTERACTION_VALUE]
        else:
            lhs = [
                STRNAMES.GROWTH_VALUE, 
                STRNAMES.SELF_INTERACTION_VALUE,
                STRNAMES.CLUSTER_INTERACTION_VALUE]
        
        # This is the residual z = y - Xb
        z = self.G.data.construct_lhs(lhs, 
            kwargs_dict={STRNAMES.GROWTH_VALUE:{
                &#39;with_perturbations&#39;: self._there_are_perturbations}})
        z = np.asarray(z).ravel()
        if self.G.data.zero_inflation_transition_policy is not None:
            z = z * self.G.data.sqrt_dt_vec[self.G.data.rows_to_include_zero_inflation]
        else:
            z = z * self.G.data.sqrt_dt_vec
        residual = np.sum(np.square(z))

        self.dof.value = self.prior.dof.value + len(z)
        self.scale.value = ((self.prior.scale.value * self.prior.dof.value) + \
           residual)/self.dof.value
        
        # shape = 2 + (len(residual)/2)
        # scale = 0.00001 + np.sum(np.square(residual))/2
        # self.value = pl.random.invgamma.sample(shape=shape, scale=scale)
        self.sample()
        self.rebuild_diag()

        self._strr = &#39;{}, empirical_variance: {:.5f}&#39;.format(self.value, 
            residual/len(z))

    def visualize(self, path: str, section: str=&#39;posterior&#39;) -&gt; Any:
        return _scalar_visualize(self, path=path, f=None, section=section)

# Clustering
# ----------
class Concentration(pl.variables.Gamma):
    &#39;&#39;&#39;Defines the posterior for the concentration parameter that is used
    in learning the cluster assignments.
    The posterior is implemented as it is describes in &#39;Bayesian Inference
    for Density Estimation&#39; by M. D. Escobar and M. West, 1995.

    Parameters
    ----------
    prior : mdsine2.variables.Gamma
        Prior distribution
    value : float, int
        - Initial value of the concentration
        - Default value is the mean of the prior
    n_iter : int
        How many times to resample
    &#39;&#39;&#39;
    def __init__(self, prior: variables.Gamma, value: Union[float, int]=None, 
        n_iter: int=None, **kwargs):
        kwargs[&#39;name&#39;] = STRNAMES.CONCENTRATION
        # initialize shape and scale as the same as the priors
        # we will be updating this later
        pl.variables.Gamma.__init__(self, shape=prior.shape.value, scale=prior.scale.value,
            dtype=float, **kwargs)
        self.add_prior(prior)

    def initialize(self, value_option: str, hyperparam_option: str, n_iter: int=None, 
        value: Union[int, float]=None, shape: Union[int, float]=None, scale: Union[int, float]=None, 
        delay: int=0):
        &#39;&#39;&#39;Initialize the hyperparameters of the beta prior

        Parameters
        ----------
        value_option : str
            - Options to initialize the value
            - &#39;manual&#39;
                - Set the value manually, `value` must also be specified
            - &#39;auto&#39;, &#39;prior-mean&#39;
                - Set to the mean of the prior
        hyperparam_option : str
            - Options ot initialize the hyperparameters
            - Options
                - &#39;manual&#39;
                    - Set the values manually. `shape` and `scale` must also be specified
                - &#39;auto&#39;, &#39;diffuse&#39;
                    - shape = 1e-5, scale= 1e5
        shape, scale : int, float
            - User specified values
            - Only necessary if `hyperparam_option` == &#39;manual&#39;
        &#39;&#39;&#39;
        if not pl.isint(delay):
            raise TypeError(&#39;`delay` ({}) must be an int&#39;.format(type(delay)))
        if delay &lt; 0:
            raise ValueError(&#39;`delay` ({}) must be &gt;= 0&#39;.format(delay))
        self.delay = delay

        if hyperparam_option == &#39;manual&#39;:
            self.prior.shape.override_value(shape)
            self.prior.scale.override_value(scale)
            self.n_iter = n_iter

        elif hyperparam_option in [&#39;diffuse&#39;, &#39;auto&#39;]:
            self.prior.shape.override_value(1e-5)
            self.prior.scale.override_value(1e5)
            self.n_iter = 20
        else:
            raise ValueError(&#39;hyperparam_option `{}` not recognized&#39;.format(hyperparam_option))

        if value_option == &#39;manual&#39;:
            self.value = value
        elif value_option in [&#39;auto&#39;, &#39;prior-mean&#39;]:
            self.value = self.prior.mean()
        else:
            raise ValueError(&#39;value_option `{}` not recognized&#39;.format(value_option))

        self.shape.value = self.prior.shape.value
        self.scale.value = self.prior.scale.value
        logging.info(&#39;Cluster Concentration initialization results:\n&#39; \
            &#39;\tprior shape: {}\n\tprior scale: {}\n\tvalue: {}&#39;.format(
                self.prior.shape.value, self.prior.scale.value, self.value))

    def update(self):
        &#39;&#39;&#39;Sample the posterior of the concentration parameter
        &#39;&#39;&#39;
        if self.sample_iter &lt; self.delay:
            return

        clustering = self.G[STRNAMES.CLUSTER_INTERACTION_VALUE].clustering
        k = len(clustering)
        n = self.G.data.n_taxa
        for _ in range(self.n_iter):
            #first sample eta from a beta distribution
            eta = npr.beta(self.value+1,n)
            #sample alpha from a mixture of gammas
            pi_eta = [0.0, n]
            pi_eta[0] = (self.prior.shape.value+k-1.)/(1/(self.prior.scale.value)-np.log(eta))
            self.scale.value =  1/(1/self.prior.scale.value - np.log(eta))
            self.shape.value = self.prior.shape.value + k
            if np.random.choice([0,1], p=pi_eta/np.sum(pi_eta)) != 0:
                self.shape.value -= 1
            self.sample()

    def visualize(self, path: str, f: IO, section: str=&#39;posterior&#39;) -&gt; IO:
        &#39;&#39;&#39;Render the traces in the folder `basepath` and write the 
        learned values to the file `f`.

        Parameters
        ----------
        path : str
            This is the path to write the files to
        f : _io.TextIOWrapper
            File that we are writing the values to
        section : str
            Section of the trace to compute on. Options:
                &#39;posterior&#39; : posterior samples
                &#39;burnin&#39; : burn-in samples
                &#39;entire&#39; : both burn-in and posterior samples

        Returns
        -------
        _io.TextIOWrapper
        &#39;&#39;&#39;
        return _scalar_visualize(self, path=path, f=f, section=section)


class ClusterAssignments(pl.graph.Node):
    &#39;&#39;&#39;This is the posterior of the cluster assignments for each Taxa.

    To calculate the loglikelihood of an Taxa being in a cluster, we have to
    marginalize out the cluster that the Taxa belongs to - this means marginalizing
    out the interactions going in and out of the cluster in question, along with all
    the perturbations associated with it.

    Parameters
    ----------
    clustering : pylab.cluster.Clustering
        - Defines the clusters
    concentration : pylab.Variable or subclass of pylab.Variable
        - Defines the concentration parameter for the base distribution
    m : int, Optional
        - Number of auxiliary variables defined in the model
        - Default is 1
    mp : str, None
        - This is the type of multiprocessing it is going to be. Options:
            - None
                - No multiprocessing
            - &#39;full-#&#39;
                - Send out to the different processors, where &#39;#&#39; is the number of
                  processors to make
            - &#39;debug&#39;
                - Send out to the different classes but stay on a single core. This
                  is necessary for benchmarking and easier debugging.
    &#39;&#39;&#39;
    def __init__(self, clustering: pl.Clustering, concentration: Concentration,
        m: int=1, mp: str=None, **kwargs):
        self.clustering = clustering # mdsine2.pylab.cluster.Clustering object
        self.concentration = concentration # mdsine2.posterior.Concentration
        self.m = m # int
        self.mp = mp # float
        self._strtime = -1

        # if self.mp is not None:
        #     self.update = self.update_mp
        # else:
        #     self.update = self.update_slow_fast
        self.update = self.update_slow_fast #self.update_mp

        kwargs[&#39;name&#39;] = STRNAMES.CLUSTERING
        pl.graph.Node.__init__(self, **kwargs)

    def __str__(self) -&gt; str:
        try:
            if self.mp == &#39;debug&#39;:
                mpstr = &#39;no mp&#39;
            else:
                mpstr = &#39;mp&#39;
        except:
            mpstr = &#39;NA&#39;
        return str(self.clustering) + &#39;\n{} - Total time: {}&#39;.format(mpstr,
            self._strtime)

    def __getstate__(self):
        state = self.__dict__.copy()
        state.pop(&#39;pool&#39;, None)
        state.pop(&#39;actors&#39;, None)
        return state

    def __setstate__(self, state):
        self.__dict__.update(state)
        self.pool = []
        self.actors = None

    @property
    def value(self) -&gt; pl.Clustering:
        &#39;&#39;&#39;This is so that the cluster assignments get printed in inference.MCMC
        &#39;&#39;&#39;
        return self.clustering

    @property
    def sample_iter(self) -&gt; int:
        &#39;&#39;&#39;Sample iteration
        &#39;&#39;&#39;
        return self.clustering.n_clusters.sample_iter

    def initialize(self, value_option: str, hyperparam_option: str=None, value: Union[np.ndarray, str, List]=None, 
        n_clusters: Union[int, str]=None, delay: int=0, run_every_n_iterations: int=1):
        &#39;&#39;&#39;Initialize the cluster assingments - there are no hyperparamters to
        initialize because the concentration is initialized somewhere else

        Note - if `n_clusters` is not specified and the cluster initialization
        method requires it - it will be set to the expected number of clusters
        which = log(n_taxa)/log(2)

        Parameters
        ----------
        value_option : str
            - The different methods to initialize the clusters. Options:
                - &#39;manual&#39;
                    - Manually set the cluster assignments
                - &#39;no-clusters&#39;
                    - Every Taxa in their own cluster
                - &#39;random&#39;
                    - Every Taxa is randomly assigned to the number of clusters. `n_clusters` required
                - &#39;taxonomy&#39;
                    - Cluster Taxa based on their taxonomic similarity. `n_clusters` required
                - &#39;sequence&#39;
                    - Cluster Taxa based on their sequence similarity. `n_clusters` required
                - &#39;phylogeny&#39;
                    - Cluster Taxa based on their phylogenetic similarity. `n_clusters` required
                - &#39;spearman&#39;, &#39;auto&#39;
                    - Creates a distance matrix based on the spearman rank similarity
                      between two trajectories. We use the raw data. `n_clusters` required
                - &#39;fixed-clustering&#39;
                    - Sets the clustering assignment to the most likely clustering configuration
                      specified in the graph at the location `value` (`value` is a str).
                    - We take the mean coclusterings and do agglomerative clustering on that matrix
                      with the `mode` number of clusters.
        hyperparam_option : None
            Not used in this function - only here for API consistency
        value : list of list
            - Cluster assingments for each of the Taxa
            - Only necessary if `value_option` == &#39;manual&#39;
        n_clusters : int, str
            - Necessary if `value_option` is not &#39;manual&#39; or &#39;no-clusters&#39;.
            - If str, options: &#39;expected&#39;, &#39;auto&#39;: log_2(n_taxa)
        run_every_n_iterations : int
            Only run the update every `run_every_n_iterations` iterations
        &#39;&#39;&#39;
        from sklearn.cluster import AgglomerativeClustering
        from .util import generate_cluster_assignments_posthoc
        taxa = self.G.data.taxa

        self.run_every_n_iterations = run_every_n_iterations
        self.delay = delay

        if value_option not in [&#39;manual&#39;, &#39;no-clusters&#39;, &#39;fixed-clustering&#39;]:
            if pl.isstr(n_clusters):
                if n_clusters in [&#39;expected&#39;, &#39;auto&#39;]:
                    n_clusters = expected_n_clusters(self.G)
                else:
                    raise ValueError(&#39;`n_clusters` ({}) not recognized&#39;.format(n_clusters))
            if not pl.isint(n_clusters):
                raise TypeError(&#39;`n_clusters` ({}) must be a str or an int&#39;.format(type(n_clusters)))
            if n_clusters &lt;= 0:
                raise ValueError(&#39;`n_clusters` ({}) must be &gt; 0&#39;.format(n_clusters))
            if n_clusters &gt; self.G.data.n_taxa:
                raise ValueError(&#39;`n_clusters` ({}) must be &lt;= than the number of s ({})&#39;.format(
                    n_clusters, self.G.data.n_taxa))

        if value_option == &#39;manual&#39;:
            # Check that all of the s are in the init and that it is in the right structure
            if not pl.isarray(value):
                raise ValueError(&#39;if `value_option` is &#34;manual&#34;, value ({}) must &#39; \
                    &#39;be of type array&#39;.format(value.__class__))
            clusters = list(value)

            idxs_to_delete = []
            for idx, cluster in enumerate(clusters):
                if not pl.isarray(cluster):
                    raise ValueError(&#39;cluster at index `{}` ({}) is not an array&#39;.format(
                        idx, cluster))
                cluster = list(cluster)
                if len(cluster) == 0:
                    logging.warning(&#39;Cluster index {} has 0 elements, deleting&#39;.format(
                        idx))
                    idxs_to_delete.append(idx)
            if len(idxs_to_delete) &gt; 0:
                clusters = np.delete(clusters, idxs_to_delete).tolist()

            all_oidxs = OrderedSet()
            for cluster in clusters:
                for oidx in cluster:
                    if not pl.isint(oidx):
                        raise ValueError(&#39;`oidx` ({}) must be an int&#39;.format(oidx.__class__))
                    if oidx &gt;= len(taxa):
                        raise ValueError(&#39;oidx `{}` not in our TaxaSet&#39;.format(oidx))
                    all_oidxs.add(oidx)

            for oidx in range(len(taxa)):
                if oidx not in all_oidxs:
                    raise ValueError(&#39;oidx `{}` in TaxaSet not in `value` ({})&#39;.format(
                        oidx, value))
            # Now everything is checked and valid

        elif value_option == &#39;fixed-clustering&#39;:
            logging.info(&#39;Fixed topology initialization&#39;)
            if not pl.isstr(value):
                raise TypeError(&#39;`value` ({}) must be a str&#39;.format(value))

            CHAIN2 = pl.inference.BaseMCMC.load(value)
            CLUSTERING2 = CHAIN2.graph[STRNAMES.CLUSTERING_OBJ]
            TAXA2 = CHAIN2.graph.data.taxa
            taxa_curr = self.G.data.taxa
            for taxon in TAXA2:
                if taxon.name not in taxa_curr:
                    raise ValueError(&#39;Cannot perform fixed topology because the  {} in &#39; \
                        &#39;the passed in clustering is not in this clustering: {}&#39;.format(
                            taxon.name, taxa_curr.names.order))
            for taxon in taxa_curr:
                if taxon.name not in TAXA2:
                    raise ValueError(&#39;Cannot perform fixed topology because the  {} in &#39; \
                        &#39;the current clustering is not in the passed in clustering: {}&#39;.format(
                            taxon.name, TAXA2.names.order))

            # Get the most likely cluster configuration and set as the value for the passed in cluster
            ret = generate_cluster_assignments_posthoc(CLUSTERING2, n_clusters=&#39;mode&#39;, set_as_value=False)
            CLUSTERING2.from_array(ret)
            logging.info(&#39;Clustering set to:\n{}&#39;.format(str(CLUSTERING2)))

            # Set the passed in cluster assignment as the current cluster assignment
            # Need to be careful because the indices of the s might not line up
            clusters = []
            for cluster in CLUSTERING2:
                anames = [taxa_curr[TAXA2.names.order[aidx]].name for aidx in cluster.members]
                aidxs = [taxa_curr[aname].idx for aname in anames]
                clusters.append(aidxs)

        elif value_option == &#39;no-clusters&#39;:
            clusters = []
            for oidx in range(len(taxa)):
                clusters.append([oidx])

        elif value_option == &#39;random&#39;:
            clusters = {}
            for oidx in range(len(taxa)):
                idx = npr.choice(n_clusters)
                if idx in clusters:
                    clusters[idx].append(oidx)
                else:
                    clusters[idx] = [oidx]
            c = []
            for cid in clusters.keys():
                c.append(clusters[cid])
            clusters = c

        elif value_option == &#39;taxonomy&#39;:
            # Create an affinity matrix, we can precompute the self-similarity to 1
            M = np.diag(np.ones(len(taxa), dtype=float))
            for i, oid1 in enumerate(taxa.ids.order):
                for j, oid2 in enumerate(taxa.ids.order):
                    if i == j:
                        continue
                    M[i,j] = taxa.taxonomic_similarity(oid1=oid1, oid2=oid2)

            c = AgglomerativeClustering(n_clusters=n_clusters, affinity=&#39;precomputed&#39;, linkage=&#39;complete&#39;)
            assignments = c.fit_predict(1-M)

            # Convert assignments into clusters
            clusters = {}
            for oidx,cidx in enumerate(assignments):
                if cidx not in clusters:
                    clusters[cidx] = []
                clusters[cidx].append(oidx)
            clusters = [val for val in clusters.values()]

        elif value_option == &#39;sequence&#39;:
            from pylab import diversity

            logging.info(&#39;Making affinity matrix from sequences&#39;)
            evenness = np.diag(np.ones(len(self.G.data.taxa), dtype=float))

            for i in range(len(self.G.data.taxa)):
                for j in range(len(self.G.data.taxa)):
                    if j &lt;= i:
                        continue
                    # Subtract because we want to make a similarity matrix
                    dist = 1-diversity.beta.hamming(
                        list(self.G.data.taxa[i].sequence),
                        list(self.G.data.taxa[j].sequence))
                    evenness[i,j] = dist
                    evenness[j,i] = dist

            c = AgglomerativeClustering(n_clusters=n_clusters, affinity=&#39;precomputed&#39;, linkage=&#39;average&#39;)
            assignments = c.fit_predict(evenness)
            clusters = {}
            for oidx,cidx in enumerate(assignments):
                if cidx not in clusters:
                    clusters[cidx] = []
                clusters[cidx].append(oidx)
            clusters = [val for val in clusters.values()]

        elif value_option == &#39;spearman&#39;:
            # Use spearman correlation to create a distance matrix
            # Use agglomerative clustering to make the clusters based
            # on distance matrix (distance = 1 - pearson(x,y))
            dm = np.zeros(shape=(len(taxa), len(taxa)))
            data = []
            for ridx in range(self.G.data.n_replicates):
                data.append(self.G.data.abs_data[ridx])
            data = np.hstack(data)
            for i in range(len(taxa)):
                for j in range(i+1):
                    distance = (1 - scipy.stats.spearmanr(data[i, :], data[j, :])[0])/2
                    dm[i,j] = distance
                    dm[j,i] = distance

            c = AgglomerativeClustering(n_clusters=n_clusters, affinity=&#39;precomputed&#39;, linkage=&#39;complete&#39;)
            assignments = c.fit_predict(dm)

            # convert into clusters
            clusters = {}
            for oidx, cidx in enumerate(assignments):
                if cidx not in clusters:
                    clusters[cidx] = []
                clusters[cidx].append(oidx)
            clusters = [val for val in clusters.values()]

        elif value_option == &#39;phylogeny&#39;:
            raise NotImplementedError(&#39;`phylogeny` not implemented yet&#39;)

        else:
            raise ValueError(&#39;`value_option` &#34;{}&#34; not recognized&#39;.format(value_option))

        # Move all the taxa into their assigned clusters
        for cluster in clusters:
            cid = None
            for oidx in cluster:
                if cid is None:
                    # make new cluster
                    cid = self.clustering.make_new_cluster_with(idx=oidx)
                else:
                    self.clustering.move_item(idx=oidx, cid=cid)
        logging.info(&#39;Cluster Assingments initialization results:\n{}&#39;.format(
            str(self.clustering)))
        self._there_are_perturbations = self.G.perturbations is not None

        # Initialize the multiprocessors if necessary
        if self.mp is not None:
            if not pl.isstr(self.mp):
                raise TypeError(&#39;`mp` ({}) must be a str&#39;.format(type(self.mp)))
            if &#39;full&#39; in self.mp:
                n_cpus = self.mp.split(&#39;-&#39;)[1]
                if n_cpus == &#39;auto&#39;:
                    self.n_cpus = psutil.cpu_count(logical=False)
                else:
                    try:
                        self.n_cpus = int(n_cpus)
                    except:
                        raise ValueError(&#39;`mp` ({}) not recognized&#39;.format(self.mp))
                self.pool = pl.multiprocessing.PersistentPool(ptype=&#39;dasw&#39;, G=self.G)
            elif self.mp == &#39;debug&#39;:
                self.pool = None
            else:
                raise ValueError(&#39;`mp` ({}) not recognized&#39;.format(self.mp))
        else:
            self.pool = None

        self.ndts_bias = []
        self.n_taxa = len(self.G.data.taxa)
        self.n_replicates = self.G.data.n_replicates
        self.n_dts_for_replicate = self.G.data.n_dts_for_replicate
        self.total_dts = np.sum(self.n_dts_for_replicate)
        for ridx in range(self.G.data.n_replicates):
            self.ndts_bias.append(
                np.arange(0, self.G.data.n_dts_for_replicate[ridx] * self.n_taxa, self.n_taxa))
        self.replicate_bias = np.zeros(self.n_replicates, dtype=int)
        for ridx in range(1, self.n_replicates):
            self.replicate_bias[ridx] = self.replicate_bias[ridx-1] + \
                self.n_taxa * self.n_dts_for_replicate[ridx - 1]

    def visualize(self, basepath: str, f: IO, section: str=&#39;posterior&#39;, 
        taxa_formatter: str=&#39;%(paperformat)s&#39;, yticklabels: str=&#39;%(paperformat)s %(index)s&#39;, 
        xticklabels: str=&#39;%(index)s&#39;, tax_fmt: str=&#39;%(family)s&#39;) -&gt; IO:
        &#39;&#39;&#39;Render the traces in the folder `basepath` and write the 
        learned values to the file `f`.

        Parameters
        ----------
        basepath : str
            This is the loction to write the files to
        f : _io.TextIOWrapper
            File that we are writing the values to
        section : str
            Section of the trace to compute on. Options:
                &#39;posterior&#39; : posterior samples
                &#39;burnin&#39; : burn-in samples
                &#39;entire&#39; : both burn-in and posterior samples
        taxa_formatter : str, None
            This is the format of the label to return for each . If None, it will return
            the taxon&#39;s name
        yticklabels, xticklabels : str
            These are the formats to plot the y-axis and x0axis, respectively.
        tax_fmt : str
            This is the format to render the taxonomic distiribution plot

        Returns
        -------
        _io.TextIOWrapper
        &#39;&#39;&#39;
        from matplotlib.colors import LogNorm
        taxa = self.G.data.taxa
        f.write(&#39;\n\n###################################\n&#39;)
        f.write(self.name)
        f.write(&#39;\n###################################\n&#39;)
        if not self.G.inference.is_in_inference_order(self):
            f.write(&#39;`{}` not learned. These were the fixed cluster assignments\n&#39;.format(self.name))
            for cidx, cluster in enumerate(self.clustering):
                f.write(&#39;Cluster {}:\n&#39;.format(cidx+1))
                for aidx in cluster.members:
                    label = pl.taxaname_formatter(format=taxa_formatter, taxon=taxa[aidx], taxa=taxa)
                    f.write(&#39;\t- {}\n&#39;.format(label))
            return f

        # Coclusters
        cocluster_trace = self.clustering.coclusters.get_trace_from_disk(section=section)
        coclusters = pl.variables.summary(cocluster_trace, section=section)[&#39;mean&#39;]
        for i in range(coclusters.shape[0]):
            coclusters[i,i] = np.nan

        # N clusters
        visualization.render_trace(var=self.clustering.n_clusters, plt_type=&#39;both&#39;, 
            section=section, include_burnin=True, rasterized=True)
        fig = plt.gcf()
        fig.suptitle(&#39;Number of Clusters&#39;)
        plt.savefig(os.path.join(basepath, &#39;n_clusters.pdf&#39;))
        plt.close()

        ret = generate_cluster_assignments_posthoc(clustering=self.clustering, n_clusters=&#39;mode&#39;, 
            section=section, set_as_value=True)
        data = []
        for taxon in taxa:
            data.append([taxon.name, ret[taxon.idx]])
        df = pd.DataFrame(data, columns=[&#39;name&#39;, &#39;Cluster Assignment&#39;])
        df.to_csv(os.path.join(basepath, &#39;clusterassignments.tsv&#39;), sep=&#39;\t&#39;, index=False, header=True)
        order = _make_cluster_order(graph=self.G, section=section)

        visualization.render_cocluster_probabilities(
            coclusters=coclusters, taxa=self.G.data.taxa, order=order,
            yticklabels=yticklabels, include_tick_marks=False, xticklabels=xticklabels,
            title=&#39;Cluster Assignments&#39;)

        # Write out cocluster values
        coclusters = coclusters[order, :]
        coclusters = coclusters[:, order]
        labels = [taxa[aidx].name for aidx in order]
        df = pd.DataFrame(coclusters, index=labels, columns=labels)
        df.to_csv(os.path.join(basepath, &#39;coclusters.tsv&#39;), index=True, header=True)

        fig = plt.gcf()
        fig.tight_layout()
        plt.savefig(os.path.join(basepath, &#39;coclusters.pdf&#39;))
        plt.close()

        # Make taxonomic distribution plot
        df = generate_taxonomic_distribution_over_clusters_posthoc(
            mcmc=self.G.inference, tax_fmt=tax_fmt)

        df[df==0] = np.nan
        fig = plt.figure()
        ax = fig.add_subplot(111)
        ax = sns.heatmap(df, ax=ax, cmap=&#39;Blues&#39;,
            norm=LogNorm(vmin=np.nanmin(df.to_numpy()), vmax=np.nanmax(df.to_numpy())))
        ax.set_title(&#39;Taxonomic abundance per cluster&#39;)
        ax.set_xlabel(&#39;Clusters&#39;)
        ax.set_ylabel(tax_fmt.replace(&#39;%(&#39;, &#39;&#39;).replace(&#39;)s&#39;, &#39;&#39;).capitalize())
        fig.tight_layout()
        plt.savefig(os.path.join(basepath, &#39;taxonomic_distribution.pdf&#39;))
        plt.close()

        f.write(&#39;Mode number of clusters: {}\n&#39;.format(len(self.clustering)))
        for cidx, cluster in enumerate(self.clustering):
            f.write(&#39;Cluster {} - Size {}\n&#39;.format(cidx, len(cluster)))
            for oidx in cluster.members:
                label = pl.taxaname_formatter(format=taxa_formatter, taxon=taxa[oidx], taxa=taxa)
                f.write(&#39;\t- {}\n&#39;.format(label))

        return f

    def set_trace(self):
        self.clustering.set_trace()

    def remove_local_trace(self):
        &#39;&#39;&#39;Delete the local trace
        &#39;&#39;&#39;
        self.clustering.trace = None

    def add_trace(self):
        self.clustering.add_trace()

    def kill(self):
        if pl.ispersistentpool(self.pool):
            # For pylab multiprocessing, explicitly kill them
            self.pool.kill()
        return

    # Update super safe - meant to be used during debugging
    # =====================================================
    def update_slow(self):
        &#39;&#39;&#39; This is updating the new cluster. Depending on the iteration you do
        either split-merge Metropolis-Hasting update or a regular Gibbs update. To
        get highest mixing we alternate between each.
        &#39;&#39;&#39;
        if self.clustering.n_clusters.sample_iter &lt; self.delay:
            return

        if self.clustering.n_clusters.sample_iter % self.run_every_n_iterations != 0:
           return

        start_time = time.time()
        oidxs = npr.permutation(np.arange(len(self.G.data.taxa)))

        for oidx in oidxs:
            self.gibbs_update_single_taxon_slow(oidx=oidx)
        self._strtime = time.time() - start_time

    def gibbs_update_single_taxon_slow(self, oidx: int):
        &#39;&#39;&#39;The update function is based off of Algorithm 8 in &#39;Markov Chain
        Sampling Methods for Dirichlet Process Mixture Models&#39; by Radford M.
        Neal, 2000.

        Calculate the marginal likelihood of the taxon in every cluster
        and a new cluster then sample from `self.sample_categorical_log`
        to get the cluster assignment.

        Parameters
        ----------
        oidx : int
            Taxa index that we are updating the cluster assignment of
        &#39;&#39;&#39;
        curr_cluster = self.clustering.idx2cid[oidx]
        concentration = self.concentration.value

        # start as a dictionary then send values to `sample_categorical_log`
        LOG_P = []
        LOG_KEYS = []

        # Calculate current cluster
        # =========================
        # If the element is already in its own cluster, use the new cluster case
        if self.clustering.clusters[curr_cluster].size == 1:
            a = np.log(concentration/self.m)
        else:
            a = np.log(self.clustering.clusters[curr_cluster].size - 1)
        LOG_P.append(a + self.calculate_marginal_loglikelihood_slow()[&#39;ret&#39;])
        LOG_KEYS.append(curr_cluster)

        # Calculate going to every other cluster
        # ======================================
        for cid in self.clustering.order:
            if curr_cluster == cid:
                continue

            # Move Taxa and recompute the matrices
            self.clustering.move_item(idx=oidx,cid=cid)
            self.G[STRNAMES.CLUSTER_INTERACTION_INDICATOR].update_cnt_indicators()
            self.G.data.design_matrices[STRNAMES.CLUSTER_INTERACTION_VALUE].M.build()
            if self._there_are_perturbations:
                self.G.data.design_matrices[STRNAMES.PERT_VALUE].M.build()

            LOG_P.append(np.log(self.clustering.clusters[cid].size - 1) + \
                self.calculate_marginal_loglikelihood_slow()[&#39;ret&#39;])
            LOG_KEYS.append(cid)


        # Calculate new cluster
        # =====================
        cid=self.clustering.make_new_cluster_with(idx=oidx)
        self.G[STRNAMES.CLUSTER_INTERACTION_INDICATOR].update_cnt_indicators()
        self.G.data.design_matrices[STRNAMES.CLUSTER_INTERACTION_VALUE].M.build()
        if self._there_are_perturbations:
            self.G.data.design_matrices[STRNAMES.PERT_VALUE].M.build()
        
        LOG_KEYS.append(cid)
        LOG_P.append(np.log(concentration/self.m) + \
            self.calculate_marginal_loglikelihood_slow()[&#39;ret&#39;])

        # Sample the assignment
        # =====================
        idx = sample_categorical_log(LOG_P)
        assigned_cid = LOG_KEYS[idx]
        curr_clus = self.clustering.idx2cid[oidx]

        if assigned_cid != curr_clus:
            self.clustering.move_item(idx=oidx,cid=assigned_cid)
            self.G[STRNAMES.CLUSTER_INTERACTION_INDICATOR].update_cnt_indicators()

            # Change the mixing matrix for the interactions and (potentially) perturbations
            self.G.data.design_matrices[STRNAMES.CLUSTER_INTERACTION_VALUE].M.build()
            if self._there_are_perturbations:
                self.G.data.design_matrices[STRNAMES.PERT_VALUE].M.build()

    def calculate_marginal_loglikelihood_slow(self):
        &#39;&#39;&#39;Marginalizes out the interactions and the perturbations
        &#39;&#39;&#39;
        # Build the parameters
        # ====================
        self.G[STRNAMES.CLUSTER_INTERACTION_INDICATOR].update_cnt_indicators()
        lhs = [STRNAMES.GROWTH_VALUE, STRNAMES.SELF_INTERACTION_VALUE]
        if self._there_are_perturbations:
            rhs = [STRNAMES.PERT_VALUE, STRNAMES.CLUSTER_INTERACTION_VALUE]
        else:
            rhs = [STRNAMES.CLUSTER_INTERACTION_VALUE]

        # reconstruct the X matrices
        for v in rhs:
            self.G.data.design_matrices[v].M.build()
        
        y = self.G.data.construct_lhs(lhs, 
            kwargs_dict={STRNAMES.GROWTH_VALUE:{&#39;with_perturbations&#39;: False}})
        X = self.G.data.construct_rhs(keys=rhs, toarray=True)
        process_prec = self.G[STRNAMES.PROCESSVAR].build_matrix(cov=False, sparse=False)
        prior_prec = build_prior_covariance(G=self.G, cov=False, order=rhs, sparse=False)
        prior_var = build_prior_covariance(G=self.G, cov=True, order=rhs, sparse=False)
        prior_mean = build_prior_mean(G=self.G, order=rhs, shape=(-1,1))

        # If nothing is on, return 0
        if X.shape[1] == 0:
            return {
                &#39;a&#39;: 0,
                &#39;beta_prec&#39;: 0,
                &#39;process_prec&#39;: prior_prec,
                &#39;ret&#39;: 0,
                &#39;beta_logdet&#39;: 0,
                &#39;priorvar_logdet&#39;: 0,
                &#39;bEb&#39;: 0,
                &#39;bEbprior&#39;: 0}

        # Calculate the marginalization
        # =============================
        beta_prec = X.T @ process_prec @ X + prior_prec
        beta_cov = pinv(beta_prec, self)
        beta_mean = beta_cov @ ( X.T @ process_prec @ y + prior_prec @ prior_mean )
        beta_mean = np.asarray(beta_mean).reshape(-1,1)

        try:
            beta_logdet = log_det(beta_cov, self)
        except:
            logging.critical(&#39;Crashed in log_det&#39;)
            logging.critical(&#39;beta_cov:\n{}&#39;.format(beta_cov))
            logging.critical(&#39;prior_prec\n{}&#39;.format(prior_prec))
            raise
        priorvar_logdet = log_det(prior_var, self)
        ll2 = 0.5 * (beta_logdet - priorvar_logdet)

        bEbprior = np.asarray(prior_mean.T @ prior_prec @ prior_mean)[0,0]
        bEb = np.asarray(beta_mean.T @ beta_prec @ beta_mean)[0,0]
        ll3 = 0.5 * (bEb  - bEbprior)

        # print(&#39;prior_prec truth:\n&#39;, prior_prec)

        return {
            &#39;a&#39;: X.T @ process_prec, &#39;beta_prec&#39;: beta_prec, &#39;process_prec&#39;: prior_prec, 
            &#39;ret&#39;: ll2+ll3, &#39;beta_logdet&#39;: beta_logdet, &#39;priorvar_logdet&#39;: priorvar_logdet,
            &#39;bEb&#39;: bEb, &#39;bEbprior&#39;: bEbprior}

    # Update regular - meant to be used during inference
    # ==================================================
    # @profile
    def update_slow_fast(self):
        &#39;&#39;&#39;Much faster than `update_slow`
        &#39;&#39;&#39;

        if self.clustering.n_clusters.sample_iter &lt; self.delay:
            return

        if self.clustering.n_clusters.sample_iter % self.run_every_n_iterations != 0:
           return

        start_time = time.time()

        self.process_prec = self.G[STRNAMES.PROCESSVAR].prec.ravel() #.build_matrix(cov=False, sparse=False)
        self.process_prec_matrix = self.G[STRNAMES.PROCESSVAR].build_matrix(sparse=True, cov=False)
        lhs = [STRNAMES.GROWTH_VALUE, STRNAMES.SELF_INTERACTION_VALUE]
        self.y = self.G.data.construct_lhs(lhs, 
            kwargs_dict={STRNAMES.GROWTH_VALUE:{&#39;with_perturbations&#39;: False}})

        oidxs = npr.permutation(len(self.G.data.taxa))
        iii = 0
        for oidx in oidxs:
            logging.info(&#39;{}/{}: {}&#39;.format(iii, len(oidxs), oidx))
            self.gibbs_update_single_taxon_slow_fast(oidx=oidx)
            iii += 1
        self._strtime = time.time() - start_time
    
    # @profile
    def gibbs_update_single_taxon_slow_fast(self, oidx: int):
        &#39;&#39;&#39;The update function is based off of Algorithm 8 in &#39;Markov Chain
        Sampling Methods for Dirichlet Process Mixture Models&#39; by Radford M.
        Neal, 2000.

        Calculate the marginal likelihood of the taxon in every cluster
        and a new cluster then sample from `self.sample_categorical_log`
        to get the cluster assignment.

        Parameters
        ----------
        oidx : int
            Taxa index that we are updating the cluster assignment of
        &#39;&#39;&#39;
        curr_cluster = self.clustering.idx2cid[oidx]
        concentration = self.concentration.value

        # start as a dictionary then send values to `sample_categorical_log`
        LOG_P = []
        LOG_KEYS = []

        # Calculate current cluster
        # =========================
        # If the element is already in its own cluster, use the new cluster case
        if self.clustering.clusters[curr_cluster].size == 1:
            a = np.log(concentration/self.m)
        else:
            a = np.log(self.clustering.clusters[curr_cluster].size - 1)
        LOG_P.append(a + self.calculate_marginal_loglikelihood_slow_fast_sparse())
        LOG_KEYS.append(curr_cluster)

        # Calculate going to every other cluster
        # ======================================
        for cid in self.clustering.order:
            if curr_cluster == cid:
                continue

            # Move Taxa and recompute the matrices
            self.clustering.move_item(idx=oidx,cid=cid)
            self.G[STRNAMES.CLUSTER_INTERACTION_INDICATOR].update_cnt_indicators()
            self.G.data.design_matrices[STRNAMES.CLUSTER_INTERACTION_VALUE].M.build()
            if self._there_are_perturbations:
                self.G.data.design_matrices[STRNAMES.PERT_VALUE].M.build()

            LOG_P.append(np.log(self.clustering.clusters[cid].size - 1) + \
                self.calculate_marginal_loglikelihood_slow_fast_sparse())
            LOG_KEYS.append(cid)


        # Calculate new cluster
        # =====================
        cid=self.clustering.make_new_cluster_with(idx=oidx)
        self.G[STRNAMES.CLUSTER_INTERACTION_INDICATOR].update_cnt_indicators()
        self.G.data.design_matrices[STRNAMES.CLUSTER_INTERACTION_VALUE].M.build()
        if self._there_are_perturbations:
            self.G.data.design_matrices[STRNAMES.PERT_VALUE].M.build()
        
        LOG_KEYS.append(cid)
        LOG_P.append(np.log(concentration/self.m) + \
            self.calculate_marginal_loglikelihood_slow_fast_sparse())

        # Sample the assignment
        # =====================
        idx = sample_categorical_log(LOG_P)
        assigned_cid = LOG_KEYS[idx]
        curr_clus = self.clustering.idx2cid[oidx]

        if assigned_cid != curr_clus:
            self.clustering.move_item(idx=oidx,cid=assigned_cid)
            self.G[STRNAMES.CLUSTER_INTERACTION_INDICATOR].update_cnt_indicators()

            # Change the mixing matrix for the interactions and (potentially) perturbations
            self.G.data.design_matrices[STRNAMES.CLUSTER_INTERACTION_VALUE].M.build()
            if self._there_are_perturbations:
                self.G.data.design_matrices[STRNAMES.PERT_VALUE].M.build()

    # @profile
    def calculate_marginal_loglikelihood_slow_fast(self):
        &#39;&#39;&#39;Marginalizes out the interactions and the perturbations
        &#39;&#39;&#39;
        # Build the parameters
        # ====================
        if self._there_are_perturbations:
            rhs = [STRNAMES.PERT_VALUE, STRNAMES.CLUSTER_INTERACTION_VALUE]
        else:
            rhs = [STRNAMES.CLUSTER_INTERACTION_VALUE]
        
        
        y = self.y
        X = self.G.data.construct_rhs(keys=rhs, toarray=True)

        process_prec = self.process_prec
        prior_prec = build_prior_covariance(G=self.G, cov=False, order=rhs, sparse=False)
        prior_prec_diag = np.diag(prior_prec)
        prior_var = build_prior_covariance(G=self.G, cov=True, order=rhs, sparse=False)
        prior_mean = build_prior_mean(G=self.G, order=rhs, shape=(-1,1))

        # Calculate the marginalization
        # =============================
        a = X.T * process_prec

        beta_prec = a @ X + prior_prec
        beta_cov = pinv(beta_prec, self)
        beta_mean = beta_cov @ ( a @ y + prior_prec @ prior_mean )
        beta_mean = np.asarray(beta_mean).reshape(-1,1)

        try:
            beta_logdet = log_det(beta_cov, self)
        except:
            logging.critical(&#39;Crashed in log_det&#39;)
            logging.critical(&#39;beta_cov:\n{}&#39;.format(beta_cov))
            logging.critical(&#39;prior_prec\n{}&#39;.format(prior_prec))
            raise
        priorvar_logdet = log_det(prior_var, self)
        ll2 = 0.5 * (beta_logdet - priorvar_logdet)

        a = np.sum((prior_mean.ravel() ** 2) *prior_prec_diag)
        # np.asarray(prior_mean.T @ prior_prec @ prior_mean)[0,0]
        b = np.asarray(beta_mean.T @ beta_prec @ beta_mean)[0,0]
        ll3 = -0.5 * (a  - b)

        return ll2+ll3

    # @profile
    def calculate_marginal_loglikelihood_slow_fast_sparse(self):
        &#39;&#39;&#39;Marginalizes out the interactions and the perturbations
        &#39;&#39;&#39;
        # Build the parameters
        # ====================
        if self._there_are_perturbations:
            rhs = [STRNAMES.PERT_VALUE, STRNAMES.CLUSTER_INTERACTION_VALUE]
        else:
            rhs = [STRNAMES.CLUSTER_INTERACTION_VALUE]
        
        
        y = self.y
        X = self.G.data.construct_rhs(keys=rhs, toarray=False)

        process_prec = self.process_prec_matrix
        prior_prec = build_prior_covariance(G=self.G, cov=False, order=rhs, sparse=True)  
        prior_prec_diag = build_prior_covariance(G=self.G, cov=False, order=rhs, diag=True)        
        prior_var = build_prior_covariance(G=self.G, cov=True, order=rhs, sparse=True)
        prior_mean = build_prior_mean(G=self.G, order=rhs, shape=(-1,1))

        # Calculate the marginalization
        # =============================

        # print(&#39;X&#39;)
        # print(type(X))
        # print(X.shape)

        # print(&#39;process_prec&#39;)
        # print(type(process_prec))
        # print(process_prec.shape)

        # print(&#39;prior_prec&#39;)
        # print(type(prior_prec))
        # print(prior_prec.shape)

        # print(&#39;prior mean&#39;)
        # print(type(prior_mean))
        # print(prior_mean.shape)

        a = X.T.dot(process_prec)
        beta_prec = a.dot(X) + prior_prec
        beta_cov = pinv(beta_prec, self)
        beta_mean = beta_cov @ ( a.dot(y) + prior_prec.dot(prior_mean))
        beta_mean = np.asarray(beta_mean).reshape(-1,1)

        try:
            beta_logdet = log_det(beta_cov, self)
        except:
            logging.critical(&#39;Crashed in log_det&#39;)
            logging.critical(&#39;beta_cov:\n{}&#39;.format(beta_cov))
            logging.critical(&#39;prior_prec\n{}&#39;.format(prior_prec))
            raise
        priorvar_logdet = log_det(prior_var, self)
        ll2 = 0.5 * (beta_logdet - priorvar_logdet)

        a = np.sum((prior_mean.ravel() ** 2) *prior_prec_diag)
        # np.asarray(prior_mean.T @ prior_prec @ prior_mean)[0,0]

        # print(&#39;beta_prec.shape&#39;, beta_prec.shape)
        # print(&#39;beta_mean.shape&#39;, beta_mean.shape)

        b = np.asarray(beta_mean.T @ beta_prec.dot(beta_mean))[0,0]
        ll3 = -0.5 * (a  - b)

        return ll2+ll3

    # Update MP - meant to be used during inference
    # =============================================
    def update_mp(self):
        &#39;&#39;&#39;Implements `update_slow` but parallelizes calculating the likelihood
        of being in a cluster. NOTE that this does not parallelize on the Taxa level.

        On the first gibb step with initialize the workers that we implement with DASW (
        different arguments, single worker). For more information what this means look
        at pylab.multiprocessing documentation.

        If we initialized our pool as a pylab.multiprocessing.PersistentPool, then we 
        multiprocess the likelihood calculations for each taxa. If we didnt then this 
        implementation has the same performance as `ClusterAssignments.update_slow_fast`.
        &#39;&#39;&#39;
        if self.G.data.zero_inflation_transition_policy is not None:
            raise NotImplementedError(&#39;Multiprocessing for zero inflation data is not implemented yet.&#39; \
                &#39; Use `mp=None`&#39;)
        DMI = self.G.data.design_matrices[STRNAMES.CLUSTER_INTERACTION_VALUE]
        DMP = self.G.data.design_matrices[STRNAMES.PERT_VALUE]
        if self.clustering.n_clusters.sample_iter == 0 or self.pool == []:
            kwargs = {
                &#39;n_taxa&#39;: len(self.G.data.taxa),
                &#39;total_n_dts_per_taxon&#39;: self.G.data.total_n_dts_per_taxon,
                &#39;n_replicates&#39;: self.G.data.n_replicates,
                &#39;n_dts_for_replicate&#39;: self.G.data.n_dts_for_replicate,
                &#39;there_are_perturbations&#39;: self._there_are_perturbations,
                &#39;keypair2col_interactions&#39;: DMI.M.keypair2col,
                &#39;keypair2col_perturbations&#39;: DMP.M.keypair2col,
                &#39;n_perturbations&#39;: len(self.G.perturbations) if self._there_are_perturbations else None,
                &#39;base_Xrows&#39;: DMI.base.rows,
                &#39;base_Xcols&#39;: DMI.base.cols,
                &#39;base_Xshape&#39;: DMI.base.shape,
                &#39;base_Xpertrows&#39;: DMP.base.rows,
                &#39;base_Xpertcols&#39;: DMP.base.cols,
                &#39;base_Xpertshape&#39;: DMP.base.shape,
                &#39;n_rowsM&#39;: DMI.M.n_rows,
                &#39;n_rowsMpert&#39;: DMP.M.n_rows}

            if pl.ispersistentpool(self.pool):
                for _ in range(self.n_cpus):
                    self.pool.add_worker(SingleClusterFullParallelization(**kwargs))
            else:
                self.pool = SingleClusterFullParallelization(**kwargs)
        if self.clustering.n_clusters.sample_iter &lt; self.delay:
            return
        if self.clustering.n_clusters.sample_iter % self.run_every_n_iterations != 0:
            return
        # Send in arguments for the start of the gibbs step
        start_time = time.time()
        base_Xdata = DMI.base.data
        self.concentration = self.G[STRNAMES.CONCENTRATION].value
        y = self.G.data.construct_lhs(keys=[STRNAMES.GROWTH_VALUE, STRNAMES.SELF_INTERACTION_VALUE],
            kwargs_dict={STRNAMES.GROWTH_VALUE: {&#39;with_perturbations&#39;:False}})
        prior_var_interactions = self.G[STRNAMES.PRIOR_VAR_INTERACTIONS].value
        prior_mean_interactions = self.G[STRNAMES.PRIOR_MEAN_INTERACTIONS].value
        process_prec_diag = self.G[STRNAMES.PROCESSVAR].prec

        if self._there_are_perturbations:
            prior_var_pert = self.G[STRNAMES.PRIOR_VAR_PERT].get_single_value_of_perts()
            prior_mean_pert = self.G[STRNAMES.PRIOR_MEAN_PERT].get_single_value_of_perts()
            base_Xpertdata = DMP.base.data
        else:
            prior_var_pert = None
            prior_mean_pert = None
            base_Xpertdata = None

        kwargs = {
            &#39;base_Xdata&#39;: base_Xdata,
            &#39;base_Xpertdata&#39;: base_Xpertdata,
            &#39;concentration&#39;: self.concentration,
            &#39;m&#39;: self.m,
            &#39;y&#39;: y,
            &#39;process_prec_diag&#39;: process_prec_diag,
            &#39;prior_var_interactions&#39;: prior_var_interactions,
            &#39;prior_var_pert&#39;: prior_var_pert,
            &#39;prior_mean_interactions&#39;: prior_mean_interactions,
            &#39;prior_mean_pert&#39;: prior_mean_pert}
        
        if pl.ispersistentpool(self.pool):
            self.pool.map(&#39;initialize_gibbs&#39;, [kwargs]*self.pool.num_workers)
        else:
            self.pool.initialize_gibbs(**kwargs)

        oidxs = npr.permutation(np.arange(len(self.G.data.taxa)))
        for iii, oidx in enumerate(oidxs):
            logging.info(&#39;{}/{} - {}&#39;.format(iii, len(self.G.data.taxa), oidx))
            self.oidx = oidx
            self.gibbs_update_single_taxon_parallel()

        self._strtime = time.time() - start_time

    def gibbs_update_single_taxon_parallel(self):
        &#39;&#39;&#39;Update for a single taxon
        &#39;&#39;&#39;
        self.original_cluster = self.clustering.idx2cid[self.oidx]
        self.curr_cluster = self.original_cluster

        interactions = self.G[STRNAMES.INTERACTIONS_OBJ]
        interaction_on_idxs = interactions.get_indicators(return_idxs=True)
        if self._there_are_perturbations:
            perturbation_on_idxs = [p.indicator.cluster_arg_array() for p in self.G.perturbations]
        else:
            perturbation_on_idxs = None
        use_saved_params = False

        if pl.ispersistentpool(self.pool):
            self.pool.staged_map_start(&#39;run&#39;)
        else:
            notpool_ret = []

        # Get the likelihood of the current configuration
        if self.clustering.clusters[self.original_cluster].size == 1:
            log_mult_factor = math.log(self.concentration/self.m)
        else:
            log_mult_factor = math.log(self.clustering.clusters[self.original_cluster].size - 1)

        if use_saved_params and pl.ispersistentpool(self.pool):
            interaction_on_idxs = None
            perturbation_on_idxs = None

        cluster_config = self.clustering.toarray()

        kwargs = {
            &#39;interaction_on_idxs&#39;: interaction_on_idxs,
            &#39;perturbation_on_idxs&#39;: perturbation_on_idxs,
            &#39;cluster_config&#39;: cluster_config,
            &#39;log_mult_factor&#39;: log_mult_factor,
            &#39;cid&#39;: self.original_cluster,
            &#39;use_saved_params&#39;: use_saved_params}

        if pl.ispersistentpool(self.pool):
            self.pool.staged_map_put(kwargs)
        else:
            notpool_ret.append(self.pool.run(**kwargs))

        # Check every cluster
        for cid in self.clustering.order:
            if cid == self.original_cluster:
                continue
            self.clustering.move_item(idx=self.oidx, cid=cid)
            self.curr_cluster = cid

            if not use_saved_params:
                interaction_on_idxs = interactions.get_indicators(return_idxs=True)
                if self._there_are_perturbations:
                    perturbation_on_idxs = [p.indicator.cluster_arg_array() for p in self.G.perturbations]
                else:
                    perturbation_on_idxs = None

            cluster_config = self.clustering.toarray()
            log_mult_factor = np.log(self.clustering.clusters[self.curr_cluster].size - 1)

            kwargs = {
                &#39;interaction_on_idxs&#39;: interaction_on_idxs,
                &#39;perturbation_on_idxs&#39;: perturbation_on_idxs,
                &#39;cluster_config&#39;: cluster_config,
                &#39;log_mult_factor&#39;: log_mult_factor,
                &#39;cid&#39;: self.curr_cluster,
                &#39;use_saved_params&#39;: use_saved_params}

            if pl.ispersistentpool(self.pool):
                self.pool.staged_map_put(kwargs)
            else:
                notpool_ret.append(self.pool.run(**kwargs))

        # Make a new cluster
        self.curr_cluster = self.clustering.make_new_cluster_with(idx=self.oidx)
        cluster_config = self.clustering.toarray()
        interaction_on_idxs = interactions.get_indicators(return_idxs=True)
        if self._there_are_perturbations:
            perturbation_on_idxs = [p.indicator.cluster_arg_array() for p in self.G.perturbations]
        else:
            perturbation_on_idxs = None
        log_mult_factor = np.log(self.concentration/self.m)

        kwargs = {
            &#39;interaction_on_idxs&#39;: interaction_on_idxs,
            &#39;perturbation_on_idxs&#39;: perturbation_on_idxs,
            &#39;cluster_config&#39;: cluster_config,
            &#39;log_mult_factor&#39;: log_mult_factor,
            &#39;cid&#39;: self.curr_cluster,
            &#39;use_saved_params&#39;: False}

        # Put the values and get if necessary
        KEYS = []
        LOG_P = []
        if pl.ispersistentpool(self.pool):
            self.pool.staged_map_put(kwargs)
            ret = self.pool.staged_map_get()
        else:
            notpool_ret.append(self.pool.run(**kwargs))
            ret = notpool_ret
        for c, p in ret:
            KEYS.append(c)
            LOG_P.append(p)

        idx = sample_categorical_log(LOG_P)
        assigned_cid = KEYS[idx]

        if assigned_cid != self.original_cluster:
            logging.info(&#39;cluster changed&#39;)

        self.clustering.move_item(idx=self.oidx, cid=assigned_cid)

        self.G[STRNAMES.CLUSTER_INTERACTION_INDICATOR].update_cnt_indicators()
        self.G.data.design_matrices[STRNAMES.CLUSTER_INTERACTION_VALUE].M.build()
        if self._there_are_perturbations:
            self.G.data.design_matrices[STRNAMES.PERT_VALUE].M.build()

        
class SingleClusterFullParallelization(pl.multiprocessing.PersistentWorker):
    &#39;&#39;&#39;Make the full parallelization
        - Mixture matricies for interactions and perturbations
        - calculating the marginalization for the sent in cluster

    Parameters
    ----------
    n_taxa : int
        Total number of OTUs
    total_n_dts_per_taxon : int
        Total number of time changes for each OTU
    n_replicates : int
        Total number of replicates
    n_dts_for_replicate : np.ndarray
        Total number of time changes for each replicate
    there_are_perturbations : bool
        If True, there are perturbations
    keypair2col_interactions : np.ndarray
        These map the OTU indices of the pairs of OTUs to their column index
        in `big_X`
    keypair2col_perturbations : np.ndarray, None
        These map the OTU indices nad perturbation index to the column in
        `big_Xpert`. If there are no perturbations then this is None
    n_perturbations : int, None
        Number of perturbations. None if there are no perturbations
    base_Xrows, base_Xcols, base_Xpertrows, base_Xpertcols : np.ndarray
        These are the rows and columns necessary to build the interaction and perturbation 
        matrices, respectively. Whats passed in is the data vector and then we build
        it using sparse matrices
    n_rowsM, n_rowsMpert : int
        These are the number of rows for the mixing matrix for the interactions and
        perturbations respectively.
    &#39;&#39;&#39;
    def __init__(self, n_taxa: int, total_n_dts_per_taxon: int, n_replicates: int, 
        n_dts_for_replicate: Union[np.ndarray, List[int]], there_are_perturbations: bool, 
        keypair2col_interactions: np.ndarray, keypair2col_perturbations: np.ndarray,
        n_perturbations: int, base_Xrows: np.ndarray, base_Xcols: np.ndarray, base_Xshape: Tuple, 
        base_Xpertrows: np.ndarray, base_Xpertcols: np.ndarray,
        base_Xpertshape: Tuple, n_rowsM: int, n_rowsMpert: int):
        self.n_taxa = n_taxa
        self.total_n_dts_per_taxon = total_n_dts_per_taxon
        self.n_replicates = n_replicates
        self.n_dts_for_replicate = n_dts_for_replicate
        self.there_are_perturbations = there_are_perturbations
        self.keypair2col_interactions = keypair2col_interactions
        if self.there_are_perturbations:
            self.keypair2col_perturbations = keypair2col_perturbations
            self.n_perturbations = n_perturbations

        self.base_Xrows = base_Xrows
        self.base_Xcols = base_Xcols
        self.base_Xshape = base_Xshape
        self.base_Xpertrows = base_Xpertrows
        self.base_Xpertcols = base_Xpertcols
        self.base_Xpertshape = base_Xpertshape

        self.n_rowsM = n_rowsM
        self.n_rowsMpert = n_rowsMpert

    def initialize_gibbs(self, base_Xdata: scipy.sparse.spmatrix, base_Xpertdata: scipy.sparse.spmatrix, 
        concentration: float, m: int, y: np.ndarray, process_prec_diag: np.ndarray, 
        prior_var_interactions: float, prior_var_pert: np.ndarray, 
        prior_mean_interactions: float, prior_mean_pert: np.ndarray):
        &#39;&#39;&#39;Pass in the information that changes every Gibbs step

        Parameters
        ----------
        base_X : scipy.sparse.csc_matrix
            Sparse matrix for the interaction terms
        base_Xpert : scipy.sparse.csc_matrix, None
            Sparse matrix for the perturbation terms
            If None, there are no perturbations
        concentration : float
            This is the concentration of the system
        m : int
            This is the auxiliary variable for the marginalization
        y : np.ndarray
            This is the observation array
        process_prec_diag : np.ndarray
            This is the process precision diagonal
        &#39;&#39;&#39;
        self.base_X = scipy.sparse.coo_matrix(
            (base_Xdata,(self.base_Xrows,self.base_Xcols)),
            shape=self.base_Xshape).tocsc()
        
        self.concentration = concentration
        self.m = m
        self.y = y.reshape(-1,1)
        self.n_rows = len(y)
        self.n_cols_X = self.base_X.shape[1]
        self.prior_var_interactions = prior_var_interactions
        self.prior_prec_interactions = 1/prior_var_interactions
        self.prior_mean_interactions = prior_mean_interactions

        if self.there_are_perturbations:
            self.base_Xpert = scipy.sparse.coo_matrix(
                (base_Xpertdata,(self.base_Xpertrows,self.base_Xpertcols)),
                shape=self.base_Xpertshape).tocsc()
            self.prior_var_pert = prior_var_pert
            self.prior_prec_pert = 1/prior_var_pert
            self.prior_mean_pert = prior_mean_pert

        self.process_prec_matrix = scipy.sparse.dia_matrix(
            (process_prec_diag,[0]), shape=(len(process_prec_diag),len(process_prec_diag))).tocsc()

    def initialize_oidx(self, interaction_on_idxs: np.ndarray, perturbation_on_idxs: np.ndarray):
        &#39;&#39;&#39;Pass in the parameters that change for every OTU - potentially

        Parameters
        ----------
        
        &#39;&#39;&#39;
        self.saved_interaction_on_idxs = interaction_on_idxs
        self.saved_perturbation_on_idxs = perturbation_on_idxs

    # @profile
    def run(self, interaction_on_idxs: np.ndarray, perturbation_on_idxs: List[np.ndarray], 
        cluster_config: np.ndarray, log_mult_factor: float, cid: int, 
        use_saved_params: bool):
        &#39;&#39;&#39;Pass in the parameters for the specific cluster assignment for
        the OTU and run the marginalization


        Parameters
        ----------
        interaction_on_idxs : np.array(int)
            An array of indices for the interactions that are on. Assumes that the
            clustering is in the order specified in `cluster_config`
        perturbation_on_idxs : list(np.ndarray(int)), None
            If there are perturbations, then we set the perturbation idxs on
            Each element in the list are the indices of that perturbation that are on
        cluster_config : list(list(int))
            This is the cluster configuration and in cluster order.
        log_mult_factor : float
            This is the log multiplication factor that we add onto the marginalization
        use_saved_params : bool
            If True, passed in `interaction_on_idxs` and `perturbation_on_idxs` are None
            and we can use `saved_interaction_on_idxs` and `saved_perturbation_on_idxs`
        &#39;&#39;&#39;
        if use_saved_params:
            interaction_on_idxs = self.saved_interaction_on_idxs
            perturbation_on_idxs = self.saved_perturbation_on_idxs

        # We need to make the arrays for interactions and perturbations
        self.set_clustering(cluster_config=cluster_config)
        Xinteractions = self.build_interactions_matrix(on_columns=interaction_on_idxs)

        if self.there_are_perturbations:
            Xperturbations = self.build_perturbations_matrix(on_columns=perturbation_on_idxs)
            X = scipy.sparse.hstack([Xperturbations, Xinteractions])
        else:
            X = Xinteractions
        self.X = X
        self.prior_mean = self.build_prior_mean(on_interactions=interaction_on_idxs,
            on_perturbations=perturbation_on_idxs)
        self.prior_cov, self.prior_prec, self.prior_prec_diag = self.build_prior_cov_and_prec_and_diag(
            on_interactions=interaction_on_idxs, on_perturbations=perturbation_on_idxs)
        
        return cid, self.calculate_marginal_loglikelihood_slow_fast_sparse() + log_mult_factor

    def set_clustering(self, cluster_config: np.ndarray):
        self.clustering = CondensedClustering(oidx2cidx=cluster_config)
        self.iidx2cidxpair = np.zeros(shape=(len(self.clustering)*(len(self.clustering)-1), 2), 
            dtype=int)
        self.iidx2cidxpair = SingleClusterFullParallelization.make_iidx2cidxpair(
            ret=self.iidx2cidxpair,
            n_clusters=len(self.clustering))

    def build_interactions_matrix(self, on_columns: np.ndarray):
        &#39;&#39;&#39;Build the interaction matrix

        First we make the rows and columns for the mixing matrix,
        then we multiple the base matrix and the mixing matrix.
        &#39;&#39;&#39;
        rows = []
        cols = []

        # c2ciidx = Cluster-to-Cluster Interaction InDeX
        c2ciidx = 0
        for ccc in on_columns:
            tcidx = self.iidx2cidxpair[ccc, 0]
            scidx = self.iidx2cidxpair[ccc, 1]
            
            smems = self.clustering.clusters[scidx]
            tmems = self.clustering.clusters[tcidx]
            
            a = np.zeros(len(smems)*len(tmems), dtype=int)
            rows.append(SingleClusterFullParallelization.get_indices(a,
                self.keypair2col_interactions, tmems, smems))
            cols.append(np.full(len(tmems)*len(smems), fill_value=c2ciidx))
            c2ciidx += 1

        rows = np.asarray(list(itertools.chain.from_iterable(rows)))
        cols = np.asarray(list(itertools.chain.from_iterable(cols)))
        data = np.ones(len(rows), dtype=int)

        M = scipy.sparse.coo_matrix((data,(rows,cols)),
            shape=(self.n_rowsM, c2ciidx)).tocsc()
        ret = self.base_X @ M
        return ret

    def build_perturbations_matrix(self, on_columns: np.ndarray):
        if not self.there_are_perturbations:
            raise ValueError(&#39;You should not be here&#39;)
        
        keypair2col = self.keypair2col_perturbations
        rows = []
        cols = []

        col = 0
        for pidx, pert_ind_idxs in enumerate(on_columns):
            for cidx in pert_ind_idxs:
                for oidx in self.clustering.clusters[cidx]:
                    rows.append(keypair2col[oidx, pidx])
                    cols.append(col)
                col += 1
        
        data = np.ones(len(rows), dtype=np.float64)
        M = scipy.sparse.coo_matrix((data,(rows,cols)),
            shape=(self.n_rowsMpert, col)).tocsc()
        ret = self.base_Xpert @ M
        return ret

    def build_prior_mean(self, on_interactions, on_perturbations):
        &#39;&#39;&#39;Build the prior mean array

        Perturbations go first and then interactions
        &#39;&#39;&#39;
        ret = []
        for pidx, pert in enumerate(on_perturbations):
            ret = np.append(ret, 
                np.full(len(pert), fill_value=self.prior_mean_pert[pidx]))
        ret = np.append(ret, np.full(len(on_interactions), fill_value=self.prior_mean_interactions))
        return ret.reshape(-1,1)

    def build_prior_cov_and_prec_and_diag(self, on_interactions, on_perturbations):
        &#39;&#39;&#39;Build the prior covariance matrices and others
        &#39;&#39;&#39;
        ret = []
        for pidx, pert in enumerate(on_perturbations):
            ret = np.append(ret, 
                np.full(len(pert), fill_value=self.prior_var_pert[pidx]))
        prior_var_diag = np.append(ret, np.full(len(on_interactions), fill_value=self.prior_var_interactions))
        prior_prec_diag = 1/prior_var_diag

        prior_var = scipy.sparse.dia_matrix((prior_var_diag,[0]), 
            shape=(len(prior_var_diag),len(prior_var_diag))).tocsc()
        prior_prec = scipy.sparse.dia_matrix((prior_prec_diag,[0]), 
            shape=(len(prior_prec_diag),len(prior_prec_diag))).tocsc()

        return prior_var, prior_prec, prior_prec_diag

    # @profile
    def calculate_marginal_loglikelihood_slow_fast_sparse(self):
        y = self.y
        X = self.X
        process_prec = self.process_prec_matrix
        prior_mean = self.prior_mean
        prior_cov = self.prior_cov
        prior_prec = self.prior_prec
        prior_prec_diag = self.prior_prec_diag

        a = X.T.dot(process_prec)
        beta_prec = a.dot(X) + prior_prec
        beta_cov = pinv(beta_prec, self)
        beta_mean = beta_cov @ (a.dot(y) + prior_prec.dot(prior_mean))
        beta_mean = np.asarray(beta_mean).reshape(-1,1)

        try:
            beta_logdet = log_det(beta_cov, self)
        except:
            logging.critical(&#39;Crashed in log_det&#39;)
            logging.critical(&#39;beta_cov:\n{}&#39;.format(beta_cov))
            logging.critical(&#39;prior_prec\n{}&#39;.format(prior_prec))
            raise
        
        priorvar_logdet = log_det(prior_cov, self)
        ll2 = 0.5 * (beta_logdet - priorvar_logdet)

        bEbprior = np.asarray(prior_mean.T @ prior_prec.dot(prior_mean))[0,0]
        bEb = np.asarray(beta_mean.T @ beta_prec.dot(beta_mean) )[0,0]
        ll3 = 0.5 * (bEb - bEbprior)

        self.a = a
        self.beta_prec = beta_prec

        return ll2 + ll3

    @staticmethod
    @numba.jit(nopython=True, cache=True)
    def get_indices(a, keypair2col, tmems, smems):
        &#39;&#39;&#39;Use Just in Time compilation to reduce the &#39;getting&#39; time
        by about 95%

        Parameters
        ----------
        keypair2col : np.ndarray
            Maps (target_oidx, source_oidx) pair to the place the interaction
            index would be on a full interactio design matrix on the OTU level
        tmems, smems : np.ndarray
            These are the OTU indices in the target cluster and the source cluster
            respectively
        &#39;&#39;&#39;
        i = 0
        for tidx in tmems:
            for sidx in smems:
                a[i] = keypair2col[tidx, sidx]
                i += 1
        return a

    @staticmethod
    def make_iidx2cidxpair(ret, n_clusters):
        &#39;&#39;&#39;Map the index of a cluster interaction to (dst,src) of clusters

        Parameters
        ----------
        n_clusters : int
            Number of clusters

        Returns
        -------
        np.ndarray(n_interactions,2)
            First column is the destination cluster index, second column is the source
            cluster index
        &#39;&#39;&#39;
        i = 0
        for dst_cidx in range(n_clusters):
            for src_cidx in range(n_clusters):
                if dst_cidx == src_cidx:
                    continue
                ret[i,0] = dst_cidx
                ret[i,1] = src_cidx
                i += 1
        return ret


class CondensedClustering:
    &#39;&#39;&#39;Condensed clustering object that is not associated with the graph

    Parameters
    ----------
    oidx2cidx : np.ndarray
        Maps the cluster assignment to each taxon.
        index -&gt; Taxa index
        output -&gt; cluster index

    &#39;&#39;&#39;
    def __init__(self, oidx2cidx):
        self.clusters = []
        self.oidx2cidx = oidx2cidx
        a = {}
        for oidx, cidx in enumerate(self.oidx2cidx):
            if cidx not in a:
                a[cidx] = [oidx]
            else:
                a[cidx].append(oidx)
        cidx = 0
        while cidx in a:
            self.clusters.append(np.asarray(a[cidx], dtype=int))
            cidx += 1

    def __len__(self):
        return len(self.clusters)


# Filtering
# ---------
class TrajectorySet(pl.graph.Node):
    &#39;&#39;&#39;This aggregates a set of trajectories from each set

    Parameters
    ----------
    G : pylab.graph.Graph
        Graph object to attach it to
    &#39;&#39;&#39;
    def __init__(self, G: Graph, **kwargs):
        name = STRNAMES.LATENT_TRAJECTORY
        pl.graph.Node.__init__(self, name=name, G=G)
        self.value = []
        n_taxa = self.G.data.n_taxa

        for ridx, subj in enumerate(self.G.data.subjects):
            n_timepoints = self.G.data.n_timepoints_for_replicate[ridx]

            # initialize values to zeros for initialization
            self.value.append(pl.variables.Variable(
                name=name+&#39;_{}&#39;.format(subj.name), G=G, shape=(n_taxa, n_timepoints),
                value=np.zeros((n_taxa, n_timepoints), dtype=float), **kwargs))
        prior = pl.variables.Normal(
            loc=pl.variables.Constant(name=self.name+&#39;_prior_loc&#39;, value=0, G=self.G),
            scale2=pl.variables.Constant(name=self.name+&#39;_prior_scale2&#39;, value=1, G=self.G),
            name=self.name+&#39;_prior&#39;, G=self.G)
        self.add_prior(prior)

    def __getitem__(self, ridx: int) -&gt; variables.Variable:
        return self.value[ridx]

    @property
    def sample_iter(self) -&gt; int:
        return self.value[0].sample_iter

    def reset_value_size(self):
        &#39;&#39;&#39;Change the size of the trajectory when we set the intermediate timepoints
        &#39;&#39;&#39;
        n_taxa = self.G.data.n_taxa
        for ridx in range(len(self.value)):
            n_timepoints = self.G.data.n_timepoints_for_replicate[ridx]
            self.value[ridx].value = np.zeros((n_taxa, n_timepoints),dtype=float)
            self.value[ridx].set_value_shape(self.value[ridx].value.shape)

    def _vectorize(self) -&gt; np.ndarray:
        &#39;&#39;&#39;Get all the data in vector form
        &#39;&#39;&#39;
        vals = np.array([])
        for data in self.value:
            vals = np.append(vals, data.value)
        return vals

    def set_trace(self, *args, **kwargs):
        for ridx in range(len(self.value)):
            self.value[ridx].set_trace(*args, **kwargs)

    def add_trace(self):
        for ridx in range(len(self.value)):
            # Set the zero inflation values to nans
            self.value[ridx].value[~self.G[STRNAMES.ZERO_INFLATION].value[ridx]] = np.nan
            self.value[ridx].add_trace()

    def visualize(self, ridx: int, section: str, basepath: str, taxa_formatter: str, 
        vmin: Union[float, int]=None, vmax: Union[float, int]=None):
        &#39;&#39;&#39;Visualize the replicate at index `ridx`
        &#39;&#39;&#39;
        subjset = self.G.data.subjects
        taxa = subjset.taxa
        subj = subjset.iloc(ridx)
        obj = self.value[ridx]
        logging.info(&#39;Retrieving trace for subject {}&#39;.format(subj.name))

        given_data = subj.matrix()[&#39;abs&#39;]
        given_times = subj.times
        
        trace = obj.get_trace_from_disk(section=section)
        summ = pl.summary(trace)
        data_times = self.G.data.times[ridx]
        index = [taxon.name for taxon in taxa]

        # Make the tables
        for k,arr in summ.items():
            df = pd.DataFrame(arr, index=index, columns=data_times)
            df.to_csv(os.path.join(basepath, &#39;{}.tsv&#39;.format(k)), 
                sep=&#39;\t&#39;, index=True, header=True)

        percentile2_5 = np.nanpercentile(trace, q=2.5, axis=0)
        percentile97_5 = np.nanpercentile(trace, q=97.5, axis=0)
        
        acceptance_rates = []
        for oidx in range(len(subjset.taxa)):
            fig = plt.figure()
            title = pl.taxaname_formatter(format=taxa_formatter, taxon=oidx, taxa=taxa)
            title += &#39;\nSubject {}, {}&#39;.format(subj.name, taxa[oidx].name)
            fig.suptitle(title)
            ax = fig.add_subplot(111)

            med_traj = summ[&#39;median&#39;][oidx, :]
            low_traj = percentile2_5[oidx, :]
            high_traj = percentile97_5[oidx, :]

            ax.plot(data_times, med_traj, color=&#39;blue&#39;, marker=&#39;.&#39;, label=&#39;median&#39;)
            ax.fill_between(data_times, y1=low_traj, y2=high_traj, color=&#39;blue&#39;, 
                alpha=0.15, label=&#39;95th percentile&#39;)
            ax.plot(given_times, given_data[oidx, :], marker=&#39;x&#39;, color=&#39;black&#39;, 
                linestyle=&#39;:&#39;, label=&#39;data&#39;)

            ax.set_yscale(&#39;log&#39;)
            ax.set_xlabel(&#39;Time (days)&#39;)
            ax.set_ylabel(&#39;CFU/g&#39;)

            ax = visualization.shade_in_perturbations(ax, perturbations=subjset.perturbations, subj=subj)
            ax.legend(bbox_to_anchor=(1.05, 1))
            fig.tight_layout()
            plt.savefig(os.path.join(basepath, &#39;{}.pdf&#39;.format(taxa[oidx].name)))
            plt.close()

            # Calculate the acceptance rate at every taxon and timepoint
            temp = []
            for tidx in range(trace.shape[-1]):
                temp.append(pl.metropolis.acceptance_rate(x=trace[:, oidx, tidx], 
                    start=0, end=trace.shape[0]))
            acceptance_rates.append(temp)

        df = pd.DataFrame(acceptance_rates, index=index, columns=data_times)
        df.to_csv(os.path.join(basepath, &#39;acceptance_rates.tsv&#39;), sep=&#39;\t&#39;, index=True, header=True)


class FilteringLogMP(pl.graph.Node):
    &#39;&#39;&#39;This is the posterior for the latent trajectory that are
    sampled using a standard normal Metropolis-Hastings proposal.

    This is the multiprocessing version of the class. All of the computation is
    done on the subject level in parallel.

    Parallelization Modes
    ---------------------
    &#39;debug&#39;
        If this is selected, then we dont actually parallelize, but we go in
        order of the objects in sequential order. We would do this if we want
        to benchmark within each processor or do easier print statements
    &#39;full&#39;
        This is where each subject gets their own process

    This assumes that we are using the log model
    &#39;&#39;&#39;
    def __init__(self, mp: str, zero_inflation_transition_policy: Union[str,  type(None)],**kwargs):
        &#39;&#39;&#39;
        Parameters
        ----------
        mp : str
            - &#39;debug&#39;
                * Does not actually parallelize, does it serially - we do this in case we
                  want to debug and/or benchmark
            - &#39;full&#39;
                Send each replicate to a processor each
        zero_inflation_transition_policy : None, str
            - Type of zero inflation to do. If None then there is no zero inflation
        &#39;&#39;&#39;
        kwargs[&#39;name&#39;] = STRNAMES.FILTERING
        pl.graph.Node.__init__(self, **kwargs)
        self.x = TrajectorySet(G=self.G)
        self.mp = mp
        self.zero_inflation_transition_policy = zero_inflation_transition_policy

        self.print_vals = False
        self._strr = &#39;parallel&#39;

    def __str__(self) -&gt; str:
        return self._strr

    @property
    def sample_iter(self) -&gt; int:
        # It doesnt matter if we chose q or x because they are both the same
        return self.x.sample_iter

    def initialize(self, x_value_option: str, a0: Union[float, str], a1: Union[float, str], 
        v1: Union[float, int], v2: Union[float, int], essential_timepoints: Union[np.ndarray, str], 
        tune: Tuple[int, int], proposal_init_scale: Union[float, int],
        intermediate_step: Union[Tuple[str, Tuple], np.ndarray, List, type(None)], 
        intermediate_interpolation: str=None, delay: int=0, bandwidth: Union[float, int]=None, 
        window: int=None, target_acceptance_rate: Union[str, float]=0.44, 
        calculate_qpcr_loglik: bool=True):
        &#39;&#39;&#39;Initialize the values of the error model (values for the
        latent and the auxiliary trajectory). Additionally this sets
        the intermediate time points

        Initialize the values of the prior.

        Parameters
        ----------
        x_value_option : str
            - Option to initialize the value of the latent trajectory.
            - Options
                - &#39;coupling&#39;
                    - Sample the values around the data with extremely low variance.
                    - This also truncates the data so that it stays &gt; 0.
                - &#39;moving-avg&#39;
                    - Initialize the values using a moving average around the points.
                    - The bandwidth of the filter is by number of days, not the order
                      of timepoints. You must also provide the argument `bandwidth`.
                - &#39;loess&#39;, &#39;auto&#39;
                    - Implements the initialization of the values using LOESS (Locally
                    - Estimated Scatterplot Smoothing) algorithm. You must also provide
                      the `window` parameter
        tune : tuple(int, int)
            - This is how often to tune the individual covariances
            - The first element indicates which MCMC sample to stop the tuning
            - The second element is how often to update the proposal covariance
        a0, a1 : float, str
            These are the hyperparameters to calculate the dispersion of the
            negative binomial.
        v1, v2 : float, int, str
            These are the values used to calulcate the coupling variance between
            x and q
        intermediate_step : tuple(str, args), array, None
            - This is the type of interemediate timestep to intialize and the arguments
              for them. If this is None, then we do no intermediate timesteps.
            - Options:
                * &#39;step&#39;
                    - args: (stride (numeric), eps (numeric))
                    - We simulate at each timepoint every `stride` days.
                    - We do not set an intermediate time point if it is within `eps`
                      days of a given data point.
                * &#39;preserve-density&#39;
                    - args: (n (int), eps (numeric))
                    - We preserve the denisty of the given data by only simulating data
                    - `n` times between each essential datapoint. If a timepoint is within
                    - `eps` days of a given timepoint then we do not make an intermediate
                      point there.
                * &#39;manual;
                    - args: np.ndarray
                    - These are the points that we want to set. If these are not given times
                      then we set them as timepoints
        intermediate_interpolation : str
            - This is the type of interpolation to perform on the intermediate timepoints.
            - Options:
                - &#39;linear-interpolation&#39;, &#39;auto&#39;
                    - Perform linear interpolation between the two closest given timepoints
        essential_timepoints : np.ndarray, str, None
            - These are the timepoints that must be included in each subject. If one of the
              subjects has a missing timepoint there then we use an intermediate time point
              that this timepoint. It is initialized with linear interpolation. If all of the
              timepoints specified in this vector are included in a subject then nothing is
              done. If it is a str:
                - &#39;union&#39;, &#39;auto&#39;
                    * We take a union of all the timepoints in each subject and make sure
                      that all of the subjects have all those points.
        bandwidth : float
            This is the day bandwidth of the filter if the initialization method is
            done with &#39;moving-avg&#39;.
        window : int
            This is the window term for the LOESS initialization scheme. This is
            only used if value_initialization is done with &#39;loess&#39;
        target_acceptance_rate : numeric
            This is the target acceptance rate for each time point individually
        calculate_qpcr_loglik : bool
            If True, calculate the loglikelihood of the qPCR measurements during the 
            proposal
        &#39;&#39;&#39;
        if not pl.isint(delay):
            raise TypeError(&#39;`delay` ({}) must be an int&#39;.format(type(delay)))
        if delay &lt; 0:
            raise ValueError(&#39;`delay` ({}) must be &gt;= 0&#39;.format(delay))
        self.delay = delay
        self._there_are_perturbations = self.G.perturbations is not None

        # Set the hyperparameters
        if not pl.isfloat(target_acceptance_rate):
            raise TypeError(&#39;`target_acceptance_rate` must be a float&#39;.format(
                type(target_acceptance_rate)))
        if target_acceptance_rate &lt; 0 or target_acceptance_rate &gt; 1:
            raise ValueError(&#39;`target_acceptance_rate` ({}) must be in (0,1)&#39;.format(
                target_acceptance_rate))
        if not pl.istuple(tune):
            raise TypeError(&#39;`tune` ({}) must be a tuple&#39;.format(type(tune)))
        if len(tune) != 2:
            raise ValueError(&#39;`tune` ({}) must have 2 elements&#39;.format(len(tune)))
        if not pl.isint(tune[0]):
            raise TypeError(&#39;`tune` ({}) 1st parameter must be an int&#39;.format(type(tune[0])))
        if tune[0] &lt; 0:
            raise ValueError(&#39;`tune` ({}) 1st parameter must be &gt; 0&#39;.format(tune[0]))
        if not pl.isint(tune[1]):
            raise TypeError(&#39;`tune` ({}) 2nd parameter must be an int&#39;.format(type(tune[1])))
        if tune[1] &lt; 0:
            raise ValueError(&#39;`tune` ({}) 2nd parameter must be &gt; 0&#39;.format(tune[1]))
        
        if not pl.isnumeric(a0):
            raise TypeError(&#39;`a0` ({}) must be a numeric type&#39;.format(type(a0)))
        elif a0 &lt;= 0:
            raise ValueError(&#39;`a0` ({}) must be &gt; 0&#39;.format(a0))
        if not pl.isnumeric(a1):
            raise TypeError(&#39;`a1` ({}) must be a numeric type&#39;.format(type(a1)))
        elif a1 &lt;= 0:
            raise ValueError(&#39;`a1` ({}) must be &gt; 0&#39;.format(a1))

        if not pl.isnumeric(proposal_init_scale):
            raise TypeError(&#39;`proposal_init_scale` ({}) must be a numeric type (int, float)&#39;.format(
                type(proposal_init_scale)))
        if proposal_init_scale &lt; 0:
            raise ValueError(&#39;`proposal_init_scale` ({}) must be positive&#39;.format(
                proposal_init_scale))

        self.tune = tune
        self.a0 = a0
        self.a1 = a1
        self.target_acceptance_rate = target_acceptance_rate
        self.proposal_init_scale = proposal_init_scale
        self.v1 = v1
        self.v2 = v2

        # Set the essential timepoints (check to see if there is any missing data)
        if essential_timepoints is not None:
            logging.info(&#39;Setting up the essential timepoints&#39;)
            if pl.isstr(essential_timepoints):
                if essential_timepoints in [&#39;auto&#39;, &#39;union&#39;]:
                    essential_timepoints = OrderedSet()
                    for ts in self.G.data.times:
                        essential_timepoints = essential_timepoints.union(OrderedSet(list(ts)))
                    essential_timepoints = np.sort(list(essential_timepoints))
                else:
                    raise ValueError(&#39;`essential_timepoints` ({}) not recognized&#39;.format(
                        essential_timepoints))
            elif not pl.isarray(essential_timepoints):
                raise TypeError(&#39;`essential_timepoints` ({}) must be a str or an array&#39;.format(
                    type(essential_timepoints)))
            logging.info(&#39;Essential timepoints: {}&#39;.format(essential_timepoints))
            self.G.data.set_timepoints(times=essential_timepoints, eps=None, reset_timepoints=True)
            self.x.reset_value_size()

        # Set the intermediate timepoints if necessary
        if intermediate_step is not None:
            # Set the intermediate timepoints in the data
            if not pl.istuple(intermediate_step):
                raise TypeError(&#39;`intermediate_step` ({}) must be a tuple&#39;.format(
                    type(intermediate_step)))
            if len(intermediate_step) != 2:
                raise ValueError(&#39;`intermediate_step` ({}) must be length 2&#39;.format(
                    len(intermediate_step)))
            f, args = intermediate_step
            if not pl.isstr(f):
                raise TypeError(&#39;intermediate_step type ({}) must be a str&#39;.format(type(f)))
            if f == &#39;step&#39;:
                if not pl.istuple(args):
                    raise TypeError(&#39;`args` ({}) must be a tuple&#39;.format(type(args)))
                if len(args) != 2:
                    raise TypeError(&#39;`args` ({}) must have 2 arguments&#39;.format(len(args)))
                step, eps = args
                self.G.data.set_timepoints(timestep=step, eps=eps, reset_timepoints=False)
            elif f == &#39;preserve-density&#39;:
                if not pl.istuple(args):
                    raise TypeError(&#39;`args` ({}) must be a tuple&#39;.format(type(args)))
                if len(args) != 2:
                    raise TypeError(&#39;`args` ({}) must have 2 arguments&#39;.format(len(args)))
                n, eps = args
                if not pl.isint(n):
                    raise TypeError(&#39;`n` ({}) must be an int&#39;.format(type(n)))

                # For each timepoint, add `n` intermediate timepoints
                for ridx in range(self.G.data.n_replicates):
                    times = []
                    for i in range(len(self.G.data.times[ridx])-1):
                        t0 = self.G.data.times[ridx][i]
                        t1 = self.G.data.times[ridx][i+1]
                        step = (t1-t0)/(n+1)
                        times = np.append(times, np.arange(t0,t1,step=step))
                    times = np.sort(np.unique(times))
                    # print(&#39;\n\ntimes to put in&#39;, times)
                    self.G.data.set_timepoints(times=times, eps=eps, ridx=ridx, reset_timepoints=False)
                    # print(&#39;times for ridx {}&#39;.format(self.G.data.times[ridx]))
                    # print(&#39;len times&#39;, len(self.G.data.times[ridx]))
                    # print(&#39;data shape&#39;, self.G.data.data[ridx].shape)

                # sys.exit()
            elif f == &#39;manual&#39;:
                raise NotImplementedError(&#39;Not Implemented&#39;)
            else:
                raise ValueError(&#39;`intermediate_step type ({}) not recognized&#39;.format(f))
            self.x.reset_value_size()

        if intermediate_interpolation is not None:
            if intermediate_interpolation in [&#39;linear-interpolation&#39;, &#39;auto&#39;]:
                for ridx in range(self.G.data.n_replicates):
                    for tidx in range(self.G.data.n_timepoints_for_replicate[ridx]):
                        if tidx not in self.G.data.given_timeindices[ridx]:
                            # We need to interpolate this time point
                            # get the previous given and next given timepoint
                            prev_tidx = None
                            for ii in range(tidx-1,-1,-1):
                                if ii in self.G.data.given_timeindices[ridx]:
                                    prev_tidx = ii
                                    break
                            if prev_tidx is None:
                                # Set to the same as the closest forward timepoint then continue
                                next_idx = None
                                for ii in range(tidx+1, self.G.data.n_timepoints_for_replicate[ridx]):
                                    if ii in self.G.data.given_timeindices[ridx]:
                                        next_idx = ii
                                        break
                                self.G.data.data[ridx][:,tidx] = self.G.data.data[ridx][:,next_idx]
                                continue

                            next_tidx = None
                            for ii in range(tidx+1, self.G.data.n_timepoints_for_replicate[ridx]):
                                if ii in self.G.data.given_timeindices[ridx]:
                                    next_tidx = ii
                                    break
                            if next_tidx is None:
                                # Set to the previous timepoint then continue
                                self.G.data.data[ridx][:,tidx] = self.G.data.data[ridx][:,prev_tidx]
                                continue

                            # Interpolate from prev_tidx to next_tidx
                            x = self.G.data.times[ridx][tidx]
                            x0 = self.G.data.times[ridx][prev_tidx]
                            y0 = self.G.data.data[ridx][:,prev_tidx]
                            x1 = self.G.data.times[ridx][next_tidx]
                            y1 = self.G.data.data[ridx][:,next_tidx]
                            self.G.data.data[ridx][:,tidx] = y0 * (1-((x-x0)/(x1-x0))) + y1 * (1-((x1-x)/(x1-x0)))
            else:
                raise ValueError(&#39;`intermediate_interpolation` ({}) not recognized&#39;.format(intermediate_interpolation))

        # Initialize the latent trajectory
        if not pl.isstr(x_value_option):
            raise TypeError(&#39;`x_value_option` ({}) is not a str&#39;.format(type(x_value_option)))
        if x_value_option == &#39;coupling&#39;:
            self._init_coupling()
        elif x_value_option == &#39;moving-avg&#39;:
            if not pl.isnumeric(bandwidth):
                raise TypeError(&#39;`bandwidth` ({}) must be a numeric&#39;.format(type(bandwidth)))
            if bandwidth &lt;= 0:
                raise ValueError(&#39;`bandwidth` ({}) must be positive&#39;.format(bandwidth))
            self.bandwidth = bandwidth
            self._init_moving_avg()
        elif x_value_option in [&#39;loess&#39;, &#39;auto&#39;]:
            if window is None:
                raise TypeError(&#39;If `value_option` is loess, then `window` must be specified&#39;)
            if not pl.isint(window):
                raise TypeError(&#39;`window` ({}) must be an int&#39;.format(type(window)))
            if window &lt;= 0:
                raise ValueError(&#39;`window` ({}) must be &gt; 0&#39;.format(window))
            self.window = window
            self._init_loess()
        else:
            raise ValueError(&#39;`x_value_option` ({}) not recognized&#39;.format(x_value_option))

        # Get necessary data and set the parallel objects
        if self._there_are_perturbations:
            pert_starts = []
            pert_ends = []
            for perturbation in self.G.perturbations:
                pert_starts.append(perturbation.starts)
                pert_ends.append(perturbation.ends)
        else:
            pert_starts = None
            pert_ends = None

        if self.mp is None:
            self.mp = &#39;debug&#39;
        if not pl.isstr(self.mp):
            raise TypeError(&#39;`mp` ({}) must either be a string or None&#39;.format(type(self.mp)))
        if self.mp == &#39;debug&#39;:
            self.pool = []
        elif self.mp == &#39;full&#39;:
            self.pool = pl.multiprocessing.PersistentPool(G=self.G, ptype=&#39;sadw&#39;)
            self.worker_pids = []
        else:
            raise ValueError(&#39;`mp` ({}) not recognized&#39;.format(self.mp))

        for ridx, subj in enumerate(self.G.data.subjects):
            # Set up qPCR measurements and reads to send
            qpcr_log_measurements = {}
            for t in self.G.data.given_timepoints[ridx]:
                qpcr_log_measurements[t] = self.G.data.qpcr[ridx][t].log_data
            reads = self.G.data.subjects.iloc(ridx).reads

            worker = SubjectLogTrajectorySetMP()
            worker.initialize(
                zero_inflation_transition_policy=self.zero_inflation_transition_policy,
                times=self.G.data.times[ridx],
                qpcr_log_measurements=qpcr_log_measurements,
                reads=reads,
                there_are_intermediate_timepoints=True,
                there_are_perturbations=self._there_are_perturbations,
                pv_global=self.G[STRNAMES.PROCESSVAR].global_variance,
                x_prior_mean=np.log(1e7),
                x_prior_std=1e10,
                tune=tune[1],
                delay=delay,
                end_iter=tune[0],
                proposal_init_scale=proposal_init_scale,
                a0=a0,
                a1=a1,
                x=self.x[ridx].value,
                pert_starts=np.asarray(pert_starts),
                pert_ends=np.asarray(pert_ends),
                ridx=ridx,
                subjname=subj.name,
                calculate_qpcr_loglik=calculate_qpcr_loglik,
                h5py_xname=self.x[ridx].name,
                target_acceptance_rate=self.target_acceptance_rate)
            if self.mp == &#39;debug&#39;:
                self.pool.append(worker)
            elif self.mp == &#39;full&#39;:
                pid = self.pool.add_worker(worker)
                self.worker_pids.append(pid)

        # Set the data to the latent values
        self.set_latent_as_data(update_values=False)

        self.total_n_datapoints = 0
        for ridx in range(self.G.data.n_replicates):
            self.total_n_datapoints += self.x[ridx].value.shape[0] * self.x[ridx].value.shape[1]

    def _init_coupling(self):
        &#39;&#39;&#39;Initialize `x` by sampling around the data using a small
        variance using a truncated normal distribution
        &#39;&#39;&#39;
        for ridx, tidx, oidx in self.x.iter_indices():
            val = self.G.data.data[ridx][oidx,tidx]
            self.x[ridx][oidx,tidx] = pl.random.truncnormal.sample(
                loc=val, scale=math.sqrt(self.v1 * (val ** 2) + self.v2),
                low=0, high=float(&#39;inf&#39;))

    def _init_moving_avg(self):
        &#39;&#39;&#39;Initializes `x` by using a moving
        average over the data - using `self.bandwidth` as the bandwidth
        of number of days - it then samples around that point using the
        coupling variance.

        If there are no other points within the bandwidth around the point,
        then it just samples around the current timepoint with the coupling
        variance.
        &#39;&#39;&#39;
        for ridx in range(self.G.data.n_replicates):
            for tidx in range(self.G.data.n_timepoints_for_replicate[ridx]):
                tidx_low = np.searchsorted(
                    self.G.data.times[ridx], self.G.data.times[ridx][tidx]-self.bandwidth)
                tidx_high = np.searchsorted(
                    self.G.data.times[ridx], self.G.data.times[ridx][tidx]+self.bandwidth)

                for oidx in range(len(self.G.data.taxa)):
                    val = np.mean(self.G.data.data[ridx][oidx, tidx_low: tidx_high])
                    self.x[ridx][oidx,tidx] = pl.random.truncnormal.sample(
                        loc=val, scale=math.sqrt(self.v1 * (val ** 2) + self.v2),
                        low=0, high=float(&#39;inf&#39;))

    def _init_loess(self):
        &#39;&#39;&#39;Initialize the data using LOESS algorithm and then samples around that
        the coupling variance we implement the LOESS algorithm in the module
        `fit_loess.py`
        &#39;&#39;&#39;
        for ridx in range(self.G.data.n_replicates):
            xx = self.G.data.times[ridx]
            for oidx in range(len(self.G.data.taxa)):
                yy = self.G.data.data[ridx][oidx, :]
                loess = _Loess(xx, yy)

                for tidx, t in enumerate(self.G.data.times[ridx]):
                    val = loess.estimate(t, window=self.window)
                    self.x[ridx][oidx,tidx] = pl.random.truncnormal.sample(
                        loc=val, scale=math.sqrt(self.v1 * (val ** 2) + self.v2),
                        low=0, high=float(&#39;inf&#39;))

                    if np.isnan(self.x[ridx][oidx, tidx]):
                        print(&#39;crashed here&#39;, ridx, tidx, oidx)
                        print(&#39;mean&#39;, val)
                        print(&#39;t&#39;, t)
                        print(&#39;yy&#39;, yy)
                        print(&#39;std&#39;, math.sqrt(self.v1 * (val ** 2) + self.v2))
                        raise ValueError(&#39;&#39;)

    def set_latent_as_data(self, update_values: bool=True):
        &#39;&#39;&#39;Change the values in the data matrix so that it is the latent variables
        &#39;&#39;&#39;
        data = []
        for obj in self.x.value:
            data.append(obj.value)
        self.G.data.data = data
        if update_values:
            self.G.data.update_values()

    def add_trace(self):
        self.x.add_trace()

    def set_trace(self, *args, **kwargs):
        self.x.set_trace(*args, **kwargs)

    def kill(self):
        if self.mp == &#39;full&#39;:
            self.pool.kill()

    def update(self):
        &#39;&#39;&#39;Send out to each parallel object
        &#39;&#39;&#39;
        if self.sample_iter &lt; self.delay:
            return
        start_time = time.time()

        growth = self.G[STRNAMES.GROWTH_VALUE].value.ravel()
        self_interactions = self.G[STRNAMES.SELF_INTERACTION_VALUE].value.ravel()
        pv = self.G[STRNAMES.PROCESSVAR].value
        interactions = self.G[STRNAMES.INTERACTIONS_OBJ].get_datalevel_value_matrix(
            set_neg_indicators_to_nan=False)
        perts = None
        if self._there_are_perturbations:
            perts = []
            for perturbation in self.G.perturbations:
                perts.append(perturbation.item_array().reshape(-1,1))
            perts = np.hstack(perts)

        # zero_inflation = [self.G[STRNAMES.ZERO_INFLATION].value[ridx] for ridx in range(self.G.data.n_replicates)]
        qpcr_vars = []
        for aaa in self.G[STRNAMES.QPCR_VARIANCES].value:
            qpcr_vars.append(aaa.value)
        
        
        kwargs = {&#39;growth&#39;:growth, &#39;self_interactions&#39;:self_interactions,
            &#39;pv&#39;:pv, &#39;interactions&#39;:interactions, &#39;perturbations&#39;:perts, 
            &#39;zero_inflation_data&#39;: None, &#39;qpcr_variances&#39;:qpcr_vars}

        str_acc = [None]*self.G.data.n_replicates
        mpstr = None
        if self.mp == &#39;debug&#39;:

            for ridx in range(self.G.data.n_replicates):
                _, x, acc_rate = self.pool[ridx].persistent_run(**kwargs)
                self.x[ridx].value = x
                str_acc[ridx] = &#39;{:.3f}&#39;.format(acc_rate)
                mpstr = &#39;no-mp&#39;

        else:
            # raise NotImplementedError(&#39;Multiprocessing for filtering with zero inflation &#39; \
            #     &#39;is not implemented&#39;)
            ret = self.pool.map(func=&#39;persistent_run&#39;, args=kwargs)
            for ridx, x, acc_rate in ret:
                self.x[ridx].value = x
                str_acc[ridx] = &#39;{:.3f}&#39;.format(acc_rate)
            mpstr = &#39;mp&#39;

        self.set_latent_as_data()

        t = time.time() - start_time
        try:
            self._strr = &#39;{} - Time: {:.4f}, Acc: {}, data/sec: {:.2f}&#39;.format(mpstr, t,
                str(str_acc).replace(&#34;&#39;&#34;,&#39;&#39;), self.total_n_datapoints/t)
        except:
            self._strr = &#39;NA&#39;

    def visualize(self, basepath: str, section: str=&#39;posterior&#39;, taxa_formatter: str=&#39;%(name)s&#39;, 
        vmin: float=None, vmax: float=None):
        &#39;&#39;&#39;Plot the posterior for each filtering object

        Parameters
        ----------
        basepath : str
            This is the loction to write the files to
        section : str
            Section of the trace to compute on. Options:
                &#39;posterior&#39; : posterior samples
                &#39;burnin&#39; : burn-in samples
                &#39;entire&#39; : both burn-in and posterior samples
        taxa_formatter : str, None
            This is the format of the label to return for each . If None, it will return
            the taxon&#39;s name
        vmin, vmax : float
            These are the maximum and minimum values to plot the abundance of
        &#39;&#39;&#39;
        for ridx, replicate in enumerate(self.x.value):
            subj = self.G.data.subjects.iloc(ridx)
            path = os.path.join(basepath, &#39;Subject_{}&#39;.format(subj.name))
            os.makedirs(path, exist_ok=True)
            self.x.visualize(ridx=ridx, section=section, basepath=path, 
                taxa_formatter=taxa_formatter)


class SubjectLogTrajectorySetMP(pl.multiprocessing.PersistentWorker):
    &#39;&#39;&#39;This performs filtering on a multiprocessing level. We send the
    other parameters of the model and return the filtered `x` and values.
    With multiprocessing, this class has ~91% efficiency. Additionally, the code
    in this class is optimized to be ~20X faster than the code in `Filtering`. This
    assumes we have the log model.

    It might seem unneccessary to have so many local attributes, but it speeds up
    the inference considerably if we index a value from an array once and store it
    as a float instead of repeatedly indexing the array - the difference in
    reality is super small but we do this so often that it adds up to ~40% speedup
    as supposed to not doing it - so we do this as often as possible - This speedup
    is even greater for indexing keys of dictionaries and getting parameters of objects.

    General efficiency speedups
    ---------------------------
    All of these are done relative to a non-optimized filtering implementation:
    - Specialized sampling and logpdf functions. About 95% faster than
      scipy or numpy functions. All of these add up to a ~35% speed up
    - Explicit function definitions:
      instead of doing `self.prior.logpdf(...)`, we do
      `pl.random.normal.logpdf(...)`, about a ~10% overall speedup
    - Precomputation of values so that the least amout of computation
      is done on a data level - All of these add up to a ~25% speed up
    Benchmarked on a MacPro

    Non/trivial efficiency speedups w.r.t. non-multiprocessed filtering
    -------------------------------------------------------------------
    All of the speed ups are done relative to the current implementation
    in non-multiprocessed filtering.
    - Whenever possible we replace a 2D variable like `self.x` with a
      `curr_x`, which is 1D, because indexing a 1D array is 10-20% faster
      than a 2D array. All of these add up to a ~8% speed up
    - We only every compute the forward dynamics (not the reverse), because
      we can use the forward of the previous timepoint as the reverse for
      the next timepoint. This is about 45% faster and adds up to a ~40%
      speedup.
    - Whenever possible, we replace a dictionary like `read_depths`
      with a float because indexing a dict is 12-20% slower than a 1D
      array. All these add up to a ~7% speed up
    - We precompute AS MUCH AS POSSIBLE in `update` and in `initialize`,
      even simple this as `self.curr_tidx_minus_1`: all of these add up
      to about ~5% speedup
    - If an attribute of a class is being referenced more than once in
      a subroutine, we &#34;get&#34; it by making it a local variable. Example:
      `tidx = self.tidx`. This has about a 5% speed up PER ADDITIONAL
      CALL within the subroutine. All of these add up to ~2.5% speed up.
    - If an indexed value gets indexed more than once within a subroutine,
      we &#34;get&#34; the value by making it a local variable. All of these
      add up to ~4% speed up.
    - We &#34;get&#34; all of the means and stds of the qPCR data-structures so we
      do not reference an object. This is about 22% faster and adds up
      to a ~3% speed up.
    Benchmarked on a MacPro
    &#39;&#39;&#39;
    def __init__(self):
        &#39;&#39;&#39;Set all local variables to None
        &#39;&#39;&#39;
        return

    def initialize(self, times: np.ndarray, qpcr_log_measurements: Dict[float, np.ndarray], 
        reads: Dict[float, np.ndarray], there_are_intermediate_timepoints: bool,
        there_are_perturbations: bool, pv_global: bool, x_prior_mean: Union[float, int],
        x_prior_std: Union[float, int], tune: int, delay: int, end_iter: int, proposal_init_scale: float, 
        a0: float, a1: float, x: np.ndarray, calculate_qpcr_loglik: bool,
        pert_starts: np.ndarray, pert_ends: np.ndarray, ridx: int, subjname: str, 
        h5py_xname: str, target_acceptance_rate: float, zero_inflation_transition_policy: Any):
        &#39;&#39;&#39;Initialize the object at the beginning of the inference

        n_o = Number of Taxa
        n_gT = Number of given time points
        n_T = Total number of time points, including intermediate
        n_P = Number of Perturbations

        Parameters
        ----------
        times : np.array((n_T, ))
            Times for each of the time points
        qpcr_log_measurements : dict(t -&gt; np.ndarray(float))
            These are the qPCR observations for every timepoint in log space.
        reads : dict (float -&gt; np.ndarray((n_o, )))
            The counts for each of the given timepoints. Each value is an
            array for the counts for each of the Taxa
        there_are_intermediate_timepoints : bool
            If True, then there are intermediate timepoints, else there are only
            given timepoints
        there_are_perturbations : bool
            If True, that means there are perturbations, else there are no
            perturbations
        pv_global : bool
            If True, it means the process variance is is global for each Taxa. If
            False it means that there is a separate `pv` for each Taxa
        pv : float, np.ndarray
            This is the process variance value. This is a float if `pv_global` is True
            and it is an array if `pv_global` is False.
        x_prior_mean, x_prior_std : numeric
            This is the prior mean and std for `x` used when sampling the reverse for
            the first timepoint
        tune : int
            How often we should update the proposal for each Taxa
        delay : int
            How many MCMC iterations we should delay the start of updating
        end_iter : int
            What iteration we should stop updating the proposal
        proposal_init_scale : float
            Scale to multiply the initial covariance of the poposal
        a0, a1 : floats
            These are the negative binomial dispersion parameters that specify how
            much noise there is in the counts
        x : np.ndarray((n_o, n_T))
            This is the x initialization
        pert_starts, pert_ends : np.ndarray((n_P, ))
            The starts and ends for each one of the perturbations
        ridx : int
            This is the replicate index that this object corresponds to
        subjname : str
            This is the name of the replicate
        h5py_xname : str
            This is the name for the x in the h5py object
        target_acceptance_rate : float
            This is the target acceptance rate for each point
        calculate_qpcr_loglik : bool
            If True, calculate the loglikelihood of the qPCR measurements during the proposal
        zero_inflation_transition_policy : str
            Zero inflation policy
        &#39;&#39;&#39;
        self.subjname = subjname
        self.h5py_xname = h5py_xname
        self.target_acceptance_rate = target_acceptance_rate
        self.zero_inflation_transition_policy = zero_inflation_transition_policy

        self.times = times
        self.qpcr_log_measurements = qpcr_log_measurements
        self.reads = reads
        self.there_are_intermediate_timepoints = there_are_intermediate_timepoints
        self.there_are_perturbations = there_are_perturbations
        self.pv_global = pv_global
        if not pv_global:
            raise TypeError(&#39;Filtering with MP not implemented for non global process variance&#39;)
        self.x_prior_mean = x_prior_mean
        self.x_prior_std = x_prior_std
        self.tune = tune
        self.delay = 0
        self.end_iter = end_iter
        self.proposal_init_scale = proposal_init_scale
        self.a0 = a0
        self.a1 = a1
        self.n_taxa = x.shape[0]
        self.n_timepoints = len(times)
        self.n_timepoints_minus_1 = len(times)-1
        self.logx = np.log(x)
        self.x = x

        # Get the perturbations for this subject
        if self.there_are_perturbations:
            self.pert_starts = []
            self.pert_ends = []
            for pidx in range(len(pert_starts)):
                self.pert_starts.append(pert_starts[pidx][subjname])
                self.pert_ends.append(pert_ends[pidx][subjname])

        self.total_n_points = self.x.shape[0] * self.x.shape[1]
        self.ridx = ridx
        self.calculate_qpcr_loglik = calculate_qpcr_loglik

        self.sample_iter = 0
        self.n_data_points = self.x.shape[0] * self.x.shape[1]

        # latent state
        self.sum_q = np.sum(self.x, axis=0)
        self.trace_iter = 0

        # proposal
        self.proposal_std = np.log(1.5) #np.log(3)
        self.acceptances = 0
        self.n_props_total = 0
        self.n_props_local = 0
        self.total_acceptances = 0
        self.add_trace = True

        # Intermediate timepoints
        if self.there_are_intermediate_timepoints:
            self.is_intermediate_timepoint = {}
            self.data_loglik = self.data_loglik_w_intermediates
            for t in self.times:
                self.is_intermediate_timepoint[t] = t not in self.reads
        else:
            self.data_loglik = self.data_loglik_wo_intermediates

        # Reads
        self.read_depths = {}
        for t in self.reads:
            self.read_depths[t] = float(np.sum(self.reads[t]))

        # t
        self.dts = np.zeros(self.n_timepoints_minus_1)
        self.sqrt_dts = np.zeros(self.n_timepoints_minus_1)
        for k in range(self.n_timepoints_minus_1):
            self.dts[k] = self.times[k+1] - self.times[k]
            self.sqrt_dts[k] = np.sqrt(self.dts[k])
        self.t2tidx = {}
        for tidx, t in enumerate(self.times):
            self.t2tidx[t] = tidx

        self.cnt_accepted_times = np.zeros(len(self.times))

        # Perturbations
        # -------------
        # in_pert_transition : np.ndarray(dtype=bool)
        #   This is a bool array where if it is a true it means that the
        #   forward and reverse growth rates are different
        # fully_in_pert : np.ndarray(dtype=int)
        #   This is an int-array where it tells you which perturbation you are fully in
        #   (the forward and reverse growth rates are the same but not the default).
        #   If there is no perturbation then the value is -1. If it is not -1, then the
        #   number corresponds to what perturbation index you are in.
        #
        # Edge cases
        # ----------
        #   * missing data for start
        #       There could be a situation where there was no sample collection
        #       on the day that they started a perturbation. In this case we
        #       assume that the next time point is the `start` of the perturbation.
        #       i.e. the next time point is the perturbation transition.
        #   * missing data for end
        #       There could be a situation where no sample was collected when the
        #       perturbation ended. In this case we assume that the pervious time
        #       point was the end of the perturbation.
        if self.there_are_perturbations:
            self.in_pert_transition = np.zeros(self.n_timepoints, dtype=bool)
            self.fully_in_pert = np.ones(self.n_timepoints, dtype=int) * -1
            for pidx, t in enumerate(self.pert_starts):
                if t == self.times[-1] or t == self.times[0]:
                    raise ValueError(&#39;The code right now does not support either a perturbation that &#39; \
                        &#39;started on the first day or ended on the last day. The code where this is &#39; \
                        &#39;incompatible is when we checking if we are in a perturbation transition&#39;)
                if t &gt; np.max(self.times):
                    continue
                if t not in self.t2tidx:
                    # Use the next time point
                    tidx = np.searchsorted(self.times, t)
                else:
                    tidx = self.t2tidx[t]
                self.in_pert_transition[tidx] = True
            for pidx, t in enumerate(self.pert_ends):
                if t == self.times[-1] or t == self.times[0]:
                    raise ValueError(&#39;The code right now does not support either a perturbation that &#39; \
                        &#39;started on the first day or ended on the last day. The code where this is &#39; \
                        &#39;incompatible is when we checking if we are in a perturbation transition&#39;)
                if t &lt; np.min(self.times) or t &gt; np.max(self.times):
                    continue
                if t not in self.t2tidx:
                    # Use the previous time point
                    tidx = np.searchsorted(self.times, t) - 1
                else:
                    tidx = self.t2tidx[t]
                self.in_pert_transition[tidx] = True

            # check if anything is weird
            if np.sum(self.in_pert_transition) % 2 != 0:
                raise ValueError(&#39;The number of in_pert_transition periods must be even ({})&#39; \
                    &#39;. There is either something wrong with the data (start and end day are &#39; \
                    &#39;the same) or with the algorithm ({})&#39;.format(
                        np.sum(self.in_pert_transition),
                        self.in_pert_transition))

            # Make the fully in perturbation times
            for pidx in range(len(self.pert_ends)):
                try:
                    start_tidx = self.t2tidx[self.pert_starts[pidx]] + 1
                    end_tidx = self.t2tidx[self.pert_ends[pidx]]
                except:
                    # This means there is a missing datapoint at either the
                    # start or end of the perturbation
                    start_t = self.pert_starts[pidx]
                    end_t = self.pert_ends[pidx]
                    start_tidx = np.searchsorted(self.times, start_t)
                    end_tidx = np.searchsorted(self.times, end_t) - 1

                self.fully_in_pert[start_tidx:end_tidx] = pidx
    
    # @profile
    def persistent_run(self, growth: np.ndarray, self_interactions: np.ndarray, 
        pv: Union[float, int, np.ndarray], interactions: np.ndarray,
        perturbations: np.ndarray, qpcr_variances: np.ndarray, 
        zero_inflation_data: Any) -&gt; Tuple[int, np.ndarray, float]:
        &#39;&#39;&#39;Run an update of the values for a single gibbs step for all of the data points
        in this replicate

        Parameters
        ----------
        growth : np.ndarray((n_taxa, ))
            Growth rates for each Taxa
        self_interactions : np.ndarray((n_taxa, ))
            Self-interactions for each Taxa
        pv : numeric, np.ndarray
            This is the process variance
        interactions : np.ndarray((n_taxa, n_taxa))
            These are the Taxa-Taxa interactions
        perturbations : np.ndarray((n_perturbations, n_taxa))
            Perturbation values in the right perturbation order, per Taxa
        zero_inflation : np.ndarray
            These are the points that are delibertly pushed down to zero
        qpcr_variances : np.ndarray
            These are the sampled qPCR variances as an array - they are in
            time order

        Returns
        -------
        (int, np.ndarray, float)
            1 This is the replicate index
            2 This is the updated latent state for logx
            3 This is the acceptance rate for this past update.
        &#39;&#39;&#39;
        self.master_growth_rate = growth

        if self.sample_iter &lt; self.delay:
            self.sample_iter += 1
            return self.ridx, self.x, np.nan

        self.update_proposals()
        self.n_accepted_iter = 0
        self.pv = pv
        self.pv_std = np.sqrt(pv)
        self.qpcr_stds = np.sqrt(qpcr_variances[self.ridx])
        self.qpcr_stds_d = {}
        # self.zero_inflation_data = zero_inflation_data[self.ridx]
        self.zero_inflation_data = None

        for tidx,t in enumerate(self.qpcr_log_measurements):
            self.qpcr_stds_d[t] = self.qpcr_stds[tidx]

        if self.there_are_perturbations:
            self.growth_rate_non_pert = growth.ravel()
            self.growth_rate_on_pert = growth.reshape(-1,1) * (1 + perturbations)
                
        # Go through each randomly Taxa and go in time order
        oidxs = npr.permutation(self.n_taxa)
        # print(&#39;===============================&#39;)
        # print(&#39;===============================&#39;)
        # print(&#39;ridx&#39;, self.ridx)
        for oidx in oidxs:

            # Set the necessary global parameters
            self.oidx = oidx
            self.curr_x = self.x[oidx, :]
            self.curr_logx = self.logx[oidx, :]
            self.curr_interactions = interactions[oidx, :]
            self.curr_self_interaction = self_interactions[oidx]
            # self.curr_zero_inflation = self.zero_inflation[oidx, :]

            if self.pv_global:
                self.curr_pv_std = self.pv_std
            else:
                self.curr_pv_std = self.pv_std[oidx]

            # Set for first time point
            self.tidx = 0
            self.set_attrs_for_timepoint()
            self.forward_loglik = self.default_forward_loglik
            self.reverse_loglik = self.first_timepoint_reverse
            # Calculate A matrix for forward
            self.forward_interaction_vals = np.nansum(self.x[:, self.tidx] * self.curr_interactions)
            self.update_single()
            self.reverse_loglik = self.default_reverse_loglik
            # Set for middle timepoints
            for tidx in range(1, self.n_timepoints-1):
                # Check if it needs to be zero inflated
                # if not self.curr_zero_inflation[tidx]:
                #     raise NotImplementedError(&#39;Zero inflation not implemented for logmodel&#39;)

                self.tidx = tidx
                self.set_attrs_for_timepoint()

                # Calculate A matrix for forward and reverse
                # Set the reverse of the current time step to the forward of the previous
                self.reverse_interaction_vals = self.forward_interaction_vals #np.sum(self.x[:, self.prev_tidx] * self.curr_interactions)
                self.forward_interaction_vals = np.nansum(self.x[:, self.tidx] * self.curr_interactions)

                # Run single update
                self.update_single()

            # Set for last timepoint
            self.tidx = self.n_timepoints_minus_1
            self.set_attrs_for_timepoint()
            self.forward_loglik = self.last_timepoint_forward
            # Calculate A matrix for reverse
            # Set the reverse of the current time step to the forward of the previous
            self.reverse_interaction_vals = self.forward_interaction_vals # np.sum(self.x[:, self.prev_tidx] * self.curr_interactions)
            self.update_single()

            # if self.sample_iter == 4:
            # sys.exit()

        self.sample_iter += 1
        if self.add_trace:
            self.trace_iter += 1

        # print(self.cnt_accepted_times/self.sample_iter)

        return self.ridx, self.x, self.n_accepted_iter/self.n_data_points

    def set_attrs_for_timepoint(self):
        self.prev_tidx = self.tidx-1
        self.next_tidx = self.tidx+1
        self.forward_growth_rate = self.master_growth_rate[self.oidx]
        self.reverse_growth_rate = self.master_growth_rate[self.oidx]

        if self.there_are_intermediate_timepoints:
            if not self.is_intermediate_timepoint[self.times[self.tidx]]:
                # It is not intermediate timepoints - we need to get the data
                t = self.times[self.tidx]
                self.curr_reads = self.reads[t][self.oidx]
                self.curr_read_depth = self.read_depths[t]
                self.curr_qpcr_log_measurements = self.qpcr_log_measurements[t]
                self.curr_qpcr_std = self.qpcr_stds_d[t]
        else:
            t = self.times[self.tidx]
            self.curr_reads = self.reads[t][self.oidx]
            self.curr_read_depth = self.read_depths[t]
            self.curr_qpcr_log_measurements = self.qpcr_log_measurements[t]
            self.curr_qpcr_std = self.qpcr_stds_d[t]

        # Set perturbation growth rates
        if self.there_are_perturbations:
            if self.in_pert_transition[self.tidx]:
                if self.fully_in_pert[self.tidx-1] != -1:
                    # If the previous time point is in the perturbation, that means
                    # we are going out of the perturbation
                    # self.forward_growth_rate = self.master_growth_rate[self.oidx]
                    pidx = self.fully_in_pert[self.tidx-1]
                    self.reverse_growth_rate = self.growth_rate_on_pert[self.oidx,pidx]
                else:
                    # Else we are going into a perturbation
                    # self.reverse_growth_rate = self.master_growth_rate[self.oidx]
                    pidx = self.fully_in_pert[self.tidx+1]
                    self.forward_growth_rate = self.growth_rate_on_pert[self.oidx,pidx]
            elif self.fully_in_pert[self.tidx] != -1:
                pidx = self.fully_in_pert[self.tidx]
                self.forward_growth_rate = self.growth_rate_on_pert[self.oidx,pidx]
                self.reverse_growth_rate = self.forward_growth_rate

    # @profile
    def update_single(self):
        &#39;&#39;&#39;Update a single oidx, tidx
        &#39;&#39;&#39;
        tidx = self.tidx
        oidx = self.oidx

        # Check if we should update the zero inflation policy
        if self.zero_inflation_transition_policy is not None:
            if self.zero_inflation_transition_policy == &#39;ignore&#39;:
                if not self.zero_inflation_data[oidx,tidx]:
                    self.x[oidx, tidx] = np.nan
                    self.logx[oidx, tidx] = np.nan
                    return
                else:
                    if tidx &lt; self.zero_inflation_data.shape[1]-1:
                        do_forward = self.zero_inflation_data[oidx, tidx+1]
                    else:
                        do_forward = True
                    if tidx &gt; 0:
                        do_reverse = self.zero_inflation_data[oidx, tidx-1]
                    else:
                        do_reverse = True
            else:
                raise NotImplementedError(&#39;Not Implemented&#39;)
        else:
            do_forward = True
            do_reverse = True

        try:
            logx_new = pl.random.misc.fast_sample_normal(
                loc=self.curr_logx[tidx],
                scale=self.proposal_std)
        except:
            print(&#39;mu&#39;, self.curr_logx[tidx])
            print(&#39;std&#39;, self.proposal_std)
            raise
        x_new = np.exp(logx_new)
        prev_logx_value = self.curr_logx[tidx]
        prev_x_value = self.curr_x[tidx]

        if do_forward:
            prev_aaa = self.forward_loglik()
        else:
            prev_aaa = 0
        if do_reverse:
            prev_bbb = self.reverse_loglik()
        else:
            prev_bbb = 0
        prev_ddd = self.data_loglik()

        l_old = prev_aaa + prev_bbb + prev_ddd

        self.curr_x[tidx] = x_new
        self.curr_logx[tidx] = logx_new
        self.sum_q[tidx] = self.sum_q[tidx] - prev_x_value + x_new

        if do_forward:
            new_aaa = self.forward_loglik()
        else:
            new_aaa = 0
        if do_reverse:
            new_bbb = self.reverse_loglik()
        else:
            new_bbb = 0
        new_ddd = self.data_loglik()

        l_new = new_aaa + new_bbb + new_ddd
        r_accept = l_new - l_old

        r = pl.random.misc.fast_sample_standard_uniform()
        if math.log(r) &gt; r_accept:
            self.sum_q[tidx] = self.sum_q[tidx] + prev_x_value - x_new
            self.curr_x[tidx] = prev_x_value
            self.curr_logx[tidx] = prev_logx_value
        else:
            self.x[oidx, tidx] = x_new
            self.logx[oidx, tidx] = logx_new
            self.acceptances += 1
            self.total_acceptances += 1
            self.n_accepted_iter += 1

        self.n_props_local += 1
        self.n_props_total += 1

    def update_proposals(self):
        &#39;&#39;&#39;Update the proposal if necessary
        &#39;&#39;&#39;
        if self.sample_iter &gt; self.end_iter:
            self.add_trace = False
            return
        if self.sample_iter == 0:
            return
        if self.trace_iter - self.delay == self.tune  and self.sample_iter - self.delay &gt; 0:

            # Adjust
            acc_rate = self.acceptances/self.n_props_total
            if acc_rate &lt; 0.1:
                logging.debug(&#39;Very low acceptance rate, scaling down past covariance&#39;)
                self.proposal_std *= 0.01
            elif acc_rate &lt; self.target_acceptance_rate:
                self.proposal_std /= np.sqrt(1.5)
            else:
                self.proposal_std *= np.sqrt(1.5)
            
            self.acceptances = 0
            self.n_props_local = 0

    def last_timepoint_forward(self):
        return 0

    # @profile
    def default_forward_loglik(self) -&gt; float:
        &#39;&#39;&#39;From the current timepoint (tidx) to the next timepoint (tidx+1)
        &#39;&#39;&#39;
        logmu = self.compute_dynamics(
            tidx=self.tidx,
            Axj=self.forward_interaction_vals,
            a1=self.forward_growth_rate)

        try:
            return pl.random.normal.logpdf(
                value=self.curr_logx[self.next_tidx], 
                loc=logmu, scale=self.curr_pv_std*self.sqrt_dts[self.tidx])
        except:
            return _normal_logpdf(
                value=self.curr_logx[self.next_tidx], 
                loc=logmu, scale=self.curr_pv_std*self.sqrt_dts[self.tidx])

    def first_timepoint_reverse(self) -&gt; float:
        # sample from the prior
        try:
            return pl.random.normal.logpdf(value=self.curr_logx[self.tidx],
                loc=self.x_prior_mean, scale=self.x_prior_std)
        except:
            return _normal_logpdf(value=self.curr_logx[self.tidx],
                loc=self.x_prior_mean, scale=self.x_prior_std)

    # @profile
    def default_reverse_loglik(self) -&gt; float:
        &#39;&#39;&#39;From the previous timepoint (tidx-1) to the current time point (tidx)
        &#39;&#39;&#39;
        logmu = self.compute_dynamics(
            tidx=self.prev_tidx,
            Axj=self.reverse_interaction_vals,
            a1=self.reverse_growth_rate)

        try:
            return pl.random.normal.logpdf(value=self.curr_logx[self.tidx], 
                loc=logmu, scale=self.curr_pv_std*self.sqrt_dts[self.prev_tidx])
        except:
            return _normal_logpdf(value=self.curr_logx[self.tidx], 
                loc=logmu, scale=self.curr_pv_std*self.sqrt_dts[self.prev_tidx])

    def data_loglik_w_intermediates(self) -&gt; float:
        &#39;&#39;&#39;data loglikelihood w/ intermediate timepoints
        &#39;&#39;&#39;
        if self.is_intermediate_timepoint[self.times[self.tidx]]:
            return 0
        else:
            return self.data_loglik_wo_intermediates()

    # @profile
    def data_loglik_wo_intermediates(self) -&gt; float:
        &#39;&#39;&#39;data loglikelihood with intermediate timepoints
        &#39;&#39;&#39;
        sum_q = self.sum_q[self.tidx]
        log_sum_q = math.log(sum_q)
        rel = self.curr_x[self.tidx] / sum_q

        try:
            negbin = negbin_loglikelihood_MH_condensed(
                k=self.curr_reads,
                m=self.curr_read_depth * rel,
                dispersion=self.a0/rel + self.a1)
        except:
            negbin = negbin_loglikelihood_MH_condensed_not_fast(
                k=self.curr_reads,
                m=self.curr_read_depth * rel,
                dispersion=self.a0/rel + self.a1)

        qpcr = 0
        if self.calculate_qpcr_loglik:
            for qpcr_val in self.curr_qpcr_log_measurements:
                a = pl.random.normal.logpdf(value=qpcr_val, loc=log_sum_q, scale=self.curr_qpcr_std)
                qpcr += a
        return negbin + qpcr

    def compute_dynamics(self, tidx: int, Axj: np.ndarray, a1: np.ndarray) -&gt; np.ndarray:
        &#39;&#39;&#39;Compute dynamics going into tidx+1

        a1 : growth rates and perturbations (if necessary)
        Axj : cluster interactions with the other abundances already multiplied
        tidx : time index

        Zero-inflation
        --------------
        When we get here, the current taxon at `tidx` is not a structural zero, but 
        there might be other bugs in the system that do have a structural zero there.
        Thus we do nan adds
        &#39;&#39;&#39;
        logxi = self.curr_logx[tidx]
        xi = self.curr_x[tidx]
        return logxi + (a1 - xi*self.curr_self_interaction + Axj) * self.dts[tidx]


class ZeroInflation(pl.graph.Node):
    &#39;&#39;&#39;This is the posterior distribution for the zero inflation model. These are used
    to learn when the model should use use the data and when it should not. We do not need
    to trace this object because we set the structural zeros to nans in the trace for 
    filtering.

    TODO: Parallel version of the class
    &#39;&#39;&#39;

    def __init__(self, **kwargs):
        &#39;&#39;&#39;
        Parameters
        ----------
        mp : str
            This is the type of parallelization to use. This is not implemented yet.
        &#39;&#39;&#39;
        kwargs[&#39;name&#39;] = STRNAMES.ZERO_INFLATION
        pl.graph.Node.__init__(self, **kwargs)
        self.value = []
        self._strr = &#39;NA&#39;

        for ridx in range(self.G.data.n_replicates):
            n_timepoints = self.G.data.n_timepoints_for_replicate[ridx]
            self.value.append(np.ones(shape=(len(self.G.data.taxa), n_timepoints), dtype=bool))

    def reset_value_size(self):
        &#39;&#39;&#39;Change the size of the trajectory when we set the intermediate timepoints
        &#39;&#39;&#39;
        n_taxa = self.G.data.n_taxa
        for ridx in range(len(self.value)):
            n_timepoints = self.G.data.n_timepoints_for_replicate[ridx]
            self.value[ridx] = np.ones((n_taxa, n_timepoints), dtype=bool)

    def __str__(self) -&gt; str:
        return self._strr

    def initialize(self, value_option: str, delay: int=0):
        &#39;&#39;&#39;Initialize the values. Right now this is static and we are not learning this so
        do not do anything fancy

        Parameters
        ----------

        delay : None, int
            How much to delay starting the sampling
        &#39;&#39;&#39;
        if delay is None:
            delay = 0
        if not pl.isint(delay):
            raise TypeError(&#39;`delay` ({}) must be an int&#39;.format(type(delay)))
        self.delay = delay

        if value_option in [None, &#39;auto&#39;]:
            # Set everything to on
            self.value = []
            for ridx in range(self.G.data.n_replicates):
                n_timepoints = self.G.data.n_timepoints_for_replicate[ridx]
                self.value.append(np.ones(
                    shape=(len(self.G.data.taxa), n_timepoints), dtype=bool))
            turn_on = None
            turn_off = None

        elif value_option == &#39;mdsine-cdiff&#39;:
            # Set everything to on except for cdiff before day 28 for every subject
            self.value = []
            for ridx in range(self.G.data.n_replicates):
                n_timepoints = self.G.data.n_timepoints_for_replicate[ridx]
                self.value.append(np.ones(
                    shape=(len(self.G.data.taxa), n_timepoints), dtype=bool))

            # Get cdiff
            cdiff_idx = self.G.data.taxa[&#39;Clostridium-difficile&#39;].idx
            turn_off = []
            turn_on = []
            for ridx in range(self.G.data.n_replicates):
                for tidx, t in enumerate(self.G.data.times[ridx]):
                    for oidx in range(len(self.G.data.taxa)):
                        if t &lt; 28 and oidx == cdiff_idx:
                            self.value[ridx][cdiff_idx, tidx] = False
                            turn_off.append((ridx, tidx, cdiff_idx))
                        else:
                            turn_on.append((ridx, tidx, oidx))

        else:
            raise ValueError(&#39;`value_option` ({}) not recognized&#39;.format(value_option))

        self.G.data.set_zero_inflation(turn_on=turn_on, turn_off=turn_off)
                

# Interactions
# ------------
class PriorVarInteractions(pl.variables.SICS):
    &#39;&#39;&#39;This is the posterior of the prior variance of regression coefficients
    for the interaction (off diagonal) variables
    &#39;&#39;&#39;
    def __init__(self, prior: variables.SICS, value: Union[float, int]=None, **kwargs):

        kwargs[&#39;name&#39;] = STRNAMES.PRIOR_VAR_INTERACTIONS
        pl.variables.SICS.__init__(self, value=value,
            dtype=float, **kwargs)
        self.add_prior(prior)

    def initialize(self, value_option: str, dof_option: str, scale_option: str, value: float=None,
        mean_scaling_factor: Union[float, int]=None, dof: Union[float, int]=None, 
        scale: Union[float, int]=None, delay: int=0):
        &#39;&#39;&#39;Initialize the hyperparameters of the self interaction variance based on the
        passed in option

        Parameters
        ----------
        value_option : str
            - Initialize the value based on the specified option
            - Options
                - &#39;manual&#39;
                    - Set the value manually, `value` must also be specified
                - &#39;auto&#39;, &#39;prior-mean&#39;
                    - Set the value to the mean of the prior
        scale_option : str
            - Initialize the scale of the prior
            - Options
                - &#39;manual&#39;
                    - Set the value manually, `scale` must also be specified
                - &#39;auto&#39;, &#39;same-as-aii&#39;
                    - Set the mean the same as the self-interactions
        dof_option : str
            Initialize the dof of the parameter
            Options:
                &#39;manual&#39;: Set the value with the parameter `dof`
                &#39;diffuse&#39;: Set the value to 2.01
                &#39;strong&#39;: Set the valuye to the expected number of interactions
                &#39;auto&#39;: Set to diffuse
        dof, scale : int, float
            - User specified values
            - Only necessary if `hyperparam_option` == &#39;manual&#39;
        &#39;&#39;&#39;
        if not pl.isint(delay):
            raise TypeError(&#39;`delay` ({}) must be an int&#39;.format(type(delay)))
        if delay &lt; 0:
            raise ValueError(&#39;`delay` ({}) must be &gt;= 0&#39;.format(delay))
        self.delay = delay

        self.interactions = self.G[STRNAMES.CLUSTER_INTERACTION_VALUE]

        if not pl.isstr(dof_option):
            raise TypeError(&#39;`dof_option` ({}) must be a str&#39;.format(type(dof_option)))
        if dof_option == &#39;manual&#39;:
            if not pl.isnumeric(dof):
                raise TypeError(&#39;`dof` ({}) must be a numeric&#39;.format(type(dof)))
            if dof &lt; 0:
                raise ValueError(&#39;`dof` ({}) must be &gt; 0 for it to be a valid prior&#39;.format(dof))
        elif dof_option in [&#39;diffuse&#39;, &#39;auto&#39;]:
            dof = 2.01
        elif dof_option == &#39;strong&#39;:
            N = expected_n_clusters(G=self.G)
            dof = N * (N - 1)
        else:
            raise ValueError(&#39;`dof_option` ({}) not recognized&#39;.format(dof_option))
        self.prior.dof.override_value(dof)

        if not pl.isstr(scale_option):
            raise TypeError(&#39;`scale_option` ({}) must be a str&#39;.format(type(scale_option)))
        if scale_option == &#39;manual&#39;:
            if not pl.isnumeric(scale):
                raise TypeError(&#39;`scale` ({}) must be a numeric&#39;.format(type(scale)))
            if scale &lt; 0:
                raise ValueError(&#39;`scale` ({}) must be &gt; 0 for it to be a valid prior&#39;.format(scale))
        elif scale_option in [&#39;auto&#39;, &#39;same-as-aii&#39;]:
            mean = self.G[STRNAMES.PRIOR_VAR_SELF_INTERACTIONS].prior.mean()
            scale = mean * (self.prior.dof.value - 2) /(self.prior.dof.value)
        else:
            raise ValueError(&#39;`scale_option` ({}) not recognized&#39;.format(scale_option))
        self.prior.scale.override_value(scale)

        if not pl.isstr(value_option):
            raise TypeError(&#39;`value_option` ({}) must be a str&#39;.format(type(value_option)))
        if value_option == &#39;manual&#39;:
            if not pl.isnumeric(value):
                raise ValueError(&#39;`value` ({}) must be numeric (float,int)&#39;.format(value.__class__))
            self.value = value
        elif value_option in [&#39;auto&#39;, &#39;prior-mean&#39;]:
            if not pl.isnumeric(mean_scaling_factor):
                raise ValueError(&#39;`mean_scaling_factor` ({}) must be a numeric type &#39; \
                    &#39;(float,int)&#39;.format(mean_scaling_factor.__class__))
            self.value = self.prior.mean()
        else:
            raise ValueError(&#39;`value_option` ({}) not recognized&#39;.format(value_option))

        logging.info(&#39;Prior Variance Interactions initialization results:\n&#39; \
            &#39;\tprior dof: {}\n&#39; \
            &#39;\tprior scale: {}\n&#39; \
            &#39;\tvalue: {}&#39;.format(
                self.prior.dof.value, self.prior.scale.value, self.value))

    # @profile
    def update(self):
        &#39;&#39;&#39;Calculate the posterior of the prior variance
        &#39;&#39;&#39;
        if self.sample_iter &lt; self.delay:
            return

        x = self.interactions.obj.get_values(use_indicators=True)
        mu = self.G[STRNAMES.PRIOR_MEAN_INTERACTIONS].value

        se = np.sum(np.square(x - mu))
        n = len(x)

        self.dof.value = self.prior.dof.value + n
        self.scale.value = ((self.prior.scale.value * self.prior.dof.value) + \
           se)/self.dof.value
        self.sample()

    def visualize(self, path: str, f: IO, section: str=&#39;posterior&#39;) -&gt; IO:
        return _scalar_visualize(self, path=path, f=f, section=section)


class PriorMeanInteractions(pl.variables.Normal):
    &#39;&#39;&#39;This is the posterior mean for the interactions
    &#39;&#39;&#39;

    def __init__(self, prior: variables.Normal, **kwargs):
        kwargs[&#39;name&#39;] = STRNAMES.PRIOR_MEAN_INTERACTIONS
        pl.variables.Normal.__init__(self, loc=None, scale2=None, dtype=float, **kwargs)
        self.add_prior(prior)

    def __str__(self) -&gt; str:
        # If this fails, it is because we are dividing by 0 sampler_iter
        # If which case we just return the value 
        s = str(self.value)
        return s

    def initialize(self, value_option: str, loc_option: str, scale2_option: str, 
        value: Union[float, int]=None, loc: Union[float, int]=None, 
        scale2: Union[float, int]=None, delay: int=0):
        &#39;&#39;&#39;Initialize the hyperparameters

        Parameters
        ----------
        value_option : str
            How to set the value. Options:
                &#39;zero&#39;
                    Set to zero
                &#39;prior-mean&#39;, &#39;auto&#39;
                    Set to the mean of the prior
                &#39;manual&#39;
                    Specify with the `value` parameter
        loc_option : str
            How to set the loc of the prior
                &#39;zero&#39;, &#39;auto&#39;
                    Set to zero
                &#39;manual&#39;
                    Set with the `loc` parameter
        scale2_option : str
            &#39;same-as-aii&#39;, &#39;auto&#39;
                Set as the same variance as the self-interactions
            &#39;manual&#39;
                Set with the `scale2` parameter
        value, loc, scale2 : float
            These are only necessary if we specify manual for any of the other 
            options
        delay : int
            How much to delay the start of the update during inference
        &#39;&#39;&#39;
        if not pl.isint(delay):
            raise TypeError(&#39;`delay` ({}) must be an int&#39;.format(type(delay)))
        if delay &lt; 0:
            raise ValueError(&#39;`delay` ({}) must be &gt;= 0&#39;.format(delay))
        self.delay = delay

        # Set the location parameter
        if not pl.isstr(loc_option):
            raise TypeError(&#39;`loc_option` ({}) must be a str&#39;.format(type(loc_option)))
        if loc_option == &#39;manual&#39;:
            if not pl.isnumeric(loc):
                raise TypeError(&#39;`loc` ({}) must be a numeric&#39;.format(type(loc)))
        elif loc_option in [&#39;zero&#39;, &#39;auto&#39;]:
            loc = 0
        else:
            raise ValueError(&#39;`loc_option` ({}) not recognized&#39;.format(loc_option))
        self.prior.loc.override_value(loc)

        # Set the scale2 parameter
        if not pl.isstr(scale2_option):
            raise TypeError(&#39;`scale2_option` ({}) must be a str&#39;.format(type(scale2_option)))
        if scale2_option == &#39;manual&#39;:
            if not pl.isnumeric(scale2):
                raise TypeError(&#39;`scale2` ({}) must be a numeric&#39;.format(type(scale2)))
            if scale2 &lt;= 0:
                raise ValueError(&#39;`scale2` ({}) must be positive&#39;.format(scale2))
        elif scale2_option in [&#39;same-as-aii&#39;, &#39;auto&#39;]:
            scale2 = self.G[STRNAMES.PRIOR_VAR_SELF_INTERACTIONS].value
        else:
            raise ValueError(&#39;`scale2_option` ({}) not recognized&#39;.format(scale2_option))
        self.prior.scale2.override_value(scale2)

        # Set the value
        if not pl.isstr(value_option):
            raise TypeError(&#39;`value_option` ({}) must be a str&#39;.format(type(value_option)))
        if value_option == &#39;manual&#39;:
            if not pl.isnumeric(value):
                raise TypeError(&#39;`value` ({}) must be a numeric&#39;.format(type(value)))
        elif value_option in [&#39;prior-mean&#39;, &#39;auto&#39;]:
            value = self.prior.loc.value
        elif value_option == &#39;zero&#39;:
            value = 0
        else:
            raise ValueError(&#39;`value_option` ({}) not recognized&#39;.format(value_option))
        self.value = value

    def update(self):
        &#39;&#39;&#39;Update using Gibbs sampling
        &#39;&#39;&#39;
        if self.sample_iter &lt; self.delay:
            return

        if self.G[STRNAMES.CLUSTER_INTERACTION_INDICATOR].num_pos_indicators == 0:
            # sample from the prior
            self.value = self.prior.sample()
            return

        x = self.G[STRNAMES.CLUSTER_INTERACTION_VALUE].value
        prec = 1/self.G[STRNAMES.PRIOR_VAR_INTERACTIONS].value

        prior_prec = 1/self.prior.scale2.value
        prior_mean = self.prior.loc.value

        self.scale2.value = 1/(prior_prec + (len(x)*prec))
        self.loc.value = self.scale2.value * ((prior_mean * prior_prec) + (np.sum(x)*prec))
        self.sample()

    def visualize(self, path: str, f: IO, section: str=&#39;posterior&#39;) -&gt; IO:
        return _scalar_visualize(self, path=path, f=f, section=section)


class ClusterInteractionIndicatorProbability(pl.variables.Beta):
    &#39;&#39;&#39;This is the posterior for the probability of a cluster being on.

    Note that in the beta prior, `a` is the number of ons, `b` is the 
    number of offs.

    Parameters
    ----------
    prior : pl.variables.Beta
        Prior probability
    kwargs : dict
        Other options like graph, value
    &#39;&#39;&#39;
    def __init__(self, prior: variables.Beta, **kwargs):
        kwargs[&#39;name&#39;] = STRNAMES.CLUSTER_INTERACTION_INDICATOR_PROB
        pl.variables.Beta.__init__(self, a=prior.a.value, b=prior.b.value,
            dtype=float, **kwargs)
        self.add_prior(prior)

    def initialize(self, value_option: str, hyperparam_option: str, 
        a: Union[int, float]=None, b: Union[int, float]=None, 
        value: float=None, N: Union[float, str]=&#39;auto&#39;, delay: int=0):
        &#39;&#39;&#39;Initialize the hyperparameters of the beta prior

        Parameters
        ----------
        value_option : str
            - Option to initialize the value by
            - Options
                - &#39;manual&#39;
                    - Set the values manually, `value` must be specified
                - &#39;auto&#39;
                    - Set to the mean of the prior
        hyperparam_option : str
            - If it is a string, then set it by the designated option
            - Options
                - &#39;manual&#39;
                    - Set the value manually. `a` and `b` must also be specified
                - &#39;weak-agnostic&#39;
                    - a=b=0.5
                - &#39;strong-dense&#39;
                    - a = N(N-1), N are the expected number of clusters
                    - b = 0.5
                - &#39;strong-sparse&#39;
                    - a = 0.5
                    - b = N(N-1), N are the expected number of clusters
                - &#39;mult-sparse-M&#39;
                    - a = 0.5
                    - b = N*M(N*M-1), N are the expected number of clusters
        N : str, int
            This is the number of clusters to set the hyperparam options to.
            If it is a str:
                &#39;auto&#39; : set to the expected number of clusters of a dirichlet process.
                &#39;fixed-clustering&#39; : set to the current number of clusters
        a, b : int, float
            - User specified values
            - Only necessary if `hyperparam_option` == &#39;manual&#39;
        &#39;&#39;&#39;
        if not pl.isint(delay):
            raise TypeError(&#39;`delay` ({}) must be an int&#39;.format(type(delay)))
        if delay &lt; 0:
            raise ValueError(&#39;`delay` ({}) must be &gt;= 0&#39;.format(delay))
        self.delay = delay

        if hyperparam_option == &#39;manual&#39;:
            if pl.isnumeric(a) and pl.isnumeric(b):
                self.prior.a.override_value(a)
                self.prior.b.override_value(b)
            else:
                raise ValueError(&#39;a ({}) and b ({}) must be numerics (float, int)&#39;.format(
                    a.__class__, b.__class__))
        elif hyperparam_option in [&#39;weak-agnostic&#39;]:
            self.prior.a.override_value(0.5)
            self.prior.b.override_value(0.5)

        # Set N
        if pl.isstr(N):
            if N == &#39;auto&#39;:
                N = expected_n_clusters(G=self.G)
            elif N == &#39;fixed-clustering&#39;:
                N = len(self.G[STRNAMES.CLUSTERING_OBJ])
            else:
                raise ValueError(&#39;`N` ({}) nto recognized&#39;.format(N))
        elif pl.isint(N):
            if N &lt; 0:
                raise ValueError(&#39;`N` ({}) must be positive&#39;.format(N))
        else:
            raise TypeError(&#39;`N` ({}) type not recognized&#39;.format(type(N)))

        if hyperparam_option == &#39;strong-dense&#39;:
            self.prior.a.override_value(N * (N - 1))
            self.prior.b.override_value(0.5)
        elif hyperparam_option == &#39;strong-sparse&#39;:
            self.prior.a.override_value(0.5)
            self.prior.b.override_value((N * (N - 1)))
        elif &#39;mult-sparse&#39; in hyperparam_option:
            try:
                M = float(hyperparam_option.replace(&#39;mult-sparse-&#39;, &#39;&#39;))
            except:
                raise ValueError(&#39;`mult-sparse` in the wrong format ({})&#39;.format(
                    hyperparam_option))
            N = N * M
            self.prior.a.override_value(0.5)
            self.prior.b.override_value((N * (N - 1)))

        if value_option == &#39;manual&#39;:
            if pl.isnumeric(value):
                self.value = value
            else:
                raise ValueError(&#39;`value` ({}) must be a numeric (float,int)&#39;.format(
                    value.__class__))
        elif value_option == &#39;auto&#39;:
            self.value = self.prior.mean()
        else:
            raise ValueError(&#39;value option &#34;{}&#34; not recognized for indicator prob&#39;.format(
                value_option))

        self.a.value = self.prior.a.value
        self.b.value = self.prior.b.value
        logging.info(&#39;Indicator Probability initialization results:\n&#39; \
            &#39;\tprior a: {}\n\tprior b: {}\n\tvalue: {}&#39;.format(
                self.prior.a.value, self.prior.b.value, self.value))

    def update(self):
        &#39;&#39;&#39;Sample the posterior given the data
        &#39;&#39;&#39;
        if self.sample_iter &lt; self.delay:
            return
        self.a.value = self.prior.a.value + \
            self.G[STRNAMES.CLUSTER_INTERACTION_INDICATOR].num_pos_indicators
        self.b.value = self.prior.b.value + \
            self.G[STRNAMES.CLUSTER_INTERACTION_INDICATOR].num_neg_indicators
        self.sample()
        return self.value

    def visualize(self, path: str, f: IO, section: str=&#39;posterior&#39;) -&gt; IO:
        return _scalar_visualize(self, path=path, f=f, section=section)


class ClusterInteractionValue(pl.variables.MVN):
    &#39;&#39;&#39;Interactions of Lotka-Voltera

    Since we initialize the interactions object in the `initialize` function,
    make sure that you have initialized the prior of the values of the interactions
    and of the indicators of the interactions before you call the initialization of
    this class

    Parameters
    ----------
    prior : mdsine2.variables.Normal
        Prior distribution
    clustering : mdsine2.Clustering
        Clustering object
    &#39;&#39;&#39;
    def __init__(self, prior: variables.Normal, clustering: pl.Clustering, **kwargs):
        kwargs[&#39;name&#39;] = STRNAMES.CLUSTER_INTERACTION_VALUE
        pl.variables.MVN.__init__(self, dtype=float, **kwargs)
        self.set_value_shape(shape=(len(self.G.data.taxa),len(self.G.data.taxa))) # Set the shape of the trace
        self.add_prior(prior)
        self.clustering = clustering # Clustering object
        self.obj = pl.contrib.Interactions( # Initialize the mdsine2.pylab.contrib.Interactions object
            clustering=self.clustering,
            use_indicators=True,
            name=STRNAMES.INTERACTIONS_OBJ, G=self.G,
            signal_when_clusters_change=False)
        self._strr = &#39;None&#39;

    def __str__(self) -&gt; str:
        return self._strr

    def __len__(self) -&gt; int:
        # Return the number of on interactions
        return self.obj.num_pos_indicators()

    def set_values(self, *args, **kwargs):
        &#39;&#39;&#39;Set the values from an array
        &#39;&#39;&#39;
        self.obj.set_values(*args, **kwargs)

    def initialize(self, value_option: str, hyperparam_option: str=None, 
        value: np.ndarray=None, indicators: np.ndarray=None, delay: int=0):
        &#39;&#39;&#39;Initialize the interactions object.

        Parameters
        ----------
        value_option : str
            - This is how to initialize the values
            - Options:
                - &#39;manual&#39;
                    - Set the values of the interactions manually. `value` and `indicators`
                      must also be specified. We assume the values are only set for when
                      `indicators` is True, and that the order of the `indicators` and `values`
                      correspond to how we iterate over the interactions
                    - Example
                        * 3 Clusters
                        * indicators = [True, False, False, True, False, True]
                        * value = [0.2, 0.8, -0.35]
                - &#39;all-off&#39;, &#39;auto&#39;
                    - Set all of the interactions and the indicators to 0
                - &#39;all-on&#39;
                    - Set all of the indicators to on and all the values to 0
        delay : int
            How many MCMC iterations to delay starting to update
        See also
        --------
        `mdsine2.pylab.cluster.Interactions.set_values`
        &#39;&#39;&#39;
        self.obj.set_signal_when_clusters_change(True)
        self.G[STRNAMES.INTERACTIONS_OBJ].value_initializer = self.prior.sample
        self.G[STRNAMES.INTERACTIONS_OBJ].indicator_initializer = self.G[STRNAMES.CLUSTER_INTERACTION_INDICATOR_PROB].prior.sample

        self._there_are_perturbations = self.G.perturbations is not None

        if not pl.isint(delay):
            raise TypeError(&#39;`delay` ({}) must be an int&#39;.format(type(delay)))
        if delay &lt; 0:
            raise ValueError(&#39;`delay` ({}) must be &gt;= 0&#39;.format(delay))
        self.delay = delay

        if not pl.isstr(value_option):
            raise TypeError(&#39;`value_option` ({}) must be a str&#39;.format(type(value_option)))
        if value_option in [&#39;auto&#39;, &#39;all-off&#39;]:
            for interaction in self.obj:
                interaction.value = 0
                interaction.indicator = False
        elif value_option == &#39;all-on&#39;:
            for interaction in self.obj:
                interaction.value = 0
                interaction.indicator = True
        elif value_option == &#39;manual&#39;:
            if not np.all(pl.itercheck([value, indicators], pl.isarray)):
                raise TypeError(&#39;`value` ({}) and `indicators` ({}) must be arrays&#39;.format(
                    type(value), type(indicators)))
            if len(value) != np.sum(indicators):
                raise ValueError(&#39;Length of `value` ({}) must equal the number of positive &#39; \
                    &#39;values in `indicators` ({})&#39;.format(len(value), np.sum(indicators)))
            if len(indicators) != self.obj.size:
                raise ValueError(&#39;The length of `indicators` ({}) must be the same as the &#39; \
                    &#39;number of possible interactions ({})&#39;.format(len(indicators), self.obj.size))
            ii = 0
            for i,interaction in enumerate(self.obj):
                interaction.indicator = indicators[i]
                if interaction.indicator:
                    interaction.value = value[ii]
                    ii += 1
        else:
            raise ValueError(&#39;`value_option` ({}) not recognized&#39;.format(value_option))

        self._strr = str(self.obj.get_values(use_indicators=True))
        self.value = self.obj.get_values(use_indicators=True)

    def update(self):
        &#39;&#39;&#39;Update the values (where the indicators are positive) using a multivariate normal
        distribution - call this from regress coeff if you want to update the interactions
        conditional on all the other parameters.
        &#39;&#39;&#39;
        if self.obj.sample_iter &lt; self.delay:
            return
        if self.obj.num_pos_indicators() == 0:
            # logging.info(&#39;No positive indicators, skipping&#39;)
            self._strr = &#39;[]&#39;
            return

        rhs = [
            STRNAMES.CLUSTER_INTERACTION_VALUE]
        lhs = [
            STRNAMES.GROWTH_VALUE,
            STRNAMES.SELF_INTERACTION_VALUE]
        X = self.G.data.construct_rhs(keys=rhs)
        y = self.G.data.construct_lhs(keys=lhs,
            kwargs_dict={STRNAMES.GROWTH_VALUE:{
                &#39;with_perturbations&#39;:self._there_are_perturbations}})
        process_prec = self.G[STRNAMES.PROCESSVAR].build_matrix(
            cov=False, sparse=True)
        prior_prec = build_prior_covariance(G=self.G, cov=False,
            order=rhs, sparse=True)

        pm = prior_prec @ (self.prior.mean.value * np.ones(prior_prec.shape[0]).reshape(-1,1))

        prec = X.T @ process_prec @ X + prior_prec
        cov = pinv(prec, self)
        mean = (cov @ (X.T @ process_prec.dot(y) + pm)).ravel()

        # print(np.hstack((y, self.G.data.lhs.vector.reshape(-1,1))))
        # for perturbation in self.G.perturbations:
        #     print()
        #     print(perturbation.magnitude.cluster_array())
        #     print(perturbation.indicator.cluster_array())

        self.mean.value = mean
        self.cov.value = cov
        value = self.sample()
        self.obj.set_values(arr=value, use_indicators=True)
        self.update_str()

        if np.any(np.isnan(self.value)):
            logging.critical(&#39;mean: {}&#39;.format(self.mean.value))
            logging.critical(&#39;nan in cov: {}&#39;.format(np.any(np.isnan(self.cov.value))))
            logging.critical(&#39;value: {}&#39;.format(self.value))
            raise ValueError(&#39;`Values in {} are nan: {}&#39;.format(self.name, self.value))

    def update_str(self):
        self._strr = str(self.obj.get_values(use_indicators=True))

    def set_trace(self):
        self.obj.set_trace()

    def add_trace(self):
        self.obj.add_trace()

    def visualize(self, basepath: str, section: str=&#39;posterior&#39;, taxa_formatter: str=&#39;%(paperformat)s&#39;, 
        yticklabels: str=&#39;%(paperformat)s %(index)s&#39;, xticklabels: str=&#39;%(index)s&#39;,
        fixed_clustering: bool=False):
        &#39;&#39;&#39;Render the interaction matrices in the folder `basepath`

        Parameters
        ----------
        basepath : str
            This is the loction to write the files to
        section : str
            Section of the trace to compute on. Options:
                &#39;posterior&#39; : posterior samples
                &#39;burnin&#39; : burn-in samples
                &#39;entire&#39; : both burn-in and posterior samples
        taxa_formatter : str, None
            This is the format of the label to return for each Taxa. If None, it will return
            the taxon&#39;s name
        yticklabels, xticklabels : str
            These are the formats to plot the y-axis and x0axis, respectively.
        fixed_clustering : bool
            If True, plot the variable as if clustering was fixed. Since the
            cluster assignments never change, all of the taxa within the cluster
            have identical values, so we can choose any taxon within the cluster to
            represent it
        &#39;&#39;&#39;
        if not self.G.inference.tracer.is_being_traced(self.obj.name):
            logging.info(&#39;Interactions are not being learned&#39;)
        
        # Get cluster ordering
        taxa = self.G.data.taxa
        summ = pl.summary(self.obj, set_nan_to_0=True, section=section)
        clustering = self.G[STRNAMES.CLUSTERING_OBJ]

        if fixed_clustering:
            # Condense each matrix to cluster level. Make the labels clusters
            order = None
            labels = [&#39;Cluster {}&#39;.format(iii+1) for iii in range(len(clustering))]
            yticklabels = [&#39;{cname} {idx}&#39;.format(cname=labels[idx], idx=idx) for idx in range(len(clustering))]
            xticklabels = [&#39;{}&#39;.format(i) for i in range(len(clustering))]
            taxa = None

            for k in summ:
                M_taxa = summ[k]
                M_clus = []
                for cluster1 in clustering:
                    temp = []
                    aidx = list(cluster1.members)[0]
                    for cluster2 in clustering:
                        if cluster1.id == cluster2.id:
                            temp.append(np.nan)
                            continue
                        bidx = list(cluster2.members)[0]
                        temp.append(M_taxa[aidx, bidx])
                    M_clus.append(temp)
                summ[k] = np.asarray(M_clus)
        else:
            order = _make_cluster_order(graph=self.G, section=section)
            labels = [taxa[aidx].name for aidx in order]
            taxa = self.G.data.taxa

        for k,M in summ.items():

            visualization.render_interaction_strength(interaction_matrix=M,
                log_scale=True, taxa=taxa, yticklabels=yticklabels,
                xticklabels=xticklabels, order=order, 
                title=&#39;{} Interactions&#39;.format(k.capitalize()))
            fig = plt.gcf()
            fig.tight_layout()
            plt.savefig(os.path.join(basepath, &#39;{}_matrix.pdf&#39;.format(k)))
            plt.close()

            if not fixed_clustering:
                M = M[order, :]
                M = M[:, order]
            
            df = pd.DataFrame(M, index=labels, columns=labels)
            df.to_csv(os.path.join(basepath, &#39;{}_matrix.tsv&#39;.format(k)),
                sep=&#39;\t&#39;, index=True, header=True)


class ClusterInteractionIndicators(pl.variables.Variable):
    &#39;&#39;&#39;This is the posterior of the Indicator variables on the interactions
    between clusters. These clusters are not fixed.
    If `value` is not `None`, then we set that to be the initial indicators
    of the cluster interactions

    Parameters
    ----------
    prior : pl.variables.Beta
        This is the prior of the variable
    mp : str, None
        If `None`, then there is no multiprocessing.
        If it is a str, then there are two options:
            &#39;debug&#39;: pool is done sequentially and not sent to processors
            &#39;full&#39;: pool is done at different processors
    relative : bool
        Whether you update using the relative marginal likelihood or not.
    &#39;&#39;&#39;
    def __init__(self, prior: variables.Beta, mp: str=None, relative: bool=True, **kwargs):
        if not pl.isbool(relative):
            raise TypeError(&#39;`relative` ({}) must be a bool&#39;.format(type(relative)))
        if relative:
            if mp is not None:
                raise ValueError(&#39;Multiprocessing is slower for rel. Turn mp off&#39;)
            self.update = self.update_relative
        else:
            self.update = self.update_slow

        if mp is not None:
            if not pl.isstr(mp):
                raise TypeError(&#39;`mp` ({}) must be a str&#39;.format(type(mp)))
            if mp not in [&#39;full&#39;, &#39;debug&#39;]:
                raise ValueError(&#39;`mp` ({}) not recognized&#39;.format(mp))

        kwargs[&#39;name&#39;] = STRNAMES.CLUSTER_INTERACTION_INDICATOR
        pl.variables.Variable.__init__(self, dtype=bool, **kwargs)
        self.n_taxa = len(self.G.data.taxa)
        self.set_value_shape(shape=(self.n_taxa, self.n_taxa))
        self.add_prior(prior)
        self.clustering = self.G[STRNAMES.CLUSTERING_OBJ]
        self.mp = mp
        self.relative = relative

        # parameters used during update
        self.X = None
        self.y = None
        self.process_prec_matrix = None
        self._strr = &#39;None&#39;

    def initialize(self, delay: int=0, run_every_n_iterations: int=1):
        &#39;&#39;&#39;Do nothing, the indicators are set in `ClusterInteractionValue`.

        Parameters
        ----------
        delay : int
            How many iterations to delay starting to update the values
        run_every_n_iterations : int
            Which iteration to run on
        &#39;&#39;&#39;
        if not pl.isint(delay):
            raise TypeError(&#39;`delay` ({}) must be an int&#39;.format(type(delay)))
        if delay &lt; 0:
            raise ValueError(&#39;`delay` ({}) must be &gt;= 0&#39;.format(delay))
        if not pl.isint(run_every_n_iterations):
            raise TypeError(&#39;`run_every_n_iterations` ({}) must be an int&#39;.format(
                type(run_every_n_iterations)))
        if run_every_n_iterations &lt;= 0:
            raise ValueError(&#39;`run_every_n_iterations` ({}) must be &gt; 0&#39;.format(
                run_every_n_iterations))

        self.delay = delay
        self.run_every_n_iterations = run_every_n_iterations
        self._there_are_perturbations = self.G.perturbations is not None
        self.update_cnt_indicators()
        self.interactions = self.G[STRNAMES.INTERACTIONS_OBJ]
        self.n_taxa = len(self.G.data.taxa)

        # These are for the function `self._make_idx_for_clusters`
        self.ndts_bias = []
        self.n_replicates = self.G.data.n_replicates
        self.n_dts_for_replicate = self.G.data.n_dts_for_replicate
        self.total_dts = np.sum(self.n_dts_for_replicate)
        self.replicate_bias = np.zeros(self.n_replicates, dtype=int)
        for ridx in range(1, self.n_replicates):
            self.replicate_bias[ridx] = self.replicate_bias[ridx-1] + \
                self.n_taxa * self.n_dts_for_replicate[ridx - 1]
        for ridx in range(self.G.data.n_replicates):
            self.ndts_bias.append(
                np.arange(0, self.G.data.n_dts_for_replicate[ridx] * self.n_taxa, self.n_taxa))

        # Makes a dictionary that maps the taxon index to the rows that it the  in
        self.oidx2rows = {}
        for oidx in range(self.n_taxa):
            idxs = np.zeros(self.total_dts, dtype=int)
            i = 0
            for ridx in range(self.n_replicates):
                temp = np.arange(0, self.n_dts_for_replicate[ridx] * self.n_taxa, self.n_taxa)
                temp = temp + oidx
                temp = temp + self.replicate_bias[ridx]
                l = len(temp)
                idxs[i:i+l] = temp
                i += l
            self.oidx2rows[oidx] = idxs

    def add_trace(self):
        self.value = self.G[STRNAMES.INTERACTIONS_OBJ].get_datalevel_indicator_matrix()
        pl.variables.Variable.add_trace(self)

    def update_cnt_indicators(self):
        self.num_pos_indicators = self.G[STRNAMES.INTERACTIONS_OBJ].num_pos_indicators()
        self.num_neg_indicators = self.G[STRNAMES.INTERACTIONS_OBJ].num_neg_indicators()

    def __str__(self) -&gt; str:
        return self._strr

    # @profile
    def update_slow(self):
        &#39;&#39;&#39;Permute the order that the indices that are updated.

        Build the full master interaction matrix that we can then slice
        &#39;&#39;&#39;
        start = time.time()
        if self.sample_iter &lt; self.delay:
            # for interaction in self.interactions:
            #     interaction.indicator=False
            self._strr = &#39;{}\ntotal time: {}&#39;.format(
                self.interactions.get_indicators(), time.time()-start)
            return
        if self.sample_iter % self.run_every_n_iterations != 0:
            return

        idxs = npr.permutation(self.interactions.size)
        for idx in idxs:
            self.update_single_idx_slow(idx=idx)

        self.update_cnt_indicators()
        # Since slicing is literally so slow, it is faster to build than just slicing M
        self.G.data.design_matrices[STRNAMES.CLUSTER_INTERACTION_VALUE].M.build(
            build=True, build_for_neg_ind=False)
        iii = self.interactions.get_indicators()
        n_on = np.sum(iii)
        self._strr = &#39;{}\ntotal time: {}, n_interactions: {}/{}, {:.2f}&#39;.format(
            iii, time.time()-start, n_on, len(iii), n_on/len(iii))

    def update_single_idx_slow(self, idx: int):
        &#39;&#39;&#39;Update the likelihood for interaction `idx`

        Parameters
        ----------
        idx : int
            This is the index of the interaction we are updating
        &#39;&#39;&#39;
        prior_ll_on = np.log(self.G[STRNAMES.CLUSTER_INTERACTION_INDICATOR_PROB].value)
        prior_ll_off = np.log(1 - self.G[STRNAMES.CLUSTER_INTERACTION_INDICATOR_PROB].value)

        d_on = self.calculate_marginal_loglikelihood(idx=idx, val=True)
        d_off = self.calculate_marginal_loglikelihood(idx=idx, val=False)

        ll_on = d_on[&#39;ret&#39;] + prior_ll_on
        ll_off = d_off[&#39;ret&#39;] + prior_ll_off
        dd = [ll_off, ll_on]

        res = bool(sample_categorical_log(dd))
        self.interactions.iloc(idx).indicator = res
        self.update_cnt_indicators()

    # @profile
    def _make_idx_vector_for_clusters(self) -&gt; Dict[int, np.ndarray]:
        &#39;&#39;&#39;Creates a dictionary that maps the cluster id to the
        rows that correspond to each Taxa in the cluster.

        We cannot cast this with numba because it does not support Fortran style
        raveling :(.

        Returns
        -------
        dict: int -&gt; np.ndarray
            Maps the cluster ID to the row indices corresponding to it
        &#39;&#39;&#39;
        clusters = [np.asarray(oidxs, dtype=int).reshape(-1,1) \
            for oidxs in self.clustering.tolistoflists()]

        d = {}
        cids = self.clustering.order

        for cidx,cid in enumerate(cids):
            a = np.zeros(len(clusters[cidx]) * self.total_dts, dtype=int)
            i = 0
            for ridx in range(self.n_replicates):
                idxs = np.zeros(
                    (len(clusters[cidx]),
                    self.n_dts_for_replicate[ridx]), int)
                idxs = idxs + clusters[cidx]
                idxs = idxs + self.ndts_bias[ridx]
                idxs = idxs + self.replicate_bias[ridx]
                idxs = idxs.ravel(&#39;F&#39;)
                l = len(idxs)
                a[i:i+l] = idxs
                i += l

            d[cid] = a
        
        if self.G.data.zero_inflation_transition_policy is not None:
            # We need to convert the indices that are meant from no zero inflation to 
            # ones that take into account zero inflation - use the array from 
            # `data.Data._setrows_to_include_zero_inflation`. If the index should be
            # included, then we subtract the number of indexes that are previously off
            # before that index. If it should not be included then we exclude it
            prevoff_arr = self.G.data.off_previously_arr_zero_inflation
            rows_to_include = self.G.data.rows_to_include_zero_inflation
            for cid in d:
                arr = d[cid]
                new_arr = np.zeros(len(arr), dtype=int)
                n = 0
                for idx in arr:
                    if rows_to_include[idx]:
                        new_arr[n] = idx - prevoff_arr[idx]
                        n += 1

                new_arr = new_arr[:n]
                d[cid] = new_arr
        return d

    # @profile
    def make_rel_params(self):
        &#39;&#39;&#39;We make the parameters needed to update the relative log-likelihod.
        This function is called once at the beginning of the update.

        Parameters that we create with this function
        --------------------------------------------
        - ys : dict (int -&gt; np.ndarray)
            - Maps the target cluster id to the observation matrix that it
              corresponds to (only the Taxa in the target cluster). This 
              array already has the growth and self-interactions subtracted
              out:
                * $ \frac{log(x_{k+1}) - log(x_{k})}{dt} - a_{1,k} - a_{2,k}x_{k} $
        - process_precs : dict (int -&gt; np.ndarray)
            - Maps the target cluster id to the vector of the process precision
              that corresponds to the target cluster (only the Taxa in the target
              cluster). This is a 1D array that corresponds to the diagonal of what
              would be the precision matrix.
        - interactionXs : dict (int -&gt; np.ndarray)
            - Maps the target cluster id to the matrix of the design matrix of the
              interactions. Only includes the rows that correspond to the Taxa in the
              target cluster. It includes every single column as if all of the indicators
              are on. We only index out the columns when we are doing the marginalization.
        - prior_prec_interaction : float
            - Prior precision of the interaction value. We then use this
              value to make the diagonal of the prior precision.
        - prior_var_interaction : float
            - Prior variance of the interaction value.
        - prior_mean_interaction : float
            - Prior mean of the interaction values. We use this value
              to make the prior mean vector during the marginalization.
        - n_on_master : int
            - How many interactions are on at any one time. We adjust this
              throughout the update depending on what interactions we turn off and
              on.
        - prior_ll_on : float
            - Prior log likelihood of a positive interaction
        - prior_ll_off : float
            - Prior log likelihood of the negative interaction
        - priorvar_logdet_diff : float
            - This is the prior variance log determinant that we add when the indicator
              is positive.

        Parameters created if there are perturbations
        ---------------------------------------------
        - perturbationsXs : dict (int -&gt; np.ndarray)
            - Maps the target cluster id to the design matrix that corresponds to 
              the on perturbations of the target clusters. This is preindexed in
              both rows and columns
        - prior_prec_perturbations : dict (int -&gt; np.ndarray)
            - Maps the target cluster id to the diagonal of the prior precision
              of the perturbations
        - prior_var_perturbations : dict (int -&gt; np.ndarray)
            - Maps the target cluster id to the diagonal of the prior variance 
              of the perturbations
        - prior_mean_perturbations : dict (int -&gt; np.ndarray)
            - Maps the target cluster id to the vector of the prior mean of the
              perturbations
        &#39;&#39;&#39;
        # Get the row indices for each cluster
        row_idxs = self._make_idx_vector_for_clusters()

        # Create ys
        self.ys = {}
        y = self.G.data.construct_lhs(keys=[
            STRNAMES.SELF_INTERACTION_VALUE, STRNAMES.GROWTH_VALUE],
            kwargs_dict={STRNAMES.GROWTH_VALUE:{&#39;with_perturbations&#39;: False}})
        for tcid in self.clustering.order:
            self.ys[tcid] = y[row_idxs[tcid], :]

        # Create process_precs
        self.process_precs = {}
        process_prec_diag = self.G[STRNAMES.PROCESSVAR].prec
        for tcid in self.clustering.order:
            self.process_precs[tcid] = process_prec_diag[row_idxs[tcid]]

        # Make interactionXs
        self.interactionXs = {}
        self.G.data.design_matrices[STRNAMES.CLUSTER_INTERACTION_VALUE].M.build(
            build=True, build_for_neg_ind=True)
        XM_master = self.G.data.design_matrices[STRNAMES.CLUSTER_INTERACTION_VALUE].toarray()
        for tcid in self.clustering.order:
            self.interactionXs[tcid] = XM_master[row_idxs[tcid], :]

        # Make prior parameters
        self.prior_var_interaction = self.G[STRNAMES.PRIOR_VAR_INTERACTIONS].value
        self.prior_prec_interaction = 1/self.prior_var_interaction
        self.prior_mean_interaction = self.G[STRNAMES.PRIOR_MEAN_INTERACTIONS].value
        self.prior_ll_on = np.log(self.prior.value)
        self.prior_ll_off = np.log(1 - self.prior.value)
        self.n_on_master = self.interactions.num_pos_indicators()

        # Make priorvar_logdet
        self.priorvar_logdet = np.log(self.prior_var_interaction)

        if self._there_are_perturbations:
            XMpert_master = self.G.data.design_matrices[STRNAMES.PERT_VALUE].toarray()

            # Make perturbationsXs
            self.perturbationsXs = {}
            for tcid in self.clustering.order:
                rows = row_idxs[tcid]
                cols = []
                i = 0
                for perturbation in self.G.perturbations:
                    for cid in perturbation.indicator.value:
                        if perturbation.indicator.value[cid]:
                            if cid == tcid:
                                cols.append(i)
                            i += 1
                cols = np.asarray(cols, dtype=int)

                self.perturbationsXs[tcid] = pl.util.fast_index(M=XMpert_master,
                    rows=rows, cols=cols)

            # Make prior perturbation parameters
            self.prior_mean_perturbations = {}
            self.prior_var_perturbations = {}
            self.prior_prec_perturbations = {}
            for tcid in self.clustering.order:
                mean = []
                var = []
                for perturbation in self.G.perturbations:
                    if perturbation.indicator.value[tcid]:
                        # This is on, get the parameters
                        mean.append(perturbation.magnitude.prior.loc.value)
                        var.append(perturbation.magnitude.prior.scale2.value)
                self.prior_mean_perturbations[tcid] = np.asarray(mean)
                self.prior_var_perturbations[tcid] = np.asarray(var)
                self.prior_prec_perturbations[tcid] = 1/self.prior_var_perturbations[tcid]

            # Make priorvar_det_perturbations
            self.priorvar_det_perturbations = 0
            for perturbation in self.G.perturbations:
                self.priorvar_det_perturbations += \
                    perturbation.indicator.num_on_clusters() * \
                    perturbation.magnitude.prior.scale2.value

    # @profile
    def update_relative(self):
        &#39;&#39;&#39;Update the indicators variables by calculating the relative loglikelihoods
        of it being on as supposed to off. Because this is a relative loglikelihood,
        we only need to take into account the following parameters of the model:
            - Only the Taxa in the target cluster of the interaction
            - Only the positively indicated interactions going into the
              target cluster.

        This is 1000&#39;s of times faster than `update` because we are operating on matrices
        that are MUCH smaller than in a full system. These matrices are also considered dense
        so we do all of our computations without sparse matrices.

        We permute the order that the indices are updated for more robust mixing.
        &#39;&#39;&#39;
        start = time.time()
        if self.sample_iter &lt; self.delay:
            self._strr = &#39;{}\ntotal time: {}&#39;.format(
                self.interactions.get_indicators(), time.time()-start)
            return
        if self.sample_iter % self.run_every_n_iterations != 0:
            return

        idxs = npr.permutation(self.interactions.size)

        self.make_rel_params()
        for idx in idxs:
            self.update_single_idx_fast(idx=idx)

        self.update_cnt_indicators()
        # Since slicing is literally so slow, it is faster to build than just slicing M
        self.G.data.design_matrices[STRNAMES.CLUSTER_INTERACTION_VALUE].M.build(
            build=True, build_for_neg_ind=False)
        iii = self.interactions.get_indicators()
        n_on = np.sum(iii)
        self._strr = &#39;{}\ntotal time: {}, n_interactions: {}/{}, {:.2f}&#39;.format(
            iii, time.time()-start, n_on, len(iii), n_on/len(iii))

    def calculate_marginal_loglikelihood(self, idx: int, val: bool) -&gt; Dict[str, Union[float, int]]:
        &#39;&#39;&#39;Calculate the likelihood of interaction `idx` with the value `val`
        &#39;&#39;&#39;
        # Build and initialize
        self.interactions.iloc(idx).indicator = val
        self.update_cnt_indicators()
        self.G.data.design_matrices[STRNAMES.CLUSTER_INTERACTION_VALUE].M.build()

        lhs = [STRNAMES.GROWTH_VALUE, STRNAMES.SELF_INTERACTION_VALUE]
        if self._there_are_perturbations:
            rhs = [STRNAMES.PERT_VALUE, STRNAMES.CLUSTER_INTERACTION_VALUE]
        else:
            rhs = [STRNAMES.CLUSTER_INTERACTION_VALUE]

        y = self.G.data.construct_lhs(lhs, 
            kwargs_dict={STRNAMES.GROWTH_VALUE:{&#39;with_perturbations&#39;: False}})
        X = self.G.data.construct_rhs(rhs, toarray=True)

        if X.shape[1] == 0:
            return {
            &#39;ret&#39;: 0,
            &#39;beta_logdet&#39;: 0,
            &#39;priorvar_logdet&#39;: 0,
            &#39;bEb&#39;: 0,
            &#39;bEbprior&#39;: 0}

        process_prec = self.G[STRNAMES.PROCESSVAR].build_matrix(cov=False, sparse=False)
        prior_prec = build_prior_covariance(G=self.G, cov=False, order=rhs, sparse=False)
        prior_var = build_prior_covariance(G=self.G, cov=True, order=rhs, sparse=False)
        prior_mean = build_prior_mean(G=self.G, order=rhs, shape=(-1,1))


        # Calculate the posterior
        beta_prec = X.T @ process_prec @ X + prior_prec
        beta_cov = pinv(beta_prec, self)
        beta_mean = beta_cov @ ( X.T @ process_prec @ y + prior_prec @ prior_mean )
        beta_mean = np.asarray(beta_mean).reshape(-1,1)

        # Perform the marginalization
        try:
            beta_logdet = log_det(beta_cov, self)
        except:
            logging.critical(&#39;Crashed in log_det&#39;)
            logging.critical(&#39;beta_cov:\n{}&#39;.format(beta_cov))
            logging.critical(&#39;prior_prec\n{}&#39;.format(prior_prec))
            raise
        priorvar_logdet = log_det(prior_var, self)
        ll2 = 0.5 * (beta_logdet - priorvar_logdet)

        a = np.asarray(prior_mean.T @ prior_prec @ prior_mean)[0,0]
        b = np.asarray(beta_mean.T @ beta_prec @ beta_mean)[0,0]
        ll3 = -0.5 * (a  - b)

        return {&#39;ret&#39;: ll2+ll3, &#39;beta_logdet&#39;: beta_logdet, &#39;priorvar_logdet&#39;: priorvar_logdet,
            &#39;bEb&#39;: b, &#39;bEbprior&#39;: a}

    def update_single_idx_fast(self, idx: int):
        &#39;&#39;&#39;Calculate the relative log likelihood of changing the indicator of the
        interaction at index `idx`.

        This is about 20X faster than `update_single_idx_slow`.

        Parameters
        ----------
        idx : int
            This is the index of the interaction index we are sampling
        &#39;&#39;&#39;
        # Get the current interaction by the index
        self.curr_interaction = self.interactions.iloc(idx)
        start_sign = self.curr_interaction.indicator
        
        tcid = self.curr_interaction.target_cid
        self.curr_interaction.indicator = False
        self.col_idxs = np.asarray(
            self.interactions.get_arg_indicators(target_cid=tcid),
            dtype=int)

        if not start_sign:
            self.n_on_master += 1
            self.num_pos_indicators += 1
            self.num_neg_indicators -= 1

        d_on = self.calculate_relative_marginal_loglikelihood(idx=idx, val=True)

        self.n_on_master -= 1
        self.num_pos_indicators -= 1
        self.num_neg_indicators += 1
        d_off = self.calculate_relative_marginal_loglikelihood(idx=idx, val=False)

        ll_on = d_on + self.prior_ll_on
        ll_off = d_off + self.prior_ll_off

        dd = [ll_off, ll_on]
        res = bool(sample_categorical_log(dd))
        if res:
            self.n_on_master += 1
            self.num_pos_indicators += 1
            self.num_neg_indicators -= 1
            self.curr_interaction.indicator = True

    def calculate_relative_marginal_loglikelihood(self, idx: int, val: bool) -&gt; float:
        &#39;&#39;&#39;Calculate the relative marginal log likelihood for the interaction index
        `idx` with the indicator `val`

        Parameters
        ----------
        idx : int
            This is the index of the interaction
        val : bool
            This is the value to calculate it as
        &#39;&#39;&#39;
        tcid = self.curr_interaction.target_cid

        y = self.ys[tcid]
        process_prec = self.process_precs[tcid]

        # Make X, prior mean, and prior_var
        if val:
            cols = np.append(self.col_idxs, idx)
        else:
            cols = self.col_idxs
        
        X = self.interactionXs[tcid][:, cols]
        prior_mean = np.full(len(cols), self.prior_mean_interaction)
        prior_prec_diag = np.full(len(cols), self.prior_prec_interaction)

        if self._there_are_perturbations:
            Xpert = self.perturbationsXs[tcid]
            X = np.hstack((X, Xpert))

            prior_mean = np.append(
                prior_mean,
                self.prior_mean_perturbations[tcid])
            
            prior_prec_diag = np.append(
                prior_prec_diag,
                self.prior_prec_perturbations[tcid])

        if X.shape[1] == 0:
            return 0

        prior_prec = np.diag(prior_prec_diag)
        pm = (prior_prec_diag * prior_mean).reshape(-1,1)

        # Do the marginalization
        a = X.T * process_prec

        beta_prec = (a @ X) + prior_prec
        beta_cov = pinv(beta_prec, self)
        beta_mean = beta_cov @ ((a @ y) + pm )

        bEb = (beta_mean.T @ beta_prec @ beta_mean)[0,0]
        try:
            beta_logdet = log_det(beta_cov, self)
        except:
            logging.critical(&#39;Crashed in log_det&#39;)
            logging.critical(&#39;beta_cov:\n{}&#39;.format(beta_cov))
            logging.critical(&#39;prior_prec\n{}&#39;.format(prior_prec))
            raise
        
        if val:
            bEbprior = (self.prior_mean_interaction**2)/self.prior_var_interaction
            priorvar_logdet = self.priorvar_logdet
        else:
            bEbprior = 0
            priorvar_logdet = 0

        ll2 = 0.5 * (beta_logdet - priorvar_logdet)
        ll3 = 0.5 * (bEb - bEbprior)
        return ll2 + ll3
            
    def kill(self):
        pass

    def visualize(self, basepath: str, section: str=&#39;posterior&#39;, taxa_formatter: str=&#39;%(paperformat)s&#39;, 
        yticklabels: str=&#39;%(paperformat)s %(index)s&#39;, xticklabels: str=&#39;%(index)s&#39;, vmax: Union[float, int]=10,
        fixed_clustering: bool=False):
        &#39;&#39;&#39;Render the interaction matrices in the folder `basepath`

        Parameters
        ----------
        basepath : str
            This is the loction to write the files to
        section : str
            Section of the trace to compute on. Options:
                &#39;posterior&#39; : posterior samples
                &#39;burnin&#39; : burn-in samples
                &#39;entire&#39; : both burn-in and posterior samples
        taxa_formatter : str, None
            This is the format of the label to return for each . If None, it will return
            the taxon/s name
        yticklabels, xticklabels : str
            These are the formats to plot the y-axis and x0axis, respectively.
        fixed_clustering : bool
            If True, plot the variable as if clustering was fixed. Since the
            cluster assignments never change, all of the taxa within the cluster
            have identical values, so we can choose any taxon within the cluster to
            represent it
        &#39;&#39;&#39;
        from . util import generate_interation_bayes_factors_posthoc

        if not self.G.inference.tracer.is_being_traced(self.G[STRNAMES.INTERACTIONS_OBJ]):
            logging.info(&#39;Interactions are not being learned&#39;)
        
        # Get cluster ordering
        taxa = self.G.data.taxa
        order = _make_cluster_order(graph=self.G, section=section)
        clustering = self.G[STRNAMES.CLUSTERING_OBJ]

        # Plot the bayes factors
        bfs = generate_interation_bayes_factors_posthoc(mcmc=self.G.inference, section=section)

        if fixed_clustering:
            labels = [&#39;Cluster {}&#39;.format(iii+1) for iii in range(len(clustering))]
            yticklabels = [&#39;{cname} {idx}&#39;.format(cname=labels[idx], idx=idx) for idx in range(len(clustering))]
            xticklabels = [&#39;{}&#39;.format(i) for i in range(len(clustering))]
            order = None
            taxa = None
            title = &#39;Cluster Interaction Bayes Factors&#39;

            bf_cluster = []
            for cluster1 in clustering:
                temp = []
                aidx = list(cluster1.members)[0]
                for cluster2 in clustering:
                    if cluster1.id == cluster2.id:
                        temp.append(np.nan)
                        continue
                    bidx = list(cluster2.members)[0]
                    temp.append(bfs[aidx, bidx])
                bf_cluster.append(temp)
            bfs = np.asarray(bf_cluster)
        else:
            labels = [taxa[aidx].name for aidx in order]
            taxa = self.G.data.taxa
            title = &#39;Microbe Interaction Bayes Factors&#39;

        visualization.render_bayes_factors(bfs, taxa=taxa, order=order, max_value=vmax, 
            xticklabels=xticklabels, yticklabels=yticklabels, title=title)
        fig = plt.gcf()
        fig.tight_layout()
        plt.savefig(os.path.join(basepath, &#39;bayes_factors.pdf&#39;))
        plt.close()

        if not fixed_clustering:
            bfs = bfs[order, :]
            bfs = bfs[:, order]
        df = pd.DataFrame(bfs, index=labels, columns=labels)
        df.to_csv(os.path.join(basepath, &#39;bayes_factors.tsv&#39;), sep=&#39;\t&#39;, index=True, header=True)


# Logistic Growth
# ---------------
class PriorVarMH(pl.variables.SICS):
    &#39;&#39;&#39;This is the posterior for the prior variance of either the growth
    or self-interaction parameter. We update with a MH update since this 
    prior is not conjugate.

    Parameters
    ----------
    prior : pl.variables.SICS
        This is the prior of this distribution - which is a Squared
        Inverse Chi Squared (SICS) distribution
    child_name : str
        This is the name of the variable that this is a prior variance
        for. This is either the name of the growth parameter or the 
        self-interactions parameter
    kwargs : dict
        These are the other parameters for the initialization.
    &#39;&#39;&#39;

    def __init__(self, prior: variables.SICS, child_name: str, **kwargs):
        if child_name == STRNAMES.GROWTH_VALUE:
            kwargs[&#39;name&#39;] = STRNAMES.PRIOR_VAR_GROWTH
        elif child_name == STRNAMES.SELF_INTERACTION_VALUE:
            kwargs[&#39;name&#39;] = STRNAMES.PRIOR_VAR_SELF_INTERACTIONS
        else:
            raise ValueError(&#39;`child_name` ({}) not recognized&#39;.format(child_name))
        pl.variables.SICS.__init__(self, dtype=float, **kwargs)
        self.child_name = child_name
        self.add_prior(prior)
        self.proposal = pl.variables.SICS(dof=None, scale=None, value=None)

    def __str__(self) -&gt; str:
        # If this fails, it is because we are dividing by 0 sampler_iter
        # If which case we just return the value 
        try:
            s = &#39;Value: {}, Acceptance rate: {}&#39;.format(
                self.value, np.mean(self.acceptances[
                    np.max([self.sample_iter-50, 0]):self.sample_iter]))
        except:
            s = str(self.value)
        return s

    def initialize(self, value_option: str, dof_option: str, scale_option: str, 
        proposal_option: str, target_acceptance_rate: Union[float,str], tune: Union[float, str], 
        end_tune: Union[str, int], value: float=None, dof: float=None, scale: float=None, 
        proposal_dof: float=None, delay: int=0):
        &#39;&#39;&#39;Initialize the parameters of the distribution and the 
        proposal distribution

        Parameters
        ----------
        value_option : str
            Different ways to initialize the values
            Options
                &#39;manual&#39;
                    Set the value manually, `value` must also be specified
                &#39;unregularized&#39;
                    Do unregularized regression and the value is set to the
                    variance of the growth values
                &#39;prior-mean&#39;, &#39;auto&#39;
                    Set the value to the prior of the mean
        scale_option : str
            Different ways to initialize the scale of the prior
            Options
                &#39;manual&#39;
                    Set the value manually, `scale` must also be specified
                &#39;auto&#39;, &#39;inflated-median&#39;
                    We set the scale such that the mean of the prior is
                    equal to the median growth values calculated
                    with linear regression squared and inflated by 100.
        dof_option : str
            How informative the prior should be (setting the dof)
                &#39;diffuse&#39;: set to the mimumum value (2)
                &#39;weak&#39;: set so that 10% of the posterior comes from the prior
                &#39;strong&#39;: set so that 50% of the posterior comes from the prior
                &#39;manual&#39;: set to the value provided in the parameter `shape`
                &#39;auto&#39;: Set to &#39;weak&#39;
        proposal_option : str
            How to set the initial dof of the proposal - this will get adjusted with
            tuning
                &#39;tight&#39;, &#39;auto&#39;
                    Set the dof to be 15, relatively strong initially
                &#39;diffuse&#39;
                    Set the dof to be 2.5, relatively diffuse initially
                &#39;manual&#39;
                    Set the dof with the parameter `proposal_dof&#39;
        target_acceptance_rate : float, str
            This is the target_acceptance rate. Options:
                &#39;auto&#39;, &#39;optimal&#39;
                    Set to 0.44
                float
                    This is the value you want
        tune : str, int
            This is how often you want to update the proposal dof
                int
                &#39;auto&#39;
                    Set to every 50 iterations
        end_tune : str, int
            This is when to stop the tuning
                &#39;half-burnin&#39;, &#39;auto&#39;
                    Half of burnin, rounded down
                int
        delay : int
            How many iterations to delay updating the value of the variance
        &#39;&#39;&#39;
        if not pl.isint(delay):
            raise TypeError(&#39;`delay` ({}) must be an int&#39;.format(type(delay)))
        if delay &lt; 0:
            raise ValueError(&#39;`delay` ({}) must be &gt;= 0&#39;.format(delay))
        self.delay = delay

        # Set the proposal dof
        if not pl.isstr(proposal_option):
            raise TypeError(&#39;`proposal_option` ({}) must be a str&#39;.format(
                type(proposal_option)))
        elif proposal_option == &#39;manual&#39;:
            if not pl.isnumeric(proposal_dof):
                raise TypeError(&#39;`proposal_dof` ({}) must be a numeric&#39;.format(
                    type(proposal_dof)))
            if proposal_dof &lt; 2:
                raise ValueError(&#39;`proposal_dof` ({}) not proper&#39;.format(proposal_dof))
        elif proposal_option in [&#39;tight&#39;, &#39;auto&#39;]:
            proposal_dof = 15
        elif proposal_option == &#39;diffuse&#39;:
            proposal_dof = 2.5
        else:
            raise ValueError(&#39;`proposal_option` ({}) not recognized&#39;.format(
                proposal_option))
        self.proposal.dof.value = proposal_dof

        # Set the propsal parameters
        if pl.isstr(target_acceptance_rate):
            if target_acceptance_rate in [&#39;optimal&#39;, &#39;auto&#39;]:
                target_acceptance_rate = 0.44
            else:
                raise ValueError(&#39;`target_acceptance_rate` ({}) not recognized&#39;.format(
                    target_acceptance_rate))
        elif pl.isfloat(target_acceptance_rate):
            if target_acceptance_rate &lt; 0 or target_acceptance_rate &gt; 1:
                raise ValueError(&#39;`target_acceptance_rate` ({}) out of range&#39;.format(
                    target_acceptance_rate))
        else:
            raise TypeError(&#39;`target_acceptance_rate` ({}) type not recognized&#39;.format(
                type(target_acceptance_rate)))
        self.target_acceptance_rate = target_acceptance_rate

        if pl.isstr(tune):
            if tune in [&#39;auto&#39;]:
                tune = 50
            else:
                raise ValueError(&#39;`tune` ({}) not recognized&#39;.format(tune))
        elif pl.isint(tune):
            if tune &lt; 0:
                raise ValueError(&#39;`tune` ({}) must be &gt; 0&#39;.format(
                    tune))
        else:
            raise TypeError(&#39;`tune` ({}) type not recognized&#39;.format(type(tune)))
        self.tune = tune

        if pl.isstr(end_tune):
            if end_tune in [&#39;auto&#39;, &#39;half-burnin&#39;]:
                end_tune = int(self.G.inference.burnin/2)
            else:
                raise ValueError(&#39;`tune` ({}) not recognized&#39;.format(end_tune))
        elif pl.isint(end_tune):
            if end_tune &lt; 0 or end_tune &gt; self.G.inference.burnin:
                raise ValueError(&#39;`end_tune` ({}) out of range (0, {})&#39;.format(
                    end_tune, self.G.inference.burnin))
        else:
            raise TypeError(&#39;`end_tune` ({}) type not recognized&#39;.format(type(end_tune)))
        self.end_tune = end_tune

        # Set the prior dof
        if not pl.isstr(dof_option):
            raise TypeError(&#39;`dof_option` ({}) must be a str&#39;.format(type(dof_option)))
        if dof_option == &#39;manual&#39;:
            if not pl.isnumeric(dof):
                raise TypeError(&#39;`dof` ({}) must be a numeric&#39;.format(type(dof)))
            if dof &lt; 2:
                raise ValueError(&#39;`dof` ({}) must be &gt;= 2&#39;.format(dof))
        elif dof_option == &#39;diffuse&#39;:
            dof = 2.5
        elif dof_option in [&#39;weak&#39;, &#39;auto&#39;]:
            dof = len(self.G.data.taxa)/9
        elif dof_option == &#39;strong&#39;:
            dof = len(self.G.data.taxa)/2
        else:
            raise ValueError(&#39;`dof_option` ({}) not recognized&#39;.format(dof_option))
        if dof &lt; 2:
            raise ValueError(&#39;`dof` ({}) must be strictly larger than 2 to be a proper&#39; \
                &#39; prior&#39;.format(dof))
        self.prior.dof.override_value(dof)

        # Set the prior scale
        if not pl.isstr(scale_option):
            raise TypeError(&#39;`scale_option` ({}) must be a str&#39;.format(type(scale_option)))
        if scale_option == &#39;manual&#39;:
            if not pl.isnumeric(scale):
                raise TypeError(&#39;`scale` ({}) must be a numeric&#39;.format(type(scale)))
            if scale &lt;= 0:
                raise ValueError(&#39;`scale` ({}) must be positive&#39;.format(scale))
        elif scale_option in [&#39;auto&#39;, &#39;inflated-median&#39;]:
            # Perform linear regression
            rhs = [STRNAMES.GROWTH_VALUE, STRNAMES.SELF_INTERACTION_VALUE]
            X = self.G.data.construct_rhs(keys=rhs,
                kwargs_dict={STRNAMES.GROWTH_VALUE:{&#39;with_perturbations&#39;:False}},
                index_out_perturbations=True)
            y = self.G.data.construct_lhs(index_out_perturbations=True)

            prec = X.T @ X
            cov = pinv(prec, self)
            mean = cov @ X.T @ y
            if self.child_name == STRNAMES.GROWTH_VALUE:
                mean = 1e4*(np.median(mean[:self.G.data.n_taxa]) ** 2)
            else:
                mean = 1e4*(np.median(mean[self.G.data.n_taxa:]) ** 2)

            # Calculate the scale
            scale = mean * (self.prior.dof.value - 2) / self.prior.dof.value
        else:
            raise ValueError(&#39;`scale_option` ({}) not recognized&#39;.format(scale_option))
        self.prior.scale.override_value(scale)

        # Set the initial value of the prior
        if not pl.isstr(value_option):
            raise TypeError(&#39;`value_option` ({}) must be a str&#39;.format(type(value_option)))
        if value_option == &#39;manual&#39;:
            if not pl.isnumeric(value):
                raise ValueError(&#39;If `value_option` == &#34;manual&#34;, value ({}) &#39; \
                    &#39;must be a numeric (float, int)&#39;.format(value.__class__))
        elif value_option in [&#39;inflated-median&#39;]:
            # No interactions
            rhs = [
                STRNAMES.GROWTH_VALUE,
                STRNAMES.SELF_INTERACTION_VALUE]
            X = self.G.data.construct_rhs(keys=rhs,
                kwargs_dict={STRNAMES.GROWTH_VALUE:{&#39;with_perturbations&#39;:False}},
                index_out_perturbations=True)
            y = self.G.data.construct_lhs(index_out_perturbations=True)

            prec = X.T @ X
            cov = pinv(prec, self)
            mean = cov @ X.T @ y
            if self.child_name == STRNAMES.GROWTH_VALUE:
                value = 1e4*(np.median(mean[:self.G.data.n_taxa]) ** 2)
            else:
                value = 1e4*(np.median(mean[self.G.data.n_taxa:]) ** 2)
        elif value_option in [&#39;prior-mean&#39;, &#39;auto&#39;]:
            value = self.prior.mean()
        else:
            raise ValueError(&#39;`value_option` &#34;{}&#34; not recognized&#39;.format(value_option))
        self.value = value

    def update_dof(self):
        &#39;&#39;&#39;Updat the `dof` parameter so that we adjust the acceptance
        rate to `target_acceptance_rate`
        &#39;&#39;&#39;
        if self.sample_iter == 0:
            self.temp_acceptances = 0
            self.acceptances = np.zeros(self.G.inference.n_samples, dtype=bool)
        
        elif self.sample_iter &gt; self.end_tune:
            # Don&#39;t do any more updates
            return
        
        elif self.sample_iter % self.tune == 0:
            # Update dof
            acceptance_rate = self.temp_acceptances / self.tune
            if acceptance_rate &gt; self.target_acceptance_rate:
                self.proposal.dof.value = self.proposal.dof.value * 1.5
            else:
                self.proposal.dof.value = self.proposal.dof.value / 1.5
            self.temp_acceptances = 0

    def update(self):
        &#39;&#39;&#39;First we check if we need to tune the dof, which we do during
        the first half of burnin. We calculate the likelihoods in logspace
        &#39;&#39;&#39;
        if self.sample_iter &lt; self.delay:
            return
        self.update_dof()

        # Get necessary data of the respective parameter
        var = self.G[self.child_name]
        x = var.value.ravel()
        mu = var.prior.loc.value
        low = var.low
        high = var.high

        # propose a new value
        prev_value = self.value
        prev_value_std = math.sqrt(prev_value)
        self.proposal.scale.value = self.value
        new_value = self.proposal.sample() # Sample a new value
        new_value_std = math.sqrt(new_value)

        # Calculate the target distribution ll
        prev_target_ll = 0
        for i in range(len(x)):
            prev_target_ll += pl.random.truncnormal.logpdf(
                value=x[i], loc=mu, scale=prev_value_std,
                low=low, high=high)
        new_target_ll = 0
        for i in range(len(x)):
            new_target_ll += pl.random.truncnormal.logpdf(
                value=x[i], loc=mu, scale=new_value_std,
                low=low, high=high)

        # Normalize by the ll of the proposal
        prev_prop_ll = self.proposal.logpdf(value=prev_value)
        new_prop_ll = self.proposal.logpdf(value=new_value)

        # Accept or reject
        r = (new_target_ll - prev_prop_ll) - \
            (prev_target_ll - new_prop_ll)
        u = np.log(pl.random.misc.fast_sample_standard_uniform())

        if r &gt;= u:
            self.acceptances[self.sample_iter] = True
            self.value = new_value
            self.temp_acceptances += 1
        else:
            self.value = prev_value

    def visualize(self, path: str, section: str=&#39;posterior&#39;) -&gt; pd.DataFrame:
        &#39;&#39;&#39;Render the traces in the folder `basepath` and returns a
        `pandas.DataFrame` with summary statistics

        Parameters
        ----------
        path : str
            This is the path to write the files to
        f : _io.TextIOWrapper
            File that we are writing the values to
        section : str
            Section of the trace to compute on. Options:
                &#39;posterior&#39; : posterior samples
                &#39;burnin&#39; : burn-in samples
                &#39;entire&#39; : both burn-in and posterior samples

        Returns
        -------
        pandas.DataFrame
        &#39;&#39;&#39;
        if not self.G.inference.tracer.is_being_traced(self):
            logging.info(&#39;`{}` not learned\n\tValue: {}\n&#39;.format(self.name, self.value))
            return pd.DataFrame()
        summ = pl.summary(self, section=section)
        data = [[self.name] + [v for _,v in summ.items()]]
        columns = [&#39;name&#39;] + [k for k in summ]
        index = [&#39;variance&#39;]
        df = pd.DataFrame(data, columns=columns, index=index)

        # Plot the traces
        ax1, ax2 = visualization.render_trace(var=self, plt_type=&#39;both&#39;, section=section,
            include_burnin=True, log_scale=True, rasterized=True)

        # Plot the prior over the posterior
        l,h = ax1.get_xlim()
        xs = np.arange(l,h,step=(h-l)/100) 
        ys = []
        for x in xs:
            ys.append(pl.random.sics.pdf(value=x, 
                dof=self.prior.dof.value,
                scale=self.prior.scale.value))
        ax1.plot(xs, ys, label=&#39;prior&#39;, alpha=0.5, color=&#39;red&#39;, rasterized=True)
        ax1.legend()

        # Plot the acceptance rate over the trace
        ax3 = ax2.twinx()
        ax3 = visualization.render_acceptance_rate_trace(var=self, ax=ax3, 
            label=&#39;Acceptance Rate&#39;, color=&#39;red&#39;, scatter=False, rasterized=True)
        ax3.legend()
        fig = plt.gcf()
        fig.tight_layout()
        fig.suptitle(self.name)
        plt.savefig(path)
        plt.close()

        return df


class PriorMeanMH(pl.variables.TruncatedNormal):
    &#39;&#39;&#39;This implements the posterior for the prior mean of the either
    the growths or the self-interactions

    Parameters
    ----------
    prior : variables.TruncatedNormal
        Prior distribution
    child_name : str
        Name of the child
    &#39;&#39;&#39;
    def __init__(self, prior: variables.TruncatedNormal, child_name: str, **kwargs):
        if child_name == STRNAMES.GROWTH_VALUE:
            kwargs[&#39;name&#39;] = STRNAMES.PRIOR_MEAN_GROWTH
        elif child_name == STRNAMES.SELF_INTERACTION_VALUE:
            kwargs[&#39;name&#39;] = STRNAMES.PRIOR_MEAN_SELF_INTERACTIONS
        else:
            raise ValueError(&#39;`child_name` ({}) not recognized&#39;.format(child_name))
        pl.variables.TruncatedNormal.__init__(self, loc=None, scale2=None, dtype=float, **kwargs)
        self.child_name = child_name
        self.add_prior(prior)
        self.proposal = pl.variables.TruncatedNormal(loc=None, scale2=None, value=None)

    def __str__(self) -&gt; str:
        # If this fails, it is because we are dividing by 0 sampler_iter
        # If which case we just return the value 
        try:
            s = &#39;Value: {}, Acceptance rate: {}&#39;.format(
                self.value, np.mean(self.acceptances[
                    np.max([self.sample_iter-50, 0]):self.sample_iter]))
        except:
            s = str(self.value)
        return s

    def initialize(self, value_option: str, loc_option: str, scale2_option: str,
        truncation_settings: Union[str, Tuple[float, float]], proposal_option: str, 
        target_acceptance_rate: Union[str, float], tune: Union[str, int], 
        end_tune: Union[str, int], value: float=None, loc: float=None, scale: float=None, 
        proposal_var: float=None, delay: int=0):
        &#39;&#39;&#39;These are the parameters to initialize the parameters
        of the class. Depending whether it is a self-interaction
        or a growth, it does it differently.

        Parameters
        ----------
        value_option : str
            How to initialize the value. Options:
                &#39;auto&#39;, &#39;prior-mean&#39;
                    Set to the prior mean
                &#39;linear-regression&#39;
                    Set the values from an unregularized linear regression
                &#39;manual&#39;
                    `value` must also be specified
        truncation_settings: str, tuple
            How to set the truncation parameters. The proposal trucation will
            be set the same way.
                tuple - (low,high)
                    These are the truncation parameters
                &#39;auto&#39;
                    If self-interactions, &#39;negative&#39;. If growths, &#39;positive&#39;
                &#39;positive&#39;
                    (0, \infty)
                &#39;negative&#39;
                    (-\infty, 0)
                &#39;in-vivo&#39;
                    Not implemented
        loc_option : str
            How to set the mean
                &#39;auto&#39;, &#39;median-linear-regression&#39;
                    Set the mean to the median of the values from an
                    unregularized linear-regression
                &#39;manual&#39;
                    `loc` must also be specified
        scale2_option : str
            How to set the standard deviation
                &#39;auto&#39;, &#39;diffuse-linear-regression&#39;
                    Set the variance to 10^4 * median(a_l)
                &#39;manaul&#39;
                    `scale2` must also be specified.
        proposal_option : str
            How to initialize the proposal variance:
                &#39;auto&#39;
                    loc**2 / 100
                &#39;manual&#39;
                    `proposal_var` must also be supplied
        target_acceptance_rate : str, float
            If float, this is the target acceptance rate
            If str: 
                &#39;optimal&#39;, &#39;auto&#39;: 0.44
        tune : str, int
            How often to tune the proposal. If str:
                &#39;auto&#39;: 50
        end_tune : str, int
            When to stop tuning the proposal. If str:
                &#39;auto&#39;, &#39;half-burnin&#39;: Half of burnin
        &#39;&#39;&#39;
        self._there_are_perturbations = self.G.perturbations is not None
        if not pl.isint(delay):
            raise TypeError(&#39;`delay` ({}) must be an int&#39;.format(type(delay)))
        if delay &lt; 0:
            raise ValueError(&#39;`delay` ({}) must be &gt;= 0&#39;.format(delay))
        self.delay = delay

        # Set the propsal parameters
        if pl.isstr(target_acceptance_rate):
            if target_acceptance_rate in [&#39;optimal&#39;, &#39;auto&#39;]:
                target_acceptance_rate = 0.44
            else:
                raise ValueError(&#39;`target_acceptance_rate` ({}) not recognized&#39;.format(
                    target_acceptance_rate))
        elif pl.isfloat(target_acceptance_rate):
            if target_acceptance_rate &lt; 0 or target_acceptance_rate &gt; 1:
                raise ValueError(&#39;`target_acceptance_rate` ({}) out of range&#39;.format(
                    target_acceptance_rate))
        else:
            raise TypeError(&#39;`target_acceptance_rate` ({}) type not recognized&#39;.format(
                type(target_acceptance_rate)))
        self.target_acceptance_rate = target_acceptance_rate

        if pl.isstr(tune):
            if tune in [&#39;auto&#39;]:
                tune = 50
            else:
                raise ValueError(&#39;`tune` ({}) not recognized&#39;.format(tune))
        elif pl.isint(tune):
            if tune &lt; 0:
                raise ValueError(&#39;`tune` ({}) must be &gt; 0&#39;.format(
                    tune))
        else:
            raise TypeError(&#39;`tune` ({}) type not recognized&#39;.format(type(tune)))
        self.tune = tune

        if pl.isstr(end_tune):
            if end_tune in [&#39;auto&#39;, &#39;half-burnin&#39;]:
                end_tune = int(self.G.inference.burnin/2)
            else:
                raise ValueError(&#39;`tune` ({}) not recognized&#39;.format(end_tune))
        elif pl.isint(end_tune):
            if end_tune &lt; 0 or end_tune &gt; self.G.inference.burnin:
                raise ValueError(&#39;`end_tune` ({}) out of range (0, {})&#39;.format(
                    end_tune, self.G.inference.burnin))
        else:
            raise TypeError(&#39;`end_tune` ({}) type not recognized&#39;.format(type(end_tune)))
        self.end_tune = end_tune

        # Set the truncation settings
        if truncation_settings is None:
            truncation_settings = &#39;positive&#39;
        if pl.isstr(truncation_settings):
            if truncation_settings == &#39;positive&#39;:
                self.low = 0.
                self.high = float(&#39;inf&#39;)
            elif truncation_settings == &#39;in-vivo&#39;:
                self.low = 0.1
                self.high = np.log(10)
            else:
                raise ValueError(&#39;`truncation_settings` ({}) not recognized&#39;.format(
                    truncation_settings))
        elif pl.istuple(truncation_settings):
            if len(truncation_settings) != 2:
                raise ValueError(&#39;If `truncation_settings` is a tuple, it must have a &#39; \
                    &#39;length of 2 ({})&#39;.format(len(truncation_settings)))
            l,h = truncation_settings

            if (not pl.isnumeric(l)) or (not pl.isnumeric(h)):
                raise TypeError(&#39;`low` ({}) and `high` ({}) must be numerics&#39;.format(
                    type(l), type(h)))
            if l &lt; 0 or h &lt; 0:
                raise ValueError(&#39;`low` ({}) and `high` ({}) must be &gt;= 0&#39;.format(l,h))
            if h &lt;= l:
                raise ValueError(&#39;`low` ({}) must be strictly less than high ({})&#39;.format(l,h))
            self.high = h
            self.low = l
        else:
            raise TypeError(&#39;`truncation_settings` ({}) type not recognized&#39;)
        self.proposal.high = self.high
        self.proposal.low = self.low

        # Set the loc
        if not pl.isstr(loc_option):
            raise TypeError(&#39;`loc_option` ({}) must be a str&#39;.format(type(loc_option)))
        if loc_option == &#39;manual&#39;:
            if not pl.isnumeric(loc):
                raise TypeError(&#39;`loc` ({}) must be a numeric&#39;.format(type(loc)))
        elif loc_option in [&#39;auto&#39;, &#39;median-linear-regression&#39;]:
            # Perform linear regression
            if self.child_name == STRNAMES.GROWTH_VALUE:
                rhs = [STRNAMES.GROWTH_VALUE, STRNAMES.SELF_INTERACTION_VALUE]
                lhs = []
            else:
                rhs = [STRNAMES.SELF_INTERACTION_VALUE]
                lhs = [STRNAMES.GROWTH_VALUE]
            X = self.G.data.construct_rhs(keys=rhs,
                kwargs_dict={STRNAMES.GROWTH_VALUE:{&#39;with_perturbations&#39;:False}},
                index_out_perturbations=True)
            y = self.G.data.construct_lhs(keys=lhs, 
                kwargs_dict={STRNAMES.GROWTH_VALUE:{&#39;with_perturbations&#39;:False}},
                index_out_perturbations=True)

            prec = X.T @ X
            cov = pinv(prec, self)
            loc = cov @ X.T @ y

            if self.child_name == STRNAMES.GROWTH_VALUE:
                loc = np.median(loc[:self.G.data.n_taxa])
            else:
                loc = np.median(loc)
        else:
            raise ValueError(&#39;`loc_option` ({}) not recognized&#39;.format(loc_option))
        self.prior.loc.override_value(loc)

        # Set the scale
        if not pl.isstr(scale2_option):
            raise TypeError(&#39;`scale2_option` ({}) must be a str&#39;.format(type(scale2_option)))
        if scale2_option == &#39;manual&#39;:
            if not pl.isnumeric(scale):
                raise TypeError(&#39;`scale` ({}) must be a numeric&#39;.format(type(scale)))
        elif scale2_option in [&#39;auto&#39;, &#39;diffuse-linear-regression&#39;]:
            # Perform linear regression
            if self.child_name == STRNAMES.GROWTH_VALUE:
                rhs = [STRNAMES.GROWTH_VALUE, STRNAMES.SELF_INTERACTION_VALUE]
                lhs = []
            else:
                rhs = [STRNAMES.SELF_INTERACTION_VALUE]
                lhs = [STRNAMES.GROWTH_VALUE]
            X = self.G.data.construct_rhs(keys=rhs,
                kwargs_dict={STRNAMES.GROWTH_VALUE:{&#39;with_perturbations&#39;:False}},
                index_out_perturbations=True)
            y = self.G.data.construct_lhs(keys=lhs, 
                kwargs_dict={STRNAMES.GROWTH_VALUE:{&#39;with_perturbations&#39;:False}},
                index_out_perturbations=True)

            prec = X.T @ X
            cov = pinv(prec, self)
            mean = cov @ X.T @ y

            if self.child_name == STRNAMES.GROWTH_VALUE:
                mean = np.median(mean[:self.G.data.n_taxa])
            else:
                mean = np.median(mean)
            scale2 = 1e4 * (mean**2)
        else:
            raise ValueError(&#39;`scale2_option` ({}) not recognized&#39;.format(scale2_option))
        self.prior.scale2.override_value(scale2)

        # Set the value
        if not pl.isstr(value_option):
            raise TypeError(&#39;`value_option` ({}) must be a str&#39;.format(type(value_option)))
        if value_option == &#39;manual&#39;:
            if not pl.isnumeric(value):
                raise TypeError(&#39;`value` ({}) must be a numeric&#39;.format(type(value)))
        elif value_option in [&#39;linear-regression&#39;]:
            # Perform linear regression
            if self.child_name == STRNAMES.GROWTH_VALUE:
                rhs = [STRNAMES.GROWTH_VALUE, STRNAMES.SELF_INTERACTION_VALUE]
                lhs = []
            else:
                rhs = [STRNAMES.SELF_INTERACTION_VALUE]
                lhs = [STRNAMES.GROWTH_VALUE]
            X = self.G.data.construct_rhs(keys=rhs,
                kwargs_dict={STRNAMES.GROWTH_VALUE:{&#39;with_perturbations&#39;:False}},
                index_out_perturbations=True)
            y = self.G.data.construct_lhs(keys=lhs, 
                kwargs_dict={STRNAMES.GROWTH_VALUE:{&#39;with_perturbations&#39;:False}},
                index_out_perturbations=True)

            prec = X.T @ X
            cov = pinv(prec, self)
            mean = cov @ X.T @ y

            if self.child_name == STRNAMES.GROWTH_VALUE:
                value = mean[:self.G.data.n_taxa]
            else:
                value = mean
        elif value_option in [&#39;auto&#39;, &#39;prior-mean&#39;]:
            value = self.prior.loc.value
        else:
            raise ValueError(&#39;`value_option` ({}) not recognized&#39;.format(value_option))
        self.value = value

        # Set the proposal variance
        if not pl.isstr(proposal_option):
            raise TypeError(&#39;`proposal_option` ({}) must be a str&#39;.format(
                type(proposal_option)))
        elif proposal_option == &#39;manual&#39;:
            if not pl.isnumeric(proposal_var):
                raise TypeError(&#39;`proposal_var` ({}) must be a numeric&#39;.format(
                    type(proposal_var)))
            if proposal_var &lt;= 0:
                raise ValueError(&#39;`proposal_var` ({}) not proper&#39;.format(proposal_var))
        elif proposal_option in [&#39;auto&#39;]:
            proposal_var = (self.value ** 2)/10
        else:
            raise ValueError(&#39;`proposal_option` ({}) not recognized&#39;.format(
                proposal_option))
        self.proposal.scale2.value = proposal_var

    def update_var(self):
        &#39;&#39;&#39;Update the `var` parameter so that we adjust the acceptance
        rate to `target_acceptance_rate`
        &#39;&#39;&#39;
        if self.sample_iter == 0:
            self.temp_acceptances = 0
            self.acceptances = np.zeros(self.G.inference.n_samples, dtype=bool)
        
        elif self.sample_iter &gt; self.end_tune:
            # Don&#39;t do any more updates
            return
        
        elif self.sample_iter % self.tune == 0:
            # Update var
            acceptance_rate = self.temp_acceptances / self.tune
            if acceptance_rate &gt; self.target_acceptance_rate:
                self.proposal.scale2.value *= 1.5
            else:
                self.proposal.scale2.value /= 1.5
            self.temp_acceptances = 0

    def update(self):
        &#39;&#39;&#39;First we check if we need to tune the var, which we do during
        the first half of burnin. We calculate the likelihoods in logspace
        &#39;&#39;&#39;
        if self.sample_iter &lt; self.delay:
            return
        self.update_var()
        proposal_std = np.sqrt(self.proposal.scale2.value)

        # Get necessary data of the respective parameter
        variable = self.G[self.child_name]
        x = variable.value.ravel()
        std = np.sqrt(variable.prior.scale2.value)

        low = variable.low
        high = variable.high

        # propose a new value for the mean
        prev_mean = self.value
        self.proposal.loc.value = self.value
        new_mean = self.proposal.sample() # Sample a new value

        # Calculate the target distribution ll
        prev_target_ll = pl.random.truncnormal.logpdf( 
            value=prev_mean, loc=self.prior.loc.value, 
            scale=np.sqrt(self.prior.scale2.value), low=self.low,
            high=self.high)
        for i in range(len(x)):
            prev_target_ll += pl.random.truncnormal.logpdf(
                value=x[i], loc=prev_mean, scale=std,
                low=low, high=high)
        new_target_ll = pl.random.truncnormal.logpdf( 
            value=new_mean, loc=self.prior.loc.value, 
            scale=np.sqrt(self.prior.scale2.value), low=self.low,
            high=self.high)
        for i in range(len(x)):
            new_target_ll += pl.random.truncnormal.logpdf(
                value=x[i], loc=new_mean, scale=std,
                low=low, high=high)

        # Normalize by the ll of the proposal
        prev_prop_ll = pl.random.truncnormal.logpdf(
            value=prev_mean, loc=new_mean, scale=proposal_std,
            low=low, high=high)
        
        new_prop_ll = pl.random.truncnormal.logpdf(
            value=new_mean, loc=prev_mean, scale=proposal_std,
            low=low, high=high)

        # Accept or reject
        r = (new_target_ll - prev_prop_ll) - \
            (prev_target_ll - new_prop_ll)
        u = np.log(pl.random.misc.fast_sample_standard_uniform())

        if r &gt;= u:
            self.acceptances[self.sample_iter] = True
            self.value = new_mean
            self.temp_acceptances += 1
        else:
            self.value = prev_mean

    def visualize(self, path: str, section: str=&#39;posterior&#39;) -&gt; pd.DataFrame:
        &#39;&#39;&#39;Render the traces in the folder `basepath` and returns a
        `pandas.DataFrame` with summary statistics

        Parameters
        ----------
        path : str
            This is the path to write the files to
        f : _io.TextIOWrapper
            File that we are writing the values to
        section : str
            Section of the trace to compute on. Options:
                &#39;posterior&#39; : posterior samples
                &#39;burnin&#39; : burn-in samples
                &#39;entire&#39; : both burn-in and posterior samples

        Returns
        -------
        pandas.DataFrame
        &#39;&#39;&#39;
        if not self.G.inference.tracer.is_being_traced(self):
            logging.info(&#39;`{}` not learned\n\tValue: {}\n&#39;.format(self.name, self.value))
            return pd.DataFrame()
        summ = pl.summary(self, section=section)
        data = [[self.name] + [v for _,v in summ.items()]]
        columns = [&#39;name&#39;] + [k for k in summ]
        index = [&#39;mean&#39;]
        df = pd.DataFrame(data, columns=columns, index=index)

        # Plot the traces
        ax1, ax2 = visualization.render_trace(var=self, plt_type=&#39;both&#39;, section=section,
            include_burnin=True, log_scale=True, rasterized=True)

        # Plot the prior over the posterior
        l,h = ax1.get_xlim()
        xs = np.arange(l,h,step=(h-l)/100) 
        ys = []
        for x in xs:
            ys.append(pl.random.normal.pdf(value=x, 
                loc=self.prior.loc.value,
                scale=np.sqrt(self.prior.scale2.value)))
        ax1.plot(xs, ys, label=&#39;prior&#39;, alpha=0.5, color=&#39;red&#39;, rasterized=True)
        ax1.legend()

        # Plot the acceptance rate over the trace
        ax3 = ax2.twinx()
        ax3 = visualization.render_acceptance_rate_trace(var=self, ax=ax3, 
            label=&#39;Acceptance Rate&#39;, color=&#39;red&#39;, scatter=False, rasterized=True)
        ax3.legend()
        fig = plt.gcf()
        fig.tight_layout()
        fig.suptitle(self.name)
        plt.savefig(path)
        plt.close()

        return df


class Growth(pl.variables.TruncatedNormal):
    &#39;&#39;&#39;Growth values of Lotka-Voltera

    Parameters
    ----------
    prior : mdsine2.variables.TruncatedNormal
        Prior distribution
    &#39;&#39;&#39;
    def __init__(self, prior: variables.TruncatedNormal, **kwargs):
        kwargs[&#39;name&#39;] = STRNAMES.GROWTH_VALUE
        pl.variables.TruncatedNormal.__init__(self, loc=None, scale2=None, low=0.,
            high=float(&#39;inf&#39;), dtype=float, **kwargs)
        self.set_value_shape(shape=(len(self.G.data.taxa),))
        self.add_prior(prior)
        self.delay = 0
        self._initialized = False

    def __str__(self) -&gt; str:
        return str(self.value)

    def update_str(self):
        return

    def initialize(self, value_option: str, truncation_settings: Union[str, Tuple[float, float]],
        value: np.ndarray=None, delay: int=0):
        &#39;&#39;&#39;Initialize the growth values and hyperparamters

        Parameters
        ----------
        value_option : str
            - How to initialize the values. Options:
                - &#39;manual&#39;
                    - Set the values manually. `value` must also be specified.
                - &#39;linear regression&#39;
                    - Set the values of the growth using linear regression
                - &#39;ones&#39;
                    - Set all of the values to 1.
                - &#39;auto&#39;
                    - Alias for &#39;ones&#39;
                - &#39;prior-mean&#39;
                    - Set to the mean of the prior
        value : array
            Only necessary if `value_option` is &#39;manual&#39;
        delay : int
            How many MCMC iterations to delay starting to update
        truncation_settings : str, tuple, None
            - These are the settings of how you set the upper and lower limit of the
              truncated distribution. If it is None, it will default to &#39;standard&#39;. Options:
                - &#39;positive&#39;, None
                    - Only constrains the values to being positive
                    - low=0., high=float(&#39;inf&#39;)
                - &#39;in-vivo&#39;, &#39;auto&#39;
                    - Tighter constraint on the growth values.
                    - low=0.1, high=ln(10)
                    - These values have the following meaning
                        - The slowest growing microbe will grow an order of magnitude in ~10 days
                        - The fastest growing microbe will grow an order of magnitude in 1 day
                - tuple(low, high)
                    - These are manually specified values for the low and high
        &#39;&#39;&#39;
        self._initialized = True
        self._there_are_perturbations = self.G.perturbations is not None
        if not pl.isint(delay):
            raise TypeError(&#39;`delay` ({}) must be an int&#39;.format(type(delay)))
        if delay &lt; 0:
            raise ValueError(&#39;`delay` ({}) must be &gt;= 0&#39;.format(delay))
        self.delay = delay

        # Truncation settings
        if truncation_settings is None:
            truncation_settings = &#39;positive&#39;
        if pl.isstr(truncation_settings):
            if truncation_settings == &#39;positive&#39;:
                self.low = 0.
                self.high = float(&#39;inf&#39;)
            elif truncation_settings in [&#39;in-vivo&#39;, &#39;auto&#39;]:
                self.low = 0.1
                self.high = math.log(10)
            else:
                raise ValueError(&#39;`truncation_settings` ({}) not recognized&#39;.format(
                    truncation_settings))
        elif pl.istuple(truncation_settings):
            if len(truncation_settings) != 2:
                raise ValueError(&#39;If `truncation_settings` is a tuple, it must have a &#39; \
                    &#39;length of 2 ({})&#39;.format(len(truncation_settings)))
            l,h = truncation_settings

            if (not pl.isnumeric(l)) or (not pl.isnumeric(h)):
                raise TypeError(&#39;`low` ({}) and `high` ({}) must be numerics&#39;.format(
                    type(l), type(h)))
            if l &lt; 0 or h &lt; 0:
                raise ValueError(&#39;`low` ({}) and `high` ({}) must be &gt;= 0&#39;.format(l,h))
            if h &lt;= l:
                raise ValueError(&#39;`low` ({}) must be strictly less than high ({})&#39;.format(l,h))
            self.high = h
            self.low = l
        else:
            raise TypeError(&#39;`truncation_settings` ({}) type not recognized&#39;)

        # Setting the value
        if not pl.isstr(value_option):
            raise TypeError(&#39;`value_option` ({}) must be a str&#39;.format(type(value_option)))
        if value_option == &#39;manual&#39;:
            if not pl.isarray(value):
                value = np.ones(len(self.G.data.taxa))*value
            if len(value) != self.G.data.n_taxa:
                raise ValueError(&#39;`value` ({}) must be ({}) long&#39;.format(
                    len(value), len(self.G.data.taxa)))
            self.value = value
        elif value_option == &#39;linear-regression&#39;:
            rhs = [
                STRNAMES.GROWTH_VALUE,
                STRNAMES.SELF_INTERACTION_VALUE
            ]
            lhs = []
            X = self.G.data.construct_rhs(
                keys=rhs, kwargs_dict={STRNAMES.GROWTH_VALUE:{&#39;with_perturbations&#39;:False}},
                index_out_perturbations=True)
            y = self.G.data.construct_lhs(keys=lhs, index_out_perturbations=True)

            prec = X.T @ X
            cov = pinv(prec, self)
            mean = (cov @ X.transpose().dot(y)).ravel()
            self.value = np.absolute(mean[:len(self.G.data.taxa)])
        elif value_option in [&#39;auto&#39;, &#39;ones&#39;]:
            self.value = np.ones(len(self.G.data.taxa), dtype=float)
        elif value_option == &#39;prior-mean&#39;:
            self.value = self.prior.mean() * np.ones(self.G.data.n_taxa)
        else:
            raise ValueError(&#39;`value_option` ({}) not recognized&#39;.format(value_option))

        logging.info(&#39;Growth value initialization: {}&#39;.format(self.value))
        logging.info(&#39;Growth prior mean: {}&#39;.format(self.prior.loc.value))
        logging.info(&#39;Growth truncation settings: {}&#39;.format((self.low, self.high)))

    def update(self):
        &#39;&#39;&#39;Update the values using a truncated normal
        &#39;&#39;&#39;
        if self.sample_iter &lt; self.delay:
            return

        self.calculate_posterior()
        self.sample()

        if not pl.isarray(self.value):
            # This will happen if there is 1 Taxa
            self.value = np.array([self.value])

        if np.any(np.isnan(self.value)):
            logging.critical(&#39;mean: {}&#39;.format(self.loc.value))
            logging.critical(&#39;var: {}&#39;.format(self.scale2.value))
            logging.critical(&#39;value: {}&#39;.format(self.value))
            raise ValueError(&#39;`Values in {} are nan: {}&#39;.format(self.name, self.value))

        if self._there_are_perturbations:
            # If there are perturbations then we need to update their
            # matrix because the growths changed
            self.G.data.design_matrices[STRNAMES.PERT_VALUE].update_values()

    def calculate_posterior(self):
        rhs = [STRNAMES.GROWTH_VALUE]
        if self._there_are_perturbations:
            lhs = [
                STRNAMES.SELF_INTERACTION_VALUE,
                STRNAMES.CLUSTER_INTERACTION_VALUE]
        else:
            lhs = [
                STRNAMES.SELF_INTERACTION_VALUE,
                STRNAMES.CLUSTER_INTERACTION_VALUE]
        X = self.G.data.construct_rhs(keys=rhs,
            kwargs_dict={STRNAMES.GROWTH_VALUE:{
                &#39;with_perturbations&#39;:self._there_are_perturbations}})
        y = self.G.data.construct_lhs(keys=lhs)
        # X = X.toarray()

        process_prec = self.G[STRNAMES.PROCESSVAR].build_matrix(
            cov=False, sparse=True)

        prior_prec = build_prior_covariance(G=self.G, cov=False,
            order=rhs, sparse=True)
        prior_mean = build_prior_mean(G=self.G, order=rhs).reshape(-1,1)
        pm = prior_prec @ prior_mean

        prec = X.T @ process_prec @ X + prior_prec
        cov = pinv(prec, self)

        self.loc.value = np.asarray(cov @ (X.T @ process_prec.dot(y) + pm)).ravel()
        self.scale2.value = np.diag(cov)

    def visualize(self, basepath: str, section: str=&#39;posterior&#39;, taxa_formatter: str=&#39;%(name)s&#39;, 
        true_value: np.ndarray=None) -&gt; pd.DataFrame:
        &#39;&#39;&#39;Render the traces in the folder `basepath`. Makes a `pandas.DataFrame` table
        where the index is the Taxa name in `taxa_formatter` and the columns are
        `mean`, `median`, `25th percentile`, `75th` percentile`.

        Parameters
        ----------
        basepath : str
            This is the loction to write the files to
        section : str
            Section of the trace to compute on. Options:
                &#39;posterior&#39; : posterior samples
                &#39;burnin&#39; : burn-in samples
                &#39;entire&#39; : both burn-in and posterior samples
        taxa_formatter : str, None
            This is the format of the label to return for each Taxa. If None, it will return
            the taxa name
        true_value : np.ndarray
            Ground truth values of the variable

        Returns
        -------
        pandas.DataFrame
        &#39;&#39;&#39;
        if not self.G.inference.tracer.is_being_traced(self):
            logging.info(&#39;`{}` not learned\n\tValue: {}\n&#39;.format(self.name, self.value))
            return pd.DataFrame()

        taxa = self.G.data.subjects.taxa
        summ = pl.summary(self, section=section)
        data = []
        index = []
        columns = [&#39;name&#39;] + [k for k in summ]

        for idx in range(len(taxa)):
            if taxa_formatter is not None:
                prefix = pl.taxaname_formatter(format=taxa_formatter, taxon=taxa[idx], taxa=taxa)
            else:
                prefix = taxa[idx].name
            index.append(taxa[idx].name)
            temp = [prefix]
            for _,v in summ.items():
                temp.append(v[idx])
            data.append(temp)
        
        df = pd.DataFrame(data, columns=columns, index=index)

        if section == &#39;posterior&#39;:
            len_posterior = self.G.inference.sample_iter + 1 - self.G.inference.burnin
        elif section == &#39;burnin&#39;:
            len_posterior = self.G.inference.burnin
        else:
            len_posterior = self.G.inference.sample_iter + 1

        print(self.G.inference.sample_iter)
        print(self.G.inference.burnin)
        print(len_posterior)

        # Plot the prior on top of the posterior
        if self.G.tracer.is_being_traced(STRNAMES.PRIOR_MEAN_GROWTH):
            prior_mean_trace = self.G[STRNAMES.PRIOR_MEAN_GROWTH].get_trace_from_disk(
                    section=section)
        else:
            prior_mean_trace = self.prior.loc.value * np.ones(len_posterior, dtype=float)
        if self.G.tracer.is_being_traced(STRNAMES.PRIOR_VAR_GROWTH):
            prior_std_trace = np.sqrt(
                self.G[STRNAMES.PRIOR_VAR_GROWTH].get_trace_from_disk(section=section))
        else:
            prior_std_trace = np.sqrt(self.prior.scale2.value) * np.ones(len_posterior, dtype=float)

        for idx in range(len(taxa)):
            fig = plt.figure()
            ax_posterior = fig.add_subplot(1,2,1)
            visualization.render_trace(var=self, idx=idx, plt_type=&#39;hist&#39;,
                label=section, color=&#39;blue&#39;, ax=ax_posterior, section=section,
                include_burnin=True, rasterized=True)

            # Get the limits and only look at the posterior within 20% range +- of
            # this number
            low_x, high_x = ax_posterior.get_xlim()

            arr = np.zeros(len(prior_std_trace), dtype=float)
            for i in range(len(prior_std_trace)):
                arr[i] = pl.random.truncnormal.sample(loc=prior_mean_trace[i], scale=prior_std_trace[i], 
                    low=self.low, high=self.high)
            visualization.render_trace(var=arr, plt_type=&#39;hist&#39;, 
                label=&#39;prior&#39;, color=&#39;red&#39;, ax=ax_posterior, rasterized=True)

            if true_value is not None:
                ax_posterior.axvline(x=true_value[idx], color=&#39;red&#39;, alpha=0.65, 
                    label=&#39;True Value&#39;)

            ax_posterior.legend()
            ax_posterior.set_xlim(left=low_x*.8, right=high_x*1.2)

            # plot the trace
            ax_trace = fig.add_subplot(1,2,2)
            visualization.render_trace(var=self, idx=idx, plt_type=&#39;trace&#39;, 
                ax=ax_trace, section=section, include_burnin=True, rasterized=True)

            if true_value is not None:
                ax_trace.axhline(y=true_value[idx], color=&#39;red&#39;, alpha=0.65, 
                    label=&#39;True Value&#39;)
                ax_trace.legend()

            fig.suptitle(&#39;{}&#39;.format(index[idx]))
            fig.tight_layout()
            fig.subplots_adjust(top=0.85)
            plt.savefig(os.path.join(basepath, &#39;{}.pdf&#39;.format(taxa[idx].name)))
            plt.close()

        return df


class SelfInteractions(pl.variables.TruncatedNormal):
    &#39;&#39;&#39;self-interactions of Lotka-Voltera

    Since our dynamics subtract this parameter, this parameter must be positive

    Parameters
    ----------
    prior : mdsine2.variables.TruncatedNormal
        Prior distribution
    &#39;&#39;&#39;
    def __init__(self, prior: variables.TruncatedNormal, **kwargs):
        kwargs[&#39;name&#39;] = STRNAMES.SELF_INTERACTION_VALUE
        pl.variables.TruncatedNormal.__init__(self, loc=None, scale2=None, low=0.,
            high=float(&#39;inf&#39;), dtype=float, **kwargs)
        self.set_value_shape(shape=(len(self.G.data.taxa),))
        self.add_prior(prior)

    def __str__(self) -&gt; str:
        return str(self.value)

    def update_str(self):
        return

    def initialize(self, value_option: str, truncation_settings: Union[str, Tuple[float, float]],
        value: np.ndarray=None, delay: int=0, q: float=None, rescale_value: float=None):
        &#39;&#39;&#39;Initialize the self-interactions values and hyperparamters

        Parameters
        ----------
        value_option : str
            - How to initialize the values.
            - Options:
               - &#39;manual&#39;
                    - Set the values manually. `value` must also be specified.
                - &#39;fixed-growth&#39;
                    - Fix the growth values and then sample the self-interactions
                - &#39;strict-enforcement-partial&#39;
                    - Do an unregularized regression then take the absolute value of the numbers.
                    - We assume there are no interactions and we index out the time points that have
                      perturbations in them. We assume that we do not know the growths (the growths
                      are being regressed as well).
                - &#39;strict-enforcement-full&#39;
                    - Do an unregularized regression then take the absolute value of the numbers.
                    - We assume there are no interactions and we index out the time points that have
                      perturbations in them. We assume that we know the growths (the growths are on
                      the lhs)
                - &#39;steady-state&#39;, &#39;auto&#39;
                    - Set to the steady state values. Must also provide the quantile with the
                      parameter `q`. In here we assume that the steady state is the `q`th quantile
                      of the off perturbation data
                - &#39;prior-mean&#39;
                    - Set the value to the mean of the prior
        truncation_settings : str, 2-tuple
            - How to set the truncations for the normal distribution
            - (low,high)
                - These are the low and high values
            - &#39;negative&#39;
                - Truncated (-inf, 0)
            - &#39;positive&#39;, &#39;auto&#39;
                - Truncated (0, inf)
            - &#39;human&#39;
                - This assumes that the range of the steady state abundances of the human gut
                  fluctuate between 1e2 and  1e13. We requie that the growth value be initialized first.
                - We set the vlaues to be (growth.high/1e14, growth.low/1e2)
            - &#39;mouse&#39;
                - This assumes that the range of the steady state abundances of the mouse gut
                  fluctuate between 1e2 and  1e12. We requie that the growth value be initialized first.
                - We set the vlaues to be (growth.high/1e13, growth.low/1e2)
        value : array
            Only necessary if `value_option` is &#39;manual&#39;
        mean : array
            Only necessary if `mean_option` is &#39;manual&#39;
        delay : int
            How many MCMC iterations to delay starting to update
        rescale_value : None, float
            - This is the rescale value of the qPCR. This will rescale the truncation settings.
            - This is only used for either the &#39;mouse&#39; or &#39;human&#39; settings
        &#39;&#39;&#39;
        self._there_are_perturbations = self.G.perturbations is not None
        if not pl.isint(delay):
            raise TypeError(&#39;`delay` ({}) must be an int&#39;.format(type(delay)))
        if delay &lt; 0:
            raise ValueError(&#39;`delay` ({}) must be &gt;= 0&#39;.format(delay))
        self.delay = delay

        # Set truncation settings
        if pl.isstr(truncation_settings):
            # if truncation_settings == &#39;negative&#39;:
            #     self.low = float(&#39;-inf&#39;)
            #     self.high= 0
            if truncation_settings in [&#39;mouse&#39;, &#39;human&#39;]:
                growth = self.G[STRNAMES.GROWTH_VALUE]
                if not growth._initialized:
                    raise ValueError(&#39;Growth values `{}` must be initialized first&#39;.format(
                        STRNAMES.GROWTH_VALUE))
                if truncation_settings == &#39;mouse&#39;:
                    high = 1e13
                else:
                    high = 1e14
                low = 1e2
                if rescale_value is not None:
                    if not pl.isnumeric(rescale_value):
                        raise TypeError(&#39;`rescale_value` ({}) must be a numeric&#39;.format(
                            type(rescale_value)))
                    if rescale_value &lt;= 0:
                        raise ValueError(&#39;`rescale_value` ({}) must be &gt; 0&#39;.format(rescale_value))
                    high *= rescale_value
                    low *= rescale_value
                self.low = growth.high/low
                self.high = growth.low/high
            elif truncation_settings in [&#39;auto&#39;, &#39;positive&#39;]:
                self.low = 0
                self.high = float(&#39;inf&#39;)
            else:
                raise ValueError(&#39;`truncation_settings) ({}) not recognized&#39;.format(
                    truncation_settings))
        elif pl.istuple(truncation_settings):
            if len(truncation_settings) != 2:
                raise ValueError(&#39;If `truncation_settings` is a tuple, it must have a &#39; \
                    &#39;length of 2 ({})&#39;.format(len(truncation_settings)))
            l,h = truncation_settings

            if (not pl.isnumeric(l)) or (not pl.isnumeric(h)):
                raise TypeError(&#39;`low` ({}) and `high` ({}) must be numerics&#39;.format(
                    type(l), type(h)))
            if l &lt; 0 or h &lt; 0:
                raise ValueError(&#39;`low` ({}) and `high` ({}) must be &gt;= 0&#39;.format(l,h))
            if h &lt;= l:
                raise ValueError(&#39;`low` ({}) must be strictly less than high ({})&#39;.format(l,h))
            self.high = h
            self.low = l
        else:
            raise TypeError(&#39;`truncation_settings` ({}) must be a tuple or str&#39;.format(
                type(truncation_settings)))

        # Set value option
        if not pl.isstr(value_option):
            raise TypeError(&#39;`value_option` ({}) must be a str&#39;.format(type(value_option)))
        if value_option == &#39;manual&#39;:
            if not pl.isarray(value):
                value = np.ones(len(self.G.data.taxa))*value
            if len(value) != self.G.data.n_taxa:
                raise ValueError(&#39;`value` ({}) must be ({}) long&#39;.format(
                    len(value), len(self.G.data.taxa)))
            self.value = value
        elif value_option == &#39;fixed-growth&#39;:
            X = self.G.data.construct_rhs(keys=[STRNAMES.SELF_INTERACTION_VALUE],
                index_out_perturbations=True)
            y = self.G.data.construct_lhs(keys=[STRNAMES.GROWTH_VALUE], kwargs_dict={
                STRNAMES.GROWTH_VALUE:{&#39;with_perturbations&#39;:False}},
                index_out_perturbations=True)
            prec = X.T @ X
            cov = pinv(prec, self)
            self.value = np.absolute((cov @ X.transpose().dot(y)).ravel())
        elif &#39;strict-enforcement&#39; in value_option:
            if &#39;full&#39; in value_option:
                rhs = [STRNAMES.SELF_INTERACTION_VALUE]
                lhs = [STRNAMES.GROWTH_VALUE]
            elif &#39;partial&#39; in value_option:
                lhs = []
                rhs = [STRNAMES.GROWTH_VALUE, STRNAMES.SELF_INTERACTION_VALUE]
            else:
                raise ValueError(&#39;`value_option` ({}) not recognized&#39;.format(value_option))
            X = self.G.data.construct_rhs(
                keys=rhs, kwargs_dict={STRNAMES.GROWTH_VALUE:{&#39;with_perturbations&#39;:False}},
                index_out_perturbations=True)
            y = self.G.data.construct_lhs(keys=lhs, index_out_perturbations=True)

            prec = X.T @ X
            cov = pinv(prec, self)
            mean = (cov @ X.transpose().dot(y)).ravel()
            self.value = np.absolute(mean[len(self.G.data.taxa):])
        elif value_option == &#39;prior-mean&#39;:
            self.value = self.prior.loc.value * np.ones(self.G.data.n_taxa)
        elif value_option in [&#39;steady-state&#39;, &#39;auto&#39;]:
            # check quantile
            if not pl.isnumeric(q):
                raise TypeError(&#39;`q` ({}) must be numeric&#39;.format(type(q)))
            if q &lt; 0 or q &gt; 1:
                raise ValueError(&#39;`q` ({}) must be [0,1]&#39;.format(q))

            # Get the data off perturbation
            datas = None
            for ridx in range(self.G.data.n_replicates):
                if self._there_are_perturbations:
                    # Exclude the data thats in a perturbation
                    base_idx = 0
                    for start,end in self.G.data.tidxs_in_perturbation[ridx]:
                        if datas is None:
                            datas = self.G.data.data[ridx][:,base_idx:start]
                        else:
                            datas = np.hstack((datas, self.G.data.data[ridx][:,base_idx:start]))
                        base_idx = end
                    if end != self.G.data.data[ridx].shape[1]:
                        datas = np.hstack((datas, self.G.data.data[ridx][:,base_idx:]))
                else:
                    if datas is None:
                        datas = self.G.data.data[ridx]
                    else:
                        datas = np.hstack((datas, self.G.data.data[ridx]))

            # Set the steady-state for each Taxa
            ss = np.quantile(datas, q=q, axis=1)

            # Get the self-interactions by using the values of the growth terms
            self.value = 1/ss
        elif value_option == &#39;linear-regression&#39;:
            
            rhs = [STRNAMES.SELF_INTERACTION_VALUE]
            lhs = [STRNAMES.GROWTH_VALUE]
            X = self.G.data.construct_rhs(keys=rhs,
                index_out_perturbations=True)
            y = self.G.data.construct_lhs(keys=lhs, index_out_perturbations=True,
                kwargs_dict={STRNAMES.GROWTH_VALUE:{&#39;with_perturbations&#39;:False}})

            prec = X.T @ X
            cov = pinv(prec, self)
            mean = cov @ X.T @ y
            self.value = np.asarray(mean).ravel()
        else:
            raise ValueError(&#39;`value_option` ({}) not recognized&#39;.format(value_option))

        logging.info(&#39;Self-interactions value initialization: {}&#39;.format(self.value))
        logging.info(&#39;Self-interactions truncation settings: {}&#39;.format((self.low, self.high)))

    def update(self):
        if self.sample_iter &lt; self.delay:
            return

        self.calculate_posterior()
        self.sample()

        if not pl.isarray(self.value):
            # This will happen if there is 1 Taxa
            self.value = np.array([self.value])

        if np.any(np.isnan(self.value)):
            logging.critical(&#39;mean: {}&#39;.format(self.loc.value))
            logging.critical(&#39;var: {}&#39;.format(self.scale2.value))
            logging.critical(&#39;value: {}&#39;.format(self.value))
            raise ValueError(&#39;`Values in {} are nan: {}&#39;.format(self.name, self.value))

    def calculate_posterior(self):

        rhs = [STRNAMES.SELF_INTERACTION_VALUE]
        if self._there_are_perturbations:
            lhs = [
                STRNAMES.GROWTH_VALUE,
                STRNAMES.CLUSTER_INTERACTION_VALUE]
        else:
            lhs = [
                STRNAMES.GROWTH_VALUE,
                STRNAMES.CLUSTER_INTERACTION_VALUE]
        X = self.G.data.construct_rhs(keys=rhs)
        y = self.G.data.construct_lhs(keys=lhs, kwargs_dict={STRNAMES.GROWTH_VALUE:{
                &#39;with_perturbations&#39;:self._there_are_perturbations}})
        process_prec = self.G[STRNAMES.PROCESSVAR].build_matrix(
            cov=False, sparse=True)
        prior_prec = build_prior_covariance(G=self.G, cov=False,
            order=rhs, sparse=True)

        pm = prior_prec @ (self.prior.loc.value * np.ones(self.G.data.n_taxa).reshape(-1,1))

        prec = X.T @ process_prec @ X + prior_prec
        cov = pinv(prec, self)
        self.loc.value = np.asarray(cov @ (X.T @ process_prec.dot(y) + pm)).ravel()
        self.scale2.value = np.diag(cov)

    def visualize(self, basepath: str, section: str=&#39;posterior&#39;, taxa_formatter: str=&#39;%(name)s&#39;, 
        true_value: np.ndarray=None) -&gt; pd.DataFrame:
        &#39;&#39;&#39;Render the traces in the folder `basepath`. Makes a `pandas.DataFrame` table
        where the index is the Taxa name in `taxa_formatter` and the columns are
        `mean`, `median`, `25th percentile`, `75th` percentile`.

        Parameters
        ----------
        basepath : str
            This is the loction to write the files to
        section : str
            Section of the trace to compute on. Options:
                &#39;posterior&#39; : posterior samples
                &#39;burnin&#39; : burn-in samples
                &#39;entire&#39; : both burn-in and posterior samples
        taxa_formatter : str, None
            This is the format of the label to return for each Taxa. If None, it will return
            the taxon&#39;s name
        true_value : np.ndarray
            Ground truth values of the variable

        Returns
        -------
        pandas.DataFrame
        &#39;&#39;&#39;
        if not self.G.inference.tracer.is_being_traced(self):
            logging.info(&#39;`{}` not learned\n\tValue: {}\n&#39;.format(self.name, self.value))
            return pd.DataFrame()

        taxa = self.G.data.subjects.taxa
        summ = pl.summary(self, section=section)
        data = []
        index = []
        columns = [&#39;name&#39;] + [k for k in summ]

        for idx in range(len(taxa)):
            if taxa_formatter is not None:
                prefix = pl.taxaname_formatter(format=taxa_formatter, taxon=taxa[idx], taxa=taxa)
            else:
                prefix = taxa[idx].name
            index.append(taxa[idx].name)
            temp = [prefix]
            for _,v in summ.items():
                temp.append(v[idx])
            data.append(temp)
        
        df = pd.DataFrame(data, columns=columns, index=index)

        if section == &#39;posterior&#39;:
            len_posterior = self.G.inference.sample_iter + 1 - self.G.inference.burnin
        elif section == &#39;burnin&#39;:
            len_posterior = self.G.inference.burnin
        else:
            len_posterior = self.G.inference.sample_iter + 1

        # Plot the prior on top of the posterior
        if self.G.tracer.is_being_traced(STRNAMES.PRIOR_MEAN_GROWTH):
            prior_mean_trace = self.G[STRNAMES.PRIOR_MEAN_GROWTH].get_trace_from_disk(
                    section=section)
        else:
            prior_mean_trace = self.prior.loc.value * np.ones(len_posterior, dtype=float)
        if self.G.tracer.is_being_traced(STRNAMES.PRIOR_VAR_GROWTH):
            prior_std_trace = np.sqrt(
                self.G[STRNAMES.PRIOR_VAR_GROWTH].get_trace_from_disk(section=section))
        else:
            prior_std_trace = np.sqrt(self.prior.scale2.value) * np.ones(len_posterior, dtype=float)

        for idx in range(len(taxa)):
            fig = plt.figure()
            ax_posterior = fig.add_subplot(1,2,1)
            visualization.render_trace(var=self, idx=idx, plt_type=&#39;hist&#39;,
                label=section, color=&#39;blue&#39;, ax=ax_posterior, section=section,
                include_burnin=True, rasterized=True)

            # Get the limits and only look at the posterior within 20% range +- of
            # this number
            low_x, high_x = ax_posterior.get_xlim()

            arr = np.zeros(len(prior_std_trace), dtype=float)
            for i in range(len(prior_std_trace)):
                arr[i] = pl.random.truncnormal.sample(loc=prior_mean_trace[i], scale=prior_std_trace[i], 
                    low=self.low, high=self.high)
            visualization.render_trace(var=arr, plt_type=&#39;hist&#39;, 
                label=&#39;prior&#39;, color=&#39;red&#39;, ax=ax_posterior, rasterized=True)

            if true_value is not None:
                ax_posterior.axvline(x=true_value[idx], color=&#39;red&#39;, alpha=0.65, 
                    label=&#39;True Value&#39;)

            ax_posterior.legend()
            ax_posterior.set_xlim(left=low_x*.8, right=high_x*1.2)

            # plot the trace
            ax_trace = fig.add_subplot(1,2,2)
            visualization.render_trace(var=self, idx=idx, plt_type=&#39;trace&#39;, 
                ax=ax_trace, section=section, include_burnin=True, rasterized=True)

            if true_value is not None:
                ax_trace.axhline(y=true_value[idx], color=&#39;red&#39;, alpha=0.65, 
                    label=&#39;True Value&#39;)
                ax_trace.legend()

            fig.suptitle(&#39;{}&#39;.format(index[idx]))
            fig.tight_layout()
            fig.subplots_adjust(top=0.85)
            plt.savefig(os.path.join(basepath, &#39;{}.pdf&#39;.format(taxa[idx].name)))
            plt.close()

        return df


class GLVParameters(pl.variables.MVN):
    &#39;&#39;&#39;This is the posterior of the regression coefficients.
    The current posterior assumes a prior mean of 0.

    This class samples the growth, self-interactions, and cluster
    interactions jointly.

    Parameters
    ----------
    growth : posterior.Growth
        This is the class that has the growth variables
    self_interactions : posterior.SelfInteractions
        The self interaction terms for the Taxa
    interactions : ClusterInteractionValue
        These are the cluster interaction values
    pert_mag : PerturbationMagnitudes, None
        These are the magnitudes of the perturbation parameters (per clsuter)
        Set to None if there are no perturbations
    &#39;&#39;&#39;
    def __init__(self, growth: Growth, self_interactions: SelfInteractions, 
        interactions: ClusterInteractionValue, pert_mag: &#34;PerturbationMagnitudes&#34;, **kwargs):

        if not issubclass(growth.__class__, Growth):
            raise ValueError(&#39;`growth` ({}) must be a subclass of the Growth &#39; \
                &#39;class&#39;.format(type(growth)))
        if not issubclass(self_interactions.__class__, SelfInteractions):
            raise ValueError(&#39;`self_interactions` ({}) must be a subclass of the SelfInteractions &#39; \
                &#39;class&#39;.format(type(self_interactions)))
        if not issubclass(interactions.__class__, ClusterInteractionValue):
            raise ValueError(&#39;`interactions` ({}) must be a subclass of the Interactions &#39; \
                &#39;class&#39;.format(type(interactions)))
        if pert_mag is not None:
            if not issubclass(pert_mag.__class__, PerturbationMagnitudes):
                raise ValueError(&#39;`pert_mag` ({}) must be a subclass of the PerturbationMagnitudes &#39; \
                    &#39;class&#39;.format(type(pert_mag)))


        kwargs[&#39;name&#39;] = STRNAMES.GLV_PARAMETERS
        pl.variables.MVN.__init__(self, mean=None, cov=None, dtype=float, **kwargs)

        self.n_taxa = self.G.data.n_taxa
        self.growth = growth
        self.self_interactions = self_interactions
        self.interactions = interactions
        self.pert_mag = pert_mag
        self.clustering = interactions.clustering

        # These serve no functional purpose but we do it so that they are
        # connected in the graph structure. Each of these should have their
        # prior already initialized
        self.add_parent(self.growth)
        self.add_parent(self.self_interactions)
        self.add_parent(self.interactions)

    def __str__(self) -&gt; str:
        &#39;&#39;&#39;Make it more readable
        &#39;&#39;&#39;
        try:
            a = &#39;Growth:\n{}\nSelf Interactions:\n{}\nInteractions:\n{}\nPerturbations:\n{}\n&#39; \
                &#39;Acceptances:\n{}&#39;.format(
                self.growth.value, self.self_interactions.value,
                str(self.G[STRNAMES.CLUSTER_INTERACTION_VALUE]),
                str(self.pert_mag), np.mean(
                    self.acceptances[ np.max([self.sample_iter-50, 0]):self.sample_iter], axis=0))
        except:
            a = &#39;Growth:\n{}\nSelf Interactions:\n{}\nInteractions:\n{}\nPerturbations:\n{}&#39;.format(
                self.growth.value, self.self_interactions.value,
                str(self.G[STRNAMES.CLUSTER_INTERACTION_VALUE]),
                str(self.pert_mag))
        return a

    def initialize(self, update_jointly_pert_inter: bool):
        &#39;&#39;&#39;The interior objects are initialized by themselves. Define which variables
        get updated together.

        Note that the interactions and perturbations will always be updated before the
        growth rates and the self-interactions

        Interactions and perturbations
        ------------------------------
        These are conjugate and have a normal prior. If these are said to be updated
        jointly then we can sample directly with Gibbs sampling.

        Growths and self-interactions
        -----------------------------
        These are conjugate and have a truncated normal prior. If they are set to be 
        updated together, then we must do MH because we cannot sample from a truncated
        multivariate gaussian.

        Parameters
        ----------
        update_jointly_pert_inter : bool
            If True, update the interactions and the perturbations jointly.
            If False, update the interactions and perturbations separately - you
            randomly choose which one to update first.
        &#39;&#39;&#39;
        self._there_are_perturbations = self.G.perturbations is not None
        if not pl.isbool(update_jointly_pert_inter):
            raise TypeError(&#39;`update_jointly_pert_inter` ({}) must be a bool&#39;.format(
                type(update_jointly_pert_inter)))

        self.update_jointly_pert_inter = update_jointly_pert_inter
        self.sample_iter = 0
                
    # @profile
    def asarray(self) -&gt; np.ndarray:
        &#39;&#39;&#39;Builds the full regression coefficient vector.
        &#39;&#39;&#39;
        # build the entire thing
        a = np.append(self.growth.value, self.self_interactions.value)
        a = np.append(a, self.interactions.obj.get_values(use_indicators=True))
        return a

    def update(self):
        &#39;&#39;&#39;Either updated jointly using multivariate normal or update independently
        using truncated normal distributions for growth and self-interactions.

        Always update the one that the interactions is in first
        &#39;&#39;&#39;
        self._update_perts_and_inter()
        if self._there_are_perturbations:
            self.G.data.design_matrices[STRNAMES.GROWTH_VALUE].build_with_perturbations()
        
        # Update growth and self-interactions
        if pl.random.misc.fast_sample_standard_uniform():
            self.growth.update()
            self.self_interactions.update()
        else:
            self.self_interactions.update()
            self.growth.update()

        self.sample_iter += 1

        if self._there_are_perturbations:
            # If there are perturbations then we need to update their
            # matrix because the growths changed
            self.G.data.design_matrices[STRNAMES.PERT_VALUE].update_values()

    # @profile
    def _update_perts_and_inter(self):
        &#39;&#39;&#39;Update the with Gibbs sampling of a multivariate normal.

        Parameters
        ----------
        args : tuple
            This is a tuple of length &gt; 1 that holds the variables on what to update
            together
        &#39;&#39;&#39;
        if not self.update_jointly_pert_inter:
            # Update separately
            # -----------------
            if pl.random.misc.fast_sample_standard_uniform() &lt; 0.5:
                self.G[STRNAMES.CLUSTER_INTERACTION_VALUE].update()
                if self._there_are_perturbations:
                    self.G[STRNAMES.PERT_VALUE].update()
            else:
                if self._there_are_perturbations:
                    self.G[STRNAMES.PERT_VALUE].update()
                self.G[STRNAMES.CLUSTER_INTERACTION_VALUE].update()
        else:
            # Update jointly
            # --------------
            rhs = [] # Right hand side. This is what we are marginalizing over
            lhs = [] # Left hand side. This is what we are conditioning on
            if self.interactions.obj.sample_iter &gt;= self.interactions.delay:
                rhs.append(STRNAMES.CLUSTER_INTERACTION_VALUE)
            else:
                lhs.append(STRNAMES.CLUSTER_INTERACTION_VALUE)
            if self._there_are_perturbations:
                if self.pert_mag.sample_iter &gt;= self.pert_mag.delay:
                    rhs.append(STRNAMES.PERT_VALUE)
                else:
                    lhs.append(STRNAMES.PERT_VALUE)

            if len(rhs) == 0:
                return

            lhs += [STRNAMES.GROWTH_VALUE, STRNAMES.SELF_INTERACTION_VALUE]
            X = self.G.data.construct_rhs(keys=rhs)
            if X.shape[1] == 0:
                logging.info(&#39;No columns, skipping&#39;)
                return
            y = self.G.data.construct_lhs(keys=lhs,
                kwargs_dict={STRNAMES.GROWTH_VALUE:{&#39;with_perturbations&#39;: False}})

            process_prec = self.G[STRNAMES.PROCESSVAR].build_matrix(
                cov=False, sparse=True)
            prior_prec = build_prior_covariance(G=self.G, cov=False,
                order=rhs, sparse=True)
            prior_means = build_prior_mean(G=self.G,order=rhs).reshape(-1,1)

            # Make the prior covariance matrix and process varaince
            prec = X.T @ process_prec @ X + prior_prec
            self.cov.value = pinv(prec, self)
            self.mean.value = np.asarray(self.cov.value @ (X.T @ process_prec.dot(y) + \
                prior_prec @ prior_means)).ravel()

            # sample posterior jointly and then assign the values to each coefficient
            # type, respectfully
            try:
                value = self.sample()
            except:
                logging.critical(&#39;failed here, updating separately&#39;)
                self.pert_mag.update()
                self.interactions.update()
                return

            i = 0
            if STRNAMES.CLUSTER_INTERACTION_VALUE in rhs:
                l = self.interactions.obj.num_pos_indicators()
                self.interactions.value = value[:l]
                self.interactions.set_values(arr=value[:l], use_indicators=True)
                self.interactions.update_str()
                i += l
            if self._there_are_perturbations:
                if STRNAMES.PERT_VALUE in rhs:
                    self.pert_mag.value = value[i:]
                    self.pert_mag.set_values(arr=value[i:], use_indicators=True)
                    self.pert_mag.update_str()
                    self.G.data.design_matrices[STRNAMES.GROWTH_VALUE].update_value()
                    # self.G.data.design_matrices[STRNAMES.PERT_VALUE].build()

    def add_trace(self):
        &#39;&#39;&#39;Trace values for growth, self-interactions, and cluster interaction values
        &#39;&#39;&#39;
        self.growth.add_trace()
        self.self_interactions.add_trace()
        self.interactions.add_trace()
        if self._there_are_perturbations:
            self.pert_mag.add_trace()

    def set_trace(self):
        self.growth.set_trace()
        self.self_interactions.set_trace()
        self.interactions.set_trace()
        if self._there_are_perturbations:
            self.pert_mag.set_trace()


# Perturbations
# -------------
class PerturbationMagnitudes(pl.variables.Normal):
    &#39;&#39;&#39;These update the perturbation values jointly.
    &#39;&#39;&#39;
    def __init__(self, **kwargs):
        &#39;&#39;&#39;Parameters
        &#39;&#39;&#39;

        kwargs[&#39;name&#39;] = STRNAMES.PERT_VALUE
        pl.variables.Normal.__init__(self, loc=None, scale2=None, dtype=float, **kwargs)
        self.perturbations = self.G.perturbations # mdsine2.pylab.base.Perturbations 

    def __str__(self) -&gt; str:
        s = &#39;Perturbation Magnitudes (multiplicative)&#39;
        for perturbation in self.perturbations:
            s += &#39;\n\t perturbation {}: {}&#39;.format(
                perturbation.name, perturbation.cluster_array(only_pos_ind=True))
        return s

    def __len__(self) -&gt; int:
        &#39;&#39;&#39;Return the number of on indicators
        &#39;&#39;&#39;
        n = 0
        for perturbation in self.perturbations:
            n += perturbation.indicator.num_on_clusters()
        return n

    def set_values(self, arr: np.ndarray, use_indicators: bool=True):
        &#39;&#39;&#39;Set the values of the perturbation of them stacked one on top of each other

        Parameters
        ----------
        arr : np.ndarray
            Values for all of the perturbations in order
        use_indicators : bool
            If True, the values only refer to the on indicators
        &#39;&#39;&#39;
        i = 0
        for perturbation in self.perturbations:
            l = perturbation.indicator.num_on_clusters()
            perturbation.set_values_from_array(values=arr[i:i+l],
                use_indicators=use_indicators)
            i += l

    def update_str(self):
        return

    @property
    def sample_iter(self):
        return self.perturbations[0].sample_iter

    def initialize(self, value_option: str, value: np.ndarray=None, 
        loc: Union[float, int]=None, delay: int=0):
        &#39;&#39;&#39;Initialize the prior and the value of the perturbation. We assume that
        each perturbation has the same hyperparameters for the prior

        Parameters
        ----------
        value_option : str
            How to initialize the values. Options:
                &#39;manual&#39;
                    Set the value manually, `value` must also be specified
                &#39;zero&#39;
                    Set all the values to zero.
                &#39;auto&#39;, &#39;prior-mean&#39;
                    Initialize to the same value as the prior mean
        delay : int, None
            How many MCMC iterations to delay the update of the values.
        loc, value : int, float, array
            - Only necessary if any of the options are &#39;manual&#39;
        &#39;&#39;&#39;
        if delay is None:
            delay = 0
        if not pl.isint(delay):
            raise TypeError(&#39;`delay` ({}) must be an int&#39;.format(type(delay)))
        if delay &lt; 0:
            raise ValueError(&#39;`delay` ({}) must be &gt;= 0&#39;.format(delay))
        self.delay = delay
        for perturbation in self.perturbations:
            perturbation.magnitude.set_signal_when_clusters_change(True)

        # Set the value of the perturbations
        if not pl.isstr(value_option):
            raise TypeError(&#39;`value_option` ({}) must be a str&#39;.format(type(value_option)))
        if value_option == &#39;manual&#39;:
            for pidx, perturbation in enumerate(self.perturbations):
                v = value[pidx]
                for cidx, val in enumerate(v):
                    cid = perturbation.clustering.order[cidx]
                    perturbation.indicator.value[cid] = not np.isnan(val)
                    perturbation.magnitude.value[cid] = val if not np.isnan(val) else 0

        elif value_option == &#39;zero&#39;:
            for perturbation in self.perturbations:
                for cid in perturbation.clustering.order:
                    perturbation.magnitude.value[cid] = 0
        elif value_option in [&#39;auto&#39;, &#39;prior-mean&#39;]:
            for perturbation in self.perturbations:
                loc = perturbation.magnitude.prior.loc.value
                for cid in perturbation.clustering.order:
                    perturbation.magnitude.value[cid] = loc
        else:
            raise ValueError(&#39;`value_option` ({}) not recognized&#39;.format(value_option))


        s = &#39;Perturbation magnitude initialization results:\n&#39;
        for perturbation in self.perturbations:
            if perturbation.name is not None:
                a = perturbation.name
            s += &#39;\tPerturbation {}:\n&#39; \
                &#39;\t\tvalue: {}\n&#39;.format(a, perturbation.magnitude.cluster_array())
        logging.info(s)

    def update(self):
        &#39;&#39;&#39;Update with a gibbs step jointly
        &#39;&#39;&#39;
        if self.sample_iter &lt; self.delay:
            return

        n_on = [perturbation.indicator.num_on_clusters() for perturbation in \
            self.perturbations]
        
        if n_on == 0:
            return

        rhs = [STRNAMES.PERT_VALUE]
        lhs = [
            STRNAMES.GROWTH_VALUE,
            STRNAMES.SELF_INTERACTION_VALUE,
            STRNAMES.CLUSTER_INTERACTION_VALUE]
        X = self.G.data.construct_rhs(keys=rhs, toarray=True)
        y = self.G.data.construct_lhs(keys=lhs,
            kwargs_dict={STRNAMES.GROWTH_VALUE:{
                &#39;with_perturbations&#39;:False}})

        process_prec = self.G[STRNAMES.PROCESSVAR].prec
        prior_prec = build_prior_covariance(G=self.G, cov=False, order=rhs, sparse=False)

        prior_mean = build_prior_mean(G=self.G, order=rhs).reshape(-1,1)

        a = X.T * process_prec
        prec = a @ X + prior_prec
        cov = pinv(prec, self)
        mean = np.asarray(cov @ (a @ y + prior_prec @ prior_mean)).ravel()

        # print(&#39;\n\ny\n&#39;,np.hstack((y, self.G.data.lhs.vector.reshape(-1,1))))
        # print(self.G[STRNAMES.CLUSTER_INTERACTION_VALUE].value)
        # print(self.G[STRNAMES.GROWTH_VALUE].value)
        # print(self.G[STRNAMES.SELF_INTERACTION_VALUE].value)

        self.loc.value = mean
        self.scale2.value = np.diag(cov)
        value = self.sample()

        if np.any(np.isnan(value)):
            logging.critical(&#39;mean: {}&#39;.format(self.loc.value))
            logging.critical(&#39;var: {}&#39;.format(self.scale2.value))
            logging.critical(&#39;value: {}&#39;.format(self.value))
            logging.critical(&#39;prior mean: {}&#39;.format(prior_mean.ravel()))
            raise ValueError(&#39;`Values in {} are nan: {}&#39;.format(self.name, self.value))

        i = 0
        for pidx, perturbation in enumerate(self.perturbations):
            perturbation.set_values_from_array(value[i:i+n_on[pidx]], use_indicators=True)
            i += n_on[pidx]

        # Rebuild the design matrix
        self.G.data.design_matrices[STRNAMES.GROWTH_VALUE].build_with_perturbations()

    def set_trace(self, *args, **kwargs):
        for perturbation in self.perturbations:
            perturbation.set_trace(*args, **kwargs)

    def add_trace(self, *args, **kwargs):
        for perturbation in self.perturbations:
            perturbation.add_trace(*args, **kwargs)

    def asarray(self) -&gt; np.ndarray:
        &#39;&#39;&#39;Get an array of the perturbation magnitudes
        &#39;&#39;&#39;
        a = []
        for perturbation in self.perturbations:
            a.append(perturbation.cluster_array(only_pos_ind=True))
        return np.asarray(list(itertools.chain.from_iterable(a)))

    def toarray(self) -&gt; np.ndarray:
        &#39;&#39;&#39;Alias for asarray
        &#39;&#39;&#39;
        return self.asarray()

    def visualize(self, basepath: str, pidx: int, section: str=&#39;posterior&#39;, taxa_formatter: str=&#39;%(name)s&#39;,
        fixed_clustering: bool=False):
        &#39;&#39;&#39;Render the traces in the folder `basepath`. Makes a `pandas.DataFrame` table
        where the index is the Taxa name in `taxa_formatter` and the columns are
        `mean`, `median`, `25th percentile`, `75th` percentile`.

        Parameters
        ----------
        basepath : str
            This is the loction to write the files to
        section : str
            Section of the trace to compute on. Options:
                &#39;posterior&#39; : posterior samples
                &#39;burnin&#39; : burn-in samples
                &#39;entire&#39; : both burn-in and posterior samples
        taxa_formatter : str, None
            This is the format of the label to return for each Taxa. If None, it will return
            the taxon&#39;s name
        fixed_clustering : bool
            If True, plot the variable as if clustering was fixed. Since the
            cluster assignments never change, all of the taxa within the cluster
            have identical values, so we can choose any taxon within the cluster to
            represent it
        &#39;&#39;&#39;
        perturbation = self.perturbations[pidx]
        taxa = self.G.data.taxa
        clustering = self.G[STRNAMES.CLUSTERING_OBJ]

        summ = pl.summary(perturbation, set_nan_to_0=True, section=section)
        data = np.asarray([arr for _,arr in summ.items()]).T
        if fixed_clustering:
            data_clustering = []
            for cluster in clustering:
                aidx = list(cluster.members)[0]
                data_clustering.append(data[aidx, :])
            index = [&#39;Cluster {}&#39;.format(cidx + 1) for cidx in range(len(clustering))]
            data = data_clustering
        else:
            index = [taxon.name for taxon in self.G.data.taxa]
        df = pd.DataFrame(data, columns=[k for k in summ], 
            index=index)
        df.to_csv(os.path.join(basepath, &#39;values.tsv&#39;), sep=&#39;\t&#39;, index=True, header=True)

        if section == &#39;posterior&#39;:
            len_posterior = self.G.inference.sample_iter + 1 - self.G.inference.burnin
        elif section == &#39;burnin&#39;:
            len_posterior = self.G.inference.burnin
        else:
            len_posterior = self.G.inference.sample_iter + 1

        # Make the prior
        if self.G.inference.tracer.is_being_traced(STRNAMES.PRIOR_MEAN_PERT):
            prior_mean_trace = perturbation.magnitude.prior.loc.get_trace_from_disk(section=section)
        else:
            prior_mean_trace = np.ones(len_posterior) + perturbation.magnitude.prior.loc.value
        if self.G.inference.tracer.is_being_traced(STRNAMES.PRIOR_VAR_PERT):
            prior_std_trace = np.sqrt(perturbation.magnitude.prior.scale2.get_trace_from_disk(section=section))
        else:
            prior_std_trace = np.ones(len_posterior) + np.sqrt(perturbation.magnitude.prior.scale2.value)
        
        if fixed_clustering:
            rang = len(clustering)
        else:
            rang = len(self.G.data.taxa)

        for iii in range(rang):
            if fixed_clustering:
                cluster = clustering[iii]
                oidx = list(cluster.members)[0]
            else:
                oidx = iii
            fig = plt.figure()
            ax_posterior = fig.add_subplot(1,2,1)
            visualization.render_trace(var=perturbation, idx=oidx, plt_type=&#39;hist&#39;,
                label=section, color=&#39;blue&#39;, ax=ax_posterior, section=section,
                include_burnin=True, rasterized=True)

            low_x, high_x = ax_posterior.get_xlim()
            arr = np.zeros(len(prior_std_trace), dtype=float)
            for i in range(len(prior_std_trace)):
                arr[i] = pl.random.normal.sample(loc=prior_mean_trace[i], scale=prior_std_trace[i])
            visualization.render_trace(var=arr, plt_type=&#39;hist&#39;, 
                label=&#39;prior&#39;, color=&#39;red&#39;, ax=ax_posterior, rasterized=True)

            ax_posterior.legend()
            ax_posterior.set_xlim(left=low_x*.8, right=high_x*1.2)

            # plot the trace
            ax_trace = fig.add_subplot(1,2,2)
            visualization.render_trace(var=perturbation, idx=oidx, plt_type=&#39;trace&#39;, 
                ax=ax_trace, section=section, include_burnin=True, rasterized=True)

            if fixed_clustering:
                fig.suptitle(&#39;Cluster {}&#39;.format(iii+1))
            else:
                fig.suptitle(&#39;{}&#39;.format(pl.taxaname_formatter(
                    format=taxa_formatter, taxon=oidx, taxa=self.G.data.taxa)))
            fig.tight_layout()
            fig.subplots_adjust(top=0.85)
            if fixed_clustering:
                plt.savefig(os.path.join(basepath, &#39;cluster{}.pdf&#39;.format(iii+1)))
            else:
                plt.savefig(os.path.join(basepath, &#39;{}.pdf&#39;.format(taxa[oidx].name)))
            plt.close()


class PerturbationProbabilities(pl.Node):
    &#39;&#39;&#39;This is the probability for a positive interaction for a perturbation
    &#39;&#39;&#39;
    def __init__(self, **kwargs):
        kwargs[&#39;name&#39;] = STRNAMES.PERT_INDICATOR_PROB
        pl.Node.__init__(self, **kwargs)
        self.perturbations = self.G.perturbations # mdsine2.pylab.base.Perturbations

    def __str__(self) -&gt; str:
        s = &#39;Perturbation Indicator probabilities&#39;
        for perturbation in self.perturbations:
            s += &#39;\n\tperturbation {}: {}&#39;.format(
                perturbation.name,
                perturbation.probability.value)
        return s

    @property
    def sample_iter(self) -&gt; int:
        return self.perturbations[0].probability.sample_iter

    def initialize(self, value_option: str, hyperparam_option: str, a: float=None, 
        b: float=None, value: Union[float, int]=None, N: Union[str, int]=&#39;auto&#39;, delay: int=0):
        &#39;&#39;&#39;Initialize the hyperparameters of the prior and the value. Each
        perturbation has the same prior.

        Parameters
        ----------
        value_option : str
            How to initialize the values. Options:
                &#39;manual&#39;
                    Set the value manually, `value` must also be specified
                &#39;auto&#39;, &#39;prior-mean&#39;
                    Initialize the value as the prior mean
        hyperparam_option : str
            How to initialize `a` and `b`. Options:
                &#39;manual&#39;
                    Set the value manually. `a` and `b` must also be specified
                &#39;weak-agnostic&#39; or &#39;auto&#39;
                    a=b=0.5
                &#39;strong-dense&#39;
                    a = N, N are the expected number of clusters
                    b = 0.5
                &#39;strong-sparse&#39;
                    a = 0.5
                    b = N, N are the expected number of clusters
                &#39;mult-sparse-M&#39;
                    a = 0.5
                    b = N*M(N*M-1), N are the expected number of clusters
        N : str, int
            This is the number of clusters to set the hyperparam options to 
            (if they are dependent on the number of cluster). If &#39;auto&#39;, set to the expected number
            of clusters from a dirichlet process. Else use this number (must be an int).
        delay : int
            How many MCMC iterations to delay starting the update of the variable
        value, a, b : int, float
            User specified values
            Only necessary if `hyperparam_option` == &#39;manual&#39;
        &#39;&#39;&#39;
        if not pl.isint(delay):
            raise TypeError(&#39;`delay` ({}) must be an int&#39;.format(type(delay)))
        if delay &lt; 0:
            raise ValueError(&#39;`delay` ({}) must be &gt;= 0&#39;.format(delay))
        self.delay = delay

        # Set the hyper-parameters
        if not pl.isstr(hyperparam_option):
            raise ValueError(&#39;`hyperparam_option` ({}) must be a str&#39;.format(type(hyperparam_option)))
        if hyperparam_option == &#39;manual&#39;:
            if (not pl.isnumeric(a)) or (not pl.isnumeric(b)):
                raise TypeError(&#39;If `hyperparam_option` is &#34;manual&#34; then `a` ({})&#39; \
                    &#39; and `b` ({}) must be numerics&#39;.format(type(a), type(b)))
        elif hyperparam_option in [&#39;auto&#39;, &#39;weak-agnostic&#39;]:
            a = 0.5
            b = 0.5

        # Set N
        if pl.isstr(N):
            if N == &#39;auto&#39;:
                N = expected_n_clusters(G=self.G)
            elif N == &#39;fixed-clustering&#39;:
                N = len(self.G[STRNAMES.CLUSTERING_OBJ])
            else:
                raise ValueError(&#39;`N` ({}) nto recognized&#39;.format(N))
        elif pl.isint(N):
            if N &lt; 0:
                raise ValueError(&#39;`N` ({}) must be positive&#39;.format(N))
        else:
            raise TypeError(&#39;`N` ({}) type not recognized&#39;.format(type(N)))

        
        if hyperparam_option == &#39;strong-dense&#39;:
            a = N
            b = 0.5
        elif hyperparam_option == &#39;strong-sparse&#39;:
            a = 0.5
            b = N
        elif &#39;mult-sparse&#39; in hyperparam_option:
            try:
                M = float(hyperparam_option.replace(&#39;mult-sparse-&#39;, &#39;&#39;))
            except:
                raise ValueError(&#39;`mult-sparse` in the wrong format ({})&#39;.format(
                    hyperparam_option))
            N = N * M
            a = 0.5
            b = N

        for perturbation in self.perturbations:
            perturbation.probability.prior.a.override_value(a)
            perturbation.probability.prior.b.override_value(b)

        # Set the value
        if not pl.isstr(value_option):
            raise TypeError(&#39;`value_option` ({}) must be a str&#39;.format(type(value_option)))
        if value_option == &#39;manual&#39;:
            if not pl.isnumeric(value):
                raise TypeError(&#39;If `value_option` is &#34;manual&#34; then `value` ({})&#39; \
                    &#39; must be a numeric&#39;.format(type(value)))
            for perturbation in self.perturbations:
                perturbation.probability.value = value
        elif value_option in [&#39;auto&#39;, &#39;prior-mean&#39;]:
            for perturbation in self.perturbations:
                perturbation.probability.value = perturbation.probability.prior.mean()
        else:
            raise ValueError(&#39;`value_option` ({}) not recognized&#39;.format(value_option))

        s = &#39;Perturbation indicator probability initialization results:\n&#39;
        for i, perturbation in enumerate(self.perturbations):
            s += &#39;\tPerturbation {}:\n&#39; \
                &#39;\t\tprior a: {}\n&#39; \
                &#39;\t\tprior b: {}\n&#39; \
                &#39;\t\tvalue: {}\n&#39;.format(i,
                    perturbation.probability.prior.a.value,
                    perturbation.probability.prior.b.value,
                    perturbation.probability.value)
        logging.info(s)

    def update(self):
        &#39;&#39;&#39;Update according to how many positive and negative indicators there
        are
        &#39;&#39;&#39;
        if self.sample_iter &lt; self.delay:
            return
        for perturbation in self.perturbations:
            num_pos = perturbation.indicator.num_on_clusters()
            num_neg = len(perturbation.clustering.clusters) - num_pos
            perturbation.probability.a.value = perturbation.probability.prior.a.value + num_pos
            perturbation.probability.b.value = perturbation.probability.prior.b.value + num_neg
            perturbation.probability.sample()

    def set_trace(self, *args, **kwargs):
        for perturbation in self.perturbations:
            perturbation.probability.set_trace(*args, **kwargs)

    def add_trace(self, *args, **kwargs):
        for perturbation in self.perturbations:
            perturbation.probability.add_trace(*args, **kwargs)

    def visualize(self, path: str, f: IO, pidx: int, section: str=&#39;posterior&#39;) -&gt; IO:
        &#39;&#39;&#39;Visualize the `pidx`th perturbation prior magnitude

        Parameters
        ----------
        obj : mdsine2.Variable
        path : str
            This is the path to write the files to
        f : _io.TextIOWrapper
            File that we are writing the values to
        section : str
            Section of the trace to compute on. Options:
                &#39;posterior&#39; : posterior samples
                &#39;burnin&#39; : burn-in samples
                &#39;entire&#39; : both burn-in and posterior samples

        Returns
        -------
        _io.TextIOWrapper
        &#39;&#39;&#39;
        if not self.G.inference.is_in_inference_order(self.name):
            return f
        return _scalar_visualize(path=path, f=f, section=section,
            obj=self.perturbations[pidx].probability, log_scale=False)


class PerturbationIndicators(pl.Node):
    &#39;&#39;&#39;This is the indicator for a perturbation

    We only need to trace once for the perturbations. Our default is to only
    trace from the magnitudes. Thus, we only trace the indicators (here) if
    we are learning here and not learning the magnitudes.
    &#39;&#39;&#39;
    def __init__(self, need_to_trace: bool, relative: bool, **kwargs):
        &#39;&#39;&#39;Parameters
        &#39;&#39;&#39;
        kwargs[&#39;name&#39;] = STRNAMES.PERT_INDICATOR
        pl.Node.__init__(self, **kwargs)
        self.need_to_trace = need_to_trace
        self.perturbations = self.G.perturbations # mdsine2.pyab.base.Perturbations
        self.clustering = None # mdsine2.pylab.cluster.Clustering
        self._time_taken = None
        if relative:
            self.update = self.update_relative
        else:
            self.update = self.update_slow

    def __str__(self) -&gt; str:
        s = &#39;Perturbation Indicators - time: {}s&#39;.format(self._time_taken)
        for perturbation in self.perturbations:
            arr = perturbation.indicator.cluster_bool_array()
            s += &#39;\nperturbation {} ({}/{}): {}&#39;.format(perturbation.name,
                np.sum(arr), len(arr), arr)
        return s

    @property
    def sample_iter(self) -&gt; int:
        return self.perturbations[0].sample_iter

    def add_trace(self):
        &#39;&#39;&#39;Only trace if perturbation indicators are being learned and the
        perturbation value is not being learned
        &#39;&#39;&#39;
        if self.need_to_trace:
            for perturbation in self.perturbations:
                perturbation.add_trace()

    def set_trace(self, *args, **kwargs):
        &#39;&#39;&#39;Only trace if perturbation indicators are being learned and the
        perturbation value is not being learned
        &#39;&#39;&#39;
        if self.need_to_trace:
            for perturbation in self.perturbations:
                perturbation.set_trace(*args, **kwargs)

    def initialize(self, value_option: str, p: float=None, delay: int=0):
        &#39;&#39;&#39;Initialize the based on the passed in option.

        Parameters
        ----------
        value_option : str
            - Different ways to initialize the values. Options:
                - &#39;auto&#39;, &#39;all-off&#39;
                    - Turn all of the indicators off
                - &#39;all-on&#39;
                    - Turn all the indicators on
                - &#39;random&#39;
                    - Randomly assign the indicator with probability `p`
        p : float
            Only required if `value_option` == &#39;random&#39;
        delay : int
            How many Gibbs steps to delay updating the values
        &#39;&#39;&#39;
        # print(&#39;in pert ind&#39;)
        if not pl.isint(delay):
            raise TypeError(&#39;`delay` ({}) must be an int&#39;.format(type(delay)))
        if delay &lt; 0:
            raise ValueError(&#39;`delay` ({}) must be &gt;= 0&#39;.format(delay))
        self.delay = delay

        for perturbation in self.perturbations:
            perturbation.indicator.set_signal_when_clusters_change(True)
        self.clustering = self.G.perturbations[0].indicator.clustering # mdsine2.pylab.cluster.Clustering

        # Set the value
        if not pl.isstr(value_option):
            raise ValueError(&#39;`value_option` ({}) must be a str&#39;.format(type(value_option)))
        if value_option in [&#39;all-off&#39;, &#39;auto&#39;]:
            value = False
        elif value_option == &#39;all-on&#39;:
            value = True
        elif value_option == &#39;random&#39;:
            if not pl.isfloat(p):
                raise TypeError(&#39;`p` ({}) must be a float&#39;.format(type(p)))
            if p &lt; 0 or p &gt; 1:
                raise ValueError(&#39;`p` ({}) must be [0,1]&#39;.format(p))
        else:
            raise ValueError(&#39;`value_option` ({}) not recognized&#39;.format(value_option))
        for perturbation in self.perturbations:
            for cid in perturbation.clustering.clusters:
                if value_option == &#39;random&#39;:
                    perturbation.indicator.value[cid] = bool(pl.random.bernoulli.sample(p))
                else:
                    perturbation.indicator.value[cid] = value

        # These are for the function `self._make_idx_for_clusters`
        self.ndts_bias = []
        self.n_replicates = self.G.data.n_replicates
        self.n_perturbations = len(self.G.perturbations)
        self.n_dts_for_replicate = self.G.data.n_dts_for_replicate
        self.total_dts = np.sum(self.n_dts_for_replicate)
        self.replicate_bias = np.zeros(self.n_replicates, dtype=int)
        self.n_taxa = len(self.G.data.taxa)
        for ridx in range(1, self.n_replicates):
            self.replicate_bias[ridx] = self.replicate_bias[ridx-1] + \
                self.n_taxa * self.n_dts_for_replicate[ridx - 1]
        for ridx in range(self.G.data.n_replicates):
            self.ndts_bias.append(
                np.arange(0, self.G.data.n_dts_for_replicate[ridx] * self.n_taxa, self.n_taxa))

        s = &#39;Perturbation indicator initialization results:\n&#39;
        for i, perturbation in enumerate(self.perturbations):
            s += &#39;\tPerturbation {}:\n&#39; \
                &#39;\t\tindicator: {}\n&#39;.format(i, perturbation.indicator.cluster_bool_array())
        logging.info(s)

    def _make_idx_for_clusters(self) -&gt; Dict[int, np.ndarray]:
        &#39;&#39;&#39;Creates a dictionary that maps the cluster id to the
        rows that correspond to each Taxa in the cluster.

        We cannot cast this with numba because it does not support Fortran style
        raveling :(.

        Returns
        -------
        dict: int -&gt; np.ndarray
            Maps the cluster ID to the row indices corresponding to it
        &#39;&#39;&#39;
        clusters = [np.asarray(oidxs, dtype=int).reshape(-1,1) \
            for oidxs in self.clustering.tolistoflists()]
        n_dts=self.G.data.n_dts_for_replicate

        d = {}
        cids = self.clustering.order

        for cidx,cid in enumerate(cids):
            a = np.zeros(len(clusters[cidx]) * self.total_dts, dtype=int)
            i = 0
            for ridx in range(self.n_replicates):
                idxs = np.zeros(
                    (len(clusters[cidx]),
                    self.n_dts_for_replicate[ridx]), int)
                idxs = idxs + clusters[cidx]
                idxs = idxs + self.ndts_bias[ridx]
                idxs = idxs + self.replicate_bias[ridx]
                idxs = idxs.ravel(&#39;F&#39;)
                l = len(idxs)
                a[i:i+l] = idxs
                i += l

            d[cid] = a
        
        if self.G.data.zero_inflation_transition_policy is not None:
            # We need to convert the indices that are meant from no zero inflation to 
            # ones that take into account zero inflation - use the array from 
            # `data.Data._setrows_to_include_zero_inflation`. If the index should be
            # included, then we subtract the number of indexes that are previously off
            # before that index. If it should not be included then we exclude it
            prevoff_arr = self.G.data.off_previously_arr_zero_inflation
            rows_to_include = self.G.data.zero_inflation_transition_policy
            for cid in d:
                arr = d[cid]
                new_arr = np.zeros(len(arr), dtype=int)
                n = 0
                for i, idx in enumerate(arr):
                    if rows_to_include[idx]:
                        new_arr[n] = idx - prevoff_arr[i]
                        n += 1
                new_arr = new_arr[:n]
        return d

    # @profile
    def make_rel_params(self):
        &#39;&#39;&#39;We make the parameters needed to update the relative log-likelihod.
        This function is called once at the beginning of the update.

        THIS ASSUMES THAT EACH PERTURBATION CLUSTERS ARE DEFINED BY THE SAME CLUSTERS
            - To make this separate, make a higher level list for each perturbation index
              for each individual perturbation

        Parameters that we create with this function
        --------------------------------------------
        - ys : dict (int -&gt; np.ndarray)
            - Maps the target cluster id to the observation matrix that it
              corresponds to (only the Taxa in the target cluster). This 
              array already has the growth and self-interactions subtracted
              out:
                - $ \frac{log(x_{k+1}) - log(x_{k})}{dt} - a_{1,k} - a_{2,k}x_{k} $
        - process_precs : dict (int -&gt; np.ndarray)
            - Maps the target cluster id to the vector of the process precision
              that corresponds to the target cluster (only the Taxa in the target
              cluster). This is a 1D array that corresponds to the diagonal of what
              would be the precision matrix.
        - interactionXs : dict (int -&gt; np.ndarray)
            - Maps the target cluster id to the design matrix for the interactions
              going into that cluster. We pre-index it with the rows and columns
        - prior_prec_interaction : dict (int -&gt; np.ndarray)
            - Maps the target cluster id to to the diagonal of the prior precision 
              for the interaction values.
        - prior_mean_interaction : dict (int -&gt; np.ndarray)
            - Maps the target cluster id to to the diagonal of the prior mean 
              for the interaction values.
        - prior_ll_ons : np.ndarray
            - Prior log likelihood of a positive indicator. These are separate for each
              perturbation.
        - prior_ll_offs : np.ndarray
            - Prior log likelihood of the negative indicator. These are separate for each
              perturbation.
        - priorvar_logdet_diffs : np.ndarray
            - This is the prior variance log determinant that we add when the indicator
              is positive. This is different for each perturbation.
        - perturbationsXs : dict (int -&gt; np.ndarray)
            - Maps the target cluster id to the design matrix that corresponds to 
              the on perturbations of the target clusters. This is preindexed by the 
              rows but not the columns - the columns assume that all of the perturbations
              are on and we index the ones that we want.
        - prior_prec_perturbations : np.ndarray
            - This is the prior precision of the magnitude for each of the perturbations. Use
              the perturbation index to get the value
        - prior_mean_perturbations : np.ndarray
            - This is the prior mean of the magnitude for each one of the perturbations. Use
              the perturbation index to get the value
        &#39;&#39;&#39;
        row_idxs = self._make_idx_for_clusters()

        # Create ys
        self.ys = {}
        y = self.G.data.construct_lhs(keys=[
            STRNAMES.SELF_INTERACTION_VALUE, STRNAMES.GROWTH_VALUE],
            kwargs_dict={STRNAMES.GROWTH_VALUE:{&#39;with_perturbations&#39;: False}})
        for tcid in self.clustering.order:
            self.ys[tcid] = y[row_idxs[tcid], :]

        # Create process_precs
        self.process_precs = {}
        process_prec_diag = self.G[STRNAMES.PROCESSVAR].prec
        for tcid in self.clustering.order:
            self.process_precs[tcid] = process_prec_diag[row_idxs[tcid]]

        # Make interactionXs
        self.interactionXs = {}
        interactions = self.G[STRNAMES.INTERACTIONS_OBJ]
        XM_master = self.G.data.design_matrices[STRNAMES.CLUSTER_INTERACTION_VALUE].toarray()
        for tcid in self.clustering.order:
            cols = []
            for i, interaction in enumerate(interactions.iter_valid()):
                if interaction.target_cid == tcid:
                    if interaction.indicator:
                        cols.append(i)
            cols = np.asarray(cols, dtype=int)
            self.interactionXs[tcid] = pl.util.fast_index(M=XM_master, 
                rows=row_idxs[tcid], cols=cols)

        # Make prior parameters for interactions
        self.prior_prec_interaction = 1/self.G[STRNAMES.PRIOR_VAR_INTERACTIONS].value
        self.prior_mean_interaction = self.G[STRNAMES.PRIOR_MEAN_INTERACTIONS].value

        # Make the perturbation parameters
        self.prior_ll_ons = []
        self.prior_ll_offs = []
        self.priorvar_logdet_diffs = []
        self.prior_prec_perturbations = []
        self.prior_mean_perturbations = []

        for perturbation in self.G.perturbations:
            prob_on = perturbation.probability.value
            self.prior_ll_ons.append(np.log(prob_on))
            self.prior_ll_offs.append(np.log(1 - prob_on))
            
            self.priorvar_logdet_diffs.append(
                np.log(perturbation.magnitude.prior.scale2.value))

            self.prior_prec_perturbations.append( 
                1/perturbation.magnitude.prior.scale2.value)

            self.prior_mean_perturbations.append(
                perturbation.magnitude.prior.loc.value)

        # Make perturbation matrices
        self.perturbationsXs = {}
        self.G.data.design_matrices[STRNAMES.PERT_VALUE].M.build(build=True, 
            build_for_neg_ind=True)
        Xpert_master = self.G.data.design_matrices[STRNAMES.PERT_VALUE].toarray()
        for tcid in self.clustering.order:
            self.perturbationsXs[tcid] = Xpert_master[row_idxs[tcid], :]

        self.n_clusters = len(self.clustering.order)
        self.clustering_order = self.clustering.order

        self.col2pidxcidx = []
        for pidx in range(len(self.perturbations)):
            for cidx in range(len(self.clustering.order)):
                self.col2pidxcidx.append((pidx, cidx))

        self.arr = []
        for perturbation in self.perturbations:
            self.arr = np.append(
                self.arr, 
                perturbation.indicator.cluster_bool_array())
        self.arr = np.asarray(self.arr, dtype=bool)

    # @profile
    def update_relative(self):
        &#39;&#39;&#39;Update each perturbation indicator for the given cluster by
        calculating the realtive loglikelihoods of it being on/off as
        supposed to as is. Because this is a relative loglikelihood, we
        only need to take into account the following parameters of the
        model:
            - Only the Taxa in the cluster in question
            - Only the perturbations for that cluster
            - Only the interactions going into the cluster

        Because these matrices are considerably smaller and considered &#39;dense&#39;, we
        do the operations in numpy instead of scipy sparse.

        We permute the order that the indices are updated for more robust mixing
        &#39;&#39;&#39;
        if self.sample_iter &lt; self.delay:
            return
        start_time = time.time()

        self.make_rel_params()

        # Iterate over each perturbation indicator variable
        iidxs = npr.permutation(len(self.arr))
        for iidx in iidxs:
            self.update_single_idx_fast(idx=iidx)

        # Set the perturbation indicators from arr
        i = 0
        for perturbation in self.perturbations:
            for cid in self.clustering.order:
                perturbation.indicator.value[cid] = self.arr[i]
                i += 1

        # rebuild the growth design matrix
        self.G.data.design_matrices[STRNAMES.PERT_VALUE].M.build()
        self.G.data.design_matrices[STRNAMES.GROWTH_VALUE].build_with_perturbations()
        self._time_taken = time.time() - start_time

    # @profile
    def update_single_idx_fast(self, idx: int):
        &#39;&#39;&#39;Do a Gibbs step for a single cluster
        &#39;&#39;&#39;
        pidx, cidx = self.col2pidxcidx[idx]

        prior_ll_on = self.prior_ll_ons[pidx]
        prior_ll_off = self.prior_ll_offs[pidx]

        d_on = self.calculate_relative_marginal_loglikelihood(idx=idx, val=True)
        d_off = self.calculate_relative_marginal_loglikelihood(idx=idx, val=False)

        ll_on = d_on + prior_ll_on
        ll_off = d_off + prior_ll_off
        dd = [ll_off, ll_on]

        # print(&#39;\nindicator&#39;, idx)
        # print(&#39;fast\n\ttotal: {}\n\tbeta_logdet_diff: {}\n\t&#39; \
        #     &#39;priorvar_logdet_diff: {}\n\tbEb_diff: {}\n\t&#39; \
        #     &#39;bEbprior_diff: {}&#39;.format(
        #         ll_on - ll_off,
        #         d_on[&#39;beta_logdet&#39;] - d_off[&#39;beta_logdet&#39;],
        #         d_on[&#39;priorvar_logdet&#39;] - d_off[&#39;priorvar_logdet&#39;],
        #         d_on[&#39;bEb&#39;] - d_off[&#39;bEb&#39;],
        #         d_on[&#39;bEbprior&#39;] - d_off[&#39;bEbprior&#39;]))
        # self.update_single_idx_slow(idx)

        res = bool(sample_categorical_log(dd))
        self.arr[idx] = res

    # @profile
    def calculate_relative_marginal_loglikelihood(self, idx: int, val: bool) -&gt; float:
        &#39;&#39;&#39;Calculate the relative marginal loglikelihood of switching the `idx`&#39;th index
        of the perturbation matrix to `val`

        Parameters
        ----------
        idx : int
            This is the index of the indicator we are sampling
        val : bool
            This is the value we are testing it at.

        Returns
        -------
        float
        &#39;&#39;&#39;
        # Create and get the data
        self.arr[idx] = val
        pidx, cidx = self.col2pidxcidx[idx]
        tcid = self.clustering_order[cidx]

        y = self.ys[tcid]
        process_prec = self.process_precs[tcid]
        X = self.interactionXs[tcid]

        prior_mean = []
        prior_prec_diag = []
        cols = []
        for temp_pidx in range(len(self.perturbations)):
            col = int(cidx + temp_pidx * self.n_clusters)
            if self.arr[col]:
                cols.append(col)
                prior_mean.append(self.prior_mean_perturbations[temp_pidx])
                prior_prec_diag.append(self.prior_prec_perturbations[temp_pidx])
        Xpert = self.perturbationsXs[tcid][:, cols]

        if Xpert.shape[1] + X.shape[1] == 0:
            # return {
            #     &#39;ret&#39;: 0,
            #     &#39;beta_logdet&#39;: 0,
            #     &#39;priorvar_logdet&#39;: 0,
            #     &#39;bEb&#39;: 0,
            #     &#39;bEbprior&#39;: 0}
            return 0

        prior_mean = np.append(
            prior_mean,
            np.full(X.shape[1], self.prior_mean_interaction))
        prior_prec_diag = np.append(
            prior_prec_diag,
            np.full(X.shape[1], self.prior_prec_interaction))
        X = np.hstack((Xpert, X))
        prior_prec = np.diag(prior_prec_diag)
        pm = (prior_prec_diag * prior_mean).reshape(-1,1)

        # Do the marginalization
        a = X.T * process_prec
        beta_prec = (a @ X) + prior_prec
        beta_cov = pinv(beta_prec, self)
        beta_mean = beta_cov @ ((a @ y) + pm )

        bEb = (beta_mean.T @ beta_prec @ beta_mean)[0,0]
        try:
            beta_logdet = log_det(beta_cov, self)
        except:
            logging.critical(&#39;Crashed in log_det&#39;)
            logging.critical(&#39;beta_cov:\n{}&#39;.format(beta_cov))
            logging.critical(&#39;prior_prec\n{}&#39;.format(prior_prec))
            raise
        
        if val:
            bEbprior = (self.prior_mean_perturbations[pidx]**2) * \
                self.prior_prec_perturbations[pidx]
            priorvar_logdet = self.priorvar_logdet_diffs[pidx]
        else:
            bEbprior = 0
            priorvar_logdet = 0

        ll2 = 0.5 * (beta_logdet - priorvar_logdet)
        ll3 = 0.5 * (bEb - bEbprior)

        # return {
        #     &#39;ret&#39;: ll2+ll3,
        #     &#39;beta_logdet&#39;: beta_logdet,
        #     &#39;priorvar_logdet&#39;: priorvar_logdet,
        #     &#39;bEb&#39;: bEb,
        #     &#39;bEbprior&#39;: bEbprior}
        return ll2 + ll3        

    # @profile
    def update_slow(self):
        &#39;&#39;&#39;Update each cluster indicator variable for the perturbation
        &#39;&#39;&#39;
        start_time = time.time()

        if self.sample_iter &lt; self.delay:
            return

        n_clusters = len(self.clustering.order)
        n_perturbations = len(self.perturbations)
        idxs = npr.permutation(int(n_clusters*n_perturbations))
        for idx in idxs:
            self.update_single_idx_slow(idx=idx)

        # rebuild the growth design matrix
        self.G.data.design_matrices[STRNAMES.PERT_VALUE].M.build()
        self.G.data.design_matrices[STRNAMES.GROWTH_VALUE].build_with_perturbations()
        self._time_taken = time.time() - start_time

    # @profile
    def update_single_idx_slow(self, idx: int):
        &#39;&#39;&#39;Do a Gibbs step for a single cluster and perturbation

        Parameters
        ----------
        idx : int
            This is the index of the indicator in vectorized form
        &#39;&#39;&#39;
        cidx = idx % self.G.data.n_taxa
        cid = self.clustering.order[cidx]
        
        pidx = idx // self.G.data.n_taxa
        perturbation = self.perturbations[pidx]

        d_on = self.calculate_marginal_loglikelihood(cid=cid, val=True,
            perturbation=perturbation)
        d_off = self.calculate_marginal_loglikelihood(cid=cid, val=False,
            perturbation=perturbation)

        prior_ll_on = np.log(perturbation.probability.value)
        prior_ll_off = np.log(1 - perturbation.probability.value)

        ll_on = d_on[&#39;ret&#39;] + prior_ll_on
        ll_off = d_off[&#39;ret&#39;] + prior_ll_off
        dd = [ll_off, ll_on]

        res = bool(sample_categorical_log(dd))
        if perturbation.indicator.value[cid] != res:
            perturbation.indicator.value[cid] = res
            self.G.data.design_matrices[STRNAMES.PERT_VALUE].build()

    # @profile
    def calculate_marginal_loglikelihood(self, cid: int, val: bool, 
        perturbation: pl.contrib.ClusterPerturbationEffect) -&gt; Dict[str, float]:
        &#39;&#39;&#39;Calculate the log marginal likelihood with the perturbations integrated
        out
        &#39;&#39;&#39;
        # Set parameters
        perturbation.indicator.value[cid] = val
        self.G.data.design_matrices[STRNAMES.PERT_VALUE].M.build()

        # Make matrices
        rhs = [STRNAMES.PERT_VALUE, STRNAMES.CLUSTER_INTERACTION_VALUE]
        lhs = [STRNAMES.GROWTH_VALUE, STRNAMES.SELF_INTERACTION_VALUE]
        X = self.G.data.construct_rhs(keys=rhs)
        y = self.G.data.construct_lhs(keys=lhs, 
            kwargs_dict={STRNAMES.GROWTH_VALUE:{&#39;with_perturbations&#39;: False}})
        
        if X.shape[1] == 0:
            return {
            &#39;ret&#39;: 0,
            &#39;beta_logdet&#39;: 0,
            &#39;priorvar_logdet&#39;: 0,
            &#39;bEb&#39;: 0,
            &#39;bEbprior&#39;: 0}

        process_prec = self.G[STRNAMES.PROCESSVAR].build_matrix(cov=False, sparse=False)
        prior_prec = build_prior_covariance(G=self.G, cov=False, order=rhs, sparse=False)
        prior_var = build_prior_covariance(G=self.G, cov=True, order=rhs, sparse=False)
        prior_mean = build_prior_mean(G=self.G, order=rhs, shape=(-1,1))

        # Calculate the posterior
        beta_prec = X.T @ process_prec @ X + prior_prec
        beta_cov = pinv(beta_prec, self)
        beta_mean = beta_cov @ ( X.T @ process_prec @ y + prior_prec @ prior_mean )
        beta_mean = np.asarray(beta_mean).reshape(-1,1)

        # Perform the marginalization
        try:
            beta_logdet = log_det(beta_cov, self)
        except:
            logging.critical(&#39;Crashed in log_det&#39;)
            logging.critical(&#39;beta_cov:\n{}&#39;.format(beta_cov))
            logging.critical(&#39;prior_prec\n{}&#39;.format(prior_prec))
            raise
        priorvar_logdet = log_det(prior_var, self)
        ll2 = 0.5 * (beta_logdet - priorvar_logdet)

        a = np.asarray(prior_mean.T @ prior_prec @ prior_mean)[0,0]
        b = np.asarray(beta_mean.T @ beta_prec @ beta_mean)[0,0]
        ll3 = -0.5 * (a  - b)

        return {
            &#39;ret&#39;: ll2+ll3,
            &#39;beta_logdet&#39;: beta_logdet,
            &#39;priorvar_logdet&#39;: priorvar_logdet,
            &#39;bEb&#39;: b,
            &#39;bEbprior&#39;: a}

    def total_on(self) -&gt; int:
        n = 0
        for perturbation in self.perturbations:
            n += perturbation.indicator.num_on_clusters()
        return n

    def visualize(self, path: str, pidx: int, section: str=&#39;posterior&#39;, fixed_clustering: bool=False):
        &#39;&#39;&#39;Make a table of the `pidx`th perturbation bayes factors

        Parameters
        ----------
        path : str
            This is the path to write the bayes factors
        f : _io.TextIOWrapper
            File that we are writing the values to
        section : str
            Section of the trace to compute on. Options:
                &#39;posterior&#39; : posterior samples
                &#39;burnin&#39; : burn-in samples
                &#39;entire&#39; : both burn-in and posterior samples
        fixed_clustering : bool
            If True, plot the variable as if clustering was fixed. Since the
            cluster assignments never change, all of the taxa within the cluster
            have identical values, so we can choose any taxon within the cluster to
            represent it
        &#39;&#39;&#39;
        from .util import generate_perturbation_bayes_factors_posthoc

        perturbation = self.perturbations[pidx]
        bayes_factors = generate_perturbation_bayes_factors_posthoc(
            mcmc=self.G.inference, perturbation=perturbation, section=section)
        if fixed_clustering:
            # Condense the taxa level bayes factors to cluster level bayes factors
            clustering = self.G[STRNAMES.CLUSTERING_OBJ]
            bf_clusters = []
            for cluster in clustering:
                aidx = list(cluster.members)[0]
                bf_clusters.append(bayes_factors[aidx])
            bf_clusters = np.asarray(bf_clusters)
            df = pd.DataFrame([bf_clusters], index=[perturbation.name],
                columns=[&#39;Cluster {}&#39;.format(cidx+1) for cidx in range(len(clustering))])
        else:
            df = pd.DataFrame([bayes_factors], index=[perturbation.name], 
                columns=[taxon.name for taxon in self.G.data.taxa])
        df.to_csv(path, sep=&#39;\t&#39;, index=True, header=True)


class PriorVarPerturbations(pl.Variable):
    &#39;&#39;&#39;Class iterates over the perturbations for updating the variance of the prior
    for each perturbation

    All perturbations get the same hyperparameters
    &#39;&#39;&#39;
    def __init__(self, **kwargs):
        kwargs[&#39;name&#39;] = STRNAMES.PRIOR_VAR_PERT
        pl.Variable.__init__(self, **kwargs)
        self.perturbations = self.G.perturbations # mdsine2.pylab.base.Perturbations

        if self.perturbations is None:
            raise TypeError(&#39;Only instantiate this object if there are perturbations&#39;)

    def __str__(self) -&gt; str:
        s = &#39;Perturbation Magnitude Prior Variances&#39;
        for perturbation in self.perturbations:
            s += &#39;\n\tperturbation {}: {}&#39;.format(
                perturbation.name,
                perturbation.magnitude.prior.scale2.value)
        return s

    @property
    def sample_iter(self) -&gt; int:
        return self.perturbations[0].magnitude.prior.scale2.sample_iter

    def initialize(self, **kwargs):
        &#39;&#39;&#39;Every prior variance on the perturbations gets the same hyperparameters
        &#39;&#39;&#39;
        for perturbation in self.perturbations:
            perturbation.magnitude.prior.scale2.initialize(**kwargs)

    def update(self):
        for perturbation in self.perturbations:
            perturbation.magnitude.prior.scale2.update()

    def set_trace(self, *args, **kwargs):
        for perturbation in self.perturbations:
            perturbation.magnitude.prior.scale2.set_trace(*args, **kwargs)

    def add_trace(self, *args, **kwargs):
        for perturbation in self.perturbations:
            perturbation.magnitude.prior.scale2.add_trace(*args, **kwargs)

    def get_single_value_of_perts(self) -&gt; np.ndarray:
        &#39;&#39;&#39;Get the variance for each perturbation
        &#39;&#39;&#39;
        return np.asarray([p.magnitude.prior.scale2.value for p in self.perturbations])

    def diag(self, only_pos_ind: bool=True) -&gt; np.ndarray:
        &#39;&#39;&#39;Return the diagonal of the prior variances stacked up in order

        Parameters
        ----------
        only_pos_ind : bool
            If True, only put in the values for the positively indicated clusters
            for each perturbation

        Returns
        -------
        np.ndarray
        &#39;&#39;&#39;
        ret = []
        for perturbation in self.perturbations:
            if only_pos_ind:
                n = perturbation.indicator.num_on_clusters()
            else:
                n = len(perturbation.clustering)
            ret = np.append(
                ret,
                np.ones(n, dtype=float)*perturbation.magnitude.prior.scale2.value)
        return ret

    def visualize(self, path: str, f: IO, pidx: int, section: str=&#39;posterior&#39;) -&gt; IO:
        &#39;&#39;&#39;Visualize the `pidx`th perturbation prior magnitude

        Parameters
        ----------
        obj : mdsine2.Variable
        path : str
            This is the path to write the files to
        f : _io.TextIOWrapper
            File that we are writing the values to
        section : str
            Section of the trace to compute on. Options:
                &#39;posterior&#39; : posterior samples
                &#39;burnin&#39; : burn-in samples
                &#39;entire&#39; : both burn-in and posterior samples

        Returns
        -------
        _io.TextIOWrapper
        &#39;&#39;&#39;
        if not self.G.inference.is_in_inference_order(self.name):
            return f
        perturbation = self.perturbations[pidx]
        return _scalar_visualize(path=path, f=f, section=section,
            obj=perturbation.magnitude.prior.scale2, log_scale=True)


class PriorVarPerturbationSingle(pl.variables.SICS):
    &#39;&#39;&#39;This is the posterior of the prior variance of regression coefficients
    for the interaction (off diagonal) variables
    &#39;&#39;&#39;
    def __init__(self, prior: variables.SICS, perturbation: pl.ClusterPerturbationEffect, 
        value: Union[float, int]=None, **kwargs):

        kwargs[&#39;name&#39;] = STRNAMES.PRIOR_VAR_PERT + &#39;_&#39; + perturbation.name
        pl.variables.SICS.__init__(self, value=value, dtype=float, **kwargs)
        self.add_prior(prior)
        self.perturbation = perturbation

    def initialize(self, value_option: str, dof_option: str, scale_option: str, 
        value: float=None, dof: float=None, scale: float=None, delay: int=0):
        &#39;&#39;&#39;Initialize the hyperparameters of the perturbation prior variance based on the
        passed in option

        Parameters
        ----------
        value_option : str
            - Initialize the value based on the specified option
            - Options
                &#39;manual&#39;
                    Set the value manually, `value` must also be specified
                &#39;auto&#39;, &#39;prior-mean&#39;
                    Set the value to the mean of the prior
                &#39;tight&#39;
                    value = 10^2
                &#39;diffuse&#39;
                    value = 10^4
        scale_option : str
            Initialize the scale of the prior
            Options
                &#39;manual&#39;
                    Set the value manually, `scale` must also be specified
                &#39;auto&#39;, &#39;diffuse&#39;
                    Set so that the mean of the distribution is 10^4
                &#39;tight&#39;
                    Set so that the mean of the distribution is 10^2
        dof_option : str
            Initialize the dof of the parameter
            Options:
                &#39;manual&#39;: Set the value with the parameter `dof`
                &#39;diffuse&#39;: Set the value to 2.5
                &#39;strong&#39;: Set the value to the expected number of interactions
                &#39;auto&#39;: Set to diffuse
        dof, scale : int, float
            User specified values
            Only necessary if  any of the options are &#39;manual&#39;
        &#39;&#39;&#39;
        if not pl.isint(delay):
            raise TypeError(&#39;`delay` ({}) must be an int&#39;.format(type(delay)))
        if delay &lt; 0:
            raise ValueError(&#39;`delay` ({}) must be &gt;= 0&#39;.format(delay))
        self.delay = delay

        if not pl.isstr(dof_option):
            raise TypeError(&#39;`dof_option` ({}) must be a str&#39;.format(type(dof_option)))
        if dof_option == &#39;manual&#39;:
            if not pl.isnumeric(dof):
                raise TypeError(&#39;`dof` ({}) must be a numeric&#39;.format(type(dof)))
            if dof &lt; 0:
                raise ValueError(&#39;`dof` ({}) must be &gt; 0 for it to be a valid prior&#39;.format(shape))
        elif dof_option in [&#39;diffuse&#39;, &#39;auto&#39;]:
            dof = 2.5
        elif dof_option == &#39;strong&#39;:
            dof = expected_n_clusters(G=self.G)
        else:
            raise ValueError(&#39;`dof_option` ({}) not recognized&#39;.format(dof_option))
        self.prior.dof.override_value(dof)

        if not pl.isstr(scale_option):
            raise TypeError(&#39;`scale_option` ({}) must be a str&#39;.format(type(scale_option)))
        if scale_option == &#39;manual&#39;:
            if not pl.isnumeric(scale):
                raise TypeError(&#39;`scale` ({}) must be a numeric&#39;.format(type(scale)))
            if scale &lt; 0:
                raise ValueError(&#39;`scale` ({}) must be &gt; 0 for it to be a valid prior&#39;.format(scale))
        elif scale_option in [&#39;auto&#39;, &#39;diffuse&#39;]:
            # Calculate the mean to be 10
            scale = 1e4 * (self.prior.dof.value - 2) / self.prior.dof.value
        elif scale_option == &#39;tight&#39;:
            scale = 100 * (self.prior.dof.value - 2) / self.prior.dof.value
        else:
            raise ValueError(&#39;`scale_option` ({}) not recognized&#39;.format(scale_option))
        self.prior.scale.override_value(scale)

        if not pl.isstr(value_option):
            raise TypeError(&#39;`value_option` ({}) must be a str&#39;.format(type(value_option)))
        if value_option == &#39;manual&#39;:
            if not pl.isnumeric(value):
                raise ValueError(&#39;`value` ({}) must be numeric (float,int)&#39;.format(value.__class__))
            self.value = value
        elif value_option in [&#39;auto&#39;, &#39;prior-mean&#39;]:
            self.value = self.prior.mean()
        elif value_option == &#39;diffuse&#39;:
            self.value = 1e4
        elif value_option == &#39;tight&#39;:
            self.value = 1e2
        else:
            raise ValueError(&#39;`value_option` ({}) not recognized&#39;.format(value_option))

        logging.info(&#39;Prior Variance Interactions initialization results:\n&#39; \
            &#39;\tprior dof: {}\n&#39; \
            &#39;\tprior scale: {}\n&#39; \
            &#39;\tvalue: {}&#39;.format(
                self.prior.dof.value, self.prior.scale.value, self.value))

    # @profile
    def update(self):
        &#39;&#39;&#39;Calculate the posterior of the prior variance
        &#39;&#39;&#39;
        if self.sample_iter &lt; self.delay:
            return

        x = self.perturbation.cluster_array(only_pos_ind=True)
        mu = self.perturbation.magnitude.prior.loc.value

        se = np.sum(np.square(x - mu))
        n = len(x)

        self.dof.value = self.prior.dof.value + n
        self.scale.value = ((self.prior.scale.value * self.prior.dof.value) + \
           se)/self.dof.value
        self.sample()


class PriorMeanPerturbations(pl.Variable):
    &#39;&#39;&#39;Class iterates over the perturbations for updating the mean of the prior
    for each perturbation

    All perturbations get the same hyperparameters
    &#39;&#39;&#39;
    def __init__(self, **kwargs):
        kwargs[&#39;name&#39;] = STRNAMES.PRIOR_MEAN_PERT
        pl.Variable.__init__(self, **kwargs)
        self.perturbations = self.G.perturbations # mdsine.pylab.base.Perturbations

        if self.perturbations is None:
            raise TypeError(&#39;Only instantiate this object if there are perturbations&#39;)

    def __str__(self) -&gt; str:
        s = &#39;Perturbation Magnitude Prior Means&#39;
        for perturbation in self.perturbations:
            s += &#39;\n\tperturbation {}: {}&#39;.format(
                perturbation.name,
                perturbation.magnitude.prior.loc.value)
        return s

    @property
    def sample_iter(self) -&gt; int:
        return self.perturbations[0].magnitude.prior.loc.sample_iter

    def initialize(self, **kwargs):
        &#39;&#39;&#39;Every prior variance on the perturbations gets the same hyperparameters
        &#39;&#39;&#39;
        for perturbation in self.perturbations:
            perturbation.magnitude.prior.loc.initialize(**kwargs)

    def update(self):
        for perturbation in self.perturbations:
            perturbation.magnitude.prior.loc.update()

    def set_trace(self, *args, **kwargs):
        for perturbation in self.perturbations:
            perturbation.magnitude.prior.loc.set_trace(*args, **kwargs)

    def add_trace(self, *args, **kwargs):
        for perturbation in self.perturbations:
            perturbation.magnitude.prior.loc.add_trace(*args, **kwargs)

    def get_single_value_of_perts(self) -&gt; np.ndarray:
        &#39;&#39;&#39;Get the variance for each perturbation
        &#39;&#39;&#39;
        return np.asarray([p.magnitude.prior.loc.value for p in self.perturbations])

    def toarray(self, only_pos_ind: bool=True) -&gt; np.ndarray:
        &#39;&#39;&#39;Return the diagonal of the prior variances stacked up in order

        Parameters
        ----------
        only_pos_ind : bool
            If True, only put in the values for the positively indicated clusters
            for each perturbation

        Returns
        -------
        np.ndarray
        &#39;&#39;&#39;
        ret = []
        for perturbation in self.perturbations:
            if only_pos_ind:
                n = perturbation.indicator.num_on_clusters()
            else:
                n = len(perturbation.clustering)
            ret = np.append(
                ret, np.zeros(n, dtype=float)+perturbation.magnitude.prior.loc.value)
        return ret

    def visualize(self, path: str, f: IO, pidx: int, section: str=&#39;posterior&#39;) -&gt; IO:
        &#39;&#39;&#39;Visualize the `pidx`th perturbation prior magnitude

        Parameters
        ----------
        obj : mdsine2.Variable
        path : str
            This is the path to write the files to
        f : _io.TextIOWrapper
            File that we are writing the values to
        section : str
            Section of the trace to compute on. Options:
                &#39;posterior&#39; : posterior samples
                &#39;burnin&#39; : burn-in samples
                &#39;entire&#39; : both burn-in and posterior samples

        Returns
        -------
        _io.TextIOWrapper
        &#39;&#39;&#39;
        perturbation = self.perturbations[pidx]
        if not self.G.inference.is_in_inference_order(self.name):
            return f
        return _scalar_visualize(path=path, f=f, section=section,
            obj=perturbation.magnitude.prior.loc,
            log_scale=False)


class PriorMeanPerturbationSingle(pl.variables.Normal):
    
    def __init__(self, prior: variables.Normal, perturbation: pl.ClusterPerturbationEffect, **kwargs):

        kwargs[&#39;name&#39;] = STRNAMES.PRIOR_MEAN_PERT + &#39;_&#39; + perturbation.name
        pl.variables.Normal.__init__(self, loc=None, scale2=None, dtype=float, **kwargs)
        self.add_prior(prior)
        self.perturbation = perturbation

    def initialize(self, value_option: str, loc_option: str, scale2_option: str, 
        value: float=None, loc: float=None, scale2: float=None, delay: int=0):
        &#39;&#39;&#39;Initialize the hyperparameters

        Parameters
        ----------
        value_option : str
            How to set the value. Options:
                &#39;zero&#39;
                    Set to zero
                &#39;prior-mean&#39;, &#39;auto&#39;
                    Set to the mean of the prior
                &#39;manual&#39;
                    Specify with the `value` parameter
        loc_option : str
            How to set the mean of the prior
                &#39;zero&#39;, &#39;auto&#39;
                    Set to zero
                &#39;manual&#39;
                    Set with the `loc` parameter
        scale2_option : str
            &#39;diffuse&#39;, &#39;auto&#39;
                Variance is set to 10e4
            &#39;tight&#39;
                Variance is set to 1e2
            &#39;manual&#39;
                Set with the `scale2` parameter
        value, loc, scale2 : float
            These are only necessary if we specify manual for any of the other 
            options
        delay : int
            How much to delay the start of the update during inference
        &#39;&#39;&#39;
        if not pl.isint(delay):
            raise TypeError(&#39;`delay` ({}) must be an int&#39;.format(type(delay)))
        if delay &lt; 0:
            raise ValueError(&#39;`delay` ({}) must be &gt;= 0&#39;.format(delay))
        self.delay = delay

        # Set the loc
        if not pl.isstr(loc_option):
            raise TypeError(&#39;`loc_option` ({}) must be a str&#39;.format(type(loc_option)))
        if loc_option == &#39;manual&#39;:
            if not pl.isnumeric(loc):
                raise TypeError(&#39;`loc` ({}) must be a numeric&#39;.format(type(loc)))
        elif loc_option in [&#39;zero&#39;, &#39;auto&#39;]:
            loc = 0
        else:
            raise ValueError(&#39;`loc_option` ({}) not recognized&#39;.format(loc_option))
        self.prior.loc.override_value(loc)

        # Set the scale2
        if not pl.isstr(scale2_option):
            raise TypeError(&#39;`scale2_option` ({}) must be a str&#39;.format(type(scale2_option)))
        if scale2_option == &#39;manual&#39;:
            if not pl.isnumeric(scale2):
                raise TypeError(&#39;`scale2` ({}) must be a numeric&#39;.format(type(scale2)))
            if scale2 &lt;= 0:
                raise ValueError(&#39;`scale2` ({}) must be positive&#39;.format(scale2))
        elif scale2_option in [&#39;diffuse&#39;, &#39;auto&#39;]:
            scale2 = 1e4
        elif scale2_option == &#39;tight&#39;:
            scale2 = 1e2
        else:
            raise ValueError(&#39;`scale2_option` ({}) not recognized&#39;.format(scale2_option))
        self.prior.scale2.override_value(scale2)

        # Set the value
        if not pl.isstr(value_option):
            raise TypeError(&#39;`value_option` ({}) must be a str&#39;.format(type(value_option)))
        if value_option == &#39;manual&#39;:
            if not pl.isnumeric(value):
                raise TypeError(&#39;`value` ({}) must be a numeric&#39;.format(type(value)))
        elif value_option in [&#39;prior-mean&#39;, &#39;auto&#39;]:
            value = self.prior.loc.value
        elif value_option == &#39;zero&#39;:
            value = 0
        else:
            raise ValueError(&#39;`value_option` ({}) not recognized&#39;.format(value_option))
        self.value = value
        
    def update(self):
        &#39;&#39;&#39;Update using a Gibbs update
        &#39;&#39;&#39;
        if self.sample_iter &lt; self.delay:
            return

        x = self.perturbation.cluster_array(only_pos_ind=True)
        prec = 1/self.perturbation.magnitude.prior.scale2.value

        prior_prec = 1/self.prior.scale2.value
        prior_mean = self.prior.loc.value

        self.scale2.value = 1/(prior_prec + (len(x)*prec))
        self.loc.value = self.scale2.value * ((prior_mean * prior_prec) + (np.sum(x)*prec))
        self.sample()


# qPCR variance (NOTE THESE IS NOT LEARNED IN THE MODEL)
# ------------------------------------------------------
class _qPCRBase(pl.Variable):
    &#39;&#39;&#39;Base class for qPCR measurements
    &#39;&#39;&#39;
    def __init__(self, L, **kwargs):

        pl.Variable.__init__(self, **kwargs)
        self._sample_iter = 0
        self.L = L
        self.n_replicates = self.G.data.n_replicates
        self.value = []

    def __getitem__(self, key):
        return self.value[key]

    def __str__(self):
        # Make them into an array?
        try:
            s = &#39;&#39;
            for a in self.value:
                s += str(a) + &#39;\n&#39;
        except:
            s = &#39;not set&#39;
        return s

    def update(self):
        &#39;&#39;&#39;Update each of the qPCR variances
        &#39;&#39;&#39;
        for a in self.value:
            a.update()
        self._sample_iter += 1

    def initialize(self, **kwargs):
        &#39;&#39;&#39;Every variance gets the same initialization
        &#39;&#39;&#39;
        for a in self.value:
            a.initialize(**kwargs)

    def add_trace(self, *args, **kwargs):
        for a in self.value:
            a.add_trace(*args, **kwargs)

    def set_trace(self, *args, **kwargs):
        for a in self.value:
            a.set_trace(*args, **kwargs)


class _qPCRPriorAggVar(_qPCRBase):
    &#39;&#39;&#39;Base class for `qPCRDegsOfFreedoms` and `qPCRScales`
    &#39;&#39;&#39;
    def __init__(self, L, child,**kwargs):
        _qPCRBase.__init__(self, L, **kwargs)
        for l in range(self.L):
            self.value.append(
                child(L=L, l=l, **kwargs))

    def add_qpcr_measurement(self, ridx, tidx, l):
        &#39;&#39;&#39;Add a qPCR measurement for subject `ridx` at time index
        `tidx` to qPCR set `l`

        Parameters
        ----------
        ridx : int
            Subject index
        tidx : int
            Time index
        l : int
            qPCR set index
        &#39;&#39;&#39;
        if not pl.isint(ridx):
            raise TypeError(&#39;`ridx` ({}) must be an int&#39;.format(type(ridx)))
        if ridx &gt;= self.G.data.n_replicates:
            raise ValueError(&#39;`ridx` ({}) out of range ({})&#39;.format(ridx, 
                self.G.data.n_replicates))
        if not pl.isint(tidx):
            raise TypeError(&#39;`tidx` ({}) must be an int&#39;.format(type(tidx)))
        if tidx &gt;= len(self.G.data.given_timepoints[ridx]):
            raise ValueError(&#39;`tidx` ({}) out of range ({})&#39;.format(tidx, 
                len(self.G.data.given_timepoints[ridx])))
        if not pl.isint(l):
            raise TypeError(&#39;`l` ({}) must be an int&#39;.format(type(l)))
        if l &gt;= self.L:
            raise ValueError(&#39;`l` ({}) out of range ({})&#39;.format(tidx, 
                self.L))
        self.value[l].add_qpcr_measurement(ridx=ridx, tidx=tidx)

    def set_shape(self):
        for a in self.value:
            a.set_shape()


class qPCRVariances(_qPCRBase):
    &#39;&#39;&#39;Aggregation class for qPCR variance for a set of qPCR variances. 
    The qPCR variances are 

    Parameters
    ----------
    L : int
        How many qPCR variance groupings there are
    &#39;&#39;&#39;
    def __init__(self, **kwargs):
        kwargs[&#39;name&#39;] = STRNAMES.QPCR_VARIANCES
        _qPCRBase.__init__(self, **kwargs)
        self.G.data.qpcr_variances = self
        
        for ridx in range(self.n_replicates):
            self.value.append( 
                qPCRVarianceReplicate(ridx=ridx, **kwargs))

    def add_qpcr_measurement(self, ridx, tidx, l):
        &#39;&#39;&#39;Add a qPCR measurement for subject `ridx` at time index
        `tidx` to qPCR set `l`

        Parameters
        ----------
        ridx : int
            Subject index
        tidx : int
            Time index
        l : int
            qPCR set index
        &#39;&#39;&#39;
        if not pl.isint(ridx):
            raise TypeError(&#39;`ridx` ({}) must be an int&#39;.format(type(ridx)))
        if ridx &gt;= self.G.data.n_replicates:
            raise ValueError(&#39;`ridx` ({}) out of range ({})&#39;.format(ridx, 
                self.G.data.n_replicates))
        if not pl.isint(tidx):
            raise TypeError(&#39;`tidx` ({}) must be an int&#39;.format(type(tidx)))
        if tidx &gt;= len(self.G.data.given_timepoints[ridx]):
            raise ValueError(&#39;`tidx` ({}) out of range ({})&#39;.format(tidx, 
                len(self.G.data.given_timepoints[ridx])))
        if not pl.isint(l):
            raise TypeError(&#39;`l` ({}) must be an int&#39;.format(type(l)))
        if l &gt;= self.L:
            raise ValueError(&#39;`l` ({}) out of range ({})&#39;.format(tidx, 
                self.L))
        self.value[ridx].add_qpcr_measurement(tidx=tidx, l=l)        


class qPCRVarianceReplicate(pl.variables.SICS):
    &#39;&#39;&#39;Posterior for a set of single qPCR variances for replicate `ridx`

    Parameters
    ----------
    ridx : int
        Which subject replicate index this set of qPCR variances belongs
        to.
    L : int
        How many qPCR variance groupings there are
    &#39;&#39;&#39;
    def __init__(self, ridx, L, **kwargs):
        self.ridx = ridx
        self.L = L
        kwargs[&#39;name&#39;] = STRNAMES.QPCR_VARIANCES + &#39;_{}&#39;.format(ridx)
        pl.variables.SICS.__init__(self, **kwargs)
        self.priors_idx = np.full(len(self.G.data.given_timepoints[ridx]), -1, dtype=int)
        self.set_value_shape(shape=(len(self.G.data.given_timepoints[ridx]),))

    def initialize(self, value_option, value=None, inflated=None):
        &#39;&#39;&#39;Initialize the values. We do not set any hyperparameters because those are
        set in their own classes.

        Parameters
        ----------
        value_option : str
            How to initialize the variances
                &#39;empirical&#39;, &#39;auto&#39;
                    Set to the empirical variance of the respective measurements
                &#39;inflated&#39;
                    Set to an inflated value of the empirical variance.
                &#39;manual&#39;
                    Set the values manually
        value : float, np.ndarray(float)
            If float, set all the values to the same number. If array then set the 
            values to each of the parameters
        inflated : float, None
            Necessary if `value_option` == &#39;inflated&#39;
        &#39;&#39;&#39;
        if not pl.isstr(value_option):
            raise TypeError(&#39;`value_option` ({}) must be a str&#39;.format(type(value_option)))
        if value_option in [&#39;empirical&#39;, &#39;auto&#39;]:
            self.value = np.zeros(len(self.G.data.qpcr[self.ridx]), dtype=float)
            for idx, t in enumerate(self.G.data.qpcr[self.ridx]):
                self.value[idx] = np.var(self.G.data.qpcr[self.ridx][t].log_data)

        elif value_option == &#39;inflated&#39;:
            if not pl.isnumeric(inflated):
                raise TypeError(&#39;`inflated` ({}) must be a numeric&#39;.format(type(inflated)))
            if inflated &lt; 0:
                raise ValueError(&#39;`inflated` ({}) must be positive&#39;.format(inflated))
            # Set each variance by the empirical variance * inflated
            self.value = np.zeros(len(self.G.data.qpcr[self.ridx]), dtype=float)
            for idx, t in enumerate(self.G.data.qpcr[self.ridx]):
                self.value[idx] = np.var(self.G.data.qpcr[self.ridx][t].log_data) * inflated

        elif value_option == &#39;manual&#39;:
            raise NotImplementedError(&#39;Need to implement&#39;)
        else:
            raise ValueError(&#39;`value_option` ({}) not recognized&#39;.format(value_option))

        # Set the qPCR measurements
        self.qpcr_measurements = []
        for tidx, t in enumerate(self.G.data.given_timepoints[self.ridx]):
            self.qpcr_measurements.append(self.G.data.qpcr[self.ridx][t].log_data)

    def update(self):

        prior_dofs = []
        prior_scales = []
        for l in range(self.L):
            prior_dofs.append(self.G[STRNAMES.QPCR_DOFS].value[l].value)
            prior_scales.append(self.G[STRNAMES.QPCR_SCALES].value[l].value)

        for tidx in range(len(self.priors_idx)):
            t = self.G.data.given_timepoints[self.ridx][tidx]
            l = self.priors_idx[tidx]
            prior_dof = prior_dofs[l]
            prior_scale = prior_scales[l]

            # qPCR measurements (these are already in log space)
            values = self.qpcr_measurements[tidx]

            # Current mean is the log of the sum of latent abundance
            tidx_in_arr = self.G.data.timepoint2index[self.ridx][t]
            mean = np.log(np.sum(self.G.data.data[self.ridx][:, tidx_in_arr]))

            # Calculate the residual sum
            resid_sum = np.sum(np.square(values - mean))

            # posterior
            dof = prior_dof + len(values)
            scale = ((prior_scale * prior_dof) + resid_sum)/dof
            self.value[tidx] = pl.random.sics.sample(dof, scale)

    def add_qpcr_measurement(self, tidx, l):
        &#39;&#39;&#39;Add qPCR measurement for subject index `ridx` and time index `tidx`

        Parameters
        ----------
        ridx : int
            Subject index
        tidx : int
            Time index
        &#39;&#39;&#39;
        if not pl.isint(tidx):
            raise TypeError(&#39;`tidx` ({}) must be an int&#39;.format(type(tidx)))
        if tidx &gt;= len(self.G.data.given_timepoints[self.ridx]):
            raise ValueError(&#39;`tidx` ({}) out of range ({})&#39;.format(tidx, 
                len(self.G.data.given_timepoints[self.ridx])))
        if not pl.isint(l):
            raise TypeError(&#39;`l` ({}) must be an int&#39;.format(type(l)))
        self.priors_idx[tidx] = l


class qPCRDegsOfFreedoms(_qPCRPriorAggVar):
    &#39;&#39;&#39;Aggregation class for a degree of freedom parameter of qPCR variance

    Parameters
    ----------
    L : int
        How many qPCR variance groupings there are
    &#39;&#39;&#39;
    def __init__(self, L, **kwargs):
        kwargs[&#39;name&#39;] = STRNAMES.QPCR_DOFS
        _qPCRPriorAggVar.__init__(self, L=L, child=qPCRDegsOfFreedomL, **kwargs)


class qPCRDegsOfFreedomL(pl.variables.Uniform):
    &#39;&#39;&#39;Posterior for a single qPCR degrees of freedom parameter for a SICS set
    
    Parameters
    ----------
    L : int
        How many qPCR variance groupings there are
    l : int
        Which specific grouping this hyperprior is
    &#39;&#39;&#39;
    def __init__(self, L, l, **kwargs):

        self.L = L
        self.l = l
        kwargs[&#39;name&#39;] = STRNAMES.QPCR_DOFS + &#39;_{}&#39;.format(l)
        pl.variables.Uniform.__init__(self, **kwargs)

        self.data_locs = []
        self.proposal = pl.variables.TruncatedNormal(loc=None, scale2=None, value=None)

    def __str__(self):
        # If this fails, it is because we are dividing by 0 sampler_iter
        # If which case we just return the value 
        try:
            s = &#39;Value: {}, Acceptance rate: {}&#39;.format(
                self.value, np.mean(self.acceptances[
                    np.max([self.sample_iter-50, 0]):self.sample_iter]))
        except:
            s = str(self.value)
        return s

    def set_shape(self):
        &#39;&#39;&#39;Set the shape of the array (how many qPCR variances this is a prior for)
        &#39;&#39;&#39;
        self.set_value_shape(shape=(len(self.data_locs), ))

    def add_qpcr_measurement(self, ridx, tidx):
        &#39;&#39;&#39;Add the qPCR measurement for subject index `ridx` and time index
        `tidx` to 
        &#39;&#39;&#39;
        self.data_locs.append((ridx, tidx))

    def initialize(self, value_option, low_option, high_option, proposal_option, 
        target_acceptance_rate, tune, end_tune, value=None, low=None, high=None, 
        proposal_var=None, delay=0):
        &#39;&#39;&#39;Initialize the values and hyperparameters. The proposal truncation is 
        always set to the same as the parameterization of the prior.

        Parameters
        ----------
        value_option : str
            How to initialize the value. Options:
                &#39;auto&#39;, &#39;diffuse&#39;
                    Set the value to 2.5
                &#39;strong&#39;
                    Set to be 50% of the data
                &#39;manual&#39;
                    `value` must also be specified
        low_option : str
            How to set the low parameter of the prior
            &#39;auto&#39;, &#39;valid&#39;
                Set to 2 so that the prior stays proper during inference
            &#39;zero&#39;
                Set to 0
            &#39;manual&#39;
                Specify the value with the parameter `low`
        high_option : str
            How to set the high parameter of the prior
            &#39;auto&#39;, &#39;med&#39;
                Set to 10 X the maximum in the set
            &#39;high&#39;
                Set to 100 X the maximum in the set
            &#39;low&#39;
                Set to 1 X the maximum in the set
            &#39;manual&#39;
                Set the value with the parameter `high`
        proposal_option : str
            How to initialize the proposal variance:
                &#39;auto&#39;
                    mean**2 / 100
                &#39;manual&#39;
                    `proposal_var` must also be supplied
        &#39;&#39;&#39;
        if not pl.isint(delay):
            raise TypeError(&#39;`delay` ({}) must be an int&#39;.format(type(delay)))
        if delay &lt; 0:
            raise ValueError(&#39;`delay` ({}) must be &gt;= 0&#39;.format(delay))
        self.delay = delay

        self.qpcr_data = []
        for ridx, tidx in self.data_locs:
            t = self.G.data.given_timepoints[ridx][tidx]
            self.qpcr_data = np.append(self.qpcr_data, 
                self.G.data.qpcr[ridx][t].log_data)
        
        # Set the prior low
        if not pl.isstr(low_option):
            raise TypeError(&#39;`low_option` ({}) must be a str&#39;.format(type(low_option)))
        if low_option == &#39;manual&#39;:
            if not pl.isnumeric(low):
                raise TypeError(&#39;`low` ({}) must be a numeric&#39;.format(type(low)))
            if low &lt; 2:
                raise ValueError(&#39;`low` ({}) must be &gt;= 2&#39;.format(low))
        elif low_option in [&#39;valid&#39;, &#39;auto&#39;]:
            low = 2
        elif low_option == &#39;zero&#39;:
            low = 0
        else:
            raise ValueError(&#39;`low_option` ({}) not recognized&#39;.format(low_option))
        self.prior.low.override_value(low)  

        # Set the prior high
        if not pl.isstr(high_option):
            raise TypeError(&#39;`high_option` ({}) must be a str&#39;.format(type(high_option)))
        if high_option == &#39;manual&#39;:
            if not pl.isnumeric(high):
                raise TypeError(&#39;`high` ({}) must be a numeric&#39;.format(type(high)))
        elif high_option in [&#39;med&#39;, &#39;auto&#39;]:
            high = 10 * len(self.data_locs)
        elif high_option == &#39;low&#39;:
            high = len(self.data_locs)
        elif high_option == &#39;high&#39;:
            high = 100 * len(self.data_locs)
        else:
            raise ValueError(&#39;`high_option` ({}) not recognized&#39;.format(high_option))
        if high &lt; self.prior.low.value:
            raise ValueError(&#39;`high` ({}) must be &gt;= low ({})&#39;.format(high, 
                self.prior.low.value))
        self.prior.high.override_value(high)

        # Set the value
        if not pl.isstr(value_option):
            raise TypeError(&#39;`value_option` ({}) must be a str&#39;.format(type(value_option)))
        if value_option == &#39;manual&#39;:
            if not pl.isnumeric(value):
                raise TypeError(&#39;`value` ({}) must be a numeric&#39;.format(type(value)))
        elif value_option in [&#39;auto&#39;, &#39;diffuse&#39;]:
            value = 2.5
        elif value_option == &#39;strong&#39;:
            value = len(self.data_locs)
        else:
            raise ValueError(&#39;`value_option` ({}) not recognized&#39;.format(value_option))
        self.value = value
        if self.value &lt;= self.prior.low.value or self.value &gt;= self.prior.high.value:
            raise ValueError(&#39;`value` ({}) out of range ({})&#39;.format(self.value))

        # Set the propsal parameters
        if pl.isstr(target_acceptance_rate):
            if target_acceptance_rate in [&#39;optimal&#39;, &#39;auto&#39;]:
                target_acceptance_rate = 0.44
            else:
                raise ValueError(&#39;`target_acceptance_rate` ({}) not recognized&#39;.format(
                    target_acceptance_rate))
        elif pl.isfloat(target_acceptance_rate):
            if target_acceptance_rate &lt; 0 or target_acceptance_rate &gt; 1:
                raise ValueError(&#39;`target_acceptance_rate` ({}) out of range&#39;.format(
                    target_acceptance_rate))
        else:
            raise TypeError(&#39;`target_acceptance_rate` ({}) type not recognized&#39;.format(
                type(target_acceptance_rate)))
        self.target_acceptance_rate = target_acceptance_rate
        
        if pl.isstr(tune):
            if tune in [&#39;auto&#39;]:
                tune = 50
            else:
                raise ValueError(&#39;`tune` ({}) not recognized&#39;.format(tune))
        elif pl.isint(tune):
            if tune &lt; 0:
                raise ValueError(&#39;`tune` ({}) must be &gt; 0&#39;.format(
                    tune))
        else:
            raise TypeError(&#39;`tune` ({}) type not recognized&#39;.format(type(tune)))
        self.tune = tune

        if pl.isstr(end_tune):
            if end_tune in [&#39;auto&#39;, &#39;half-burnin&#39;]:
                end_tune = int(self.G.inference.burnin/2)
            else:
                raise ValueError(&#39;`tune` ({}) not recognized&#39;.format(end_tune))
        elif pl.isint(end_tune):
            if end_tune &lt; 0 or end_tune &gt; self.G.inference.burnin:
                raise ValueError(&#39;`end_tune` ({}) out of range (0, {})&#39;.format(
                    end_tune, self.G.inference.burnin))
        else:
            raise TypeError(&#39;`end_tune` ({}) type not recognized&#39;.format(type(end_tune)))
        self.end_tune = end_tune

        # Set the proposal variance
        if not pl.isstr(proposal_option):
            raise TypeError(&#39;`proposal_option` ({}) must be a str&#39;.format(
                type(proposal_option)))
        elif proposal_option == &#39;manual&#39;:
            if not pl.isnumeric(proposal_var):
                raise TypeError(&#39;`proposal_var` ({}) must be a numeric&#39;.format(
                    type(proposal_var)))
            if proposal_var &lt;= 0:
                raise ValueError(&#39;`proposal_var` ({}) not proper&#39;.format(proposal_var))
        elif proposal_option in [&#39;auto&#39;]:
            proposal_var = (self.value ** 2)/10
        else:
            raise ValueError(&#39;`proposal_option` ({}) not recognized&#39;.format(
                proposal_option))
        self.proposal.scale2.value = proposal_var
        self.proposal.low = self.prior.low.value
        self.proposal.high = self.prior.high.value

    def update_var(self):
        &#39;&#39;&#39;Update the variance of the proposal
        &#39;&#39;&#39;
        if self.sample_iter == 0:
            self.temp_acceptances = 0
            self.acceptances = np.zeros(self.G.inference.n_samples, dtype=bool)
        
        elif self.sample_iter &gt; self.end_tune:
            # Don&#39;t do any more updates
            return
        
        elif self.sample_iter % self.tune == 0:
            # Update var
            acceptance_rate = self.temp_acceptances / self.tune
            if acceptance_rate &gt; self.target_acceptance_rate:
                self.proposal.scale2.value *= 1.5
            else:
                self.proposal.scale2.value /= 1.5
            self.temp_acceptances = 0

    def update(self):
        &#39;&#39;&#39;First we update the proposal (if necessary) and then we do a MH step
        &#39;&#39;&#39;
        self.update_var()
        proposal_std = np.sqrt(self.proposal.scale2.value)

        # Get the data
        xs = []
        for ridx, tidx in self.data_locs:
            xs.append(self.G[STRNAMES.QPCR_VARIANCES].value[ridx].value[tidx])

        # Get the scale
        scale = self.G[STRNAMES.QPCR_SCALES].value[self.l].value

        # Propose a new value for the dof
        prev_dof = self.value
        self.proposal.loc.value = self.value
        new_dof = self.proposal.sample()

        if new_dof &lt; self.prior.low.value or new_dof &gt; self.prior.high.value:
            # Automatic reject
            self.value = prev_dof
            return

        # Calculate the target distribution log likelihood
        prev_target_ll = 0
        for x in xs:
            prev_target_ll += pl.random.sics.logpdf(value=x,
                scale=scale, dof=prev_dof)
        new_target_ll = 0
        for x in xs:
            new_target_ll += pl.random.sics.logpdf(value=x,
                scale=scale, dof=new_dof)

        # Normalize by the loglikelihood of the proposal
        prev_prop_ll = pl.random.truncnormal.logpdf(
            value=prev_dof, loc=new_dof, scale=proposal_std,
            low=self.proposal.low, high=self.proposal.high)
        new_prop_ll = pl.random.truncnormal.logpdf(
            value=new_dof, loc=prev_dof, scale=proposal_std,
            low=self.proposal.low, high=self.proposal.high)

        # Accept or reject
        r = (new_target_ll - prev_prop_ll) - \
            (prev_target_ll - new_prop_ll)
        u = np.log(pl.random.misc.fast_sample_standard_uniform())
        if r &gt;= u:
            self.acceptances[self.sample_iter] = True
            self.value = new_dof
            self.temp_acceptances += 1
        else:
            self.value = prev_dof


class qPCRScales(_qPCRPriorAggVar):
    &#39;&#39;&#39;Aggregation class for a scale parameter of qPCR variance

    Parameters
    ----------
    L : int
        How many qPCR variance groupings there are
    &#39;&#39;&#39;
    def __init__(self, L, **kwargs):
        kwargs[&#39;name&#39;] = STRNAMES.QPCR_SCALES
        _qPCRPriorAggVar.__init__(self, L=L, child=qPCRScaleL, **kwargs)


class qPCRScaleL(pl.variables.SICS):
    &#39;&#39;&#39;Posterior for a single qPCR scale set
    &#39;&#39;&#39;
    def __init__(self, L, l, **kwargs):

        self.L = L
        self.l = l
        kwargs[&#39;name&#39;] = STRNAMES.QPCR_SCALES + &#39;_{}&#39;.format(l)
        pl.variables.Uniform.__init__(self, **kwargs)

        self.data_locs = []
        self.proposal = pl.variables.TruncatedNormal(loc=None, scale2=None, value=None)

    def __str__(self):
        # If this fails, it is because we are dividing by 0 sampler_iter
        # If which case we just return the value 
        try:
            s = &#39;Value: {}, Acceptance rate: {}&#39;.format(
                self.value, np.mean(self.acceptances[
                    np.max([self.sample_iter-50, 0]):self.sample_iter]))
        except:
            s = str(self.value)
        return s

    def set_shape(self):
        &#39;&#39;&#39;Set the shape of the array (how many qPCR variances this is a prior for)
        &#39;&#39;&#39;
        self.set_value_shape(shape=(len(self.data_locs), ))

    def add_qpcr_measurement(self, ridx, tidx):
        &#39;&#39;&#39;Add the qPCR measurement for subject index `ridx` and time index
        `tidx` to 
        &#39;&#39;&#39;
        self.data_locs.append((ridx, tidx))

    def initialize(self, value_option, scale_option, dof_option, proposal_option, 
        target_acceptance_rate, tune, end_tune, value=None, dof=None, scale=None,
        proposal_var=None, delay=0):
        &#39;&#39;&#39;Initialize the values and hyperparameters

        Parameters
        ----------
        value_option : str
            How to initialize the value. Options:
                &#39;auto&#39;, &#39;prior-mean&#39;
                    Set to the prior mean
                &#39;manual&#39;
                    `value` must also be specified
        dof_option : str
            How to set the prior dof
                &#39;auto&#39;, &#39;diffuse&#39;
                    2.5
                &#39;manual&#39;:
                    set with `dof`
        scale_option : str
            How to set the prior scale
                &#39;empirical&#39;, &#39;auto&#39;
                    Set to the variance of the data assigned to the set
                &#39;manual&#39;
                    Set with the parameter `scale`
        proposal_option : str
            How to initialize the proposal variance:
                &#39;auto&#39;
                    mean**2 / 100
                &#39;manual&#39;
                    `proposal_var` must also be supplied     
        &#39;&#39;&#39;
        if not pl.isint(delay):
            raise TypeError(&#39;`delay` ({}) must be an int&#39;.format(type(delay)))
        if delay &lt; 0:
            raise ValueError(&#39;`delay` ({}) must be &gt;= 0&#39;.format(delay))
        self.delay = delay

        self.qpcr_data = []
        for ridx, tidx in self.data_locs:
            t = self.G.data.given_timepoints[ridx][tidx]
            self.qpcr_data = np.append(self.qpcr_data, 
                self.G.data.qpcr[ridx][t].log_data)

        # Set the prior dof
        if not pl.isstr(dof_option):
            raise TypeError(&#39;`dof_option` ({}) must be a str&#39;.format(type(dof_option)))
        if dof_option == &#39;manual&#39;:
            if not pl.isnumeric(dof):
                raise TypeError(&#39;`dof` ({}) must be a numeric&#39;.format(type(dof)))
            if dof &lt; 2:
                raise ValueError(&#39;`dof` ({}) must be &gt;= 2&#39;.format(dof))
        elif dof_option in [&#39;diffuse&#39;, &#39;auto&#39;]:
            dof = 2.5
        else:
            raise ValueError(&#39;`dof_option` ({}) not recognized&#39;.format(dof_option))
        if dof &lt; 2:
            raise ValueError(&#39;`dof` ({}) must be strictly larger than 2 to be a proper&#39; \
                &#39; prior&#39;.format(dof))
        self.prior.dof.override_value(dof)

        # Set the prior scale
        if not pl.isstr(scale_option):
            raise TypeError(&#39;`scale_option` ({}) must be a str&#39;.format(type(scale_option)))
        if scale_option == &#39;manual&#39;:
            if not pl.isnumeric(scale):
                raise TypeError(&#39;`scale` ({}) must be a numeric&#39;.format(type(scale)))
            if scale &lt;= 0:
                raise ValueError(&#39;`scale` ({}) must be positive&#39;.format(scale))
        elif scale_option in [&#39;auto&#39;, &#39;empirical&#39;]:
            v = np.var(self.qpcr_data)
            scale = v * (self.prior.dof.value - 2) / self.prior.dof.value
        else:
            raise ValueError(&#39;`scale_option` ({}) not recognized&#39;.format(scale_option))
        self.prior.scale.override_value(scale)

        # Set the value
        if not pl.isstr(value_option):
            raise TypeError(&#39;`value_option` ({}) must be a str&#39;.format(type(value_option)))
        if value_option == &#39;manual&#39;:
            if not pl.isnumeric(value):
                raise TypeError(&#39;`value` ({}) must be a numeric&#39;.format(type(value)))
        elif value_option in [&#39;auto&#39;, &#39;prior-mean&#39;]:
            value = self.prior.mean()
        else:
            raise ValueError(&#39;`value_option` ({}) not recognized&#39;.format(value_option))
        self.value = value

        # Set the propsal parameters
        if pl.isstr(target_acceptance_rate):
            if target_acceptance_rate in [&#39;optimal&#39;, &#39;auto&#39;]:
                target_acceptance_rate = 0.44
            else:
                raise ValueError(&#39;`target_acceptance_rate` ({}) not recognized&#39;.format(
                    target_acceptance_rate))
        elif pl.isfloat(target_acceptance_rate):
            if target_acceptance_rate &lt; 0 or target_acceptance_rate &gt; 1:
                raise ValueError(&#39;`target_acceptance_rate` ({}) out of range&#39;.format(
                    target_acceptance_rate))
        else:
            raise TypeError(&#39;`target_acceptance_rate` ({}) type not recognized&#39;.format(
                type(target_acceptance_rate)))
        self.target_acceptance_rate = target_acceptance_rate

        if pl.isstr(tune):
            if tune in [&#39;auto&#39;]:
                tune = 50
            else:
                raise ValueError(&#39;`tune` ({}) not recognized&#39;.format(tune))
        elif pl.isint(tune):
            if tune &lt; 0:
                raise ValueError(&#39;`tune` ({}) must be &gt; 0&#39;.format(
                    tune))
        else:
            raise TypeError(&#39;`tune` ({}) type not recognized&#39;.format(type(tune)))
        self.tune = tune

        if pl.isstr(end_tune):
            if end_tune in [&#39;auto&#39;, &#39;half-burnin&#39;]:
                end_tune = int(self.G.inference.burnin/2)
            else:
                raise ValueError(&#39;`tune` ({}) not recognized&#39;.format(end_tune))
        elif pl.isint(end_tune):
            if end_tune &lt; 0 or end_tune &gt; self.G.inference.burnin:
                raise ValueError(&#39;`end_tune` ({}) out of range (0, {})&#39;.format(
                    end_tune, self.G.inference.burnin))
        else:
            raise TypeError(&#39;`end_tune` ({}) type not recognized&#39;.format(type(end_tune)))
        self.end_tune = end_tune

        # Set the proposal variance
        if not pl.isstr(proposal_option):
            raise TypeError(&#39;`proposal_option` ({}) must be a str&#39;.format(
                type(proposal_option)))
        elif proposal_option == &#39;manual&#39;:
            if not pl.isnumeric(proposal_var):
                raise TypeError(&#39;`proposal_var` ({}) must be a numeric&#39;.format(
                    type(proposal_var)))
            if proposal_var &lt;= 0:
                raise ValueError(&#39;`proposal_var` ({}) not proper&#39;.format(proposal_var))
        elif proposal_option in [&#39;auto&#39;]:
            proposal_var = (self.value ** 2)/10
        else:
            raise ValueError(&#39;`proposal_option` ({}) not recognized&#39;.format(
                proposal_option))
        self.proposal.scale2.value = proposal_var
        self.proposal.low = 0
        self.proposal.high = float(&#39;inf&#39;)

    def update_var(self):
        &#39;&#39;&#39;Update the variance of the proposal
        &#39;&#39;&#39;
        if self.sample_iter == 0:
            self.temp_acceptances = 0
            self.acceptances = np.zeros(self.G.inference.n_samples, dtype=bool)
        
        elif self.sample_iter &gt; self.end_tune:
            # Don&#39;t do any more updates
            return
        
        elif self.sample_iter % self.tune == 0:
            # Update var
            acceptance_rate = self.temp_acceptances / self.tune
            if acceptance_rate &gt; self.target_acceptance_rate:
                self.proposal.scale2.value *= 1.5
            else:
                self.proposal.scale2.value /= 1.5
            self.temp_acceptances = 0

    def update(self):
        &#39;&#39;&#39;First we update the proposal (if necessary) and then we do a MH step
        &#39;&#39;&#39;
        self.update_var()
        proposal_std = np.sqrt(self.proposal.scale2.value)

        # Get the data
        xs = []
        for ridx, tidx in self.data_locs:
            xs.append(self.G[STRNAMES.QPCR_VARIANCES].value[ridx].value[tidx])

        # Get the dof
        dof = self.G[STRNAMES.QPCR_DOFS].value[self.l].value

        # Propose a new value for the scale
        prev_scale = self.value
        self.proposal.loc.value = self.value
        new_scale = self.proposal.sample()

        # Calculate the target distribution log likelihood
        prev_target_ll = pl.random.sics.logpdf(value=prev_scale, 
            dof=self.prior.dof.value, scale=self.prior.scale.value)
        for x in xs:
            prev_target_ll += pl.random.sics.logpdf(value=x,
                scale=prev_scale, dof=dof)
        new_target_ll = pl.random.sics.logpdf(value=new_scale, 
            dof=self.prior.dof.value, scale=self.prior.scale.value)
        for x in xs:
            new_target_ll += pl.random.sics.logpdf(value=x,
                scale=new_scale, dof=dof)

        # Normalize by the loglikelihood of the proposal
        prev_prop_ll = pl.random.truncnormal.logpdf(
            value=prev_scale, loc=new_scale, scale=proposal_std,
            low=self.proposal.low, high=self.proposal.high)
        new_prop_ll = pl.random.truncnormal.logpdf(
            value=new_scale, loc=prev_scale, scale=proposal_std,
            low=self.proposal.low, high=self.proposal.high)

        # Accept or reject
        r = (new_target_ll - prev_prop_ll) - \
            (prev_target_ll - new_prop_ll)
        u = np.log(pl.random.misc.fast_sample_standard_uniform())

        if r &gt;= u:
            self.acceptances[self.sample_iter] = True
            self.value = new_scale
            self.temp_acceptances += 1
        else:
            self.value = prev_scale</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="mdsine2.posterior.build_prior_covariance"><code class="name flex">
<span>def <span class="ident">build_prior_covariance</span></span>(<span>G: <a title="mdsine2.pylab.graph.Graph" href="pylab/graph.html#mdsine2.pylab.graph.Graph">Graph</a>, cov: bool, order: List[str], sparse: bool = True, diag: bool = False) ‑> Union[scipy.sparse.base.spmatrix, numpy.ndarray]</span>
</code></dt>
<dd>
<div class="desc"><p>Build basic prior covariance or precision for the variables
specified in <code>order</code></p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>G</code></strong> :&ensp;<code>pylab.graph.Graph</code></dt>
<dd>Graph to get the variables from</dd>
<dt><strong><code>cov</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, build the covariance. If False, build the precision</dd>
<dt><strong><code>order</code></strong> :&ensp;<code>list(str)</code></dt>
<dd>Which parameters to get the priors of</dd>
<dt><strong><code>sparse</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, return as a sparse matrix</dd>
<dt><strong><code>diag</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, returns the diagonal of the matrix. If this is True, it
overwhelms the flag <code>sparse</code></dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>arr</code></strong> :&ensp;<code>np.ndarray, scipy.sparse.dia_matrix, torch.DoubleTensor</code></dt>
<dd>Prior covariance or precision matrix in either dense (np.ndarray) or
sparse (scipy.sparse.dia_matrix) form</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def build_prior_covariance(G: Graph, cov: bool, order: List[str], sparse: bool=True, 
    diag: bool=False) -&gt; Union[scipy.sparse.spmatrix, np.ndarray]:
    &#39;&#39;&#39;Build basic prior covariance or precision for the variables
    specified in `order`

    Parameters
    ----------
    G : pylab.graph.Graph
        Graph to get the variables from
    cov : bool
        If True, build the covariance. If False, build the precision
    order : list(str)
        Which parameters to get the priors of
    sparse : bool
        If True, return as a sparse matrix
    diag : bool
        If True, returns the diagonal of the matrix. If this is True, it
        overwhelms the flag `sparse`

    Returns
    -------
    arr : np.ndarray, scipy.sparse.dia_matrix, torch.DoubleTensor
        Prior covariance or precision matrix in either dense (np.ndarray) or
        sparse (scipy.sparse.dia_matrix) form
    &#39;&#39;&#39;
    n_taxa = G.data.n_taxa
    a = []
    for reprname in order:
        if reprname == STRNAMES.GROWTH_VALUE:
            a.append(np.full(n_taxa, G[STRNAMES.PRIOR_VAR_GROWTH].value))

        elif reprname == STRNAMES.SELF_INTERACTION_VALUE:
            a.append(np.full(n_taxa, G[STRNAMES.PRIOR_VAR_SELF_INTERACTIONS].value))

        elif reprname == STRNAMES.CLUSTER_INTERACTION_VALUE:
            n_interactions = G[STRNAMES.CLUSTER_INTERACTION_INDICATOR].num_pos_indicators
            a.append(np.full(n_interactions, G[STRNAMES.PRIOR_VAR_INTERACTIONS].value))

        elif reprname == STRNAMES.PERT_VALUE:
            for perturbation in G.perturbations:
                num_on = perturbation.indicator.num_on_clusters()
                a.append(np.full(
                    num_on,
                    perturbation.magnitude.prior.scale2.value))

        else:
            raise ValueError(&#39;reprname ({}) not recognized&#39;.format(reprname))

    if len(a) == 1:
        arr = np.asarray(a[0])
    else:
        arr = np.asarray(list(itertools.chain.from_iterable(a)))
    if not cov:
        arr = 1/arr
    if diag:
        return arr
    if sparse:
        return scipy.sparse.dia_matrix((arr,[0]), shape=(len(arr),len(arr))).tocsc()
    else:
        return np.diag(arr)</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.build_prior_mean"><code class="name flex">
<span>def <span class="ident">build_prior_mean</span></span>(<span>G: <a title="mdsine2.pylab.graph.Graph" href="pylab/graph.html#mdsine2.pylab.graph.Graph">Graph</a>, order: List[str], shape: Tuple = None) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Builds the prior mean vector for all the variables in <code>order</code>.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>G</code></strong> :&ensp;<code>pylab.grapg.Graph</code></dt>
<dd>Graph to index the objects</dd>
<dt><strong><code>order</code></strong> :&ensp;<code>list</code></dt>
<dd>list of objects to add the priors of. If the variable is the
cluster interactions or cluster perturbations, then we assume the
prior mean is a scalar and we set that value for every single value.</dd>
<dt><strong><code>shape</code></strong> :&ensp;<code>tuple, None</code></dt>
<dd>Shape to cast the array into</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.ndarray, torch.DoubleTensor</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def build_prior_mean(G: Graph, order: List[str], shape: Tuple=None) -&gt; np.ndarray:
    &#39;&#39;&#39;Builds the prior mean vector for all the variables in `order`.

    Parameters
    ----------
    G : pylab.grapg.Graph
        Graph to index the objects
    order : list
        list of objects to add the priors of. If the variable is the
        cluster interactions or cluster perturbations, then we assume the
        prior mean is a scalar and we set that value for every single value.
    shape : tuple, None
        Shape to cast the array into

    Returns
    -------
    np.ndarray, torch.DoubleTensor
    &#39;&#39;&#39;
    a = []
    for name in order:
        v = G[name]
        if v.name == STRNAMES.GROWTH_VALUE:
            a.append(v.prior.loc.value * np.ones(G.data.n_taxa))
        elif v.name == STRNAMES.SELF_INTERACTION_VALUE:
            a.append(v.prior.loc.value * np.ones(G.data.n_taxa))
        elif v.name == STRNAMES.CLUSTER_INTERACTION_VALUE:
            a.append(
                np.full(
                    G[STRNAMES.CLUSTER_INTERACTION_INDICATOR].num_pos_indicators,
                    v.prior.loc.value))
        elif v.name == STRNAMES.PERT_VALUE:
            for perturbation in G.perturbations:
                a.append(np.full(
                    perturbation.indicator.num_on_clusters(),
                    perturbation.magnitude.prior.loc.value))
        else:
            raise ValueError(&#39;`name` ({}) not recognized&#39;.format(name))
    if len(a) == 1:
        a = np.asarray(a[0])
    else:
        a = np.asarray(list(itertools.chain.from_iterable(a)))
    if shape is not None:
        a = a.reshape(*shape)
    return a</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.expected_n_clusters"><code class="name flex">
<span>def <span class="ident">expected_n_clusters</span></span>(<span>G: <a title="mdsine2.pylab.graph.Graph" href="pylab/graph.html#mdsine2.pylab.graph.Graph">Graph</a>) ‑> int</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate the expected number of clusters given the number of Taxa</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>G</code></strong> :&ensp;<code>pl.Graph</code></dt>
<dd>Graph object</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>int</code></dt>
<dd>Expected number of clusters</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def expected_n_clusters(G: Graph) -&gt; int:
    &#39;&#39;&#39;Calculate the expected number of clusters given the number of Taxa

    Parameters
    ----------
    G : pl.Graph
        Graph object

    Returns
    -------
    int
        Expected number of clusters
    &#39;&#39;&#39;
    conc = G[STRNAMES.CONCENTRATION].prior.mean()
    return conc * np.log((G.data.n_taxa + conc) / conc)</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.log_det"><code class="name flex">
<span>def <span class="ident">log_det</span></span>(<span>M: numpy.ndarray, var: <a title="mdsine2.pylab.variables.Variable" href="pylab/variables.html#mdsine2.pylab.variables.Variable">Variable</a>) ‑> float</span>
</code></dt>
<dd>
<div class="desc"><p>Computes pl.math.log_det but also saves the array if it crashes</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>M</code></strong> :&ensp;<code>nxn matrix (np.ndarray, scipy.sparse)</code></dt>
<dd>Matrix to calculate the log determinant</dd>
<dt><strong><code>var</code></strong> :&ensp;<code>pl.variable.Variable subclass</code></dt>
<dd>This is the variable that <code><a title="mdsine2.posterior.log_det" href="#mdsine2.posterior.log_det">log_det()</a></code> was called from</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>float</code></dt>
<dd>Log determinant of matrix</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def log_det(M: np.ndarray, var: Variable) -&gt; float:
    &#39;&#39;&#39;Computes pl.math.log_det but also saves the array if it crashes

    Parameters
    ----------
    M : nxn matrix (np.ndarray, scipy.sparse)
        Matrix to calculate the log determinant
    var : pl.variable.Variable subclass
        This is the variable that `log_det` was called from

    Returns
    -------
    float
        Log determinant of matrix
    &#39;&#39;&#39;
    if scipy.sparse.issparse(M):
        M_ = np.zeros(shape=M.shape)
        M.toarray(out=M_)
        M = M_
    try:
        # if type(M) == torch.Tensor:
        #     return torch.inverse(M)
        # else:
        return pl.math.log_det(M)
    except:
        try:
            sample_iter = var.sample_iter
        except:
            sample_iter = None
        filename = &#39;crashes/logdet_error_iter{}_var{}pinv_{}.npy&#39;.format(
            sample_iter, var.name, var.G.name)
        logging.critical(&#39;\n\n\n\n\n\n\n\nSaved array at &#34;{}&#34; - now crashing\n\n\n&#39;.format(
                filename))
        os.makedirs(&#39;crashes/&#39;, exist_ok=True)
        np.save(filename, M)
        raise</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.negbin_loglikelihood"><code class="name flex">
<span>def <span class="ident">negbin_loglikelihood</span></span>(<span>k: Union[float, int], m: Union[float, int], dispersion: Union[float, int]) ‑> float</span>
</code></dt>
<dd>
<div class="desc"><p>Loglikelihood - with parameterization in [1]</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>k</code></strong> :&ensp;<code>int</code></dt>
<dd>Observed counts</dd>
<dt><strong><code>m</code></strong> :&ensp;<code>int</code></dt>
<dd>Mean</dd>
<dt><strong><code>phi</code></strong> :&ensp;<code>float</code></dt>
<dd>Dispersion</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>float</code></dt>
<dd>Negative Binomial Log Likelihood</dd>
</dl>
<h2 id="references">References</h2>
<p>[1] TE Gibson, GK Gerber. Robust and Scalable Models of Microbiome Dynamics. ICML (2018)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def negbin_loglikelihood(k: Union[float, int], m: Union[float, int], dispersion: Union[float, int]) -&gt; float:
    &#39;&#39;&#39;Loglikelihood - with parameterization in [1]

    Parameters
    ----------
    k : int
        Observed counts
    m : int
        Mean
    phi : float
        Dispersion

    Returns
    -------
    float
        Negative Binomial Log Likelihood

    References
    ----------
    [1] TE Gibson, GK Gerber. Robust and Scalable Models of Microbiome Dynamics. ICML (2018)
    &#39;&#39;&#39;
    r = 1/dispersion
    return math.lgamma(k+r) - math.lgamma(k+1) - math.lgamma(r) \
            + r * (math.log(r) - math.log(r+m)) + k * (math.log(m) - math.log(r+m))</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.negbin_loglikelihood_MH_condensed"><code class="name flex">
<span>def <span class="ident">negbin_loglikelihood_MH_condensed</span></span>(<span>k: Union[float, int], m: Union[float, int], dispersion: Union[float, int]) ‑> float</span>
</code></dt>
<dd>
<div class="desc"><p>Loglikelihood - with parameterization in [1] - but condensed (do not calculate stuff
we do not have to)</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>k</code></strong> :&ensp;<code>int</code></dt>
<dd>Observed counts</dd>
<dt><strong><code>m</code></strong> :&ensp;<code>int</code></dt>
<dd>Mean</dd>
<dt><strong><code>phi</code></strong> :&ensp;<code>float</code></dt>
<dd>Dispersion</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>float</code></dt>
<dd>Negative Binomial Log Likelihood</dd>
</dl>
<h2 id="references">References</h2>
<p>[1] TE Gibson, GK Gerber. Robust and Scalable Models of Microbiome Dynamics. ICML (2018)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@numba.jit(nopython=True, fastmath=True, cache=False)
def negbin_loglikelihood_MH_condensed(k: Union[float, int], m: Union[float, int], dispersion: Union[float, int]) -&gt; float:
        &#39;&#39;&#39;
        Loglikelihood - with parameterization in [1] - but condensed (do not calculate stuff
        we do not have to)

        Parameters
        ----------
        k : int
            Observed counts
        m : int
            Mean
        phi : float
            Dispersion

        Returns
        -------
        float
            Negative Binomial Log Likelihood

        References
        ----------
        [1] TE Gibson, GK Gerber. Robust and Scalable Models of Microbiome Dynamics. ICML (2018)
        &#39;&#39;&#39;
        r = 1/dispersion
        rm = r+m
        return math.lgamma(k+r) - math.lgamma(r) \
            + r * (math.log(r) - math.log(rm)) + k * (math.log(m) - math.log(rm))</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.negbin_loglikelihood_MH_condensed_not_fast"><code class="name flex">
<span>def <span class="ident">negbin_loglikelihood_MH_condensed_not_fast</span></span>(<span>k: Union[float, int], m: Union[float, int], dispersion: Union[float, int]) ‑> float</span>
</code></dt>
<dd>
<div class="desc"><p>Loglikelihood - with parameterization in [1] - but condensed (do not calculate stuff
we do not have to). We use this function if <code><a title="mdsine2.posterior.negbin_loglikelihood_MH_condensed" href="#mdsine2.posterior.negbin_loglikelihood_MH_condensed">negbin_loglikelihood_MH_condensed()</a></code> fails to
compile, which can happen when doing jobs on the cluster</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>k</code></strong> :&ensp;<code>int</code></dt>
<dd>Observed counts</dd>
<dt><strong><code>m</code></strong> :&ensp;<code>int</code></dt>
<dd>Mean</dd>
<dt><strong><code>phi</code></strong> :&ensp;<code>float</code></dt>
<dd>Dispersion</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>float</code></dt>
<dd>Negative Binomial Log Likelihood</dd>
</dl>
<h2 id="references">References</h2>
<p>[1] TE Gibson, GK Gerber. Robust and Scalable Models of Microbiome Dynamics. ICML (2018)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def negbin_loglikelihood_MH_condensed_not_fast(k: Union[float, int], m: Union[float, int], dispersion: Union[float, int]) -&gt; float:
        &#39;&#39;&#39;
        Loglikelihood - with parameterization in [1] - but condensed (do not calculate stuff
        we do not have to). We use this function if `negbin_loglikelihood_MH_condensed` fails to
        compile, which can happen when doing jobs on the cluster

        Parameters
        ----------
        k : int
            Observed counts
        m : int
            Mean
        phi : float
            Dispersion

        Returns
        -------
        float
            Negative Binomial Log Likelihood

        References
        ----------
        [1] TE Gibson, GK Gerber. Robust and Scalable Models of Microbiome Dynamics. ICML (2018)
        &#39;&#39;&#39;
        r = 1/dispersion
        rm = r+m
        return math.lgamma(k+r) - math.lgamma(r) \
            + r * (math.log(r) - math.log(rm)) + k * (math.log(m) - math.log(rm))</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.pinv"><code class="name flex">
<span>def <span class="ident">pinv</span></span>(<span>M: numpy.ndarray, var: <a title="mdsine2.pylab.variables.Variable" href="pylab/variables.html#mdsine2.pylab.variables.Variable">Variable</a>) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Computes np.linalg.pinv but it also saves the array that crashed it if
it crashes.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>M</code></strong> :&ensp;<code>nxn matrix (np.ndarray, scipy.sparse)</code></dt>
<dd>Matrix to invert</dd>
<dt><strong><code>var</code></strong> :&ensp;<code>pl.variable.Variable subclass</code></dt>
<dd>This is the variable that <code><a title="mdsine2.posterior.pinv" href="#mdsine2.posterior.pinv">pinv()</a></code> was called from</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.ndarray</code></dt>
<dd>Inverse of the matrix</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pinv(M: np.ndarray, var: Variable) -&gt; np.ndarray:
    &#39;&#39;&#39;Computes np.linalg.pinv but it also saves the array that crashed it if
    it crashes.

    Parameters
    ----------
    M : nxn matrix (np.ndarray, scipy.sparse)
        Matrix to invert
    var : pl.variable.Variable subclass
        This is the variable that `pinv` was called from

    Returns
    -------
    np.ndarray
        Inverse of the matrix
    &#39;&#39;&#39;
    if scipy.sparse.issparse(M):
        M_ = np.zeros(shape=M.shape)
        M.toarray(out=M_)
        M = M_
    try:
        try:
            return np.linalg.pinv(M)
        except:
            try:
                return scipy.linalg.pinv(M)
            except:
                return scipy.linalg.inv(M)
    except:
        try:
            sample_iter = var.sample_iter
        except:
            sample_iter = None
        filename = &#39;crashes/pinv_error_iter{}_var{}pinv_{}.npy&#39;.format(
            sample_iter, var.name, var.G.name)
        logging.critical(&#39;\n\n\n\n\n\n\n\nSaved array at &#34;{}&#34; - now crashing\n\n\n&#39;.format(
                filename))
        os.makedirs(&#39;crashes/&#39;, exist_ok=True)
        np.save(filename, M)
        raise</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.prod_gaussians"><code class="name flex">
<span>def <span class="ident">prod_gaussians</span></span>(<span>means: numpy.ndarray, variances: numpy.ndarray) ‑> Tuple[float, float]</span>
</code></dt>
<dd>
<div class="desc"><p>Product of Gaussians</p>
<p>$\mu = [\mu_1, \mu_2, &hellip;, \mu_n]$
$ar = [ar_1,
ar_2, &hellip;,
ar_3]$</p>
<p>Means and variances must be in the same order.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>means</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>All of the means</dd>
<dt><strong><code>variances</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>All of the means</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>float, float</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prod_gaussians(means: np.ndarray, variances: np.ndarray) -&gt; Tuple[float, float]:
    &#39;&#39;&#39;Product of Gaussians

    $\mu = [\mu_1, \mu_2, ..., \mu_n]$
    $\var = [\var_1, \var_2, ..., \var_3]$

    Means and variances must be in the same order.

    Parameters
    ----------
    means : np.ndarray
        All of the means
    variances : np.ndarray
        All of the means

    Returns
    -------
    float, float
    &#39;&#39;&#39;
    def _calc_params(mu1, mu2, var1, var2):
        v = var1+var2
        mu = ((var1*mu2) + (var2*mu1))/(v)
        var = (var1*var2)/v
        return mu,var
    mu = means[0]
    var = variances[0]
    for i in range(1,len(means)):
        mu, var = _calc_params(mu1=mu, mu2=means[i], var1=var, var2=variances[i])
    return mu, var</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.sample_categorical_log"><code class="name flex">
<span>def <span class="ident">sample_categorical_log</span></span>(<span>log_p: Iterator[float]) ‑> int</span>
</code></dt>
<dd>
<div class="desc"><p>Generate one sample from a categorical distribution with event
probabilities provided in unnormalized log-space.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>log_p</code></strong> :&ensp;<code>array_like</code></dt>
<dd>logarithms of event probabilities, <strong><em>which need not be normalized</em></strong></dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>int</code></dt>
<dd>One sample from the categorical distribution, given as the index of that
event from log_p.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sample_categorical_log(log_p: Iterator[float]) -&gt; int:
    &#39;&#39;&#39;Generate one sample from a categorical distribution with event
    probabilities provided in unnormalized log-space.

    Parameters
    ----------
    log_p : array_like
        logarithms of event probabilities, ***which need not be normalized***

    Returns
    -------
    int
        One sample from the categorical distribution, given as the index of that
        event from log_p.
    &#39;&#39;&#39;
    try:
        exp_sample = math.log(random.random())
        events = np.logaddexp.accumulate(np.hstack([[-np.inf], log_p]))
        events -= events[-1]
        return next(x[0]-1 for x in enumerate(events) if x[1] &gt;= exp_sample)
    except:
        logging.critical(&#39;CRASHED IN `sample_categorical_log`:\nlog_p{}&#39;.format(
            log_p))
        raise</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="mdsine2.posterior.ClusterAssignments"><code class="flex name class">
<span>class <span class="ident">ClusterAssignments</span></span>
<span>(</span><span>clustering: <a title="mdsine2.pylab.cluster.Clustering" href="pylab/cluster.html#mdsine2.pylab.cluster.Clustering">Clustering</a>, concentration: <a title="mdsine2.posterior.Concentration" href="#mdsine2.posterior.Concentration">Concentration</a>, m: int = 1, mp: str = None, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>This is the posterior of the cluster assignments for each Taxa.</p>
<p>To calculate the loglikelihood of an Taxa being in a cluster, we have to
marginalize out the cluster that the Taxa belongs to - this means marginalizing
out the interactions going in and out of the cluster in question, along with all
the perturbations associated with it.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>clustering</code></strong> :&ensp;<code>pylab.cluster.Clustering</code></dt>
<dd>
<ul>
<li>Defines the clusters</li>
</ul>
</dd>
<dt><strong><code>concentration</code></strong> :&ensp;<code>pylab.Variable</code> or <code>subclass</code> of <code>pylab.Variable</code></dt>
<dd>
<ul>
<li>Defines the concentration parameter for the base distribution</li>
</ul>
</dd>
<dt><strong><code>m</code></strong> :&ensp;<code>int, Optional</code></dt>
<dd>
<ul>
<li>Number of auxiliary variables defined in the model</li>
<li>Default is 1</li>
</ul>
</dd>
<dt><strong><code>mp</code></strong> :&ensp;<code>str, None</code></dt>
<dd>
<ul>
<li>This is the type of multiprocessing it is going to be. Options:<ul>
<li>None<ul>
<li>No multiprocessing</li>
</ul>
</li>
<li>'full-#'<ul>
<li>Send out to the different processors, where '#' is the number of
processors to make</li>
</ul>
</li>
<li>'debug'<ul>
<li>Send out to the different classes but stay on a single core. This
is necessary for benchmarking and easier debugging.</li>
</ul>
</li>
</ul>
</li>
</ul>
</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ClusterAssignments(pl.graph.Node):
    &#39;&#39;&#39;This is the posterior of the cluster assignments for each Taxa.

    To calculate the loglikelihood of an Taxa being in a cluster, we have to
    marginalize out the cluster that the Taxa belongs to - this means marginalizing
    out the interactions going in and out of the cluster in question, along with all
    the perturbations associated with it.

    Parameters
    ----------
    clustering : pylab.cluster.Clustering
        - Defines the clusters
    concentration : pylab.Variable or subclass of pylab.Variable
        - Defines the concentration parameter for the base distribution
    m : int, Optional
        - Number of auxiliary variables defined in the model
        - Default is 1
    mp : str, None
        - This is the type of multiprocessing it is going to be. Options:
            - None
                - No multiprocessing
            - &#39;full-#&#39;
                - Send out to the different processors, where &#39;#&#39; is the number of
                  processors to make
            - &#39;debug&#39;
                - Send out to the different classes but stay on a single core. This
                  is necessary for benchmarking and easier debugging.
    &#39;&#39;&#39;
    def __init__(self, clustering: pl.Clustering, concentration: Concentration,
        m: int=1, mp: str=None, **kwargs):
        self.clustering = clustering # mdsine2.pylab.cluster.Clustering object
        self.concentration = concentration # mdsine2.posterior.Concentration
        self.m = m # int
        self.mp = mp # float
        self._strtime = -1

        # if self.mp is not None:
        #     self.update = self.update_mp
        # else:
        #     self.update = self.update_slow_fast
        self.update = self.update_slow_fast #self.update_mp

        kwargs[&#39;name&#39;] = STRNAMES.CLUSTERING
        pl.graph.Node.__init__(self, **kwargs)

    def __str__(self) -&gt; str:
        try:
            if self.mp == &#39;debug&#39;:
                mpstr = &#39;no mp&#39;
            else:
                mpstr = &#39;mp&#39;
        except:
            mpstr = &#39;NA&#39;
        return str(self.clustering) + &#39;\n{} - Total time: {}&#39;.format(mpstr,
            self._strtime)

    def __getstate__(self):
        state = self.__dict__.copy()
        state.pop(&#39;pool&#39;, None)
        state.pop(&#39;actors&#39;, None)
        return state

    def __setstate__(self, state):
        self.__dict__.update(state)
        self.pool = []
        self.actors = None

    @property
    def value(self) -&gt; pl.Clustering:
        &#39;&#39;&#39;This is so that the cluster assignments get printed in inference.MCMC
        &#39;&#39;&#39;
        return self.clustering

    @property
    def sample_iter(self) -&gt; int:
        &#39;&#39;&#39;Sample iteration
        &#39;&#39;&#39;
        return self.clustering.n_clusters.sample_iter

    def initialize(self, value_option: str, hyperparam_option: str=None, value: Union[np.ndarray, str, List]=None, 
        n_clusters: Union[int, str]=None, delay: int=0, run_every_n_iterations: int=1):
        &#39;&#39;&#39;Initialize the cluster assingments - there are no hyperparamters to
        initialize because the concentration is initialized somewhere else

        Note - if `n_clusters` is not specified and the cluster initialization
        method requires it - it will be set to the expected number of clusters
        which = log(n_taxa)/log(2)

        Parameters
        ----------
        value_option : str
            - The different methods to initialize the clusters. Options:
                - &#39;manual&#39;
                    - Manually set the cluster assignments
                - &#39;no-clusters&#39;
                    - Every Taxa in their own cluster
                - &#39;random&#39;
                    - Every Taxa is randomly assigned to the number of clusters. `n_clusters` required
                - &#39;taxonomy&#39;
                    - Cluster Taxa based on their taxonomic similarity. `n_clusters` required
                - &#39;sequence&#39;
                    - Cluster Taxa based on their sequence similarity. `n_clusters` required
                - &#39;phylogeny&#39;
                    - Cluster Taxa based on their phylogenetic similarity. `n_clusters` required
                - &#39;spearman&#39;, &#39;auto&#39;
                    - Creates a distance matrix based on the spearman rank similarity
                      between two trajectories. We use the raw data. `n_clusters` required
                - &#39;fixed-clustering&#39;
                    - Sets the clustering assignment to the most likely clustering configuration
                      specified in the graph at the location `value` (`value` is a str).
                    - We take the mean coclusterings and do agglomerative clustering on that matrix
                      with the `mode` number of clusters.
        hyperparam_option : None
            Not used in this function - only here for API consistency
        value : list of list
            - Cluster assingments for each of the Taxa
            - Only necessary if `value_option` == &#39;manual&#39;
        n_clusters : int, str
            - Necessary if `value_option` is not &#39;manual&#39; or &#39;no-clusters&#39;.
            - If str, options: &#39;expected&#39;, &#39;auto&#39;: log_2(n_taxa)
        run_every_n_iterations : int
            Only run the update every `run_every_n_iterations` iterations
        &#39;&#39;&#39;
        from sklearn.cluster import AgglomerativeClustering
        from .util import generate_cluster_assignments_posthoc
        taxa = self.G.data.taxa

        self.run_every_n_iterations = run_every_n_iterations
        self.delay = delay

        if value_option not in [&#39;manual&#39;, &#39;no-clusters&#39;, &#39;fixed-clustering&#39;]:
            if pl.isstr(n_clusters):
                if n_clusters in [&#39;expected&#39;, &#39;auto&#39;]:
                    n_clusters = expected_n_clusters(self.G)
                else:
                    raise ValueError(&#39;`n_clusters` ({}) not recognized&#39;.format(n_clusters))
            if not pl.isint(n_clusters):
                raise TypeError(&#39;`n_clusters` ({}) must be a str or an int&#39;.format(type(n_clusters)))
            if n_clusters &lt;= 0:
                raise ValueError(&#39;`n_clusters` ({}) must be &gt; 0&#39;.format(n_clusters))
            if n_clusters &gt; self.G.data.n_taxa:
                raise ValueError(&#39;`n_clusters` ({}) must be &lt;= than the number of s ({})&#39;.format(
                    n_clusters, self.G.data.n_taxa))

        if value_option == &#39;manual&#39;:
            # Check that all of the s are in the init and that it is in the right structure
            if not pl.isarray(value):
                raise ValueError(&#39;if `value_option` is &#34;manual&#34;, value ({}) must &#39; \
                    &#39;be of type array&#39;.format(value.__class__))
            clusters = list(value)

            idxs_to_delete = []
            for idx, cluster in enumerate(clusters):
                if not pl.isarray(cluster):
                    raise ValueError(&#39;cluster at index `{}` ({}) is not an array&#39;.format(
                        idx, cluster))
                cluster = list(cluster)
                if len(cluster) == 0:
                    logging.warning(&#39;Cluster index {} has 0 elements, deleting&#39;.format(
                        idx))
                    idxs_to_delete.append(idx)
            if len(idxs_to_delete) &gt; 0:
                clusters = np.delete(clusters, idxs_to_delete).tolist()

            all_oidxs = OrderedSet()
            for cluster in clusters:
                for oidx in cluster:
                    if not pl.isint(oidx):
                        raise ValueError(&#39;`oidx` ({}) must be an int&#39;.format(oidx.__class__))
                    if oidx &gt;= len(taxa):
                        raise ValueError(&#39;oidx `{}` not in our TaxaSet&#39;.format(oidx))
                    all_oidxs.add(oidx)

            for oidx in range(len(taxa)):
                if oidx not in all_oidxs:
                    raise ValueError(&#39;oidx `{}` in TaxaSet not in `value` ({})&#39;.format(
                        oidx, value))
            # Now everything is checked and valid

        elif value_option == &#39;fixed-clustering&#39;:
            logging.info(&#39;Fixed topology initialization&#39;)
            if not pl.isstr(value):
                raise TypeError(&#39;`value` ({}) must be a str&#39;.format(value))

            CHAIN2 = pl.inference.BaseMCMC.load(value)
            CLUSTERING2 = CHAIN2.graph[STRNAMES.CLUSTERING_OBJ]
            TAXA2 = CHAIN2.graph.data.taxa
            taxa_curr = self.G.data.taxa
            for taxon in TAXA2:
                if taxon.name not in taxa_curr:
                    raise ValueError(&#39;Cannot perform fixed topology because the  {} in &#39; \
                        &#39;the passed in clustering is not in this clustering: {}&#39;.format(
                            taxon.name, taxa_curr.names.order))
            for taxon in taxa_curr:
                if taxon.name not in TAXA2:
                    raise ValueError(&#39;Cannot perform fixed topology because the  {} in &#39; \
                        &#39;the current clustering is not in the passed in clustering: {}&#39;.format(
                            taxon.name, TAXA2.names.order))

            # Get the most likely cluster configuration and set as the value for the passed in cluster
            ret = generate_cluster_assignments_posthoc(CLUSTERING2, n_clusters=&#39;mode&#39;, set_as_value=False)
            CLUSTERING2.from_array(ret)
            logging.info(&#39;Clustering set to:\n{}&#39;.format(str(CLUSTERING2)))

            # Set the passed in cluster assignment as the current cluster assignment
            # Need to be careful because the indices of the s might not line up
            clusters = []
            for cluster in CLUSTERING2:
                anames = [taxa_curr[TAXA2.names.order[aidx]].name for aidx in cluster.members]
                aidxs = [taxa_curr[aname].idx for aname in anames]
                clusters.append(aidxs)

        elif value_option == &#39;no-clusters&#39;:
            clusters = []
            for oidx in range(len(taxa)):
                clusters.append([oidx])

        elif value_option == &#39;random&#39;:
            clusters = {}
            for oidx in range(len(taxa)):
                idx = npr.choice(n_clusters)
                if idx in clusters:
                    clusters[idx].append(oidx)
                else:
                    clusters[idx] = [oidx]
            c = []
            for cid in clusters.keys():
                c.append(clusters[cid])
            clusters = c

        elif value_option == &#39;taxonomy&#39;:
            # Create an affinity matrix, we can precompute the self-similarity to 1
            M = np.diag(np.ones(len(taxa), dtype=float))
            for i, oid1 in enumerate(taxa.ids.order):
                for j, oid2 in enumerate(taxa.ids.order):
                    if i == j:
                        continue
                    M[i,j] = taxa.taxonomic_similarity(oid1=oid1, oid2=oid2)

            c = AgglomerativeClustering(n_clusters=n_clusters, affinity=&#39;precomputed&#39;, linkage=&#39;complete&#39;)
            assignments = c.fit_predict(1-M)

            # Convert assignments into clusters
            clusters = {}
            for oidx,cidx in enumerate(assignments):
                if cidx not in clusters:
                    clusters[cidx] = []
                clusters[cidx].append(oidx)
            clusters = [val for val in clusters.values()]

        elif value_option == &#39;sequence&#39;:
            from pylab import diversity

            logging.info(&#39;Making affinity matrix from sequences&#39;)
            evenness = np.diag(np.ones(len(self.G.data.taxa), dtype=float))

            for i in range(len(self.G.data.taxa)):
                for j in range(len(self.G.data.taxa)):
                    if j &lt;= i:
                        continue
                    # Subtract because we want to make a similarity matrix
                    dist = 1-diversity.beta.hamming(
                        list(self.G.data.taxa[i].sequence),
                        list(self.G.data.taxa[j].sequence))
                    evenness[i,j] = dist
                    evenness[j,i] = dist

            c = AgglomerativeClustering(n_clusters=n_clusters, affinity=&#39;precomputed&#39;, linkage=&#39;average&#39;)
            assignments = c.fit_predict(evenness)
            clusters = {}
            for oidx,cidx in enumerate(assignments):
                if cidx not in clusters:
                    clusters[cidx] = []
                clusters[cidx].append(oidx)
            clusters = [val for val in clusters.values()]

        elif value_option == &#39;spearman&#39;:
            # Use spearman correlation to create a distance matrix
            # Use agglomerative clustering to make the clusters based
            # on distance matrix (distance = 1 - pearson(x,y))
            dm = np.zeros(shape=(len(taxa), len(taxa)))
            data = []
            for ridx in range(self.G.data.n_replicates):
                data.append(self.G.data.abs_data[ridx])
            data = np.hstack(data)
            for i in range(len(taxa)):
                for j in range(i+1):
                    distance = (1 - scipy.stats.spearmanr(data[i, :], data[j, :])[0])/2
                    dm[i,j] = distance
                    dm[j,i] = distance

            c = AgglomerativeClustering(n_clusters=n_clusters, affinity=&#39;precomputed&#39;, linkage=&#39;complete&#39;)
            assignments = c.fit_predict(dm)

            # convert into clusters
            clusters = {}
            for oidx, cidx in enumerate(assignments):
                if cidx not in clusters:
                    clusters[cidx] = []
                clusters[cidx].append(oidx)
            clusters = [val for val in clusters.values()]

        elif value_option == &#39;phylogeny&#39;:
            raise NotImplementedError(&#39;`phylogeny` not implemented yet&#39;)

        else:
            raise ValueError(&#39;`value_option` &#34;{}&#34; not recognized&#39;.format(value_option))

        # Move all the taxa into their assigned clusters
        for cluster in clusters:
            cid = None
            for oidx in cluster:
                if cid is None:
                    # make new cluster
                    cid = self.clustering.make_new_cluster_with(idx=oidx)
                else:
                    self.clustering.move_item(idx=oidx, cid=cid)
        logging.info(&#39;Cluster Assingments initialization results:\n{}&#39;.format(
            str(self.clustering)))
        self._there_are_perturbations = self.G.perturbations is not None

        # Initialize the multiprocessors if necessary
        if self.mp is not None:
            if not pl.isstr(self.mp):
                raise TypeError(&#39;`mp` ({}) must be a str&#39;.format(type(self.mp)))
            if &#39;full&#39; in self.mp:
                n_cpus = self.mp.split(&#39;-&#39;)[1]
                if n_cpus == &#39;auto&#39;:
                    self.n_cpus = psutil.cpu_count(logical=False)
                else:
                    try:
                        self.n_cpus = int(n_cpus)
                    except:
                        raise ValueError(&#39;`mp` ({}) not recognized&#39;.format(self.mp))
                self.pool = pl.multiprocessing.PersistentPool(ptype=&#39;dasw&#39;, G=self.G)
            elif self.mp == &#39;debug&#39;:
                self.pool = None
            else:
                raise ValueError(&#39;`mp` ({}) not recognized&#39;.format(self.mp))
        else:
            self.pool = None

        self.ndts_bias = []
        self.n_taxa = len(self.G.data.taxa)
        self.n_replicates = self.G.data.n_replicates
        self.n_dts_for_replicate = self.G.data.n_dts_for_replicate
        self.total_dts = np.sum(self.n_dts_for_replicate)
        for ridx in range(self.G.data.n_replicates):
            self.ndts_bias.append(
                np.arange(0, self.G.data.n_dts_for_replicate[ridx] * self.n_taxa, self.n_taxa))
        self.replicate_bias = np.zeros(self.n_replicates, dtype=int)
        for ridx in range(1, self.n_replicates):
            self.replicate_bias[ridx] = self.replicate_bias[ridx-1] + \
                self.n_taxa * self.n_dts_for_replicate[ridx - 1]

    def visualize(self, basepath: str, f: IO, section: str=&#39;posterior&#39;, 
        taxa_formatter: str=&#39;%(paperformat)s&#39;, yticklabels: str=&#39;%(paperformat)s %(index)s&#39;, 
        xticklabels: str=&#39;%(index)s&#39;, tax_fmt: str=&#39;%(family)s&#39;) -&gt; IO:
        &#39;&#39;&#39;Render the traces in the folder `basepath` and write the 
        learned values to the file `f`.

        Parameters
        ----------
        basepath : str
            This is the loction to write the files to
        f : _io.TextIOWrapper
            File that we are writing the values to
        section : str
            Section of the trace to compute on. Options:
                &#39;posterior&#39; : posterior samples
                &#39;burnin&#39; : burn-in samples
                &#39;entire&#39; : both burn-in and posterior samples
        taxa_formatter : str, None
            This is the format of the label to return for each . If None, it will return
            the taxon&#39;s name
        yticklabels, xticklabels : str
            These are the formats to plot the y-axis and x0axis, respectively.
        tax_fmt : str
            This is the format to render the taxonomic distiribution plot

        Returns
        -------
        _io.TextIOWrapper
        &#39;&#39;&#39;
        from matplotlib.colors import LogNorm
        taxa = self.G.data.taxa
        f.write(&#39;\n\n###################################\n&#39;)
        f.write(self.name)
        f.write(&#39;\n###################################\n&#39;)
        if not self.G.inference.is_in_inference_order(self):
            f.write(&#39;`{}` not learned. These were the fixed cluster assignments\n&#39;.format(self.name))
            for cidx, cluster in enumerate(self.clustering):
                f.write(&#39;Cluster {}:\n&#39;.format(cidx+1))
                for aidx in cluster.members:
                    label = pl.taxaname_formatter(format=taxa_formatter, taxon=taxa[aidx], taxa=taxa)
                    f.write(&#39;\t- {}\n&#39;.format(label))
            return f

        # Coclusters
        cocluster_trace = self.clustering.coclusters.get_trace_from_disk(section=section)
        coclusters = pl.variables.summary(cocluster_trace, section=section)[&#39;mean&#39;]
        for i in range(coclusters.shape[0]):
            coclusters[i,i] = np.nan

        # N clusters
        visualization.render_trace(var=self.clustering.n_clusters, plt_type=&#39;both&#39;, 
            section=section, include_burnin=True, rasterized=True)
        fig = plt.gcf()
        fig.suptitle(&#39;Number of Clusters&#39;)
        plt.savefig(os.path.join(basepath, &#39;n_clusters.pdf&#39;))
        plt.close()

        ret = generate_cluster_assignments_posthoc(clustering=self.clustering, n_clusters=&#39;mode&#39;, 
            section=section, set_as_value=True)
        data = []
        for taxon in taxa:
            data.append([taxon.name, ret[taxon.idx]])
        df = pd.DataFrame(data, columns=[&#39;name&#39;, &#39;Cluster Assignment&#39;])
        df.to_csv(os.path.join(basepath, &#39;clusterassignments.tsv&#39;), sep=&#39;\t&#39;, index=False, header=True)
        order = _make_cluster_order(graph=self.G, section=section)

        visualization.render_cocluster_probabilities(
            coclusters=coclusters, taxa=self.G.data.taxa, order=order,
            yticklabels=yticklabels, include_tick_marks=False, xticklabels=xticklabels,
            title=&#39;Cluster Assignments&#39;)

        # Write out cocluster values
        coclusters = coclusters[order, :]
        coclusters = coclusters[:, order]
        labels = [taxa[aidx].name for aidx in order]
        df = pd.DataFrame(coclusters, index=labels, columns=labels)
        df.to_csv(os.path.join(basepath, &#39;coclusters.tsv&#39;), index=True, header=True)

        fig = plt.gcf()
        fig.tight_layout()
        plt.savefig(os.path.join(basepath, &#39;coclusters.pdf&#39;))
        plt.close()

        # Make taxonomic distribution plot
        df = generate_taxonomic_distribution_over_clusters_posthoc(
            mcmc=self.G.inference, tax_fmt=tax_fmt)

        df[df==0] = np.nan
        fig = plt.figure()
        ax = fig.add_subplot(111)
        ax = sns.heatmap(df, ax=ax, cmap=&#39;Blues&#39;,
            norm=LogNorm(vmin=np.nanmin(df.to_numpy()), vmax=np.nanmax(df.to_numpy())))
        ax.set_title(&#39;Taxonomic abundance per cluster&#39;)
        ax.set_xlabel(&#39;Clusters&#39;)
        ax.set_ylabel(tax_fmt.replace(&#39;%(&#39;, &#39;&#39;).replace(&#39;)s&#39;, &#39;&#39;).capitalize())
        fig.tight_layout()
        plt.savefig(os.path.join(basepath, &#39;taxonomic_distribution.pdf&#39;))
        plt.close()

        f.write(&#39;Mode number of clusters: {}\n&#39;.format(len(self.clustering)))
        for cidx, cluster in enumerate(self.clustering):
            f.write(&#39;Cluster {} - Size {}\n&#39;.format(cidx, len(cluster)))
            for oidx in cluster.members:
                label = pl.taxaname_formatter(format=taxa_formatter, taxon=taxa[oidx], taxa=taxa)
                f.write(&#39;\t- {}\n&#39;.format(label))

        return f

    def set_trace(self):
        self.clustering.set_trace()

    def remove_local_trace(self):
        &#39;&#39;&#39;Delete the local trace
        &#39;&#39;&#39;
        self.clustering.trace = None

    def add_trace(self):
        self.clustering.add_trace()

    def kill(self):
        if pl.ispersistentpool(self.pool):
            # For pylab multiprocessing, explicitly kill them
            self.pool.kill()
        return

    # Update super safe - meant to be used during debugging
    # =====================================================
    def update_slow(self):
        &#39;&#39;&#39; This is updating the new cluster. Depending on the iteration you do
        either split-merge Metropolis-Hasting update or a regular Gibbs update. To
        get highest mixing we alternate between each.
        &#39;&#39;&#39;
        if self.clustering.n_clusters.sample_iter &lt; self.delay:
            return

        if self.clustering.n_clusters.sample_iter % self.run_every_n_iterations != 0:
           return

        start_time = time.time()
        oidxs = npr.permutation(np.arange(len(self.G.data.taxa)))

        for oidx in oidxs:
            self.gibbs_update_single_taxon_slow(oidx=oidx)
        self._strtime = time.time() - start_time

    def gibbs_update_single_taxon_slow(self, oidx: int):
        &#39;&#39;&#39;The update function is based off of Algorithm 8 in &#39;Markov Chain
        Sampling Methods for Dirichlet Process Mixture Models&#39; by Radford M.
        Neal, 2000.

        Calculate the marginal likelihood of the taxon in every cluster
        and a new cluster then sample from `self.sample_categorical_log`
        to get the cluster assignment.

        Parameters
        ----------
        oidx : int
            Taxa index that we are updating the cluster assignment of
        &#39;&#39;&#39;
        curr_cluster = self.clustering.idx2cid[oidx]
        concentration = self.concentration.value

        # start as a dictionary then send values to `sample_categorical_log`
        LOG_P = []
        LOG_KEYS = []

        # Calculate current cluster
        # =========================
        # If the element is already in its own cluster, use the new cluster case
        if self.clustering.clusters[curr_cluster].size == 1:
            a = np.log(concentration/self.m)
        else:
            a = np.log(self.clustering.clusters[curr_cluster].size - 1)
        LOG_P.append(a + self.calculate_marginal_loglikelihood_slow()[&#39;ret&#39;])
        LOG_KEYS.append(curr_cluster)

        # Calculate going to every other cluster
        # ======================================
        for cid in self.clustering.order:
            if curr_cluster == cid:
                continue

            # Move Taxa and recompute the matrices
            self.clustering.move_item(idx=oidx,cid=cid)
            self.G[STRNAMES.CLUSTER_INTERACTION_INDICATOR].update_cnt_indicators()
            self.G.data.design_matrices[STRNAMES.CLUSTER_INTERACTION_VALUE].M.build()
            if self._there_are_perturbations:
                self.G.data.design_matrices[STRNAMES.PERT_VALUE].M.build()

            LOG_P.append(np.log(self.clustering.clusters[cid].size - 1) + \
                self.calculate_marginal_loglikelihood_slow()[&#39;ret&#39;])
            LOG_KEYS.append(cid)


        # Calculate new cluster
        # =====================
        cid=self.clustering.make_new_cluster_with(idx=oidx)
        self.G[STRNAMES.CLUSTER_INTERACTION_INDICATOR].update_cnt_indicators()
        self.G.data.design_matrices[STRNAMES.CLUSTER_INTERACTION_VALUE].M.build()
        if self._there_are_perturbations:
            self.G.data.design_matrices[STRNAMES.PERT_VALUE].M.build()
        
        LOG_KEYS.append(cid)
        LOG_P.append(np.log(concentration/self.m) + \
            self.calculate_marginal_loglikelihood_slow()[&#39;ret&#39;])

        # Sample the assignment
        # =====================
        idx = sample_categorical_log(LOG_P)
        assigned_cid = LOG_KEYS[idx]
        curr_clus = self.clustering.idx2cid[oidx]

        if assigned_cid != curr_clus:
            self.clustering.move_item(idx=oidx,cid=assigned_cid)
            self.G[STRNAMES.CLUSTER_INTERACTION_INDICATOR].update_cnt_indicators()

            # Change the mixing matrix for the interactions and (potentially) perturbations
            self.G.data.design_matrices[STRNAMES.CLUSTER_INTERACTION_VALUE].M.build()
            if self._there_are_perturbations:
                self.G.data.design_matrices[STRNAMES.PERT_VALUE].M.build()

    def calculate_marginal_loglikelihood_slow(self):
        &#39;&#39;&#39;Marginalizes out the interactions and the perturbations
        &#39;&#39;&#39;
        # Build the parameters
        # ====================
        self.G[STRNAMES.CLUSTER_INTERACTION_INDICATOR].update_cnt_indicators()
        lhs = [STRNAMES.GROWTH_VALUE, STRNAMES.SELF_INTERACTION_VALUE]
        if self._there_are_perturbations:
            rhs = [STRNAMES.PERT_VALUE, STRNAMES.CLUSTER_INTERACTION_VALUE]
        else:
            rhs = [STRNAMES.CLUSTER_INTERACTION_VALUE]

        # reconstruct the X matrices
        for v in rhs:
            self.G.data.design_matrices[v].M.build()
        
        y = self.G.data.construct_lhs(lhs, 
            kwargs_dict={STRNAMES.GROWTH_VALUE:{&#39;with_perturbations&#39;: False}})
        X = self.G.data.construct_rhs(keys=rhs, toarray=True)
        process_prec = self.G[STRNAMES.PROCESSVAR].build_matrix(cov=False, sparse=False)
        prior_prec = build_prior_covariance(G=self.G, cov=False, order=rhs, sparse=False)
        prior_var = build_prior_covariance(G=self.G, cov=True, order=rhs, sparse=False)
        prior_mean = build_prior_mean(G=self.G, order=rhs, shape=(-1,1))

        # If nothing is on, return 0
        if X.shape[1] == 0:
            return {
                &#39;a&#39;: 0,
                &#39;beta_prec&#39;: 0,
                &#39;process_prec&#39;: prior_prec,
                &#39;ret&#39;: 0,
                &#39;beta_logdet&#39;: 0,
                &#39;priorvar_logdet&#39;: 0,
                &#39;bEb&#39;: 0,
                &#39;bEbprior&#39;: 0}

        # Calculate the marginalization
        # =============================
        beta_prec = X.T @ process_prec @ X + prior_prec
        beta_cov = pinv(beta_prec, self)
        beta_mean = beta_cov @ ( X.T @ process_prec @ y + prior_prec @ prior_mean )
        beta_mean = np.asarray(beta_mean).reshape(-1,1)

        try:
            beta_logdet = log_det(beta_cov, self)
        except:
            logging.critical(&#39;Crashed in log_det&#39;)
            logging.critical(&#39;beta_cov:\n{}&#39;.format(beta_cov))
            logging.critical(&#39;prior_prec\n{}&#39;.format(prior_prec))
            raise
        priorvar_logdet = log_det(prior_var, self)
        ll2 = 0.5 * (beta_logdet - priorvar_logdet)

        bEbprior = np.asarray(prior_mean.T @ prior_prec @ prior_mean)[0,0]
        bEb = np.asarray(beta_mean.T @ beta_prec @ beta_mean)[0,0]
        ll3 = 0.5 * (bEb  - bEbprior)

        # print(&#39;prior_prec truth:\n&#39;, prior_prec)

        return {
            &#39;a&#39;: X.T @ process_prec, &#39;beta_prec&#39;: beta_prec, &#39;process_prec&#39;: prior_prec, 
            &#39;ret&#39;: ll2+ll3, &#39;beta_logdet&#39;: beta_logdet, &#39;priorvar_logdet&#39;: priorvar_logdet,
            &#39;bEb&#39;: bEb, &#39;bEbprior&#39;: bEbprior}

    # Update regular - meant to be used during inference
    # ==================================================
    # @profile
    def update_slow_fast(self):
        &#39;&#39;&#39;Much faster than `update_slow`
        &#39;&#39;&#39;

        if self.clustering.n_clusters.sample_iter &lt; self.delay:
            return

        if self.clustering.n_clusters.sample_iter % self.run_every_n_iterations != 0:
           return

        start_time = time.time()

        self.process_prec = self.G[STRNAMES.PROCESSVAR].prec.ravel() #.build_matrix(cov=False, sparse=False)
        self.process_prec_matrix = self.G[STRNAMES.PROCESSVAR].build_matrix(sparse=True, cov=False)
        lhs = [STRNAMES.GROWTH_VALUE, STRNAMES.SELF_INTERACTION_VALUE]
        self.y = self.G.data.construct_lhs(lhs, 
            kwargs_dict={STRNAMES.GROWTH_VALUE:{&#39;with_perturbations&#39;: False}})

        oidxs = npr.permutation(len(self.G.data.taxa))
        iii = 0
        for oidx in oidxs:
            logging.info(&#39;{}/{}: {}&#39;.format(iii, len(oidxs), oidx))
            self.gibbs_update_single_taxon_slow_fast(oidx=oidx)
            iii += 1
        self._strtime = time.time() - start_time
    
    # @profile
    def gibbs_update_single_taxon_slow_fast(self, oidx: int):
        &#39;&#39;&#39;The update function is based off of Algorithm 8 in &#39;Markov Chain
        Sampling Methods for Dirichlet Process Mixture Models&#39; by Radford M.
        Neal, 2000.

        Calculate the marginal likelihood of the taxon in every cluster
        and a new cluster then sample from `self.sample_categorical_log`
        to get the cluster assignment.

        Parameters
        ----------
        oidx : int
            Taxa index that we are updating the cluster assignment of
        &#39;&#39;&#39;
        curr_cluster = self.clustering.idx2cid[oidx]
        concentration = self.concentration.value

        # start as a dictionary then send values to `sample_categorical_log`
        LOG_P = []
        LOG_KEYS = []

        # Calculate current cluster
        # =========================
        # If the element is already in its own cluster, use the new cluster case
        if self.clustering.clusters[curr_cluster].size == 1:
            a = np.log(concentration/self.m)
        else:
            a = np.log(self.clustering.clusters[curr_cluster].size - 1)
        LOG_P.append(a + self.calculate_marginal_loglikelihood_slow_fast_sparse())
        LOG_KEYS.append(curr_cluster)

        # Calculate going to every other cluster
        # ======================================
        for cid in self.clustering.order:
            if curr_cluster == cid:
                continue

            # Move Taxa and recompute the matrices
            self.clustering.move_item(idx=oidx,cid=cid)
            self.G[STRNAMES.CLUSTER_INTERACTION_INDICATOR].update_cnt_indicators()
            self.G.data.design_matrices[STRNAMES.CLUSTER_INTERACTION_VALUE].M.build()
            if self._there_are_perturbations:
                self.G.data.design_matrices[STRNAMES.PERT_VALUE].M.build()

            LOG_P.append(np.log(self.clustering.clusters[cid].size - 1) + \
                self.calculate_marginal_loglikelihood_slow_fast_sparse())
            LOG_KEYS.append(cid)


        # Calculate new cluster
        # =====================
        cid=self.clustering.make_new_cluster_with(idx=oidx)
        self.G[STRNAMES.CLUSTER_INTERACTION_INDICATOR].update_cnt_indicators()
        self.G.data.design_matrices[STRNAMES.CLUSTER_INTERACTION_VALUE].M.build()
        if self._there_are_perturbations:
            self.G.data.design_matrices[STRNAMES.PERT_VALUE].M.build()
        
        LOG_KEYS.append(cid)
        LOG_P.append(np.log(concentration/self.m) + \
            self.calculate_marginal_loglikelihood_slow_fast_sparse())

        # Sample the assignment
        # =====================
        idx = sample_categorical_log(LOG_P)
        assigned_cid = LOG_KEYS[idx]
        curr_clus = self.clustering.idx2cid[oidx]

        if assigned_cid != curr_clus:
            self.clustering.move_item(idx=oidx,cid=assigned_cid)
            self.G[STRNAMES.CLUSTER_INTERACTION_INDICATOR].update_cnt_indicators()

            # Change the mixing matrix for the interactions and (potentially) perturbations
            self.G.data.design_matrices[STRNAMES.CLUSTER_INTERACTION_VALUE].M.build()
            if self._there_are_perturbations:
                self.G.data.design_matrices[STRNAMES.PERT_VALUE].M.build()

    # @profile
    def calculate_marginal_loglikelihood_slow_fast(self):
        &#39;&#39;&#39;Marginalizes out the interactions and the perturbations
        &#39;&#39;&#39;
        # Build the parameters
        # ====================
        if self._there_are_perturbations:
            rhs = [STRNAMES.PERT_VALUE, STRNAMES.CLUSTER_INTERACTION_VALUE]
        else:
            rhs = [STRNAMES.CLUSTER_INTERACTION_VALUE]
        
        
        y = self.y
        X = self.G.data.construct_rhs(keys=rhs, toarray=True)

        process_prec = self.process_prec
        prior_prec = build_prior_covariance(G=self.G, cov=False, order=rhs, sparse=False)
        prior_prec_diag = np.diag(prior_prec)
        prior_var = build_prior_covariance(G=self.G, cov=True, order=rhs, sparse=False)
        prior_mean = build_prior_mean(G=self.G, order=rhs, shape=(-1,1))

        # Calculate the marginalization
        # =============================
        a = X.T * process_prec

        beta_prec = a @ X + prior_prec
        beta_cov = pinv(beta_prec, self)
        beta_mean = beta_cov @ ( a @ y + prior_prec @ prior_mean )
        beta_mean = np.asarray(beta_mean).reshape(-1,1)

        try:
            beta_logdet = log_det(beta_cov, self)
        except:
            logging.critical(&#39;Crashed in log_det&#39;)
            logging.critical(&#39;beta_cov:\n{}&#39;.format(beta_cov))
            logging.critical(&#39;prior_prec\n{}&#39;.format(prior_prec))
            raise
        priorvar_logdet = log_det(prior_var, self)
        ll2 = 0.5 * (beta_logdet - priorvar_logdet)

        a = np.sum((prior_mean.ravel() ** 2) *prior_prec_diag)
        # np.asarray(prior_mean.T @ prior_prec @ prior_mean)[0,0]
        b = np.asarray(beta_mean.T @ beta_prec @ beta_mean)[0,0]
        ll3 = -0.5 * (a  - b)

        return ll2+ll3

    # @profile
    def calculate_marginal_loglikelihood_slow_fast_sparse(self):
        &#39;&#39;&#39;Marginalizes out the interactions and the perturbations
        &#39;&#39;&#39;
        # Build the parameters
        # ====================
        if self._there_are_perturbations:
            rhs = [STRNAMES.PERT_VALUE, STRNAMES.CLUSTER_INTERACTION_VALUE]
        else:
            rhs = [STRNAMES.CLUSTER_INTERACTION_VALUE]
        
        
        y = self.y
        X = self.G.data.construct_rhs(keys=rhs, toarray=False)

        process_prec = self.process_prec_matrix
        prior_prec = build_prior_covariance(G=self.G, cov=False, order=rhs, sparse=True)  
        prior_prec_diag = build_prior_covariance(G=self.G, cov=False, order=rhs, diag=True)        
        prior_var = build_prior_covariance(G=self.G, cov=True, order=rhs, sparse=True)
        prior_mean = build_prior_mean(G=self.G, order=rhs, shape=(-1,1))

        # Calculate the marginalization
        # =============================

        # print(&#39;X&#39;)
        # print(type(X))
        # print(X.shape)

        # print(&#39;process_prec&#39;)
        # print(type(process_prec))
        # print(process_prec.shape)

        # print(&#39;prior_prec&#39;)
        # print(type(prior_prec))
        # print(prior_prec.shape)

        # print(&#39;prior mean&#39;)
        # print(type(prior_mean))
        # print(prior_mean.shape)

        a = X.T.dot(process_prec)
        beta_prec = a.dot(X) + prior_prec
        beta_cov = pinv(beta_prec, self)
        beta_mean = beta_cov @ ( a.dot(y) + prior_prec.dot(prior_mean))
        beta_mean = np.asarray(beta_mean).reshape(-1,1)

        try:
            beta_logdet = log_det(beta_cov, self)
        except:
            logging.critical(&#39;Crashed in log_det&#39;)
            logging.critical(&#39;beta_cov:\n{}&#39;.format(beta_cov))
            logging.critical(&#39;prior_prec\n{}&#39;.format(prior_prec))
            raise
        priorvar_logdet = log_det(prior_var, self)
        ll2 = 0.5 * (beta_logdet - priorvar_logdet)

        a = np.sum((prior_mean.ravel() ** 2) *prior_prec_diag)
        # np.asarray(prior_mean.T @ prior_prec @ prior_mean)[0,0]

        # print(&#39;beta_prec.shape&#39;, beta_prec.shape)
        # print(&#39;beta_mean.shape&#39;, beta_mean.shape)

        b = np.asarray(beta_mean.T @ beta_prec.dot(beta_mean))[0,0]
        ll3 = -0.5 * (a  - b)

        return ll2+ll3

    # Update MP - meant to be used during inference
    # =============================================
    def update_mp(self):
        &#39;&#39;&#39;Implements `update_slow` but parallelizes calculating the likelihood
        of being in a cluster. NOTE that this does not parallelize on the Taxa level.

        On the first gibb step with initialize the workers that we implement with DASW (
        different arguments, single worker). For more information what this means look
        at pylab.multiprocessing documentation.

        If we initialized our pool as a pylab.multiprocessing.PersistentPool, then we 
        multiprocess the likelihood calculations for each taxa. If we didnt then this 
        implementation has the same performance as `ClusterAssignments.update_slow_fast`.
        &#39;&#39;&#39;
        if self.G.data.zero_inflation_transition_policy is not None:
            raise NotImplementedError(&#39;Multiprocessing for zero inflation data is not implemented yet.&#39; \
                &#39; Use `mp=None`&#39;)
        DMI = self.G.data.design_matrices[STRNAMES.CLUSTER_INTERACTION_VALUE]
        DMP = self.G.data.design_matrices[STRNAMES.PERT_VALUE]
        if self.clustering.n_clusters.sample_iter == 0 or self.pool == []:
            kwargs = {
                &#39;n_taxa&#39;: len(self.G.data.taxa),
                &#39;total_n_dts_per_taxon&#39;: self.G.data.total_n_dts_per_taxon,
                &#39;n_replicates&#39;: self.G.data.n_replicates,
                &#39;n_dts_for_replicate&#39;: self.G.data.n_dts_for_replicate,
                &#39;there_are_perturbations&#39;: self._there_are_perturbations,
                &#39;keypair2col_interactions&#39;: DMI.M.keypair2col,
                &#39;keypair2col_perturbations&#39;: DMP.M.keypair2col,
                &#39;n_perturbations&#39;: len(self.G.perturbations) if self._there_are_perturbations else None,
                &#39;base_Xrows&#39;: DMI.base.rows,
                &#39;base_Xcols&#39;: DMI.base.cols,
                &#39;base_Xshape&#39;: DMI.base.shape,
                &#39;base_Xpertrows&#39;: DMP.base.rows,
                &#39;base_Xpertcols&#39;: DMP.base.cols,
                &#39;base_Xpertshape&#39;: DMP.base.shape,
                &#39;n_rowsM&#39;: DMI.M.n_rows,
                &#39;n_rowsMpert&#39;: DMP.M.n_rows}

            if pl.ispersistentpool(self.pool):
                for _ in range(self.n_cpus):
                    self.pool.add_worker(SingleClusterFullParallelization(**kwargs))
            else:
                self.pool = SingleClusterFullParallelization(**kwargs)
        if self.clustering.n_clusters.sample_iter &lt; self.delay:
            return
        if self.clustering.n_clusters.sample_iter % self.run_every_n_iterations != 0:
            return
        # Send in arguments for the start of the gibbs step
        start_time = time.time()
        base_Xdata = DMI.base.data
        self.concentration = self.G[STRNAMES.CONCENTRATION].value
        y = self.G.data.construct_lhs(keys=[STRNAMES.GROWTH_VALUE, STRNAMES.SELF_INTERACTION_VALUE],
            kwargs_dict={STRNAMES.GROWTH_VALUE: {&#39;with_perturbations&#39;:False}})
        prior_var_interactions = self.G[STRNAMES.PRIOR_VAR_INTERACTIONS].value
        prior_mean_interactions = self.G[STRNAMES.PRIOR_MEAN_INTERACTIONS].value
        process_prec_diag = self.G[STRNAMES.PROCESSVAR].prec

        if self._there_are_perturbations:
            prior_var_pert = self.G[STRNAMES.PRIOR_VAR_PERT].get_single_value_of_perts()
            prior_mean_pert = self.G[STRNAMES.PRIOR_MEAN_PERT].get_single_value_of_perts()
            base_Xpertdata = DMP.base.data
        else:
            prior_var_pert = None
            prior_mean_pert = None
            base_Xpertdata = None

        kwargs = {
            &#39;base_Xdata&#39;: base_Xdata,
            &#39;base_Xpertdata&#39;: base_Xpertdata,
            &#39;concentration&#39;: self.concentration,
            &#39;m&#39;: self.m,
            &#39;y&#39;: y,
            &#39;process_prec_diag&#39;: process_prec_diag,
            &#39;prior_var_interactions&#39;: prior_var_interactions,
            &#39;prior_var_pert&#39;: prior_var_pert,
            &#39;prior_mean_interactions&#39;: prior_mean_interactions,
            &#39;prior_mean_pert&#39;: prior_mean_pert}
        
        if pl.ispersistentpool(self.pool):
            self.pool.map(&#39;initialize_gibbs&#39;, [kwargs]*self.pool.num_workers)
        else:
            self.pool.initialize_gibbs(**kwargs)

        oidxs = npr.permutation(np.arange(len(self.G.data.taxa)))
        for iii, oidx in enumerate(oidxs):
            logging.info(&#39;{}/{} - {}&#39;.format(iii, len(self.G.data.taxa), oidx))
            self.oidx = oidx
            self.gibbs_update_single_taxon_parallel()

        self._strtime = time.time() - start_time

    def gibbs_update_single_taxon_parallel(self):
        &#39;&#39;&#39;Update for a single taxon
        &#39;&#39;&#39;
        self.original_cluster = self.clustering.idx2cid[self.oidx]
        self.curr_cluster = self.original_cluster

        interactions = self.G[STRNAMES.INTERACTIONS_OBJ]
        interaction_on_idxs = interactions.get_indicators(return_idxs=True)
        if self._there_are_perturbations:
            perturbation_on_idxs = [p.indicator.cluster_arg_array() for p in self.G.perturbations]
        else:
            perturbation_on_idxs = None
        use_saved_params = False

        if pl.ispersistentpool(self.pool):
            self.pool.staged_map_start(&#39;run&#39;)
        else:
            notpool_ret = []

        # Get the likelihood of the current configuration
        if self.clustering.clusters[self.original_cluster].size == 1:
            log_mult_factor = math.log(self.concentration/self.m)
        else:
            log_mult_factor = math.log(self.clustering.clusters[self.original_cluster].size - 1)

        if use_saved_params and pl.ispersistentpool(self.pool):
            interaction_on_idxs = None
            perturbation_on_idxs = None

        cluster_config = self.clustering.toarray()

        kwargs = {
            &#39;interaction_on_idxs&#39;: interaction_on_idxs,
            &#39;perturbation_on_idxs&#39;: perturbation_on_idxs,
            &#39;cluster_config&#39;: cluster_config,
            &#39;log_mult_factor&#39;: log_mult_factor,
            &#39;cid&#39;: self.original_cluster,
            &#39;use_saved_params&#39;: use_saved_params}

        if pl.ispersistentpool(self.pool):
            self.pool.staged_map_put(kwargs)
        else:
            notpool_ret.append(self.pool.run(**kwargs))

        # Check every cluster
        for cid in self.clustering.order:
            if cid == self.original_cluster:
                continue
            self.clustering.move_item(idx=self.oidx, cid=cid)
            self.curr_cluster = cid

            if not use_saved_params:
                interaction_on_idxs = interactions.get_indicators(return_idxs=True)
                if self._there_are_perturbations:
                    perturbation_on_idxs = [p.indicator.cluster_arg_array() for p in self.G.perturbations]
                else:
                    perturbation_on_idxs = None

            cluster_config = self.clustering.toarray()
            log_mult_factor = np.log(self.clustering.clusters[self.curr_cluster].size - 1)

            kwargs = {
                &#39;interaction_on_idxs&#39;: interaction_on_idxs,
                &#39;perturbation_on_idxs&#39;: perturbation_on_idxs,
                &#39;cluster_config&#39;: cluster_config,
                &#39;log_mult_factor&#39;: log_mult_factor,
                &#39;cid&#39;: self.curr_cluster,
                &#39;use_saved_params&#39;: use_saved_params}

            if pl.ispersistentpool(self.pool):
                self.pool.staged_map_put(kwargs)
            else:
                notpool_ret.append(self.pool.run(**kwargs))

        # Make a new cluster
        self.curr_cluster = self.clustering.make_new_cluster_with(idx=self.oidx)
        cluster_config = self.clustering.toarray()
        interaction_on_idxs = interactions.get_indicators(return_idxs=True)
        if self._there_are_perturbations:
            perturbation_on_idxs = [p.indicator.cluster_arg_array() for p in self.G.perturbations]
        else:
            perturbation_on_idxs = None
        log_mult_factor = np.log(self.concentration/self.m)

        kwargs = {
            &#39;interaction_on_idxs&#39;: interaction_on_idxs,
            &#39;perturbation_on_idxs&#39;: perturbation_on_idxs,
            &#39;cluster_config&#39;: cluster_config,
            &#39;log_mult_factor&#39;: log_mult_factor,
            &#39;cid&#39;: self.curr_cluster,
            &#39;use_saved_params&#39;: False}

        # Put the values and get if necessary
        KEYS = []
        LOG_P = []
        if pl.ispersistentpool(self.pool):
            self.pool.staged_map_put(kwargs)
            ret = self.pool.staged_map_get()
        else:
            notpool_ret.append(self.pool.run(**kwargs))
            ret = notpool_ret
        for c, p in ret:
            KEYS.append(c)
            LOG_P.append(p)

        idx = sample_categorical_log(LOG_P)
        assigned_cid = KEYS[idx]

        if assigned_cid != self.original_cluster:
            logging.info(&#39;cluster changed&#39;)

        self.clustering.move_item(idx=self.oidx, cid=assigned_cid)

        self.G[STRNAMES.CLUSTER_INTERACTION_INDICATOR].update_cnt_indicators()
        self.G.data.design_matrices[STRNAMES.CLUSTER_INTERACTION_VALUE].M.build()
        if self._there_are_perturbations:
            self.G.data.design_matrices[STRNAMES.PERT_VALUE].M.build()</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="mdsine2.pylab.graph.Node" href="pylab/graph.html#mdsine2.pylab.graph.Node">Node</a></li>
<li><a title="mdsine2.pylab.graph.BaseNode" href="pylab/graph.html#mdsine2.pylab.graph.BaseNode">BaseNode</a></li>
<li><a title="mdsine2.pylab.base.Saveable" href="pylab/base.html#mdsine2.pylab.base.Saveable">Saveable</a></li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="mdsine2.posterior.ClusterAssignments.sample_iter"><code class="name">var <span class="ident">sample_iter</span> : int</code></dt>
<dd>
<div class="desc"><p>Sample iteration</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def sample_iter(self) -&gt; int:
    &#39;&#39;&#39;Sample iteration
    &#39;&#39;&#39;
    return self.clustering.n_clusters.sample_iter</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.ClusterAssignments.value"><code class="name">var <span class="ident">value</span> : <a title="mdsine2.pylab.cluster.Clustering" href="pylab/cluster.html#mdsine2.pylab.cluster.Clustering">Clustering</a></code></dt>
<dd>
<div class="desc"><p>This is so that the cluster assignments get printed in inference.MCMC</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def value(self) -&gt; pl.Clustering:
    &#39;&#39;&#39;This is so that the cluster assignments get printed in inference.MCMC
    &#39;&#39;&#39;
    return self.clustering</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="mdsine2.posterior.ClusterAssignments.add_trace"><code class="name flex">
<span>def <span class="ident">add_trace</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add_trace(self):
    self.clustering.add_trace()</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.ClusterAssignments.calculate_marginal_loglikelihood_slow"><code class="name flex">
<span>def <span class="ident">calculate_marginal_loglikelihood_slow</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Marginalizes out the interactions and the perturbations</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def calculate_marginal_loglikelihood_slow(self):
    &#39;&#39;&#39;Marginalizes out the interactions and the perturbations
    &#39;&#39;&#39;
    # Build the parameters
    # ====================
    self.G[STRNAMES.CLUSTER_INTERACTION_INDICATOR].update_cnt_indicators()
    lhs = [STRNAMES.GROWTH_VALUE, STRNAMES.SELF_INTERACTION_VALUE]
    if self._there_are_perturbations:
        rhs = [STRNAMES.PERT_VALUE, STRNAMES.CLUSTER_INTERACTION_VALUE]
    else:
        rhs = [STRNAMES.CLUSTER_INTERACTION_VALUE]

    # reconstruct the X matrices
    for v in rhs:
        self.G.data.design_matrices[v].M.build()
    
    y = self.G.data.construct_lhs(lhs, 
        kwargs_dict={STRNAMES.GROWTH_VALUE:{&#39;with_perturbations&#39;: False}})
    X = self.G.data.construct_rhs(keys=rhs, toarray=True)
    process_prec = self.G[STRNAMES.PROCESSVAR].build_matrix(cov=False, sparse=False)
    prior_prec = build_prior_covariance(G=self.G, cov=False, order=rhs, sparse=False)
    prior_var = build_prior_covariance(G=self.G, cov=True, order=rhs, sparse=False)
    prior_mean = build_prior_mean(G=self.G, order=rhs, shape=(-1,1))

    # If nothing is on, return 0
    if X.shape[1] == 0:
        return {
            &#39;a&#39;: 0,
            &#39;beta_prec&#39;: 0,
            &#39;process_prec&#39;: prior_prec,
            &#39;ret&#39;: 0,
            &#39;beta_logdet&#39;: 0,
            &#39;priorvar_logdet&#39;: 0,
            &#39;bEb&#39;: 0,
            &#39;bEbprior&#39;: 0}

    # Calculate the marginalization
    # =============================
    beta_prec = X.T @ process_prec @ X + prior_prec
    beta_cov = pinv(beta_prec, self)
    beta_mean = beta_cov @ ( X.T @ process_prec @ y + prior_prec @ prior_mean )
    beta_mean = np.asarray(beta_mean).reshape(-1,1)

    try:
        beta_logdet = log_det(beta_cov, self)
    except:
        logging.critical(&#39;Crashed in log_det&#39;)
        logging.critical(&#39;beta_cov:\n{}&#39;.format(beta_cov))
        logging.critical(&#39;prior_prec\n{}&#39;.format(prior_prec))
        raise
    priorvar_logdet = log_det(prior_var, self)
    ll2 = 0.5 * (beta_logdet - priorvar_logdet)

    bEbprior = np.asarray(prior_mean.T @ prior_prec @ prior_mean)[0,0]
    bEb = np.asarray(beta_mean.T @ beta_prec @ beta_mean)[0,0]
    ll3 = 0.5 * (bEb  - bEbprior)

    # print(&#39;prior_prec truth:\n&#39;, prior_prec)

    return {
        &#39;a&#39;: X.T @ process_prec, &#39;beta_prec&#39;: beta_prec, &#39;process_prec&#39;: prior_prec, 
        &#39;ret&#39;: ll2+ll3, &#39;beta_logdet&#39;: beta_logdet, &#39;priorvar_logdet&#39;: priorvar_logdet,
        &#39;bEb&#39;: bEb, &#39;bEbprior&#39;: bEbprior}</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.ClusterAssignments.calculate_marginal_loglikelihood_slow_fast"><code class="name flex">
<span>def <span class="ident">calculate_marginal_loglikelihood_slow_fast</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Marginalizes out the interactions and the perturbations</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def calculate_marginal_loglikelihood_slow_fast(self):
    &#39;&#39;&#39;Marginalizes out the interactions and the perturbations
    &#39;&#39;&#39;
    # Build the parameters
    # ====================
    if self._there_are_perturbations:
        rhs = [STRNAMES.PERT_VALUE, STRNAMES.CLUSTER_INTERACTION_VALUE]
    else:
        rhs = [STRNAMES.CLUSTER_INTERACTION_VALUE]
    
    
    y = self.y
    X = self.G.data.construct_rhs(keys=rhs, toarray=True)

    process_prec = self.process_prec
    prior_prec = build_prior_covariance(G=self.G, cov=False, order=rhs, sparse=False)
    prior_prec_diag = np.diag(prior_prec)
    prior_var = build_prior_covariance(G=self.G, cov=True, order=rhs, sparse=False)
    prior_mean = build_prior_mean(G=self.G, order=rhs, shape=(-1,1))

    # Calculate the marginalization
    # =============================
    a = X.T * process_prec

    beta_prec = a @ X + prior_prec
    beta_cov = pinv(beta_prec, self)
    beta_mean = beta_cov @ ( a @ y + prior_prec @ prior_mean )
    beta_mean = np.asarray(beta_mean).reshape(-1,1)

    try:
        beta_logdet = log_det(beta_cov, self)
    except:
        logging.critical(&#39;Crashed in log_det&#39;)
        logging.critical(&#39;beta_cov:\n{}&#39;.format(beta_cov))
        logging.critical(&#39;prior_prec\n{}&#39;.format(prior_prec))
        raise
    priorvar_logdet = log_det(prior_var, self)
    ll2 = 0.5 * (beta_logdet - priorvar_logdet)

    a = np.sum((prior_mean.ravel() ** 2) *prior_prec_diag)
    # np.asarray(prior_mean.T @ prior_prec @ prior_mean)[0,0]
    b = np.asarray(beta_mean.T @ beta_prec @ beta_mean)[0,0]
    ll3 = -0.5 * (a  - b)

    return ll2+ll3</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.ClusterAssignments.calculate_marginal_loglikelihood_slow_fast_sparse"><code class="name flex">
<span>def <span class="ident">calculate_marginal_loglikelihood_slow_fast_sparse</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Marginalizes out the interactions and the perturbations</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def calculate_marginal_loglikelihood_slow_fast_sparse(self):
    &#39;&#39;&#39;Marginalizes out the interactions and the perturbations
    &#39;&#39;&#39;
    # Build the parameters
    # ====================
    if self._there_are_perturbations:
        rhs = [STRNAMES.PERT_VALUE, STRNAMES.CLUSTER_INTERACTION_VALUE]
    else:
        rhs = [STRNAMES.CLUSTER_INTERACTION_VALUE]
    
    
    y = self.y
    X = self.G.data.construct_rhs(keys=rhs, toarray=False)

    process_prec = self.process_prec_matrix
    prior_prec = build_prior_covariance(G=self.G, cov=False, order=rhs, sparse=True)  
    prior_prec_diag = build_prior_covariance(G=self.G, cov=False, order=rhs, diag=True)        
    prior_var = build_prior_covariance(G=self.G, cov=True, order=rhs, sparse=True)
    prior_mean = build_prior_mean(G=self.G, order=rhs, shape=(-1,1))

    # Calculate the marginalization
    # =============================

    # print(&#39;X&#39;)
    # print(type(X))
    # print(X.shape)

    # print(&#39;process_prec&#39;)
    # print(type(process_prec))
    # print(process_prec.shape)

    # print(&#39;prior_prec&#39;)
    # print(type(prior_prec))
    # print(prior_prec.shape)

    # print(&#39;prior mean&#39;)
    # print(type(prior_mean))
    # print(prior_mean.shape)

    a = X.T.dot(process_prec)
    beta_prec = a.dot(X) + prior_prec
    beta_cov = pinv(beta_prec, self)
    beta_mean = beta_cov @ ( a.dot(y) + prior_prec.dot(prior_mean))
    beta_mean = np.asarray(beta_mean).reshape(-1,1)

    try:
        beta_logdet = log_det(beta_cov, self)
    except:
        logging.critical(&#39;Crashed in log_det&#39;)
        logging.critical(&#39;beta_cov:\n{}&#39;.format(beta_cov))
        logging.critical(&#39;prior_prec\n{}&#39;.format(prior_prec))
        raise
    priorvar_logdet = log_det(prior_var, self)
    ll2 = 0.5 * (beta_logdet - priorvar_logdet)

    a = np.sum((prior_mean.ravel() ** 2) *prior_prec_diag)
    # np.asarray(prior_mean.T @ prior_prec @ prior_mean)[0,0]

    # print(&#39;beta_prec.shape&#39;, beta_prec.shape)
    # print(&#39;beta_mean.shape&#39;, beta_mean.shape)

    b = np.asarray(beta_mean.T @ beta_prec.dot(beta_mean))[0,0]
    ll3 = -0.5 * (a  - b)

    return ll2+ll3</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.ClusterAssignments.gibbs_update_single_taxon_parallel"><code class="name flex">
<span>def <span class="ident">gibbs_update_single_taxon_parallel</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Update for a single taxon</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def gibbs_update_single_taxon_parallel(self):
    &#39;&#39;&#39;Update for a single taxon
    &#39;&#39;&#39;
    self.original_cluster = self.clustering.idx2cid[self.oidx]
    self.curr_cluster = self.original_cluster

    interactions = self.G[STRNAMES.INTERACTIONS_OBJ]
    interaction_on_idxs = interactions.get_indicators(return_idxs=True)
    if self._there_are_perturbations:
        perturbation_on_idxs = [p.indicator.cluster_arg_array() for p in self.G.perturbations]
    else:
        perturbation_on_idxs = None
    use_saved_params = False

    if pl.ispersistentpool(self.pool):
        self.pool.staged_map_start(&#39;run&#39;)
    else:
        notpool_ret = []

    # Get the likelihood of the current configuration
    if self.clustering.clusters[self.original_cluster].size == 1:
        log_mult_factor = math.log(self.concentration/self.m)
    else:
        log_mult_factor = math.log(self.clustering.clusters[self.original_cluster].size - 1)

    if use_saved_params and pl.ispersistentpool(self.pool):
        interaction_on_idxs = None
        perturbation_on_idxs = None

    cluster_config = self.clustering.toarray()

    kwargs = {
        &#39;interaction_on_idxs&#39;: interaction_on_idxs,
        &#39;perturbation_on_idxs&#39;: perturbation_on_idxs,
        &#39;cluster_config&#39;: cluster_config,
        &#39;log_mult_factor&#39;: log_mult_factor,
        &#39;cid&#39;: self.original_cluster,
        &#39;use_saved_params&#39;: use_saved_params}

    if pl.ispersistentpool(self.pool):
        self.pool.staged_map_put(kwargs)
    else:
        notpool_ret.append(self.pool.run(**kwargs))

    # Check every cluster
    for cid in self.clustering.order:
        if cid == self.original_cluster:
            continue
        self.clustering.move_item(idx=self.oidx, cid=cid)
        self.curr_cluster = cid

        if not use_saved_params:
            interaction_on_idxs = interactions.get_indicators(return_idxs=True)
            if self._there_are_perturbations:
                perturbation_on_idxs = [p.indicator.cluster_arg_array() for p in self.G.perturbations]
            else:
                perturbation_on_idxs = None

        cluster_config = self.clustering.toarray()
        log_mult_factor = np.log(self.clustering.clusters[self.curr_cluster].size - 1)

        kwargs = {
            &#39;interaction_on_idxs&#39;: interaction_on_idxs,
            &#39;perturbation_on_idxs&#39;: perturbation_on_idxs,
            &#39;cluster_config&#39;: cluster_config,
            &#39;log_mult_factor&#39;: log_mult_factor,
            &#39;cid&#39;: self.curr_cluster,
            &#39;use_saved_params&#39;: use_saved_params}

        if pl.ispersistentpool(self.pool):
            self.pool.staged_map_put(kwargs)
        else:
            notpool_ret.append(self.pool.run(**kwargs))

    # Make a new cluster
    self.curr_cluster = self.clustering.make_new_cluster_with(idx=self.oidx)
    cluster_config = self.clustering.toarray()
    interaction_on_idxs = interactions.get_indicators(return_idxs=True)
    if self._there_are_perturbations:
        perturbation_on_idxs = [p.indicator.cluster_arg_array() for p in self.G.perturbations]
    else:
        perturbation_on_idxs = None
    log_mult_factor = np.log(self.concentration/self.m)

    kwargs = {
        &#39;interaction_on_idxs&#39;: interaction_on_idxs,
        &#39;perturbation_on_idxs&#39;: perturbation_on_idxs,
        &#39;cluster_config&#39;: cluster_config,
        &#39;log_mult_factor&#39;: log_mult_factor,
        &#39;cid&#39;: self.curr_cluster,
        &#39;use_saved_params&#39;: False}

    # Put the values and get if necessary
    KEYS = []
    LOG_P = []
    if pl.ispersistentpool(self.pool):
        self.pool.staged_map_put(kwargs)
        ret = self.pool.staged_map_get()
    else:
        notpool_ret.append(self.pool.run(**kwargs))
        ret = notpool_ret
    for c, p in ret:
        KEYS.append(c)
        LOG_P.append(p)

    idx = sample_categorical_log(LOG_P)
    assigned_cid = KEYS[idx]

    if assigned_cid != self.original_cluster:
        logging.info(&#39;cluster changed&#39;)

    self.clustering.move_item(idx=self.oidx, cid=assigned_cid)

    self.G[STRNAMES.CLUSTER_INTERACTION_INDICATOR].update_cnt_indicators()
    self.G.data.design_matrices[STRNAMES.CLUSTER_INTERACTION_VALUE].M.build()
    if self._there_are_perturbations:
        self.G.data.design_matrices[STRNAMES.PERT_VALUE].M.build()</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.ClusterAssignments.gibbs_update_single_taxon_slow"><code class="name flex">
<span>def <span class="ident">gibbs_update_single_taxon_slow</span></span>(<span>self, oidx: int)</span>
</code></dt>
<dd>
<div class="desc"><p>The update function is based off of Algorithm 8 in 'Markov Chain
Sampling Methods for Dirichlet Process Mixture Models' by Radford M.
Neal, 2000.</p>
<p>Calculate the marginal likelihood of the taxon in every cluster
and a new cluster then sample from <code>self.sample_categorical_log</code>
to get the cluster assignment.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>oidx</code></strong> :&ensp;<code>int</code></dt>
<dd>Taxa index that we are updating the cluster assignment of</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def gibbs_update_single_taxon_slow(self, oidx: int):
    &#39;&#39;&#39;The update function is based off of Algorithm 8 in &#39;Markov Chain
    Sampling Methods for Dirichlet Process Mixture Models&#39; by Radford M.
    Neal, 2000.

    Calculate the marginal likelihood of the taxon in every cluster
    and a new cluster then sample from `self.sample_categorical_log`
    to get the cluster assignment.

    Parameters
    ----------
    oidx : int
        Taxa index that we are updating the cluster assignment of
    &#39;&#39;&#39;
    curr_cluster = self.clustering.idx2cid[oidx]
    concentration = self.concentration.value

    # start as a dictionary then send values to `sample_categorical_log`
    LOG_P = []
    LOG_KEYS = []

    # Calculate current cluster
    # =========================
    # If the element is already in its own cluster, use the new cluster case
    if self.clustering.clusters[curr_cluster].size == 1:
        a = np.log(concentration/self.m)
    else:
        a = np.log(self.clustering.clusters[curr_cluster].size - 1)
    LOG_P.append(a + self.calculate_marginal_loglikelihood_slow()[&#39;ret&#39;])
    LOG_KEYS.append(curr_cluster)

    # Calculate going to every other cluster
    # ======================================
    for cid in self.clustering.order:
        if curr_cluster == cid:
            continue

        # Move Taxa and recompute the matrices
        self.clustering.move_item(idx=oidx,cid=cid)
        self.G[STRNAMES.CLUSTER_INTERACTION_INDICATOR].update_cnt_indicators()
        self.G.data.design_matrices[STRNAMES.CLUSTER_INTERACTION_VALUE].M.build()
        if self._there_are_perturbations:
            self.G.data.design_matrices[STRNAMES.PERT_VALUE].M.build()

        LOG_P.append(np.log(self.clustering.clusters[cid].size - 1) + \
            self.calculate_marginal_loglikelihood_slow()[&#39;ret&#39;])
        LOG_KEYS.append(cid)


    # Calculate new cluster
    # =====================
    cid=self.clustering.make_new_cluster_with(idx=oidx)
    self.G[STRNAMES.CLUSTER_INTERACTION_INDICATOR].update_cnt_indicators()
    self.G.data.design_matrices[STRNAMES.CLUSTER_INTERACTION_VALUE].M.build()
    if self._there_are_perturbations:
        self.G.data.design_matrices[STRNAMES.PERT_VALUE].M.build()
    
    LOG_KEYS.append(cid)
    LOG_P.append(np.log(concentration/self.m) + \
        self.calculate_marginal_loglikelihood_slow()[&#39;ret&#39;])

    # Sample the assignment
    # =====================
    idx = sample_categorical_log(LOG_P)
    assigned_cid = LOG_KEYS[idx]
    curr_clus = self.clustering.idx2cid[oidx]

    if assigned_cid != curr_clus:
        self.clustering.move_item(idx=oidx,cid=assigned_cid)
        self.G[STRNAMES.CLUSTER_INTERACTION_INDICATOR].update_cnt_indicators()

        # Change the mixing matrix for the interactions and (potentially) perturbations
        self.G.data.design_matrices[STRNAMES.CLUSTER_INTERACTION_VALUE].M.build()
        if self._there_are_perturbations:
            self.G.data.design_matrices[STRNAMES.PERT_VALUE].M.build()</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.ClusterAssignments.gibbs_update_single_taxon_slow_fast"><code class="name flex">
<span>def <span class="ident">gibbs_update_single_taxon_slow_fast</span></span>(<span>self, oidx: int)</span>
</code></dt>
<dd>
<div class="desc"><p>The update function is based off of Algorithm 8 in 'Markov Chain
Sampling Methods for Dirichlet Process Mixture Models' by Radford M.
Neal, 2000.</p>
<p>Calculate the marginal likelihood of the taxon in every cluster
and a new cluster then sample from <code>self.sample_categorical_log</code>
to get the cluster assignment.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>oidx</code></strong> :&ensp;<code>int</code></dt>
<dd>Taxa index that we are updating the cluster assignment of</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def gibbs_update_single_taxon_slow_fast(self, oidx: int):
    &#39;&#39;&#39;The update function is based off of Algorithm 8 in &#39;Markov Chain
    Sampling Methods for Dirichlet Process Mixture Models&#39; by Radford M.
    Neal, 2000.

    Calculate the marginal likelihood of the taxon in every cluster
    and a new cluster then sample from `self.sample_categorical_log`
    to get the cluster assignment.

    Parameters
    ----------
    oidx : int
        Taxa index that we are updating the cluster assignment of
    &#39;&#39;&#39;
    curr_cluster = self.clustering.idx2cid[oidx]
    concentration = self.concentration.value

    # start as a dictionary then send values to `sample_categorical_log`
    LOG_P = []
    LOG_KEYS = []

    # Calculate current cluster
    # =========================
    # If the element is already in its own cluster, use the new cluster case
    if self.clustering.clusters[curr_cluster].size == 1:
        a = np.log(concentration/self.m)
    else:
        a = np.log(self.clustering.clusters[curr_cluster].size - 1)
    LOG_P.append(a + self.calculate_marginal_loglikelihood_slow_fast_sparse())
    LOG_KEYS.append(curr_cluster)

    # Calculate going to every other cluster
    # ======================================
    for cid in self.clustering.order:
        if curr_cluster == cid:
            continue

        # Move Taxa and recompute the matrices
        self.clustering.move_item(idx=oidx,cid=cid)
        self.G[STRNAMES.CLUSTER_INTERACTION_INDICATOR].update_cnt_indicators()
        self.G.data.design_matrices[STRNAMES.CLUSTER_INTERACTION_VALUE].M.build()
        if self._there_are_perturbations:
            self.G.data.design_matrices[STRNAMES.PERT_VALUE].M.build()

        LOG_P.append(np.log(self.clustering.clusters[cid].size - 1) + \
            self.calculate_marginal_loglikelihood_slow_fast_sparse())
        LOG_KEYS.append(cid)


    # Calculate new cluster
    # =====================
    cid=self.clustering.make_new_cluster_with(idx=oidx)
    self.G[STRNAMES.CLUSTER_INTERACTION_INDICATOR].update_cnt_indicators()
    self.G.data.design_matrices[STRNAMES.CLUSTER_INTERACTION_VALUE].M.build()
    if self._there_are_perturbations:
        self.G.data.design_matrices[STRNAMES.PERT_VALUE].M.build()
    
    LOG_KEYS.append(cid)
    LOG_P.append(np.log(concentration/self.m) + \
        self.calculate_marginal_loglikelihood_slow_fast_sparse())

    # Sample the assignment
    # =====================
    idx = sample_categorical_log(LOG_P)
    assigned_cid = LOG_KEYS[idx]
    curr_clus = self.clustering.idx2cid[oidx]

    if assigned_cid != curr_clus:
        self.clustering.move_item(idx=oidx,cid=assigned_cid)
        self.G[STRNAMES.CLUSTER_INTERACTION_INDICATOR].update_cnt_indicators()

        # Change the mixing matrix for the interactions and (potentially) perturbations
        self.G.data.design_matrices[STRNAMES.CLUSTER_INTERACTION_VALUE].M.build()
        if self._there_are_perturbations:
            self.G.data.design_matrices[STRNAMES.PERT_VALUE].M.build()</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.ClusterAssignments.initialize"><code class="name flex">
<span>def <span class="ident">initialize</span></span>(<span>self, value_option: str, hyperparam_option: str = None, value: Union[numpy.ndarray, str, List] = None, n_clusters: Union[int, str] = None, delay: int = 0, run_every_n_iterations: int = 1)</span>
</code></dt>
<dd>
<div class="desc"><p>Initialize the cluster assingments - there are no hyperparamters to
initialize because the concentration is initialized somewhere else</p>
<p>Note - if <code>n_clusters</code> is not specified and the cluster initialization
method requires it - it will be set to the expected number of clusters
which = log(n_taxa)/log(2)</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>value_option</code></strong> :&ensp;<code>str</code></dt>
<dd>
<ul>
<li>The different methods to initialize the clusters. Options:<ul>
<li>'manual'<ul>
<li>Manually set the cluster assignments</li>
</ul>
</li>
<li>'no-clusters'<ul>
<li>Every Taxa in their own cluster</li>
</ul>
</li>
<li>'random'<ul>
<li>Every Taxa is randomly assigned to the number of clusters. <code>n_clusters</code> required</li>
</ul>
</li>
<li>'taxonomy'<ul>
<li>Cluster Taxa based on their taxonomic similarity. <code>n_clusters</code> required</li>
</ul>
</li>
<li>'sequence'<ul>
<li>Cluster Taxa based on their sequence similarity. <code>n_clusters</code> required</li>
</ul>
</li>
<li>'phylogeny'<ul>
<li>Cluster Taxa based on their phylogenetic similarity. <code>n_clusters</code> required</li>
</ul>
</li>
<li>'spearman', 'auto'<ul>
<li>Creates a distance matrix based on the spearman rank similarity
between two trajectories. We use the raw data. <code>n_clusters</code> required</li>
</ul>
</li>
<li>'fixed-clustering'<ul>
<li>Sets the clustering assignment to the most likely clustering configuration
specified in the graph at the location <code>value</code> (<code>value</code> is a str).</li>
<li>We take the mean coclusterings and do agglomerative clustering on that matrix
with the <code>mode</code> number of clusters.</li>
</ul>
</li>
</ul>
</li>
</ul>
</dd>
<dt><strong><code>hyperparam_option</code></strong> :&ensp;<code>None</code></dt>
<dd>Not used in this function - only here for API consistency</dd>
<dt><strong><code>value</code></strong> :&ensp;<code>list</code> of <code>list</code></dt>
<dd>
<ul>
<li>Cluster assingments for each of the Taxa</li>
<li>Only necessary if <code>value_option</code> == 'manual'</li>
</ul>
</dd>
<dt><strong><code>n_clusters</code></strong> :&ensp;<code>int, str</code></dt>
<dd>
<ul>
<li>Necessary if <code>value_option</code> is not 'manual' or 'no-clusters'.</li>
<li>If str, options: 'expected', 'auto': log_2(n_taxa)</li>
</ul>
</dd>
<dt><strong><code>run_every_n_iterations</code></strong> :&ensp;<code>int</code></dt>
<dd>Only run the update every <code>run_every_n_iterations</code> iterations</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def initialize(self, value_option: str, hyperparam_option: str=None, value: Union[np.ndarray, str, List]=None, 
    n_clusters: Union[int, str]=None, delay: int=0, run_every_n_iterations: int=1):
    &#39;&#39;&#39;Initialize the cluster assingments - there are no hyperparamters to
    initialize because the concentration is initialized somewhere else

    Note - if `n_clusters` is not specified and the cluster initialization
    method requires it - it will be set to the expected number of clusters
    which = log(n_taxa)/log(2)

    Parameters
    ----------
    value_option : str
        - The different methods to initialize the clusters. Options:
            - &#39;manual&#39;
                - Manually set the cluster assignments
            - &#39;no-clusters&#39;
                - Every Taxa in their own cluster
            - &#39;random&#39;
                - Every Taxa is randomly assigned to the number of clusters. `n_clusters` required
            - &#39;taxonomy&#39;
                - Cluster Taxa based on their taxonomic similarity. `n_clusters` required
            - &#39;sequence&#39;
                - Cluster Taxa based on their sequence similarity. `n_clusters` required
            - &#39;phylogeny&#39;
                - Cluster Taxa based on their phylogenetic similarity. `n_clusters` required
            - &#39;spearman&#39;, &#39;auto&#39;
                - Creates a distance matrix based on the spearman rank similarity
                  between two trajectories. We use the raw data. `n_clusters` required
            - &#39;fixed-clustering&#39;
                - Sets the clustering assignment to the most likely clustering configuration
                  specified in the graph at the location `value` (`value` is a str).
                - We take the mean coclusterings and do agglomerative clustering on that matrix
                  with the `mode` number of clusters.
    hyperparam_option : None
        Not used in this function - only here for API consistency
    value : list of list
        - Cluster assingments for each of the Taxa
        - Only necessary if `value_option` == &#39;manual&#39;
    n_clusters : int, str
        - Necessary if `value_option` is not &#39;manual&#39; or &#39;no-clusters&#39;.
        - If str, options: &#39;expected&#39;, &#39;auto&#39;: log_2(n_taxa)
    run_every_n_iterations : int
        Only run the update every `run_every_n_iterations` iterations
    &#39;&#39;&#39;
    from sklearn.cluster import AgglomerativeClustering
    from .util import generate_cluster_assignments_posthoc
    taxa = self.G.data.taxa

    self.run_every_n_iterations = run_every_n_iterations
    self.delay = delay

    if value_option not in [&#39;manual&#39;, &#39;no-clusters&#39;, &#39;fixed-clustering&#39;]:
        if pl.isstr(n_clusters):
            if n_clusters in [&#39;expected&#39;, &#39;auto&#39;]:
                n_clusters = expected_n_clusters(self.G)
            else:
                raise ValueError(&#39;`n_clusters` ({}) not recognized&#39;.format(n_clusters))
        if not pl.isint(n_clusters):
            raise TypeError(&#39;`n_clusters` ({}) must be a str or an int&#39;.format(type(n_clusters)))
        if n_clusters &lt;= 0:
            raise ValueError(&#39;`n_clusters` ({}) must be &gt; 0&#39;.format(n_clusters))
        if n_clusters &gt; self.G.data.n_taxa:
            raise ValueError(&#39;`n_clusters` ({}) must be &lt;= than the number of s ({})&#39;.format(
                n_clusters, self.G.data.n_taxa))

    if value_option == &#39;manual&#39;:
        # Check that all of the s are in the init and that it is in the right structure
        if not pl.isarray(value):
            raise ValueError(&#39;if `value_option` is &#34;manual&#34;, value ({}) must &#39; \
                &#39;be of type array&#39;.format(value.__class__))
        clusters = list(value)

        idxs_to_delete = []
        for idx, cluster in enumerate(clusters):
            if not pl.isarray(cluster):
                raise ValueError(&#39;cluster at index `{}` ({}) is not an array&#39;.format(
                    idx, cluster))
            cluster = list(cluster)
            if len(cluster) == 0:
                logging.warning(&#39;Cluster index {} has 0 elements, deleting&#39;.format(
                    idx))
                idxs_to_delete.append(idx)
        if len(idxs_to_delete) &gt; 0:
            clusters = np.delete(clusters, idxs_to_delete).tolist()

        all_oidxs = OrderedSet()
        for cluster in clusters:
            for oidx in cluster:
                if not pl.isint(oidx):
                    raise ValueError(&#39;`oidx` ({}) must be an int&#39;.format(oidx.__class__))
                if oidx &gt;= len(taxa):
                    raise ValueError(&#39;oidx `{}` not in our TaxaSet&#39;.format(oidx))
                all_oidxs.add(oidx)

        for oidx in range(len(taxa)):
            if oidx not in all_oidxs:
                raise ValueError(&#39;oidx `{}` in TaxaSet not in `value` ({})&#39;.format(
                    oidx, value))
        # Now everything is checked and valid

    elif value_option == &#39;fixed-clustering&#39;:
        logging.info(&#39;Fixed topology initialization&#39;)
        if not pl.isstr(value):
            raise TypeError(&#39;`value` ({}) must be a str&#39;.format(value))

        CHAIN2 = pl.inference.BaseMCMC.load(value)
        CLUSTERING2 = CHAIN2.graph[STRNAMES.CLUSTERING_OBJ]
        TAXA2 = CHAIN2.graph.data.taxa
        taxa_curr = self.G.data.taxa
        for taxon in TAXA2:
            if taxon.name not in taxa_curr:
                raise ValueError(&#39;Cannot perform fixed topology because the  {} in &#39; \
                    &#39;the passed in clustering is not in this clustering: {}&#39;.format(
                        taxon.name, taxa_curr.names.order))
        for taxon in taxa_curr:
            if taxon.name not in TAXA2:
                raise ValueError(&#39;Cannot perform fixed topology because the  {} in &#39; \
                    &#39;the current clustering is not in the passed in clustering: {}&#39;.format(
                        taxon.name, TAXA2.names.order))

        # Get the most likely cluster configuration and set as the value for the passed in cluster
        ret = generate_cluster_assignments_posthoc(CLUSTERING2, n_clusters=&#39;mode&#39;, set_as_value=False)
        CLUSTERING2.from_array(ret)
        logging.info(&#39;Clustering set to:\n{}&#39;.format(str(CLUSTERING2)))

        # Set the passed in cluster assignment as the current cluster assignment
        # Need to be careful because the indices of the s might not line up
        clusters = []
        for cluster in CLUSTERING2:
            anames = [taxa_curr[TAXA2.names.order[aidx]].name for aidx in cluster.members]
            aidxs = [taxa_curr[aname].idx for aname in anames]
            clusters.append(aidxs)

    elif value_option == &#39;no-clusters&#39;:
        clusters = []
        for oidx in range(len(taxa)):
            clusters.append([oidx])

    elif value_option == &#39;random&#39;:
        clusters = {}
        for oidx in range(len(taxa)):
            idx = npr.choice(n_clusters)
            if idx in clusters:
                clusters[idx].append(oidx)
            else:
                clusters[idx] = [oidx]
        c = []
        for cid in clusters.keys():
            c.append(clusters[cid])
        clusters = c

    elif value_option == &#39;taxonomy&#39;:
        # Create an affinity matrix, we can precompute the self-similarity to 1
        M = np.diag(np.ones(len(taxa), dtype=float))
        for i, oid1 in enumerate(taxa.ids.order):
            for j, oid2 in enumerate(taxa.ids.order):
                if i == j:
                    continue
                M[i,j] = taxa.taxonomic_similarity(oid1=oid1, oid2=oid2)

        c = AgglomerativeClustering(n_clusters=n_clusters, affinity=&#39;precomputed&#39;, linkage=&#39;complete&#39;)
        assignments = c.fit_predict(1-M)

        # Convert assignments into clusters
        clusters = {}
        for oidx,cidx in enumerate(assignments):
            if cidx not in clusters:
                clusters[cidx] = []
            clusters[cidx].append(oidx)
        clusters = [val for val in clusters.values()]

    elif value_option == &#39;sequence&#39;:
        from pylab import diversity

        logging.info(&#39;Making affinity matrix from sequences&#39;)
        evenness = np.diag(np.ones(len(self.G.data.taxa), dtype=float))

        for i in range(len(self.G.data.taxa)):
            for j in range(len(self.G.data.taxa)):
                if j &lt;= i:
                    continue
                # Subtract because we want to make a similarity matrix
                dist = 1-diversity.beta.hamming(
                    list(self.G.data.taxa[i].sequence),
                    list(self.G.data.taxa[j].sequence))
                evenness[i,j] = dist
                evenness[j,i] = dist

        c = AgglomerativeClustering(n_clusters=n_clusters, affinity=&#39;precomputed&#39;, linkage=&#39;average&#39;)
        assignments = c.fit_predict(evenness)
        clusters = {}
        for oidx,cidx in enumerate(assignments):
            if cidx not in clusters:
                clusters[cidx] = []
            clusters[cidx].append(oidx)
        clusters = [val for val in clusters.values()]

    elif value_option == &#39;spearman&#39;:
        # Use spearman correlation to create a distance matrix
        # Use agglomerative clustering to make the clusters based
        # on distance matrix (distance = 1 - pearson(x,y))
        dm = np.zeros(shape=(len(taxa), len(taxa)))
        data = []
        for ridx in range(self.G.data.n_replicates):
            data.append(self.G.data.abs_data[ridx])
        data = np.hstack(data)
        for i in range(len(taxa)):
            for j in range(i+1):
                distance = (1 - scipy.stats.spearmanr(data[i, :], data[j, :])[0])/2
                dm[i,j] = distance
                dm[j,i] = distance

        c = AgglomerativeClustering(n_clusters=n_clusters, affinity=&#39;precomputed&#39;, linkage=&#39;complete&#39;)
        assignments = c.fit_predict(dm)

        # convert into clusters
        clusters = {}
        for oidx, cidx in enumerate(assignments):
            if cidx not in clusters:
                clusters[cidx] = []
            clusters[cidx].append(oidx)
        clusters = [val for val in clusters.values()]

    elif value_option == &#39;phylogeny&#39;:
        raise NotImplementedError(&#39;`phylogeny` not implemented yet&#39;)

    else:
        raise ValueError(&#39;`value_option` &#34;{}&#34; not recognized&#39;.format(value_option))

    # Move all the taxa into their assigned clusters
    for cluster in clusters:
        cid = None
        for oidx in cluster:
            if cid is None:
                # make new cluster
                cid = self.clustering.make_new_cluster_with(idx=oidx)
            else:
                self.clustering.move_item(idx=oidx, cid=cid)
    logging.info(&#39;Cluster Assingments initialization results:\n{}&#39;.format(
        str(self.clustering)))
    self._there_are_perturbations = self.G.perturbations is not None

    # Initialize the multiprocessors if necessary
    if self.mp is not None:
        if not pl.isstr(self.mp):
            raise TypeError(&#39;`mp` ({}) must be a str&#39;.format(type(self.mp)))
        if &#39;full&#39; in self.mp:
            n_cpus = self.mp.split(&#39;-&#39;)[1]
            if n_cpus == &#39;auto&#39;:
                self.n_cpus = psutil.cpu_count(logical=False)
            else:
                try:
                    self.n_cpus = int(n_cpus)
                except:
                    raise ValueError(&#39;`mp` ({}) not recognized&#39;.format(self.mp))
            self.pool = pl.multiprocessing.PersistentPool(ptype=&#39;dasw&#39;, G=self.G)
        elif self.mp == &#39;debug&#39;:
            self.pool = None
        else:
            raise ValueError(&#39;`mp` ({}) not recognized&#39;.format(self.mp))
    else:
        self.pool = None

    self.ndts_bias = []
    self.n_taxa = len(self.G.data.taxa)
    self.n_replicates = self.G.data.n_replicates
    self.n_dts_for_replicate = self.G.data.n_dts_for_replicate
    self.total_dts = np.sum(self.n_dts_for_replicate)
    for ridx in range(self.G.data.n_replicates):
        self.ndts_bias.append(
            np.arange(0, self.G.data.n_dts_for_replicate[ridx] * self.n_taxa, self.n_taxa))
    self.replicate_bias = np.zeros(self.n_replicates, dtype=int)
    for ridx in range(1, self.n_replicates):
        self.replicate_bias[ridx] = self.replicate_bias[ridx-1] + \
            self.n_taxa * self.n_dts_for_replicate[ridx - 1]</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.ClusterAssignments.kill"><code class="name flex">
<span>def <span class="ident">kill</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def kill(self):
    if pl.ispersistentpool(self.pool):
        # For pylab multiprocessing, explicitly kill them
        self.pool.kill()
    return</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.ClusterAssignments.remove_local_trace"><code class="name flex">
<span>def <span class="ident">remove_local_trace</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Delete the local trace</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def remove_local_trace(self):
    &#39;&#39;&#39;Delete the local trace
    &#39;&#39;&#39;
    self.clustering.trace = None</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.ClusterAssignments.set_trace"><code class="name flex">
<span>def <span class="ident">set_trace</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_trace(self):
    self.clustering.set_trace()</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.ClusterAssignments.update_mp"><code class="name flex">
<span>def <span class="ident">update_mp</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Implements <code>update_slow</code> but parallelizes calculating the likelihood
of being in a cluster. NOTE that this does not parallelize on the Taxa level.</p>
<p>On the first gibb step with initialize the workers that we implement with DASW (
different arguments, single worker). For more information what this means look
at pylab.multiprocessing documentation.</p>
<p>If we initialized our pool as a pylab.multiprocessing.PersistentPool, then we
multiprocess the likelihood calculations for each taxa. If we didnt then this
implementation has the same performance as <code><a title="mdsine2.posterior.ClusterAssignments.update_slow_fast" href="#mdsine2.posterior.ClusterAssignments.update_slow_fast">ClusterAssignments.update_slow_fast()</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_mp(self):
    &#39;&#39;&#39;Implements `update_slow` but parallelizes calculating the likelihood
    of being in a cluster. NOTE that this does not parallelize on the Taxa level.

    On the first gibb step with initialize the workers that we implement with DASW (
    different arguments, single worker). For more information what this means look
    at pylab.multiprocessing documentation.

    If we initialized our pool as a pylab.multiprocessing.PersistentPool, then we 
    multiprocess the likelihood calculations for each taxa. If we didnt then this 
    implementation has the same performance as `ClusterAssignments.update_slow_fast`.
    &#39;&#39;&#39;
    if self.G.data.zero_inflation_transition_policy is not None:
        raise NotImplementedError(&#39;Multiprocessing for zero inflation data is not implemented yet.&#39; \
            &#39; Use `mp=None`&#39;)
    DMI = self.G.data.design_matrices[STRNAMES.CLUSTER_INTERACTION_VALUE]
    DMP = self.G.data.design_matrices[STRNAMES.PERT_VALUE]
    if self.clustering.n_clusters.sample_iter == 0 or self.pool == []:
        kwargs = {
            &#39;n_taxa&#39;: len(self.G.data.taxa),
            &#39;total_n_dts_per_taxon&#39;: self.G.data.total_n_dts_per_taxon,
            &#39;n_replicates&#39;: self.G.data.n_replicates,
            &#39;n_dts_for_replicate&#39;: self.G.data.n_dts_for_replicate,
            &#39;there_are_perturbations&#39;: self._there_are_perturbations,
            &#39;keypair2col_interactions&#39;: DMI.M.keypair2col,
            &#39;keypair2col_perturbations&#39;: DMP.M.keypair2col,
            &#39;n_perturbations&#39;: len(self.G.perturbations) if self._there_are_perturbations else None,
            &#39;base_Xrows&#39;: DMI.base.rows,
            &#39;base_Xcols&#39;: DMI.base.cols,
            &#39;base_Xshape&#39;: DMI.base.shape,
            &#39;base_Xpertrows&#39;: DMP.base.rows,
            &#39;base_Xpertcols&#39;: DMP.base.cols,
            &#39;base_Xpertshape&#39;: DMP.base.shape,
            &#39;n_rowsM&#39;: DMI.M.n_rows,
            &#39;n_rowsMpert&#39;: DMP.M.n_rows}

        if pl.ispersistentpool(self.pool):
            for _ in range(self.n_cpus):
                self.pool.add_worker(SingleClusterFullParallelization(**kwargs))
        else:
            self.pool = SingleClusterFullParallelization(**kwargs)
    if self.clustering.n_clusters.sample_iter &lt; self.delay:
        return
    if self.clustering.n_clusters.sample_iter % self.run_every_n_iterations != 0:
        return
    # Send in arguments for the start of the gibbs step
    start_time = time.time()
    base_Xdata = DMI.base.data
    self.concentration = self.G[STRNAMES.CONCENTRATION].value
    y = self.G.data.construct_lhs(keys=[STRNAMES.GROWTH_VALUE, STRNAMES.SELF_INTERACTION_VALUE],
        kwargs_dict={STRNAMES.GROWTH_VALUE: {&#39;with_perturbations&#39;:False}})
    prior_var_interactions = self.G[STRNAMES.PRIOR_VAR_INTERACTIONS].value
    prior_mean_interactions = self.G[STRNAMES.PRIOR_MEAN_INTERACTIONS].value
    process_prec_diag = self.G[STRNAMES.PROCESSVAR].prec

    if self._there_are_perturbations:
        prior_var_pert = self.G[STRNAMES.PRIOR_VAR_PERT].get_single_value_of_perts()
        prior_mean_pert = self.G[STRNAMES.PRIOR_MEAN_PERT].get_single_value_of_perts()
        base_Xpertdata = DMP.base.data
    else:
        prior_var_pert = None
        prior_mean_pert = None
        base_Xpertdata = None

    kwargs = {
        &#39;base_Xdata&#39;: base_Xdata,
        &#39;base_Xpertdata&#39;: base_Xpertdata,
        &#39;concentration&#39;: self.concentration,
        &#39;m&#39;: self.m,
        &#39;y&#39;: y,
        &#39;process_prec_diag&#39;: process_prec_diag,
        &#39;prior_var_interactions&#39;: prior_var_interactions,
        &#39;prior_var_pert&#39;: prior_var_pert,
        &#39;prior_mean_interactions&#39;: prior_mean_interactions,
        &#39;prior_mean_pert&#39;: prior_mean_pert}
    
    if pl.ispersistentpool(self.pool):
        self.pool.map(&#39;initialize_gibbs&#39;, [kwargs]*self.pool.num_workers)
    else:
        self.pool.initialize_gibbs(**kwargs)

    oidxs = npr.permutation(np.arange(len(self.G.data.taxa)))
    for iii, oidx in enumerate(oidxs):
        logging.info(&#39;{}/{} - {}&#39;.format(iii, len(self.G.data.taxa), oidx))
        self.oidx = oidx
        self.gibbs_update_single_taxon_parallel()

    self._strtime = time.time() - start_time</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.ClusterAssignments.update_slow"><code class="name flex">
<span>def <span class="ident">update_slow</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>This is updating the new cluster. Depending on the iteration you do
either split-merge Metropolis-Hasting update or a regular Gibbs update. To
get highest mixing we alternate between each.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_slow(self):
    &#39;&#39;&#39; This is updating the new cluster. Depending on the iteration you do
    either split-merge Metropolis-Hasting update or a regular Gibbs update. To
    get highest mixing we alternate between each.
    &#39;&#39;&#39;
    if self.clustering.n_clusters.sample_iter &lt; self.delay:
        return

    if self.clustering.n_clusters.sample_iter % self.run_every_n_iterations != 0:
       return

    start_time = time.time()
    oidxs = npr.permutation(np.arange(len(self.G.data.taxa)))

    for oidx in oidxs:
        self.gibbs_update_single_taxon_slow(oidx=oidx)
    self._strtime = time.time() - start_time</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.ClusterAssignments.update_slow_fast"><code class="name flex">
<span>def <span class="ident">update_slow_fast</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Much faster than <code>update_slow</code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_slow_fast(self):
    &#39;&#39;&#39;Much faster than `update_slow`
    &#39;&#39;&#39;

    if self.clustering.n_clusters.sample_iter &lt; self.delay:
        return

    if self.clustering.n_clusters.sample_iter % self.run_every_n_iterations != 0:
       return

    start_time = time.time()

    self.process_prec = self.G[STRNAMES.PROCESSVAR].prec.ravel() #.build_matrix(cov=False, sparse=False)
    self.process_prec_matrix = self.G[STRNAMES.PROCESSVAR].build_matrix(sparse=True, cov=False)
    lhs = [STRNAMES.GROWTH_VALUE, STRNAMES.SELF_INTERACTION_VALUE]
    self.y = self.G.data.construct_lhs(lhs, 
        kwargs_dict={STRNAMES.GROWTH_VALUE:{&#39;with_perturbations&#39;: False}})

    oidxs = npr.permutation(len(self.G.data.taxa))
    iii = 0
    for oidx in oidxs:
        logging.info(&#39;{}/{}: {}&#39;.format(iii, len(oidxs), oidx))
        self.gibbs_update_single_taxon_slow_fast(oidx=oidx)
        iii += 1
    self._strtime = time.time() - start_time</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.ClusterAssignments.visualize"><code class="name flex">
<span>def <span class="ident">visualize</span></span>(<span>self, basepath: str, f: <class 'IO'>, section: str = 'posterior', taxa_formatter: str = '%(paperformat)s', yticklabels: str = '%(paperformat)s %(index)s', xticklabels: str = '%(index)s', tax_fmt: str = '%(family)s') ‑> <class 'IO'></span>
</code></dt>
<dd>
<div class="desc"><p>Render the traces in the folder <code>basepath</code> and write the
learned values to the file <code>f</code>.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>basepath</code></strong> :&ensp;<code>str</code></dt>
<dd>This is the loction to write the files to</dd>
<dt><strong><code>f</code></strong> :&ensp;<code>_io.TextIOWrapper</code></dt>
<dd>File that we are writing the values to</dd>
<dt><strong><code>section</code></strong> :&ensp;<code>str</code></dt>
<dd>Section of the trace to compute on. Options:
'posterior' : posterior samples
'burnin' : burn-in samples
'entire' : both burn-in and posterior samples</dd>
<dt><strong><code>taxa_formatter</code></strong> :&ensp;<code>str, None</code></dt>
<dd>This is the format of the label to return for each . If None, it will return
the taxon's name</dd>
<dt><strong><code>yticklabels</code></strong>, <strong><code>xticklabels</code></strong> :&ensp;<code>str</code></dt>
<dd>These are the formats to plot the y-axis and x0axis, respectively.</dd>
<dt><strong><code>tax_fmt</code></strong> :&ensp;<code>str</code></dt>
<dd>This is the format to render the taxonomic distiribution plot</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>_io.TextIOWrapper</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def visualize(self, basepath: str, f: IO, section: str=&#39;posterior&#39;, 
    taxa_formatter: str=&#39;%(paperformat)s&#39;, yticklabels: str=&#39;%(paperformat)s %(index)s&#39;, 
    xticklabels: str=&#39;%(index)s&#39;, tax_fmt: str=&#39;%(family)s&#39;) -&gt; IO:
    &#39;&#39;&#39;Render the traces in the folder `basepath` and write the 
    learned values to the file `f`.

    Parameters
    ----------
    basepath : str
        This is the loction to write the files to
    f : _io.TextIOWrapper
        File that we are writing the values to
    section : str
        Section of the trace to compute on. Options:
            &#39;posterior&#39; : posterior samples
            &#39;burnin&#39; : burn-in samples
            &#39;entire&#39; : both burn-in and posterior samples
    taxa_formatter : str, None
        This is the format of the label to return for each . If None, it will return
        the taxon&#39;s name
    yticklabels, xticklabels : str
        These are the formats to plot the y-axis and x0axis, respectively.
    tax_fmt : str
        This is the format to render the taxonomic distiribution plot

    Returns
    -------
    _io.TextIOWrapper
    &#39;&#39;&#39;
    from matplotlib.colors import LogNorm
    taxa = self.G.data.taxa
    f.write(&#39;\n\n###################################\n&#39;)
    f.write(self.name)
    f.write(&#39;\n###################################\n&#39;)
    if not self.G.inference.is_in_inference_order(self):
        f.write(&#39;`{}` not learned. These were the fixed cluster assignments\n&#39;.format(self.name))
        for cidx, cluster in enumerate(self.clustering):
            f.write(&#39;Cluster {}:\n&#39;.format(cidx+1))
            for aidx in cluster.members:
                label = pl.taxaname_formatter(format=taxa_formatter, taxon=taxa[aidx], taxa=taxa)
                f.write(&#39;\t- {}\n&#39;.format(label))
        return f

    # Coclusters
    cocluster_trace = self.clustering.coclusters.get_trace_from_disk(section=section)
    coclusters = pl.variables.summary(cocluster_trace, section=section)[&#39;mean&#39;]
    for i in range(coclusters.shape[0]):
        coclusters[i,i] = np.nan

    # N clusters
    visualization.render_trace(var=self.clustering.n_clusters, plt_type=&#39;both&#39;, 
        section=section, include_burnin=True, rasterized=True)
    fig = plt.gcf()
    fig.suptitle(&#39;Number of Clusters&#39;)
    plt.savefig(os.path.join(basepath, &#39;n_clusters.pdf&#39;))
    plt.close()

    ret = generate_cluster_assignments_posthoc(clustering=self.clustering, n_clusters=&#39;mode&#39;, 
        section=section, set_as_value=True)
    data = []
    for taxon in taxa:
        data.append([taxon.name, ret[taxon.idx]])
    df = pd.DataFrame(data, columns=[&#39;name&#39;, &#39;Cluster Assignment&#39;])
    df.to_csv(os.path.join(basepath, &#39;clusterassignments.tsv&#39;), sep=&#39;\t&#39;, index=False, header=True)
    order = _make_cluster_order(graph=self.G, section=section)

    visualization.render_cocluster_probabilities(
        coclusters=coclusters, taxa=self.G.data.taxa, order=order,
        yticklabels=yticklabels, include_tick_marks=False, xticklabels=xticklabels,
        title=&#39;Cluster Assignments&#39;)

    # Write out cocluster values
    coclusters = coclusters[order, :]
    coclusters = coclusters[:, order]
    labels = [taxa[aidx].name for aidx in order]
    df = pd.DataFrame(coclusters, index=labels, columns=labels)
    df.to_csv(os.path.join(basepath, &#39;coclusters.tsv&#39;), index=True, header=True)

    fig = plt.gcf()
    fig.tight_layout()
    plt.savefig(os.path.join(basepath, &#39;coclusters.pdf&#39;))
    plt.close()

    # Make taxonomic distribution plot
    df = generate_taxonomic_distribution_over_clusters_posthoc(
        mcmc=self.G.inference, tax_fmt=tax_fmt)

    df[df==0] = np.nan
    fig = plt.figure()
    ax = fig.add_subplot(111)
    ax = sns.heatmap(df, ax=ax, cmap=&#39;Blues&#39;,
        norm=LogNorm(vmin=np.nanmin(df.to_numpy()), vmax=np.nanmax(df.to_numpy())))
    ax.set_title(&#39;Taxonomic abundance per cluster&#39;)
    ax.set_xlabel(&#39;Clusters&#39;)
    ax.set_ylabel(tax_fmt.replace(&#39;%(&#39;, &#39;&#39;).replace(&#39;)s&#39;, &#39;&#39;).capitalize())
    fig.tight_layout()
    plt.savefig(os.path.join(basepath, &#39;taxonomic_distribution.pdf&#39;))
    plt.close()

    f.write(&#39;Mode number of clusters: {}\n&#39;.format(len(self.clustering)))
    for cidx, cluster in enumerate(self.clustering):
        f.write(&#39;Cluster {} - Size {}\n&#39;.format(cidx, len(cluster)))
        for oidx in cluster.members:
            label = pl.taxaname_formatter(format=taxa_formatter, taxon=taxa[oidx], taxa=taxa)
            f.write(&#39;\t- {}\n&#39;.format(label))

    return f</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="mdsine2.pylab.graph.Node" href="pylab/graph.html#mdsine2.pylab.graph.Node">Node</a></b></code>:
<ul class="hlist">
<li><code><a title="mdsine2.pylab.graph.Node.add_child" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_child">add_child</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.add_parent" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_parent">add_parent</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.add_prior" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_prior">add_prior</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.add_undirected" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_undirected">add_undirected</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.degree" href="pylab/graph.html#mdsine2.pylab.graph.Node.degree">degree</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.delete" href="pylab/graph.html#mdsine2.pylab.graph.BaseNode.delete">delete</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.get_adjacent_keys" href="pylab/graph.html#mdsine2.pylab.graph.Node.get_adjacent_keys">get_adjacent_keys</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.load" href="pylab/base.html#mdsine2.pylab.base.Saveable.load">load</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.metropolis" href="pylab/graph.html#mdsine2.pylab.graph.Node.metropolis">metropolis</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.save" href="pylab/base.html#mdsine2.pylab.base.Saveable.save">save</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.set_save_location" href="pylab/base.html#mdsine2.pylab.base.Saveable.set_save_location">set_save_location</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="mdsine2.posterior.ClusterInteractionIndicatorProbability"><code class="flex name class">
<span>class <span class="ident">ClusterInteractionIndicatorProbability</span></span>
<span>(</span><span>prior: <a title="mdsine2.pylab.variables.Beta" href="pylab/variables.html#mdsine2.pylab.variables.Beta">Beta</a>, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>This is the posterior for the probability of a cluster being on.</p>
<p>Note that in the beta prior, <code>a</code> is the number of ons, <code>b</code> is the
number of offs.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>prior</code></strong> :&ensp;<code>pl.variables.Beta</code></dt>
<dd>Prior probability</dd>
<dt><strong><code>kwargs</code></strong> :&ensp;<code>dict</code></dt>
<dd>Other options like graph, value</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ClusterInteractionIndicatorProbability(pl.variables.Beta):
    &#39;&#39;&#39;This is the posterior for the probability of a cluster being on.

    Note that in the beta prior, `a` is the number of ons, `b` is the 
    number of offs.

    Parameters
    ----------
    prior : pl.variables.Beta
        Prior probability
    kwargs : dict
        Other options like graph, value
    &#39;&#39;&#39;
    def __init__(self, prior: variables.Beta, **kwargs):
        kwargs[&#39;name&#39;] = STRNAMES.CLUSTER_INTERACTION_INDICATOR_PROB
        pl.variables.Beta.__init__(self, a=prior.a.value, b=prior.b.value,
            dtype=float, **kwargs)
        self.add_prior(prior)

    def initialize(self, value_option: str, hyperparam_option: str, 
        a: Union[int, float]=None, b: Union[int, float]=None, 
        value: float=None, N: Union[float, str]=&#39;auto&#39;, delay: int=0):
        &#39;&#39;&#39;Initialize the hyperparameters of the beta prior

        Parameters
        ----------
        value_option : str
            - Option to initialize the value by
            - Options
                - &#39;manual&#39;
                    - Set the values manually, `value` must be specified
                - &#39;auto&#39;
                    - Set to the mean of the prior
        hyperparam_option : str
            - If it is a string, then set it by the designated option
            - Options
                - &#39;manual&#39;
                    - Set the value manually. `a` and `b` must also be specified
                - &#39;weak-agnostic&#39;
                    - a=b=0.5
                - &#39;strong-dense&#39;
                    - a = N(N-1), N are the expected number of clusters
                    - b = 0.5
                - &#39;strong-sparse&#39;
                    - a = 0.5
                    - b = N(N-1), N are the expected number of clusters
                - &#39;mult-sparse-M&#39;
                    - a = 0.5
                    - b = N*M(N*M-1), N are the expected number of clusters
        N : str, int
            This is the number of clusters to set the hyperparam options to.
            If it is a str:
                &#39;auto&#39; : set to the expected number of clusters of a dirichlet process.
                &#39;fixed-clustering&#39; : set to the current number of clusters
        a, b : int, float
            - User specified values
            - Only necessary if `hyperparam_option` == &#39;manual&#39;
        &#39;&#39;&#39;
        if not pl.isint(delay):
            raise TypeError(&#39;`delay` ({}) must be an int&#39;.format(type(delay)))
        if delay &lt; 0:
            raise ValueError(&#39;`delay` ({}) must be &gt;= 0&#39;.format(delay))
        self.delay = delay

        if hyperparam_option == &#39;manual&#39;:
            if pl.isnumeric(a) and pl.isnumeric(b):
                self.prior.a.override_value(a)
                self.prior.b.override_value(b)
            else:
                raise ValueError(&#39;a ({}) and b ({}) must be numerics (float, int)&#39;.format(
                    a.__class__, b.__class__))
        elif hyperparam_option in [&#39;weak-agnostic&#39;]:
            self.prior.a.override_value(0.5)
            self.prior.b.override_value(0.5)

        # Set N
        if pl.isstr(N):
            if N == &#39;auto&#39;:
                N = expected_n_clusters(G=self.G)
            elif N == &#39;fixed-clustering&#39;:
                N = len(self.G[STRNAMES.CLUSTERING_OBJ])
            else:
                raise ValueError(&#39;`N` ({}) nto recognized&#39;.format(N))
        elif pl.isint(N):
            if N &lt; 0:
                raise ValueError(&#39;`N` ({}) must be positive&#39;.format(N))
        else:
            raise TypeError(&#39;`N` ({}) type not recognized&#39;.format(type(N)))

        if hyperparam_option == &#39;strong-dense&#39;:
            self.prior.a.override_value(N * (N - 1))
            self.prior.b.override_value(0.5)
        elif hyperparam_option == &#39;strong-sparse&#39;:
            self.prior.a.override_value(0.5)
            self.prior.b.override_value((N * (N - 1)))
        elif &#39;mult-sparse&#39; in hyperparam_option:
            try:
                M = float(hyperparam_option.replace(&#39;mult-sparse-&#39;, &#39;&#39;))
            except:
                raise ValueError(&#39;`mult-sparse` in the wrong format ({})&#39;.format(
                    hyperparam_option))
            N = N * M
            self.prior.a.override_value(0.5)
            self.prior.b.override_value((N * (N - 1)))

        if value_option == &#39;manual&#39;:
            if pl.isnumeric(value):
                self.value = value
            else:
                raise ValueError(&#39;`value` ({}) must be a numeric (float,int)&#39;.format(
                    value.__class__))
        elif value_option == &#39;auto&#39;:
            self.value = self.prior.mean()
        else:
            raise ValueError(&#39;value option &#34;{}&#34; not recognized for indicator prob&#39;.format(
                value_option))

        self.a.value = self.prior.a.value
        self.b.value = self.prior.b.value
        logging.info(&#39;Indicator Probability initialization results:\n&#39; \
            &#39;\tprior a: {}\n\tprior b: {}\n\tvalue: {}&#39;.format(
                self.prior.a.value, self.prior.b.value, self.value))

    def update(self):
        &#39;&#39;&#39;Sample the posterior given the data
        &#39;&#39;&#39;
        if self.sample_iter &lt; self.delay:
            return
        self.a.value = self.prior.a.value + \
            self.G[STRNAMES.CLUSTER_INTERACTION_INDICATOR].num_pos_indicators
        self.b.value = self.prior.b.value + \
            self.G[STRNAMES.CLUSTER_INTERACTION_INDICATOR].num_neg_indicators
        self.sample()
        return self.value

    def visualize(self, path: str, f: IO, section: str=&#39;posterior&#39;) -&gt; IO:
        return _scalar_visualize(self, path=path, f=f, section=section)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="mdsine2.pylab.variables.Beta" href="pylab/variables.html#mdsine2.pylab.variables.Beta">Beta</a></li>
<li><a title="mdsine2.pylab.variables.Variable" href="pylab/variables.html#mdsine2.pylab.variables.Variable">Variable</a></li>
<li><a title="mdsine2.pylab.graph.Node" href="pylab/graph.html#mdsine2.pylab.graph.Node">Node</a></li>
<li><a title="mdsine2.pylab.graph.BaseNode" href="pylab/graph.html#mdsine2.pylab.graph.BaseNode">BaseNode</a></li>
<li><a title="mdsine2.pylab.base.Saveable" href="pylab/base.html#mdsine2.pylab.base.Saveable">Saveable</a></li>
<li>mdsine2.pylab.variables._BaseArithmeticClass</li>
<li><a title="mdsine2.pylab.base.Traceable" href="pylab/base.html#mdsine2.pylab.base.Traceable">Traceable</a></li>
<li>mdsine2.pylab.variables._RandomBase</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="mdsine2.posterior.ClusterInteractionIndicatorProbability.initialize"><code class="name flex">
<span>def <span class="ident">initialize</span></span>(<span>self, value_option: str, hyperparam_option: str, a: Union[int, float] = None, b: Union[int, float] = None, value: float = None, N: Union[float, str] = 'auto', delay: int = 0)</span>
</code></dt>
<dd>
<div class="desc"><p>Initialize the hyperparameters of the beta prior</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>value_option</code></strong> :&ensp;<code>str</code></dt>
<dd>
<ul>
<li>Option to initialize the value by</li>
<li>Options<ul>
<li>'manual'<ul>
<li>Set the values manually, <code>value</code> must be specified</li>
</ul>
</li>
<li>'auto'<ul>
<li>Set to the mean of the prior</li>
</ul>
</li>
</ul>
</li>
</ul>
</dd>
<dt><strong><code>hyperparam_option</code></strong> :&ensp;<code>str</code></dt>
<dd>
<ul>
<li>If it is a string, then set it by the designated option</li>
<li>Options<ul>
<li>'manual'<ul>
<li>Set the value manually. <code>a</code> and <code>b</code> must also be specified</li>
</ul>
</li>
<li>'weak-agnostic'<ul>
<li>a=b=0.5</li>
</ul>
</li>
<li>'strong-dense'<ul>
<li>a = N(N-1), N are the expected number of clusters</li>
<li>b = 0.5</li>
</ul>
</li>
<li>'strong-sparse'<ul>
<li>a = 0.5</li>
<li>b = N(N-1), N are the expected number of clusters</li>
</ul>
</li>
<li>'mult-sparse-M'<ul>
<li>a = 0.5</li>
<li>b = N<em>M(N</em>M-1), N are the expected number of clusters</li>
</ul>
</li>
</ul>
</li>
</ul>
</dd>
<dt><strong><code>N</code></strong> :&ensp;<code>str, int</code></dt>
<dd>This is the number of clusters to set the hyperparam options to.
If it is a str:
'auto' : set to the expected number of clusters of a dirichlet process.
'fixed-clustering' : set to the current number of clusters</dd>
<dt><strong><code>a</code></strong>, <strong><code>b</code></strong> :&ensp;<code>int, float</code></dt>
<dd>
<ul>
<li>User specified values</li>
<li>Only necessary if <code>hyperparam_option</code> == 'manual'</li>
</ul>
</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def initialize(self, value_option: str, hyperparam_option: str, 
    a: Union[int, float]=None, b: Union[int, float]=None, 
    value: float=None, N: Union[float, str]=&#39;auto&#39;, delay: int=0):
    &#39;&#39;&#39;Initialize the hyperparameters of the beta prior

    Parameters
    ----------
    value_option : str
        - Option to initialize the value by
        - Options
            - &#39;manual&#39;
                - Set the values manually, `value` must be specified
            - &#39;auto&#39;
                - Set to the mean of the prior
    hyperparam_option : str
        - If it is a string, then set it by the designated option
        - Options
            - &#39;manual&#39;
                - Set the value manually. `a` and `b` must also be specified
            - &#39;weak-agnostic&#39;
                - a=b=0.5
            - &#39;strong-dense&#39;
                - a = N(N-1), N are the expected number of clusters
                - b = 0.5
            - &#39;strong-sparse&#39;
                - a = 0.5
                - b = N(N-1), N are the expected number of clusters
            - &#39;mult-sparse-M&#39;
                - a = 0.5
                - b = N*M(N*M-1), N are the expected number of clusters
    N : str, int
        This is the number of clusters to set the hyperparam options to.
        If it is a str:
            &#39;auto&#39; : set to the expected number of clusters of a dirichlet process.
            &#39;fixed-clustering&#39; : set to the current number of clusters
    a, b : int, float
        - User specified values
        - Only necessary if `hyperparam_option` == &#39;manual&#39;
    &#39;&#39;&#39;
    if not pl.isint(delay):
        raise TypeError(&#39;`delay` ({}) must be an int&#39;.format(type(delay)))
    if delay &lt; 0:
        raise ValueError(&#39;`delay` ({}) must be &gt;= 0&#39;.format(delay))
    self.delay = delay

    if hyperparam_option == &#39;manual&#39;:
        if pl.isnumeric(a) and pl.isnumeric(b):
            self.prior.a.override_value(a)
            self.prior.b.override_value(b)
        else:
            raise ValueError(&#39;a ({}) and b ({}) must be numerics (float, int)&#39;.format(
                a.__class__, b.__class__))
    elif hyperparam_option in [&#39;weak-agnostic&#39;]:
        self.prior.a.override_value(0.5)
        self.prior.b.override_value(0.5)

    # Set N
    if pl.isstr(N):
        if N == &#39;auto&#39;:
            N = expected_n_clusters(G=self.G)
        elif N == &#39;fixed-clustering&#39;:
            N = len(self.G[STRNAMES.CLUSTERING_OBJ])
        else:
            raise ValueError(&#39;`N` ({}) nto recognized&#39;.format(N))
    elif pl.isint(N):
        if N &lt; 0:
            raise ValueError(&#39;`N` ({}) must be positive&#39;.format(N))
    else:
        raise TypeError(&#39;`N` ({}) type not recognized&#39;.format(type(N)))

    if hyperparam_option == &#39;strong-dense&#39;:
        self.prior.a.override_value(N * (N - 1))
        self.prior.b.override_value(0.5)
    elif hyperparam_option == &#39;strong-sparse&#39;:
        self.prior.a.override_value(0.5)
        self.prior.b.override_value((N * (N - 1)))
    elif &#39;mult-sparse&#39; in hyperparam_option:
        try:
            M = float(hyperparam_option.replace(&#39;mult-sparse-&#39;, &#39;&#39;))
        except:
            raise ValueError(&#39;`mult-sparse` in the wrong format ({})&#39;.format(
                hyperparam_option))
        N = N * M
        self.prior.a.override_value(0.5)
        self.prior.b.override_value((N * (N - 1)))

    if value_option == &#39;manual&#39;:
        if pl.isnumeric(value):
            self.value = value
        else:
            raise ValueError(&#39;`value` ({}) must be a numeric (float,int)&#39;.format(
                value.__class__))
    elif value_option == &#39;auto&#39;:
        self.value = self.prior.mean()
    else:
        raise ValueError(&#39;value option &#34;{}&#34; not recognized for indicator prob&#39;.format(
            value_option))

    self.a.value = self.prior.a.value
    self.b.value = self.prior.b.value
    logging.info(&#39;Indicator Probability initialization results:\n&#39; \
        &#39;\tprior a: {}\n\tprior b: {}\n\tvalue: {}&#39;.format(
            self.prior.a.value, self.prior.b.value, self.value))</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.ClusterInteractionIndicatorProbability.update"><code class="name flex">
<span>def <span class="ident">update</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Sample the posterior given the data</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update(self):
    &#39;&#39;&#39;Sample the posterior given the data
    &#39;&#39;&#39;
    if self.sample_iter &lt; self.delay:
        return
    self.a.value = self.prior.a.value + \
        self.G[STRNAMES.CLUSTER_INTERACTION_INDICATOR].num_pos_indicators
    self.b.value = self.prior.b.value + \
        self.G[STRNAMES.CLUSTER_INTERACTION_INDICATOR].num_neg_indicators
    self.sample()
    return self.value</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.ClusterInteractionIndicatorProbability.visualize"><code class="name flex">
<span>def <span class="ident">visualize</span></span>(<span>self, path: str, f: <class 'IO'>, section: str = 'posterior') ‑> <class 'IO'></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def visualize(self, path: str, f: IO, section: str=&#39;posterior&#39;) -&gt; IO:
    return _scalar_visualize(self, path=path, f=f, section=section)</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="mdsine2.pylab.variables.Beta" href="pylab/variables.html#mdsine2.pylab.variables.Beta">Beta</a></b></code>:
<ul class="hlist">
<li><code><a title="mdsine2.pylab.variables.Beta.T" href="pylab/variables.html#mdsine2.pylab.variables.Variable.T">T</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Beta.add_child" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_child">add_child</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Beta.add_parent" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_parent">add_parent</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Beta.add_prior" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_prior">add_prior</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Beta.add_trace" href="pylab/base.html#mdsine2.pylab.base.Traceable.add_trace">add_trace</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Beta.add_undirected" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_undirected">add_undirected</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Beta.degree" href="pylab/graph.html#mdsine2.pylab.graph.Node.degree">degree</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Beta.delete" href="pylab/graph.html#mdsine2.pylab.graph.BaseNode.delete">delete</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Beta.get_adjacent_keys" href="pylab/graph.html#mdsine2.pylab.graph.Node.get_adjacent_keys">get_adjacent_keys</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Beta.get_iter" href="pylab/base.html#mdsine2.pylab.base.Traceable.get_iter">get_iter</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Beta.get_trace_from_disk" href="pylab/base.html#mdsine2.pylab.base.Traceable.get_trace_from_disk">get_trace_from_disk</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Beta.load" href="pylab/base.html#mdsine2.pylab.base.Saveable.load">load</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Beta.mean" href="pylab/variables.html#mdsine2.pylab.variables.Beta.mean">mean</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Beta.metropolis" href="pylab/graph.html#mdsine2.pylab.graph.Node.metropolis">metropolis</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Beta.overwrite_entire_trace_on_disk" href="pylab/base.html#mdsine2.pylab.base.Traceable.overwrite_entire_trace_on_disk">overwrite_entire_trace_on_disk</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Beta.remove_local_trace" href="pylab/variables.html#mdsine2.pylab.variables.Variable.remove_local_trace">remove_local_trace</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Beta.sample" href="pylab/variables.html#mdsine2.pylab.variables.Beta.sample">sample</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Beta.save" href="pylab/base.html#mdsine2.pylab.base.Saveable.save">save</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Beta.set_save_location" href="pylab/base.html#mdsine2.pylab.base.Saveable.set_save_location">set_save_location</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Beta.set_trace" href="pylab/base.html#mdsine2.pylab.base.Traceable.set_trace">set_trace</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Beta.set_value_shape" href="pylab/variables.html#mdsine2.pylab.variables.Variable.set_value_shape">set_value_shape</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Beta.variance" href="pylab/variables.html#mdsine2.pylab.variables.Beta.variance">variance</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="mdsine2.posterior.ClusterInteractionIndicators"><code class="flex name class">
<span>class <span class="ident">ClusterInteractionIndicators</span></span>
<span>(</span><span>prior: <a title="mdsine2.pylab.variables.Beta" href="pylab/variables.html#mdsine2.pylab.variables.Beta">Beta</a>, mp: str = None, relative: bool = True, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>This is the posterior of the Indicator variables on the interactions
between clusters. These clusters are not fixed.
If <code>value</code> is not <code>None</code>, then we set that to be the initial indicators
of the cluster interactions</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>prior</code></strong> :&ensp;<code>pl.variables.Beta</code></dt>
<dd>This is the prior of the variable</dd>
<dt><strong><code>mp</code></strong> :&ensp;<code>str, None</code></dt>
<dd>If <code>None</code>, then there is no multiprocessing.
If it is a str, then there are two options:
'debug': pool is done sequentially and not sent to processors
'full': pool is done at different processors</dd>
<dt><strong><code>relative</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether you update using the relative marginal likelihood or not.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ClusterInteractionIndicators(pl.variables.Variable):
    &#39;&#39;&#39;This is the posterior of the Indicator variables on the interactions
    between clusters. These clusters are not fixed.
    If `value` is not `None`, then we set that to be the initial indicators
    of the cluster interactions

    Parameters
    ----------
    prior : pl.variables.Beta
        This is the prior of the variable
    mp : str, None
        If `None`, then there is no multiprocessing.
        If it is a str, then there are two options:
            &#39;debug&#39;: pool is done sequentially and not sent to processors
            &#39;full&#39;: pool is done at different processors
    relative : bool
        Whether you update using the relative marginal likelihood or not.
    &#39;&#39;&#39;
    def __init__(self, prior: variables.Beta, mp: str=None, relative: bool=True, **kwargs):
        if not pl.isbool(relative):
            raise TypeError(&#39;`relative` ({}) must be a bool&#39;.format(type(relative)))
        if relative:
            if mp is not None:
                raise ValueError(&#39;Multiprocessing is slower for rel. Turn mp off&#39;)
            self.update = self.update_relative
        else:
            self.update = self.update_slow

        if mp is not None:
            if not pl.isstr(mp):
                raise TypeError(&#39;`mp` ({}) must be a str&#39;.format(type(mp)))
            if mp not in [&#39;full&#39;, &#39;debug&#39;]:
                raise ValueError(&#39;`mp` ({}) not recognized&#39;.format(mp))

        kwargs[&#39;name&#39;] = STRNAMES.CLUSTER_INTERACTION_INDICATOR
        pl.variables.Variable.__init__(self, dtype=bool, **kwargs)
        self.n_taxa = len(self.G.data.taxa)
        self.set_value_shape(shape=(self.n_taxa, self.n_taxa))
        self.add_prior(prior)
        self.clustering = self.G[STRNAMES.CLUSTERING_OBJ]
        self.mp = mp
        self.relative = relative

        # parameters used during update
        self.X = None
        self.y = None
        self.process_prec_matrix = None
        self._strr = &#39;None&#39;

    def initialize(self, delay: int=0, run_every_n_iterations: int=1):
        &#39;&#39;&#39;Do nothing, the indicators are set in `ClusterInteractionValue`.

        Parameters
        ----------
        delay : int
            How many iterations to delay starting to update the values
        run_every_n_iterations : int
            Which iteration to run on
        &#39;&#39;&#39;
        if not pl.isint(delay):
            raise TypeError(&#39;`delay` ({}) must be an int&#39;.format(type(delay)))
        if delay &lt; 0:
            raise ValueError(&#39;`delay` ({}) must be &gt;= 0&#39;.format(delay))
        if not pl.isint(run_every_n_iterations):
            raise TypeError(&#39;`run_every_n_iterations` ({}) must be an int&#39;.format(
                type(run_every_n_iterations)))
        if run_every_n_iterations &lt;= 0:
            raise ValueError(&#39;`run_every_n_iterations` ({}) must be &gt; 0&#39;.format(
                run_every_n_iterations))

        self.delay = delay
        self.run_every_n_iterations = run_every_n_iterations
        self._there_are_perturbations = self.G.perturbations is not None
        self.update_cnt_indicators()
        self.interactions = self.G[STRNAMES.INTERACTIONS_OBJ]
        self.n_taxa = len(self.G.data.taxa)

        # These are for the function `self._make_idx_for_clusters`
        self.ndts_bias = []
        self.n_replicates = self.G.data.n_replicates
        self.n_dts_for_replicate = self.G.data.n_dts_for_replicate
        self.total_dts = np.sum(self.n_dts_for_replicate)
        self.replicate_bias = np.zeros(self.n_replicates, dtype=int)
        for ridx in range(1, self.n_replicates):
            self.replicate_bias[ridx] = self.replicate_bias[ridx-1] + \
                self.n_taxa * self.n_dts_for_replicate[ridx - 1]
        for ridx in range(self.G.data.n_replicates):
            self.ndts_bias.append(
                np.arange(0, self.G.data.n_dts_for_replicate[ridx] * self.n_taxa, self.n_taxa))

        # Makes a dictionary that maps the taxon index to the rows that it the  in
        self.oidx2rows = {}
        for oidx in range(self.n_taxa):
            idxs = np.zeros(self.total_dts, dtype=int)
            i = 0
            for ridx in range(self.n_replicates):
                temp = np.arange(0, self.n_dts_for_replicate[ridx] * self.n_taxa, self.n_taxa)
                temp = temp + oidx
                temp = temp + self.replicate_bias[ridx]
                l = len(temp)
                idxs[i:i+l] = temp
                i += l
            self.oidx2rows[oidx] = idxs

    def add_trace(self):
        self.value = self.G[STRNAMES.INTERACTIONS_OBJ].get_datalevel_indicator_matrix()
        pl.variables.Variable.add_trace(self)

    def update_cnt_indicators(self):
        self.num_pos_indicators = self.G[STRNAMES.INTERACTIONS_OBJ].num_pos_indicators()
        self.num_neg_indicators = self.G[STRNAMES.INTERACTIONS_OBJ].num_neg_indicators()

    def __str__(self) -&gt; str:
        return self._strr

    # @profile
    def update_slow(self):
        &#39;&#39;&#39;Permute the order that the indices that are updated.

        Build the full master interaction matrix that we can then slice
        &#39;&#39;&#39;
        start = time.time()
        if self.sample_iter &lt; self.delay:
            # for interaction in self.interactions:
            #     interaction.indicator=False
            self._strr = &#39;{}\ntotal time: {}&#39;.format(
                self.interactions.get_indicators(), time.time()-start)
            return
        if self.sample_iter % self.run_every_n_iterations != 0:
            return

        idxs = npr.permutation(self.interactions.size)
        for idx in idxs:
            self.update_single_idx_slow(idx=idx)

        self.update_cnt_indicators()
        # Since slicing is literally so slow, it is faster to build than just slicing M
        self.G.data.design_matrices[STRNAMES.CLUSTER_INTERACTION_VALUE].M.build(
            build=True, build_for_neg_ind=False)
        iii = self.interactions.get_indicators()
        n_on = np.sum(iii)
        self._strr = &#39;{}\ntotal time: {}, n_interactions: {}/{}, {:.2f}&#39;.format(
            iii, time.time()-start, n_on, len(iii), n_on/len(iii))

    def update_single_idx_slow(self, idx: int):
        &#39;&#39;&#39;Update the likelihood for interaction `idx`

        Parameters
        ----------
        idx : int
            This is the index of the interaction we are updating
        &#39;&#39;&#39;
        prior_ll_on = np.log(self.G[STRNAMES.CLUSTER_INTERACTION_INDICATOR_PROB].value)
        prior_ll_off = np.log(1 - self.G[STRNAMES.CLUSTER_INTERACTION_INDICATOR_PROB].value)

        d_on = self.calculate_marginal_loglikelihood(idx=idx, val=True)
        d_off = self.calculate_marginal_loglikelihood(idx=idx, val=False)

        ll_on = d_on[&#39;ret&#39;] + prior_ll_on
        ll_off = d_off[&#39;ret&#39;] + prior_ll_off
        dd = [ll_off, ll_on]

        res = bool(sample_categorical_log(dd))
        self.interactions.iloc(idx).indicator = res
        self.update_cnt_indicators()

    # @profile
    def _make_idx_vector_for_clusters(self) -&gt; Dict[int, np.ndarray]:
        &#39;&#39;&#39;Creates a dictionary that maps the cluster id to the
        rows that correspond to each Taxa in the cluster.

        We cannot cast this with numba because it does not support Fortran style
        raveling :(.

        Returns
        -------
        dict: int -&gt; np.ndarray
            Maps the cluster ID to the row indices corresponding to it
        &#39;&#39;&#39;
        clusters = [np.asarray(oidxs, dtype=int).reshape(-1,1) \
            for oidxs in self.clustering.tolistoflists()]

        d = {}
        cids = self.clustering.order

        for cidx,cid in enumerate(cids):
            a = np.zeros(len(clusters[cidx]) * self.total_dts, dtype=int)
            i = 0
            for ridx in range(self.n_replicates):
                idxs = np.zeros(
                    (len(clusters[cidx]),
                    self.n_dts_for_replicate[ridx]), int)
                idxs = idxs + clusters[cidx]
                idxs = idxs + self.ndts_bias[ridx]
                idxs = idxs + self.replicate_bias[ridx]
                idxs = idxs.ravel(&#39;F&#39;)
                l = len(idxs)
                a[i:i+l] = idxs
                i += l

            d[cid] = a
        
        if self.G.data.zero_inflation_transition_policy is not None:
            # We need to convert the indices that are meant from no zero inflation to 
            # ones that take into account zero inflation - use the array from 
            # `data.Data._setrows_to_include_zero_inflation`. If the index should be
            # included, then we subtract the number of indexes that are previously off
            # before that index. If it should not be included then we exclude it
            prevoff_arr = self.G.data.off_previously_arr_zero_inflation
            rows_to_include = self.G.data.rows_to_include_zero_inflation
            for cid in d:
                arr = d[cid]
                new_arr = np.zeros(len(arr), dtype=int)
                n = 0
                for idx in arr:
                    if rows_to_include[idx]:
                        new_arr[n] = idx - prevoff_arr[idx]
                        n += 1

                new_arr = new_arr[:n]
                d[cid] = new_arr
        return d

    # @profile
    def make_rel_params(self):
        &#39;&#39;&#39;We make the parameters needed to update the relative log-likelihod.
        This function is called once at the beginning of the update.

        Parameters that we create with this function
        --------------------------------------------
        - ys : dict (int -&gt; np.ndarray)
            - Maps the target cluster id to the observation matrix that it
              corresponds to (only the Taxa in the target cluster). This 
              array already has the growth and self-interactions subtracted
              out:
                * $ \frac{log(x_{k+1}) - log(x_{k})}{dt} - a_{1,k} - a_{2,k}x_{k} $
        - process_precs : dict (int -&gt; np.ndarray)
            - Maps the target cluster id to the vector of the process precision
              that corresponds to the target cluster (only the Taxa in the target
              cluster). This is a 1D array that corresponds to the diagonal of what
              would be the precision matrix.
        - interactionXs : dict (int -&gt; np.ndarray)
            - Maps the target cluster id to the matrix of the design matrix of the
              interactions. Only includes the rows that correspond to the Taxa in the
              target cluster. It includes every single column as if all of the indicators
              are on. We only index out the columns when we are doing the marginalization.
        - prior_prec_interaction : float
            - Prior precision of the interaction value. We then use this
              value to make the diagonal of the prior precision.
        - prior_var_interaction : float
            - Prior variance of the interaction value.
        - prior_mean_interaction : float
            - Prior mean of the interaction values. We use this value
              to make the prior mean vector during the marginalization.
        - n_on_master : int
            - How many interactions are on at any one time. We adjust this
              throughout the update depending on what interactions we turn off and
              on.
        - prior_ll_on : float
            - Prior log likelihood of a positive interaction
        - prior_ll_off : float
            - Prior log likelihood of the negative interaction
        - priorvar_logdet_diff : float
            - This is the prior variance log determinant that we add when the indicator
              is positive.

        Parameters created if there are perturbations
        ---------------------------------------------
        - perturbationsXs : dict (int -&gt; np.ndarray)
            - Maps the target cluster id to the design matrix that corresponds to 
              the on perturbations of the target clusters. This is preindexed in
              both rows and columns
        - prior_prec_perturbations : dict (int -&gt; np.ndarray)
            - Maps the target cluster id to the diagonal of the prior precision
              of the perturbations
        - prior_var_perturbations : dict (int -&gt; np.ndarray)
            - Maps the target cluster id to the diagonal of the prior variance 
              of the perturbations
        - prior_mean_perturbations : dict (int -&gt; np.ndarray)
            - Maps the target cluster id to the vector of the prior mean of the
              perturbations
        &#39;&#39;&#39;
        # Get the row indices for each cluster
        row_idxs = self._make_idx_vector_for_clusters()

        # Create ys
        self.ys = {}
        y = self.G.data.construct_lhs(keys=[
            STRNAMES.SELF_INTERACTION_VALUE, STRNAMES.GROWTH_VALUE],
            kwargs_dict={STRNAMES.GROWTH_VALUE:{&#39;with_perturbations&#39;: False}})
        for tcid in self.clustering.order:
            self.ys[tcid] = y[row_idxs[tcid], :]

        # Create process_precs
        self.process_precs = {}
        process_prec_diag = self.G[STRNAMES.PROCESSVAR].prec
        for tcid in self.clustering.order:
            self.process_precs[tcid] = process_prec_diag[row_idxs[tcid]]

        # Make interactionXs
        self.interactionXs = {}
        self.G.data.design_matrices[STRNAMES.CLUSTER_INTERACTION_VALUE].M.build(
            build=True, build_for_neg_ind=True)
        XM_master = self.G.data.design_matrices[STRNAMES.CLUSTER_INTERACTION_VALUE].toarray()
        for tcid in self.clustering.order:
            self.interactionXs[tcid] = XM_master[row_idxs[tcid], :]

        # Make prior parameters
        self.prior_var_interaction = self.G[STRNAMES.PRIOR_VAR_INTERACTIONS].value
        self.prior_prec_interaction = 1/self.prior_var_interaction
        self.prior_mean_interaction = self.G[STRNAMES.PRIOR_MEAN_INTERACTIONS].value
        self.prior_ll_on = np.log(self.prior.value)
        self.prior_ll_off = np.log(1 - self.prior.value)
        self.n_on_master = self.interactions.num_pos_indicators()

        # Make priorvar_logdet
        self.priorvar_logdet = np.log(self.prior_var_interaction)

        if self._there_are_perturbations:
            XMpert_master = self.G.data.design_matrices[STRNAMES.PERT_VALUE].toarray()

            # Make perturbationsXs
            self.perturbationsXs = {}
            for tcid in self.clustering.order:
                rows = row_idxs[tcid]
                cols = []
                i = 0
                for perturbation in self.G.perturbations:
                    for cid in perturbation.indicator.value:
                        if perturbation.indicator.value[cid]:
                            if cid == tcid:
                                cols.append(i)
                            i += 1
                cols = np.asarray(cols, dtype=int)

                self.perturbationsXs[tcid] = pl.util.fast_index(M=XMpert_master,
                    rows=rows, cols=cols)

            # Make prior perturbation parameters
            self.prior_mean_perturbations = {}
            self.prior_var_perturbations = {}
            self.prior_prec_perturbations = {}
            for tcid in self.clustering.order:
                mean = []
                var = []
                for perturbation in self.G.perturbations:
                    if perturbation.indicator.value[tcid]:
                        # This is on, get the parameters
                        mean.append(perturbation.magnitude.prior.loc.value)
                        var.append(perturbation.magnitude.prior.scale2.value)
                self.prior_mean_perturbations[tcid] = np.asarray(mean)
                self.prior_var_perturbations[tcid] = np.asarray(var)
                self.prior_prec_perturbations[tcid] = 1/self.prior_var_perturbations[tcid]

            # Make priorvar_det_perturbations
            self.priorvar_det_perturbations = 0
            for perturbation in self.G.perturbations:
                self.priorvar_det_perturbations += \
                    perturbation.indicator.num_on_clusters() * \
                    perturbation.magnitude.prior.scale2.value

    # @profile
    def update_relative(self):
        &#39;&#39;&#39;Update the indicators variables by calculating the relative loglikelihoods
        of it being on as supposed to off. Because this is a relative loglikelihood,
        we only need to take into account the following parameters of the model:
            - Only the Taxa in the target cluster of the interaction
            - Only the positively indicated interactions going into the
              target cluster.

        This is 1000&#39;s of times faster than `update` because we are operating on matrices
        that are MUCH smaller than in a full system. These matrices are also considered dense
        so we do all of our computations without sparse matrices.

        We permute the order that the indices are updated for more robust mixing.
        &#39;&#39;&#39;
        start = time.time()
        if self.sample_iter &lt; self.delay:
            self._strr = &#39;{}\ntotal time: {}&#39;.format(
                self.interactions.get_indicators(), time.time()-start)
            return
        if self.sample_iter % self.run_every_n_iterations != 0:
            return

        idxs = npr.permutation(self.interactions.size)

        self.make_rel_params()
        for idx in idxs:
            self.update_single_idx_fast(idx=idx)

        self.update_cnt_indicators()
        # Since slicing is literally so slow, it is faster to build than just slicing M
        self.G.data.design_matrices[STRNAMES.CLUSTER_INTERACTION_VALUE].M.build(
            build=True, build_for_neg_ind=False)
        iii = self.interactions.get_indicators()
        n_on = np.sum(iii)
        self._strr = &#39;{}\ntotal time: {}, n_interactions: {}/{}, {:.2f}&#39;.format(
            iii, time.time()-start, n_on, len(iii), n_on/len(iii))

    def calculate_marginal_loglikelihood(self, idx: int, val: bool) -&gt; Dict[str, Union[float, int]]:
        &#39;&#39;&#39;Calculate the likelihood of interaction `idx` with the value `val`
        &#39;&#39;&#39;
        # Build and initialize
        self.interactions.iloc(idx).indicator = val
        self.update_cnt_indicators()
        self.G.data.design_matrices[STRNAMES.CLUSTER_INTERACTION_VALUE].M.build()

        lhs = [STRNAMES.GROWTH_VALUE, STRNAMES.SELF_INTERACTION_VALUE]
        if self._there_are_perturbations:
            rhs = [STRNAMES.PERT_VALUE, STRNAMES.CLUSTER_INTERACTION_VALUE]
        else:
            rhs = [STRNAMES.CLUSTER_INTERACTION_VALUE]

        y = self.G.data.construct_lhs(lhs, 
            kwargs_dict={STRNAMES.GROWTH_VALUE:{&#39;with_perturbations&#39;: False}})
        X = self.G.data.construct_rhs(rhs, toarray=True)

        if X.shape[1] == 0:
            return {
            &#39;ret&#39;: 0,
            &#39;beta_logdet&#39;: 0,
            &#39;priorvar_logdet&#39;: 0,
            &#39;bEb&#39;: 0,
            &#39;bEbprior&#39;: 0}

        process_prec = self.G[STRNAMES.PROCESSVAR].build_matrix(cov=False, sparse=False)
        prior_prec = build_prior_covariance(G=self.G, cov=False, order=rhs, sparse=False)
        prior_var = build_prior_covariance(G=self.G, cov=True, order=rhs, sparse=False)
        prior_mean = build_prior_mean(G=self.G, order=rhs, shape=(-1,1))


        # Calculate the posterior
        beta_prec = X.T @ process_prec @ X + prior_prec
        beta_cov = pinv(beta_prec, self)
        beta_mean = beta_cov @ ( X.T @ process_prec @ y + prior_prec @ prior_mean )
        beta_mean = np.asarray(beta_mean).reshape(-1,1)

        # Perform the marginalization
        try:
            beta_logdet = log_det(beta_cov, self)
        except:
            logging.critical(&#39;Crashed in log_det&#39;)
            logging.critical(&#39;beta_cov:\n{}&#39;.format(beta_cov))
            logging.critical(&#39;prior_prec\n{}&#39;.format(prior_prec))
            raise
        priorvar_logdet = log_det(prior_var, self)
        ll2 = 0.5 * (beta_logdet - priorvar_logdet)

        a = np.asarray(prior_mean.T @ prior_prec @ prior_mean)[0,0]
        b = np.asarray(beta_mean.T @ beta_prec @ beta_mean)[0,0]
        ll3 = -0.5 * (a  - b)

        return {&#39;ret&#39;: ll2+ll3, &#39;beta_logdet&#39;: beta_logdet, &#39;priorvar_logdet&#39;: priorvar_logdet,
            &#39;bEb&#39;: b, &#39;bEbprior&#39;: a}

    def update_single_idx_fast(self, idx: int):
        &#39;&#39;&#39;Calculate the relative log likelihood of changing the indicator of the
        interaction at index `idx`.

        This is about 20X faster than `update_single_idx_slow`.

        Parameters
        ----------
        idx : int
            This is the index of the interaction index we are sampling
        &#39;&#39;&#39;
        # Get the current interaction by the index
        self.curr_interaction = self.interactions.iloc(idx)
        start_sign = self.curr_interaction.indicator
        
        tcid = self.curr_interaction.target_cid
        self.curr_interaction.indicator = False
        self.col_idxs = np.asarray(
            self.interactions.get_arg_indicators(target_cid=tcid),
            dtype=int)

        if not start_sign:
            self.n_on_master += 1
            self.num_pos_indicators += 1
            self.num_neg_indicators -= 1

        d_on = self.calculate_relative_marginal_loglikelihood(idx=idx, val=True)

        self.n_on_master -= 1
        self.num_pos_indicators -= 1
        self.num_neg_indicators += 1
        d_off = self.calculate_relative_marginal_loglikelihood(idx=idx, val=False)

        ll_on = d_on + self.prior_ll_on
        ll_off = d_off + self.prior_ll_off

        dd = [ll_off, ll_on]
        res = bool(sample_categorical_log(dd))
        if res:
            self.n_on_master += 1
            self.num_pos_indicators += 1
            self.num_neg_indicators -= 1
            self.curr_interaction.indicator = True

    def calculate_relative_marginal_loglikelihood(self, idx: int, val: bool) -&gt; float:
        &#39;&#39;&#39;Calculate the relative marginal log likelihood for the interaction index
        `idx` with the indicator `val`

        Parameters
        ----------
        idx : int
            This is the index of the interaction
        val : bool
            This is the value to calculate it as
        &#39;&#39;&#39;
        tcid = self.curr_interaction.target_cid

        y = self.ys[tcid]
        process_prec = self.process_precs[tcid]

        # Make X, prior mean, and prior_var
        if val:
            cols = np.append(self.col_idxs, idx)
        else:
            cols = self.col_idxs
        
        X = self.interactionXs[tcid][:, cols]
        prior_mean = np.full(len(cols), self.prior_mean_interaction)
        prior_prec_diag = np.full(len(cols), self.prior_prec_interaction)

        if self._there_are_perturbations:
            Xpert = self.perturbationsXs[tcid]
            X = np.hstack((X, Xpert))

            prior_mean = np.append(
                prior_mean,
                self.prior_mean_perturbations[tcid])
            
            prior_prec_diag = np.append(
                prior_prec_diag,
                self.prior_prec_perturbations[tcid])

        if X.shape[1] == 0:
            return 0

        prior_prec = np.diag(prior_prec_diag)
        pm = (prior_prec_diag * prior_mean).reshape(-1,1)

        # Do the marginalization
        a = X.T * process_prec

        beta_prec = (a @ X) + prior_prec
        beta_cov = pinv(beta_prec, self)
        beta_mean = beta_cov @ ((a @ y) + pm )

        bEb = (beta_mean.T @ beta_prec @ beta_mean)[0,0]
        try:
            beta_logdet = log_det(beta_cov, self)
        except:
            logging.critical(&#39;Crashed in log_det&#39;)
            logging.critical(&#39;beta_cov:\n{}&#39;.format(beta_cov))
            logging.critical(&#39;prior_prec\n{}&#39;.format(prior_prec))
            raise
        
        if val:
            bEbprior = (self.prior_mean_interaction**2)/self.prior_var_interaction
            priorvar_logdet = self.priorvar_logdet
        else:
            bEbprior = 0
            priorvar_logdet = 0

        ll2 = 0.5 * (beta_logdet - priorvar_logdet)
        ll3 = 0.5 * (bEb - bEbprior)
        return ll2 + ll3
            
    def kill(self):
        pass

    def visualize(self, basepath: str, section: str=&#39;posterior&#39;, taxa_formatter: str=&#39;%(paperformat)s&#39;, 
        yticklabels: str=&#39;%(paperformat)s %(index)s&#39;, xticklabels: str=&#39;%(index)s&#39;, vmax: Union[float, int]=10,
        fixed_clustering: bool=False):
        &#39;&#39;&#39;Render the interaction matrices in the folder `basepath`

        Parameters
        ----------
        basepath : str
            This is the loction to write the files to
        section : str
            Section of the trace to compute on. Options:
                &#39;posterior&#39; : posterior samples
                &#39;burnin&#39; : burn-in samples
                &#39;entire&#39; : both burn-in and posterior samples
        taxa_formatter : str, None
            This is the format of the label to return for each . If None, it will return
            the taxon/s name
        yticklabels, xticklabels : str
            These are the formats to plot the y-axis and x0axis, respectively.
        fixed_clustering : bool
            If True, plot the variable as if clustering was fixed. Since the
            cluster assignments never change, all of the taxa within the cluster
            have identical values, so we can choose any taxon within the cluster to
            represent it
        &#39;&#39;&#39;
        from . util import generate_interation_bayes_factors_posthoc

        if not self.G.inference.tracer.is_being_traced(self.G[STRNAMES.INTERACTIONS_OBJ]):
            logging.info(&#39;Interactions are not being learned&#39;)
        
        # Get cluster ordering
        taxa = self.G.data.taxa
        order = _make_cluster_order(graph=self.G, section=section)
        clustering = self.G[STRNAMES.CLUSTERING_OBJ]

        # Plot the bayes factors
        bfs = generate_interation_bayes_factors_posthoc(mcmc=self.G.inference, section=section)

        if fixed_clustering:
            labels = [&#39;Cluster {}&#39;.format(iii+1) for iii in range(len(clustering))]
            yticklabels = [&#39;{cname} {idx}&#39;.format(cname=labels[idx], idx=idx) for idx in range(len(clustering))]
            xticklabels = [&#39;{}&#39;.format(i) for i in range(len(clustering))]
            order = None
            taxa = None
            title = &#39;Cluster Interaction Bayes Factors&#39;

            bf_cluster = []
            for cluster1 in clustering:
                temp = []
                aidx = list(cluster1.members)[0]
                for cluster2 in clustering:
                    if cluster1.id == cluster2.id:
                        temp.append(np.nan)
                        continue
                    bidx = list(cluster2.members)[0]
                    temp.append(bfs[aidx, bidx])
                bf_cluster.append(temp)
            bfs = np.asarray(bf_cluster)
        else:
            labels = [taxa[aidx].name for aidx in order]
            taxa = self.G.data.taxa
            title = &#39;Microbe Interaction Bayes Factors&#39;

        visualization.render_bayes_factors(bfs, taxa=taxa, order=order, max_value=vmax, 
            xticklabels=xticklabels, yticklabels=yticklabels, title=title)
        fig = plt.gcf()
        fig.tight_layout()
        plt.savefig(os.path.join(basepath, &#39;bayes_factors.pdf&#39;))
        plt.close()

        if not fixed_clustering:
            bfs = bfs[order, :]
            bfs = bfs[:, order]
        df = pd.DataFrame(bfs, index=labels, columns=labels)
        df.to_csv(os.path.join(basepath, &#39;bayes_factors.tsv&#39;), sep=&#39;\t&#39;, index=True, header=True)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="mdsine2.pylab.variables.Variable" href="pylab/variables.html#mdsine2.pylab.variables.Variable">Variable</a></li>
<li><a title="mdsine2.pylab.graph.Node" href="pylab/graph.html#mdsine2.pylab.graph.Node">Node</a></li>
<li><a title="mdsine2.pylab.graph.BaseNode" href="pylab/graph.html#mdsine2.pylab.graph.BaseNode">BaseNode</a></li>
<li><a title="mdsine2.pylab.base.Saveable" href="pylab/base.html#mdsine2.pylab.base.Saveable">Saveable</a></li>
<li>mdsine2.pylab.variables._BaseArithmeticClass</li>
<li><a title="mdsine2.pylab.base.Traceable" href="pylab/base.html#mdsine2.pylab.base.Traceable">Traceable</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="mdsine2.posterior.ClusterInteractionIndicators.calculate_marginal_loglikelihood"><code class="name flex">
<span>def <span class="ident">calculate_marginal_loglikelihood</span></span>(<span>self, idx: int, val: bool) ‑> Dict[str, Union[float, int]]</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate the likelihood of interaction <code>idx</code> with the value <code>val</code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def calculate_marginal_loglikelihood(self, idx: int, val: bool) -&gt; Dict[str, Union[float, int]]:
    &#39;&#39;&#39;Calculate the likelihood of interaction `idx` with the value `val`
    &#39;&#39;&#39;
    # Build and initialize
    self.interactions.iloc(idx).indicator = val
    self.update_cnt_indicators()
    self.G.data.design_matrices[STRNAMES.CLUSTER_INTERACTION_VALUE].M.build()

    lhs = [STRNAMES.GROWTH_VALUE, STRNAMES.SELF_INTERACTION_VALUE]
    if self._there_are_perturbations:
        rhs = [STRNAMES.PERT_VALUE, STRNAMES.CLUSTER_INTERACTION_VALUE]
    else:
        rhs = [STRNAMES.CLUSTER_INTERACTION_VALUE]

    y = self.G.data.construct_lhs(lhs, 
        kwargs_dict={STRNAMES.GROWTH_VALUE:{&#39;with_perturbations&#39;: False}})
    X = self.G.data.construct_rhs(rhs, toarray=True)

    if X.shape[1] == 0:
        return {
        &#39;ret&#39;: 0,
        &#39;beta_logdet&#39;: 0,
        &#39;priorvar_logdet&#39;: 0,
        &#39;bEb&#39;: 0,
        &#39;bEbprior&#39;: 0}

    process_prec = self.G[STRNAMES.PROCESSVAR].build_matrix(cov=False, sparse=False)
    prior_prec = build_prior_covariance(G=self.G, cov=False, order=rhs, sparse=False)
    prior_var = build_prior_covariance(G=self.G, cov=True, order=rhs, sparse=False)
    prior_mean = build_prior_mean(G=self.G, order=rhs, shape=(-1,1))


    # Calculate the posterior
    beta_prec = X.T @ process_prec @ X + prior_prec
    beta_cov = pinv(beta_prec, self)
    beta_mean = beta_cov @ ( X.T @ process_prec @ y + prior_prec @ prior_mean )
    beta_mean = np.asarray(beta_mean).reshape(-1,1)

    # Perform the marginalization
    try:
        beta_logdet = log_det(beta_cov, self)
    except:
        logging.critical(&#39;Crashed in log_det&#39;)
        logging.critical(&#39;beta_cov:\n{}&#39;.format(beta_cov))
        logging.critical(&#39;prior_prec\n{}&#39;.format(prior_prec))
        raise
    priorvar_logdet = log_det(prior_var, self)
    ll2 = 0.5 * (beta_logdet - priorvar_logdet)

    a = np.asarray(prior_mean.T @ prior_prec @ prior_mean)[0,0]
    b = np.asarray(beta_mean.T @ beta_prec @ beta_mean)[0,0]
    ll3 = -0.5 * (a  - b)

    return {&#39;ret&#39;: ll2+ll3, &#39;beta_logdet&#39;: beta_logdet, &#39;priorvar_logdet&#39;: priorvar_logdet,
        &#39;bEb&#39;: b, &#39;bEbprior&#39;: a}</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.ClusterInteractionIndicators.calculate_relative_marginal_loglikelihood"><code class="name flex">
<span>def <span class="ident">calculate_relative_marginal_loglikelihood</span></span>(<span>self, idx: int, val: bool) ‑> float</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate the relative marginal log likelihood for the interaction index
<code>idx</code> with the indicator <code>val</code></p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>idx</code></strong> :&ensp;<code>int</code></dt>
<dd>This is the index of the interaction</dd>
<dt><strong><code>val</code></strong> :&ensp;<code>bool</code></dt>
<dd>This is the value to calculate it as</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def calculate_relative_marginal_loglikelihood(self, idx: int, val: bool) -&gt; float:
    &#39;&#39;&#39;Calculate the relative marginal log likelihood for the interaction index
    `idx` with the indicator `val`

    Parameters
    ----------
    idx : int
        This is the index of the interaction
    val : bool
        This is the value to calculate it as
    &#39;&#39;&#39;
    tcid = self.curr_interaction.target_cid

    y = self.ys[tcid]
    process_prec = self.process_precs[tcid]

    # Make X, prior mean, and prior_var
    if val:
        cols = np.append(self.col_idxs, idx)
    else:
        cols = self.col_idxs
    
    X = self.interactionXs[tcid][:, cols]
    prior_mean = np.full(len(cols), self.prior_mean_interaction)
    prior_prec_diag = np.full(len(cols), self.prior_prec_interaction)

    if self._there_are_perturbations:
        Xpert = self.perturbationsXs[tcid]
        X = np.hstack((X, Xpert))

        prior_mean = np.append(
            prior_mean,
            self.prior_mean_perturbations[tcid])
        
        prior_prec_diag = np.append(
            prior_prec_diag,
            self.prior_prec_perturbations[tcid])

    if X.shape[1] == 0:
        return 0

    prior_prec = np.diag(prior_prec_diag)
    pm = (prior_prec_diag * prior_mean).reshape(-1,1)

    # Do the marginalization
    a = X.T * process_prec

    beta_prec = (a @ X) + prior_prec
    beta_cov = pinv(beta_prec, self)
    beta_mean = beta_cov @ ((a @ y) + pm )

    bEb = (beta_mean.T @ beta_prec @ beta_mean)[0,0]
    try:
        beta_logdet = log_det(beta_cov, self)
    except:
        logging.critical(&#39;Crashed in log_det&#39;)
        logging.critical(&#39;beta_cov:\n{}&#39;.format(beta_cov))
        logging.critical(&#39;prior_prec\n{}&#39;.format(prior_prec))
        raise
    
    if val:
        bEbprior = (self.prior_mean_interaction**2)/self.prior_var_interaction
        priorvar_logdet = self.priorvar_logdet
    else:
        bEbprior = 0
        priorvar_logdet = 0

    ll2 = 0.5 * (beta_logdet - priorvar_logdet)
    ll3 = 0.5 * (bEb - bEbprior)
    return ll2 + ll3</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.ClusterInteractionIndicators.initialize"><code class="name flex">
<span>def <span class="ident">initialize</span></span>(<span>self, delay: int = 0, run_every_n_iterations: int = 1)</span>
</code></dt>
<dd>
<div class="desc"><p>Do nothing, the indicators are set in <code><a title="mdsine2.posterior.ClusterInteractionValue" href="#mdsine2.posterior.ClusterInteractionValue">ClusterInteractionValue</a></code>.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>delay</code></strong> :&ensp;<code>int</code></dt>
<dd>How many iterations to delay starting to update the values</dd>
<dt><strong><code>run_every_n_iterations</code></strong> :&ensp;<code>int</code></dt>
<dd>Which iteration to run on</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def initialize(self, delay: int=0, run_every_n_iterations: int=1):
    &#39;&#39;&#39;Do nothing, the indicators are set in `ClusterInteractionValue`.

    Parameters
    ----------
    delay : int
        How many iterations to delay starting to update the values
    run_every_n_iterations : int
        Which iteration to run on
    &#39;&#39;&#39;
    if not pl.isint(delay):
        raise TypeError(&#39;`delay` ({}) must be an int&#39;.format(type(delay)))
    if delay &lt; 0:
        raise ValueError(&#39;`delay` ({}) must be &gt;= 0&#39;.format(delay))
    if not pl.isint(run_every_n_iterations):
        raise TypeError(&#39;`run_every_n_iterations` ({}) must be an int&#39;.format(
            type(run_every_n_iterations)))
    if run_every_n_iterations &lt;= 0:
        raise ValueError(&#39;`run_every_n_iterations` ({}) must be &gt; 0&#39;.format(
            run_every_n_iterations))

    self.delay = delay
    self.run_every_n_iterations = run_every_n_iterations
    self._there_are_perturbations = self.G.perturbations is not None
    self.update_cnt_indicators()
    self.interactions = self.G[STRNAMES.INTERACTIONS_OBJ]
    self.n_taxa = len(self.G.data.taxa)

    # These are for the function `self._make_idx_for_clusters`
    self.ndts_bias = []
    self.n_replicates = self.G.data.n_replicates
    self.n_dts_for_replicate = self.G.data.n_dts_for_replicate
    self.total_dts = np.sum(self.n_dts_for_replicate)
    self.replicate_bias = np.zeros(self.n_replicates, dtype=int)
    for ridx in range(1, self.n_replicates):
        self.replicate_bias[ridx] = self.replicate_bias[ridx-1] + \
            self.n_taxa * self.n_dts_for_replicate[ridx - 1]
    for ridx in range(self.G.data.n_replicates):
        self.ndts_bias.append(
            np.arange(0, self.G.data.n_dts_for_replicate[ridx] * self.n_taxa, self.n_taxa))

    # Makes a dictionary that maps the taxon index to the rows that it the  in
    self.oidx2rows = {}
    for oidx in range(self.n_taxa):
        idxs = np.zeros(self.total_dts, dtype=int)
        i = 0
        for ridx in range(self.n_replicates):
            temp = np.arange(0, self.n_dts_for_replicate[ridx] * self.n_taxa, self.n_taxa)
            temp = temp + oidx
            temp = temp + self.replicate_bias[ridx]
            l = len(temp)
            idxs[i:i+l] = temp
            i += l
        self.oidx2rows[oidx] = idxs</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.ClusterInteractionIndicators.kill"><code class="name flex">
<span>def <span class="ident">kill</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def kill(self):
    pass</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.ClusterInteractionIndicators.make_rel_params"><code class="name flex">
<span>def <span class="ident">make_rel_params</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>We make the parameters needed to update the relative log-likelihod.
This function is called once at the beginning of the update.</p>
<h2 id="parameters-that-we-create-with-this-function">Parameters That We Create With This Function</h2>
<ul>
<li>ys : dict (int -&gt; np.ndarray)<ul>
<li>Maps the target cluster id to the observation matrix that it
corresponds to (only the Taxa in the target cluster). This
array already has the growth and self-interactions subtracted
out:<ul>
<li>$
rac{log(x_{k+1}) - log(x_{k})}{dt} - a_{1,k} - a_{2,k}x_{k} $</li>
</ul>
</li>
</ul>
</li>
<li>process_precs : dict (int -&gt; np.ndarray)<ul>
<li>Maps the target cluster id to the vector of the process precision
that corresponds to the target cluster (only the Taxa in the target
cluster). This is a 1D array that corresponds to the diagonal of what
would be the precision matrix.</li>
</ul>
</li>
<li>interactionXs : dict (int -&gt; np.ndarray)<ul>
<li>Maps the target cluster id to the matrix of the design matrix of the
interactions. Only includes the rows that correspond to the Taxa in the
target cluster. It includes every single column as if all of the indicators
are on. We only index out the columns when we are doing the marginalization.</li>
</ul>
</li>
<li>prior_prec_interaction : float<ul>
<li>Prior precision of the interaction value. We then use this
value to make the diagonal of the prior precision.</li>
</ul>
</li>
<li>prior_var_interaction : float<ul>
<li>Prior variance of the interaction value.</li>
</ul>
</li>
<li>prior_mean_interaction : float<ul>
<li>Prior mean of the interaction values. We use this value
to make the prior mean vector during the marginalization.</li>
</ul>
</li>
<li>n_on_master : int<ul>
<li>How many interactions are on at any one time. We adjust this
throughout the update depending on what interactions we turn off and
on.</li>
</ul>
</li>
<li>prior_ll_on : float<ul>
<li>Prior log likelihood of a positive interaction</li>
</ul>
</li>
<li>prior_ll_off : float<ul>
<li>Prior log likelihood of the negative interaction</li>
</ul>
</li>
<li>priorvar_logdet_diff : float<ul>
<li>This is the prior variance log determinant that we add when the indicator
is positive.</li>
</ul>
</li>
</ul>
<h2 id="parameters-created-if-there-are-perturbations">Parameters Created If There Are Perturbations</h2>
<ul>
<li>perturbationsXs : dict (int -&gt; np.ndarray)<ul>
<li>Maps the target cluster id to the design matrix that corresponds to
the on perturbations of the target clusters. This is preindexed in
both rows and columns</li>
</ul>
</li>
<li>prior_prec_perturbations : dict (int -&gt; np.ndarray)<ul>
<li>Maps the target cluster id to the diagonal of the prior precision
of the perturbations</li>
</ul>
</li>
<li>prior_var_perturbations : dict (int -&gt; np.ndarray)<ul>
<li>Maps the target cluster id to the diagonal of the prior variance
of the perturbations</li>
</ul>
</li>
<li>prior_mean_perturbations : dict (int -&gt; np.ndarray)<ul>
<li>Maps the target cluster id to the vector of the prior mean of the
perturbations</li>
</ul>
</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def make_rel_params(self):
    &#39;&#39;&#39;We make the parameters needed to update the relative log-likelihod.
    This function is called once at the beginning of the update.

    Parameters that we create with this function
    --------------------------------------------
    - ys : dict (int -&gt; np.ndarray)
        - Maps the target cluster id to the observation matrix that it
          corresponds to (only the Taxa in the target cluster). This 
          array already has the growth and self-interactions subtracted
          out:
            * $ \frac{log(x_{k+1}) - log(x_{k})}{dt} - a_{1,k} - a_{2,k}x_{k} $
    - process_precs : dict (int -&gt; np.ndarray)
        - Maps the target cluster id to the vector of the process precision
          that corresponds to the target cluster (only the Taxa in the target
          cluster). This is a 1D array that corresponds to the diagonal of what
          would be the precision matrix.
    - interactionXs : dict (int -&gt; np.ndarray)
        - Maps the target cluster id to the matrix of the design matrix of the
          interactions. Only includes the rows that correspond to the Taxa in the
          target cluster. It includes every single column as if all of the indicators
          are on. We only index out the columns when we are doing the marginalization.
    - prior_prec_interaction : float
        - Prior precision of the interaction value. We then use this
          value to make the diagonal of the prior precision.
    - prior_var_interaction : float
        - Prior variance of the interaction value.
    - prior_mean_interaction : float
        - Prior mean of the interaction values. We use this value
          to make the prior mean vector during the marginalization.
    - n_on_master : int
        - How many interactions are on at any one time. We adjust this
          throughout the update depending on what interactions we turn off and
          on.
    - prior_ll_on : float
        - Prior log likelihood of a positive interaction
    - prior_ll_off : float
        - Prior log likelihood of the negative interaction
    - priorvar_logdet_diff : float
        - This is the prior variance log determinant that we add when the indicator
          is positive.

    Parameters created if there are perturbations
    ---------------------------------------------
    - perturbationsXs : dict (int -&gt; np.ndarray)
        - Maps the target cluster id to the design matrix that corresponds to 
          the on perturbations of the target clusters. This is preindexed in
          both rows and columns
    - prior_prec_perturbations : dict (int -&gt; np.ndarray)
        - Maps the target cluster id to the diagonal of the prior precision
          of the perturbations
    - prior_var_perturbations : dict (int -&gt; np.ndarray)
        - Maps the target cluster id to the diagonal of the prior variance 
          of the perturbations
    - prior_mean_perturbations : dict (int -&gt; np.ndarray)
        - Maps the target cluster id to the vector of the prior mean of the
          perturbations
    &#39;&#39;&#39;
    # Get the row indices for each cluster
    row_idxs = self._make_idx_vector_for_clusters()

    # Create ys
    self.ys = {}
    y = self.G.data.construct_lhs(keys=[
        STRNAMES.SELF_INTERACTION_VALUE, STRNAMES.GROWTH_VALUE],
        kwargs_dict={STRNAMES.GROWTH_VALUE:{&#39;with_perturbations&#39;: False}})
    for tcid in self.clustering.order:
        self.ys[tcid] = y[row_idxs[tcid], :]

    # Create process_precs
    self.process_precs = {}
    process_prec_diag = self.G[STRNAMES.PROCESSVAR].prec
    for tcid in self.clustering.order:
        self.process_precs[tcid] = process_prec_diag[row_idxs[tcid]]

    # Make interactionXs
    self.interactionXs = {}
    self.G.data.design_matrices[STRNAMES.CLUSTER_INTERACTION_VALUE].M.build(
        build=True, build_for_neg_ind=True)
    XM_master = self.G.data.design_matrices[STRNAMES.CLUSTER_INTERACTION_VALUE].toarray()
    for tcid in self.clustering.order:
        self.interactionXs[tcid] = XM_master[row_idxs[tcid], :]

    # Make prior parameters
    self.prior_var_interaction = self.G[STRNAMES.PRIOR_VAR_INTERACTIONS].value
    self.prior_prec_interaction = 1/self.prior_var_interaction
    self.prior_mean_interaction = self.G[STRNAMES.PRIOR_MEAN_INTERACTIONS].value
    self.prior_ll_on = np.log(self.prior.value)
    self.prior_ll_off = np.log(1 - self.prior.value)
    self.n_on_master = self.interactions.num_pos_indicators()

    # Make priorvar_logdet
    self.priorvar_logdet = np.log(self.prior_var_interaction)

    if self._there_are_perturbations:
        XMpert_master = self.G.data.design_matrices[STRNAMES.PERT_VALUE].toarray()

        # Make perturbationsXs
        self.perturbationsXs = {}
        for tcid in self.clustering.order:
            rows = row_idxs[tcid]
            cols = []
            i = 0
            for perturbation in self.G.perturbations:
                for cid in perturbation.indicator.value:
                    if perturbation.indicator.value[cid]:
                        if cid == tcid:
                            cols.append(i)
                        i += 1
            cols = np.asarray(cols, dtype=int)

            self.perturbationsXs[tcid] = pl.util.fast_index(M=XMpert_master,
                rows=rows, cols=cols)

        # Make prior perturbation parameters
        self.prior_mean_perturbations = {}
        self.prior_var_perturbations = {}
        self.prior_prec_perturbations = {}
        for tcid in self.clustering.order:
            mean = []
            var = []
            for perturbation in self.G.perturbations:
                if perturbation.indicator.value[tcid]:
                    # This is on, get the parameters
                    mean.append(perturbation.magnitude.prior.loc.value)
                    var.append(perturbation.magnitude.prior.scale2.value)
            self.prior_mean_perturbations[tcid] = np.asarray(mean)
            self.prior_var_perturbations[tcid] = np.asarray(var)
            self.prior_prec_perturbations[tcid] = 1/self.prior_var_perturbations[tcid]

        # Make priorvar_det_perturbations
        self.priorvar_det_perturbations = 0
        for perturbation in self.G.perturbations:
            self.priorvar_det_perturbations += \
                perturbation.indicator.num_on_clusters() * \
                perturbation.magnitude.prior.scale2.value</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.ClusterInteractionIndicators.update_cnt_indicators"><code class="name flex">
<span>def <span class="ident">update_cnt_indicators</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_cnt_indicators(self):
    self.num_pos_indicators = self.G[STRNAMES.INTERACTIONS_OBJ].num_pos_indicators()
    self.num_neg_indicators = self.G[STRNAMES.INTERACTIONS_OBJ].num_neg_indicators()</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.ClusterInteractionIndicators.update_relative"><code class="name flex">
<span>def <span class="ident">update_relative</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Update the indicators variables by calculating the relative loglikelihoods
of it being on as supposed to off. Because this is a relative loglikelihood,
we only need to take into account the following parameters of the model:
- Only the Taxa in the target cluster of the interaction
- Only the positively indicated interactions going into the
target cluster.</p>
<p>This is 1000's of times faster than <code>update</code> because we are operating on matrices
that are MUCH smaller than in a full system. These matrices are also considered dense
so we do all of our computations without sparse matrices.</p>
<p>We permute the order that the indices are updated for more robust mixing.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_relative(self):
    &#39;&#39;&#39;Update the indicators variables by calculating the relative loglikelihoods
    of it being on as supposed to off. Because this is a relative loglikelihood,
    we only need to take into account the following parameters of the model:
        - Only the Taxa in the target cluster of the interaction
        - Only the positively indicated interactions going into the
          target cluster.

    This is 1000&#39;s of times faster than `update` because we are operating on matrices
    that are MUCH smaller than in a full system. These matrices are also considered dense
    so we do all of our computations without sparse matrices.

    We permute the order that the indices are updated for more robust mixing.
    &#39;&#39;&#39;
    start = time.time()
    if self.sample_iter &lt; self.delay:
        self._strr = &#39;{}\ntotal time: {}&#39;.format(
            self.interactions.get_indicators(), time.time()-start)
        return
    if self.sample_iter % self.run_every_n_iterations != 0:
        return

    idxs = npr.permutation(self.interactions.size)

    self.make_rel_params()
    for idx in idxs:
        self.update_single_idx_fast(idx=idx)

    self.update_cnt_indicators()
    # Since slicing is literally so slow, it is faster to build than just slicing M
    self.G.data.design_matrices[STRNAMES.CLUSTER_INTERACTION_VALUE].M.build(
        build=True, build_for_neg_ind=False)
    iii = self.interactions.get_indicators()
    n_on = np.sum(iii)
    self._strr = &#39;{}\ntotal time: {}, n_interactions: {}/{}, {:.2f}&#39;.format(
        iii, time.time()-start, n_on, len(iii), n_on/len(iii))</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.ClusterInteractionIndicators.update_single_idx_fast"><code class="name flex">
<span>def <span class="ident">update_single_idx_fast</span></span>(<span>self, idx: int)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate the relative log likelihood of changing the indicator of the
interaction at index <code>idx</code>.</p>
<p>This is about 20X faster than <code>update_single_idx_slow</code>.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>idx</code></strong> :&ensp;<code>int</code></dt>
<dd>This is the index of the interaction index we are sampling</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_single_idx_fast(self, idx: int):
    &#39;&#39;&#39;Calculate the relative log likelihood of changing the indicator of the
    interaction at index `idx`.

    This is about 20X faster than `update_single_idx_slow`.

    Parameters
    ----------
    idx : int
        This is the index of the interaction index we are sampling
    &#39;&#39;&#39;
    # Get the current interaction by the index
    self.curr_interaction = self.interactions.iloc(idx)
    start_sign = self.curr_interaction.indicator
    
    tcid = self.curr_interaction.target_cid
    self.curr_interaction.indicator = False
    self.col_idxs = np.asarray(
        self.interactions.get_arg_indicators(target_cid=tcid),
        dtype=int)

    if not start_sign:
        self.n_on_master += 1
        self.num_pos_indicators += 1
        self.num_neg_indicators -= 1

    d_on = self.calculate_relative_marginal_loglikelihood(idx=idx, val=True)

    self.n_on_master -= 1
    self.num_pos_indicators -= 1
    self.num_neg_indicators += 1
    d_off = self.calculate_relative_marginal_loglikelihood(idx=idx, val=False)

    ll_on = d_on + self.prior_ll_on
    ll_off = d_off + self.prior_ll_off

    dd = [ll_off, ll_on]
    res = bool(sample_categorical_log(dd))
    if res:
        self.n_on_master += 1
        self.num_pos_indicators += 1
        self.num_neg_indicators -= 1
        self.curr_interaction.indicator = True</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.ClusterInteractionIndicators.update_single_idx_slow"><code class="name flex">
<span>def <span class="ident">update_single_idx_slow</span></span>(<span>self, idx: int)</span>
</code></dt>
<dd>
<div class="desc"><p>Update the likelihood for interaction <code>idx</code></p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>idx</code></strong> :&ensp;<code>int</code></dt>
<dd>This is the index of the interaction we are updating</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_single_idx_slow(self, idx: int):
    &#39;&#39;&#39;Update the likelihood for interaction `idx`

    Parameters
    ----------
    idx : int
        This is the index of the interaction we are updating
    &#39;&#39;&#39;
    prior_ll_on = np.log(self.G[STRNAMES.CLUSTER_INTERACTION_INDICATOR_PROB].value)
    prior_ll_off = np.log(1 - self.G[STRNAMES.CLUSTER_INTERACTION_INDICATOR_PROB].value)

    d_on = self.calculate_marginal_loglikelihood(idx=idx, val=True)
    d_off = self.calculate_marginal_loglikelihood(idx=idx, val=False)

    ll_on = d_on[&#39;ret&#39;] + prior_ll_on
    ll_off = d_off[&#39;ret&#39;] + prior_ll_off
    dd = [ll_off, ll_on]

    res = bool(sample_categorical_log(dd))
    self.interactions.iloc(idx).indicator = res
    self.update_cnt_indicators()</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.ClusterInteractionIndicators.update_slow"><code class="name flex">
<span>def <span class="ident">update_slow</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Permute the order that the indices that are updated.</p>
<p>Build the full master interaction matrix that we can then slice</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_slow(self):
    &#39;&#39;&#39;Permute the order that the indices that are updated.

    Build the full master interaction matrix that we can then slice
    &#39;&#39;&#39;
    start = time.time()
    if self.sample_iter &lt; self.delay:
        # for interaction in self.interactions:
        #     interaction.indicator=False
        self._strr = &#39;{}\ntotal time: {}&#39;.format(
            self.interactions.get_indicators(), time.time()-start)
        return
    if self.sample_iter % self.run_every_n_iterations != 0:
        return

    idxs = npr.permutation(self.interactions.size)
    for idx in idxs:
        self.update_single_idx_slow(idx=idx)

    self.update_cnt_indicators()
    # Since slicing is literally so slow, it is faster to build than just slicing M
    self.G.data.design_matrices[STRNAMES.CLUSTER_INTERACTION_VALUE].M.build(
        build=True, build_for_neg_ind=False)
    iii = self.interactions.get_indicators()
    n_on = np.sum(iii)
    self._strr = &#39;{}\ntotal time: {}, n_interactions: {}/{}, {:.2f}&#39;.format(
        iii, time.time()-start, n_on, len(iii), n_on/len(iii))</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.ClusterInteractionIndicators.visualize"><code class="name flex">
<span>def <span class="ident">visualize</span></span>(<span>self, basepath: str, section: str = 'posterior', taxa_formatter: str = '%(paperformat)s', yticklabels: str = '%(paperformat)s %(index)s', xticklabels: str = '%(index)s', vmax: Union[float, int] = 10, fixed_clustering: bool = False)</span>
</code></dt>
<dd>
<div class="desc"><p>Render the interaction matrices in the folder <code>basepath</code></p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>basepath</code></strong> :&ensp;<code>str</code></dt>
<dd>This is the loction to write the files to</dd>
<dt><strong><code>section</code></strong> :&ensp;<code>str</code></dt>
<dd>Section of the trace to compute on. Options:
'posterior' : posterior samples
'burnin' : burn-in samples
'entire' : both burn-in and posterior samples</dd>
<dt><strong><code>taxa_formatter</code></strong> :&ensp;<code>str, None</code></dt>
<dd>This is the format of the label to return for each . If None, it will return
the taxon/s name</dd>
<dt><strong><code>yticklabels</code></strong>, <strong><code>xticklabels</code></strong> :&ensp;<code>str</code></dt>
<dd>These are the formats to plot the y-axis and x0axis, respectively.</dd>
<dt><strong><code>fixed_clustering</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, plot the variable as if clustering was fixed. Since the
cluster assignments never change, all of the taxa within the cluster
have identical values, so we can choose any taxon within the cluster to
represent it</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def visualize(self, basepath: str, section: str=&#39;posterior&#39;, taxa_formatter: str=&#39;%(paperformat)s&#39;, 
    yticklabels: str=&#39;%(paperformat)s %(index)s&#39;, xticklabels: str=&#39;%(index)s&#39;, vmax: Union[float, int]=10,
    fixed_clustering: bool=False):
    &#39;&#39;&#39;Render the interaction matrices in the folder `basepath`

    Parameters
    ----------
    basepath : str
        This is the loction to write the files to
    section : str
        Section of the trace to compute on. Options:
            &#39;posterior&#39; : posterior samples
            &#39;burnin&#39; : burn-in samples
            &#39;entire&#39; : both burn-in and posterior samples
    taxa_formatter : str, None
        This is the format of the label to return for each . If None, it will return
        the taxon/s name
    yticklabels, xticklabels : str
        These are the formats to plot the y-axis and x0axis, respectively.
    fixed_clustering : bool
        If True, plot the variable as if clustering was fixed. Since the
        cluster assignments never change, all of the taxa within the cluster
        have identical values, so we can choose any taxon within the cluster to
        represent it
    &#39;&#39;&#39;
    from . util import generate_interation_bayes_factors_posthoc

    if not self.G.inference.tracer.is_being_traced(self.G[STRNAMES.INTERACTIONS_OBJ]):
        logging.info(&#39;Interactions are not being learned&#39;)
    
    # Get cluster ordering
    taxa = self.G.data.taxa
    order = _make_cluster_order(graph=self.G, section=section)
    clustering = self.G[STRNAMES.CLUSTERING_OBJ]

    # Plot the bayes factors
    bfs = generate_interation_bayes_factors_posthoc(mcmc=self.G.inference, section=section)

    if fixed_clustering:
        labels = [&#39;Cluster {}&#39;.format(iii+1) for iii in range(len(clustering))]
        yticklabels = [&#39;{cname} {idx}&#39;.format(cname=labels[idx], idx=idx) for idx in range(len(clustering))]
        xticklabels = [&#39;{}&#39;.format(i) for i in range(len(clustering))]
        order = None
        taxa = None
        title = &#39;Cluster Interaction Bayes Factors&#39;

        bf_cluster = []
        for cluster1 in clustering:
            temp = []
            aidx = list(cluster1.members)[0]
            for cluster2 in clustering:
                if cluster1.id == cluster2.id:
                    temp.append(np.nan)
                    continue
                bidx = list(cluster2.members)[0]
                temp.append(bfs[aidx, bidx])
            bf_cluster.append(temp)
        bfs = np.asarray(bf_cluster)
    else:
        labels = [taxa[aidx].name for aidx in order]
        taxa = self.G.data.taxa
        title = &#39;Microbe Interaction Bayes Factors&#39;

    visualization.render_bayes_factors(bfs, taxa=taxa, order=order, max_value=vmax, 
        xticklabels=xticklabels, yticklabels=yticklabels, title=title)
    fig = plt.gcf()
    fig.tight_layout()
    plt.savefig(os.path.join(basepath, &#39;bayes_factors.pdf&#39;))
    plt.close()

    if not fixed_clustering:
        bfs = bfs[order, :]
        bfs = bfs[:, order]
    df = pd.DataFrame(bfs, index=labels, columns=labels)
    df.to_csv(os.path.join(basepath, &#39;bayes_factors.tsv&#39;), sep=&#39;\t&#39;, index=True, header=True)</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="mdsine2.pylab.variables.Variable" href="pylab/variables.html#mdsine2.pylab.variables.Variable">Variable</a></b></code>:
<ul class="hlist">
<li><code><a title="mdsine2.pylab.variables.Variable.T" href="pylab/variables.html#mdsine2.pylab.variables.Variable.T">T</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.add_child" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_child">add_child</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.add_parent" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_parent">add_parent</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.add_prior" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_prior">add_prior</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.add_trace" href="pylab/base.html#mdsine2.pylab.base.Traceable.add_trace">add_trace</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.add_undirected" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_undirected">add_undirected</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.degree" href="pylab/graph.html#mdsine2.pylab.graph.Node.degree">degree</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.delete" href="pylab/graph.html#mdsine2.pylab.graph.BaseNode.delete">delete</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.get_adjacent_keys" href="pylab/graph.html#mdsine2.pylab.graph.Node.get_adjacent_keys">get_adjacent_keys</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.get_iter" href="pylab/base.html#mdsine2.pylab.base.Traceable.get_iter">get_iter</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.get_trace_from_disk" href="pylab/base.html#mdsine2.pylab.base.Traceable.get_trace_from_disk">get_trace_from_disk</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.load" href="pylab/base.html#mdsine2.pylab.base.Saveable.load">load</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.metropolis" href="pylab/graph.html#mdsine2.pylab.graph.Node.metropolis">metropolis</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.overwrite_entire_trace_on_disk" href="pylab/base.html#mdsine2.pylab.base.Traceable.overwrite_entire_trace_on_disk">overwrite_entire_trace_on_disk</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.remove_local_trace" href="pylab/variables.html#mdsine2.pylab.variables.Variable.remove_local_trace">remove_local_trace</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.save" href="pylab/base.html#mdsine2.pylab.base.Saveable.save">save</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.set_save_location" href="pylab/base.html#mdsine2.pylab.base.Saveable.set_save_location">set_save_location</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.set_trace" href="pylab/base.html#mdsine2.pylab.base.Traceable.set_trace">set_trace</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.set_value_shape" href="pylab/variables.html#mdsine2.pylab.variables.Variable.set_value_shape">set_value_shape</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="mdsine2.posterior.ClusterInteractionValue"><code class="flex name class">
<span>class <span class="ident">ClusterInteractionValue</span></span>
<span>(</span><span>prior: <a title="mdsine2.pylab.variables.Normal" href="pylab/variables.html#mdsine2.pylab.variables.Normal">Normal</a>, clustering: <a title="mdsine2.pylab.cluster.Clustering" href="pylab/cluster.html#mdsine2.pylab.cluster.Clustering">Clustering</a>, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Interactions of Lotka-Voltera</p>
<p>Since we initialize the interactions object in the <code>initialize</code> function,
make sure that you have initialized the prior of the values of the interactions
and of the indicators of the interactions before you call the initialization of
this class</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>prior</code></strong> :&ensp;<code>mdsine2.variables.Normal</code></dt>
<dd>Prior distribution</dd>
<dt><strong><code>clustering</code></strong> :&ensp;<code>mdsine2.Clustering</code></dt>
<dd>Clustering object</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ClusterInteractionValue(pl.variables.MVN):
    &#39;&#39;&#39;Interactions of Lotka-Voltera

    Since we initialize the interactions object in the `initialize` function,
    make sure that you have initialized the prior of the values of the interactions
    and of the indicators of the interactions before you call the initialization of
    this class

    Parameters
    ----------
    prior : mdsine2.variables.Normal
        Prior distribution
    clustering : mdsine2.Clustering
        Clustering object
    &#39;&#39;&#39;
    def __init__(self, prior: variables.Normal, clustering: pl.Clustering, **kwargs):
        kwargs[&#39;name&#39;] = STRNAMES.CLUSTER_INTERACTION_VALUE
        pl.variables.MVN.__init__(self, dtype=float, **kwargs)
        self.set_value_shape(shape=(len(self.G.data.taxa),len(self.G.data.taxa))) # Set the shape of the trace
        self.add_prior(prior)
        self.clustering = clustering # Clustering object
        self.obj = pl.contrib.Interactions( # Initialize the mdsine2.pylab.contrib.Interactions object
            clustering=self.clustering,
            use_indicators=True,
            name=STRNAMES.INTERACTIONS_OBJ, G=self.G,
            signal_when_clusters_change=False)
        self._strr = &#39;None&#39;

    def __str__(self) -&gt; str:
        return self._strr

    def __len__(self) -&gt; int:
        # Return the number of on interactions
        return self.obj.num_pos_indicators()

    def set_values(self, *args, **kwargs):
        &#39;&#39;&#39;Set the values from an array
        &#39;&#39;&#39;
        self.obj.set_values(*args, **kwargs)

    def initialize(self, value_option: str, hyperparam_option: str=None, 
        value: np.ndarray=None, indicators: np.ndarray=None, delay: int=0):
        &#39;&#39;&#39;Initialize the interactions object.

        Parameters
        ----------
        value_option : str
            - This is how to initialize the values
            - Options:
                - &#39;manual&#39;
                    - Set the values of the interactions manually. `value` and `indicators`
                      must also be specified. We assume the values are only set for when
                      `indicators` is True, and that the order of the `indicators` and `values`
                      correspond to how we iterate over the interactions
                    - Example
                        * 3 Clusters
                        * indicators = [True, False, False, True, False, True]
                        * value = [0.2, 0.8, -0.35]
                - &#39;all-off&#39;, &#39;auto&#39;
                    - Set all of the interactions and the indicators to 0
                - &#39;all-on&#39;
                    - Set all of the indicators to on and all the values to 0
        delay : int
            How many MCMC iterations to delay starting to update
        See also
        --------
        `mdsine2.pylab.cluster.Interactions.set_values`
        &#39;&#39;&#39;
        self.obj.set_signal_when_clusters_change(True)
        self.G[STRNAMES.INTERACTIONS_OBJ].value_initializer = self.prior.sample
        self.G[STRNAMES.INTERACTIONS_OBJ].indicator_initializer = self.G[STRNAMES.CLUSTER_INTERACTION_INDICATOR_PROB].prior.sample

        self._there_are_perturbations = self.G.perturbations is not None

        if not pl.isint(delay):
            raise TypeError(&#39;`delay` ({}) must be an int&#39;.format(type(delay)))
        if delay &lt; 0:
            raise ValueError(&#39;`delay` ({}) must be &gt;= 0&#39;.format(delay))
        self.delay = delay

        if not pl.isstr(value_option):
            raise TypeError(&#39;`value_option` ({}) must be a str&#39;.format(type(value_option)))
        if value_option in [&#39;auto&#39;, &#39;all-off&#39;]:
            for interaction in self.obj:
                interaction.value = 0
                interaction.indicator = False
        elif value_option == &#39;all-on&#39;:
            for interaction in self.obj:
                interaction.value = 0
                interaction.indicator = True
        elif value_option == &#39;manual&#39;:
            if not np.all(pl.itercheck([value, indicators], pl.isarray)):
                raise TypeError(&#39;`value` ({}) and `indicators` ({}) must be arrays&#39;.format(
                    type(value), type(indicators)))
            if len(value) != np.sum(indicators):
                raise ValueError(&#39;Length of `value` ({}) must equal the number of positive &#39; \
                    &#39;values in `indicators` ({})&#39;.format(len(value), np.sum(indicators)))
            if len(indicators) != self.obj.size:
                raise ValueError(&#39;The length of `indicators` ({}) must be the same as the &#39; \
                    &#39;number of possible interactions ({})&#39;.format(len(indicators), self.obj.size))
            ii = 0
            for i,interaction in enumerate(self.obj):
                interaction.indicator = indicators[i]
                if interaction.indicator:
                    interaction.value = value[ii]
                    ii += 1
        else:
            raise ValueError(&#39;`value_option` ({}) not recognized&#39;.format(value_option))

        self._strr = str(self.obj.get_values(use_indicators=True))
        self.value = self.obj.get_values(use_indicators=True)

    def update(self):
        &#39;&#39;&#39;Update the values (where the indicators are positive) using a multivariate normal
        distribution - call this from regress coeff if you want to update the interactions
        conditional on all the other parameters.
        &#39;&#39;&#39;
        if self.obj.sample_iter &lt; self.delay:
            return
        if self.obj.num_pos_indicators() == 0:
            # logging.info(&#39;No positive indicators, skipping&#39;)
            self._strr = &#39;[]&#39;
            return

        rhs = [
            STRNAMES.CLUSTER_INTERACTION_VALUE]
        lhs = [
            STRNAMES.GROWTH_VALUE,
            STRNAMES.SELF_INTERACTION_VALUE]
        X = self.G.data.construct_rhs(keys=rhs)
        y = self.G.data.construct_lhs(keys=lhs,
            kwargs_dict={STRNAMES.GROWTH_VALUE:{
                &#39;with_perturbations&#39;:self._there_are_perturbations}})
        process_prec = self.G[STRNAMES.PROCESSVAR].build_matrix(
            cov=False, sparse=True)
        prior_prec = build_prior_covariance(G=self.G, cov=False,
            order=rhs, sparse=True)

        pm = prior_prec @ (self.prior.mean.value * np.ones(prior_prec.shape[0]).reshape(-1,1))

        prec = X.T @ process_prec @ X + prior_prec
        cov = pinv(prec, self)
        mean = (cov @ (X.T @ process_prec.dot(y) + pm)).ravel()

        # print(np.hstack((y, self.G.data.lhs.vector.reshape(-1,1))))
        # for perturbation in self.G.perturbations:
        #     print()
        #     print(perturbation.magnitude.cluster_array())
        #     print(perturbation.indicator.cluster_array())

        self.mean.value = mean
        self.cov.value = cov
        value = self.sample()
        self.obj.set_values(arr=value, use_indicators=True)
        self.update_str()

        if np.any(np.isnan(self.value)):
            logging.critical(&#39;mean: {}&#39;.format(self.mean.value))
            logging.critical(&#39;nan in cov: {}&#39;.format(np.any(np.isnan(self.cov.value))))
            logging.critical(&#39;value: {}&#39;.format(self.value))
            raise ValueError(&#39;`Values in {} are nan: {}&#39;.format(self.name, self.value))

    def update_str(self):
        self._strr = str(self.obj.get_values(use_indicators=True))

    def set_trace(self):
        self.obj.set_trace()

    def add_trace(self):
        self.obj.add_trace()

    def visualize(self, basepath: str, section: str=&#39;posterior&#39;, taxa_formatter: str=&#39;%(paperformat)s&#39;, 
        yticklabels: str=&#39;%(paperformat)s %(index)s&#39;, xticklabels: str=&#39;%(index)s&#39;,
        fixed_clustering: bool=False):
        &#39;&#39;&#39;Render the interaction matrices in the folder `basepath`

        Parameters
        ----------
        basepath : str
            This is the loction to write the files to
        section : str
            Section of the trace to compute on. Options:
                &#39;posterior&#39; : posterior samples
                &#39;burnin&#39; : burn-in samples
                &#39;entire&#39; : both burn-in and posterior samples
        taxa_formatter : str, None
            This is the format of the label to return for each Taxa. If None, it will return
            the taxon&#39;s name
        yticklabels, xticklabels : str
            These are the formats to plot the y-axis and x0axis, respectively.
        fixed_clustering : bool
            If True, plot the variable as if clustering was fixed. Since the
            cluster assignments never change, all of the taxa within the cluster
            have identical values, so we can choose any taxon within the cluster to
            represent it
        &#39;&#39;&#39;
        if not self.G.inference.tracer.is_being_traced(self.obj.name):
            logging.info(&#39;Interactions are not being learned&#39;)
        
        # Get cluster ordering
        taxa = self.G.data.taxa
        summ = pl.summary(self.obj, set_nan_to_0=True, section=section)
        clustering = self.G[STRNAMES.CLUSTERING_OBJ]

        if fixed_clustering:
            # Condense each matrix to cluster level. Make the labels clusters
            order = None
            labels = [&#39;Cluster {}&#39;.format(iii+1) for iii in range(len(clustering))]
            yticklabels = [&#39;{cname} {idx}&#39;.format(cname=labels[idx], idx=idx) for idx in range(len(clustering))]
            xticklabels = [&#39;{}&#39;.format(i) for i in range(len(clustering))]
            taxa = None

            for k in summ:
                M_taxa = summ[k]
                M_clus = []
                for cluster1 in clustering:
                    temp = []
                    aidx = list(cluster1.members)[0]
                    for cluster2 in clustering:
                        if cluster1.id == cluster2.id:
                            temp.append(np.nan)
                            continue
                        bidx = list(cluster2.members)[0]
                        temp.append(M_taxa[aidx, bidx])
                    M_clus.append(temp)
                summ[k] = np.asarray(M_clus)
        else:
            order = _make_cluster_order(graph=self.G, section=section)
            labels = [taxa[aidx].name for aidx in order]
            taxa = self.G.data.taxa

        for k,M in summ.items():

            visualization.render_interaction_strength(interaction_matrix=M,
                log_scale=True, taxa=taxa, yticklabels=yticklabels,
                xticklabels=xticklabels, order=order, 
                title=&#39;{} Interactions&#39;.format(k.capitalize()))
            fig = plt.gcf()
            fig.tight_layout()
            plt.savefig(os.path.join(basepath, &#39;{}_matrix.pdf&#39;.format(k)))
            plt.close()

            if not fixed_clustering:
                M = M[order, :]
                M = M[:, order]
            
            df = pd.DataFrame(M, index=labels, columns=labels)
            df.to_csv(os.path.join(basepath, &#39;{}_matrix.tsv&#39;.format(k)),
                sep=&#39;\t&#39;, index=True, header=True)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="mdsine2.pylab.variables.MVN" href="pylab/variables.html#mdsine2.pylab.variables.MVN">MVN</a></li>
<li><a title="mdsine2.pylab.variables.Variable" href="pylab/variables.html#mdsine2.pylab.variables.Variable">Variable</a></li>
<li><a title="mdsine2.pylab.graph.Node" href="pylab/graph.html#mdsine2.pylab.graph.Node">Node</a></li>
<li><a title="mdsine2.pylab.graph.BaseNode" href="pylab/graph.html#mdsine2.pylab.graph.BaseNode">BaseNode</a></li>
<li><a title="mdsine2.pylab.base.Saveable" href="pylab/base.html#mdsine2.pylab.base.Saveable">Saveable</a></li>
<li>mdsine2.pylab.variables._BaseArithmeticClass</li>
<li><a title="mdsine2.pylab.base.Traceable" href="pylab/base.html#mdsine2.pylab.base.Traceable">Traceable</a></li>
<li>mdsine2.pylab.variables._RandomBase</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="mdsine2.posterior.ClusterInteractionValue.initialize"><code class="name flex">
<span>def <span class="ident">initialize</span></span>(<span>self, value_option: str, hyperparam_option: str = None, value: numpy.ndarray = None, indicators: numpy.ndarray = None, delay: int = 0)</span>
</code></dt>
<dd>
<div class="desc"><p>Initialize the interactions object.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>value_option</code></strong> :&ensp;<code>str</code></dt>
<dd>
<ul>
<li>This is how to initialize the values</li>
<li>Options:<ul>
<li>'manual'<ul>
<li>Set the values of the interactions manually. <code>value</code> and <code>indicators</code>
must also be specified. We assume the values are only set for when
<code>indicators</code> is True, and that the order of the <code>indicators</code> and <code>values</code>
correspond to how we iterate over the interactions</li>
<li>Example<ul>
<li>3 Clusters</li>
<li>indicators = [True, False, False, True, False, True]</li>
<li>value = [0.2, 0.8, -0.35]</li>
</ul>
</li>
</ul>
</li>
<li>'all-off', 'auto'<ul>
<li>Set all of the interactions and the indicators to 0</li>
</ul>
</li>
<li>'all-on'<ul>
<li>Set all of the indicators to on and all the values to 0</li>
</ul>
</li>
</ul>
</li>
</ul>
</dd>
<dt><strong><code>delay</code></strong> :&ensp;<code>int</code></dt>
<dd>How many MCMC iterations to delay starting to update</dd>
</dl>
<h2 id="see-also">See Also</h2>
<p><code>mdsine2.pylab.cluster.Interactions.set_values</code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def initialize(self, value_option: str, hyperparam_option: str=None, 
    value: np.ndarray=None, indicators: np.ndarray=None, delay: int=0):
    &#39;&#39;&#39;Initialize the interactions object.

    Parameters
    ----------
    value_option : str
        - This is how to initialize the values
        - Options:
            - &#39;manual&#39;
                - Set the values of the interactions manually. `value` and `indicators`
                  must also be specified. We assume the values are only set for when
                  `indicators` is True, and that the order of the `indicators` and `values`
                  correspond to how we iterate over the interactions
                - Example
                    * 3 Clusters
                    * indicators = [True, False, False, True, False, True]
                    * value = [0.2, 0.8, -0.35]
            - &#39;all-off&#39;, &#39;auto&#39;
                - Set all of the interactions and the indicators to 0
            - &#39;all-on&#39;
                - Set all of the indicators to on and all the values to 0
    delay : int
        How many MCMC iterations to delay starting to update
    See also
    --------
    `mdsine2.pylab.cluster.Interactions.set_values`
    &#39;&#39;&#39;
    self.obj.set_signal_when_clusters_change(True)
    self.G[STRNAMES.INTERACTIONS_OBJ].value_initializer = self.prior.sample
    self.G[STRNAMES.INTERACTIONS_OBJ].indicator_initializer = self.G[STRNAMES.CLUSTER_INTERACTION_INDICATOR_PROB].prior.sample

    self._there_are_perturbations = self.G.perturbations is not None

    if not pl.isint(delay):
        raise TypeError(&#39;`delay` ({}) must be an int&#39;.format(type(delay)))
    if delay &lt; 0:
        raise ValueError(&#39;`delay` ({}) must be &gt;= 0&#39;.format(delay))
    self.delay = delay

    if not pl.isstr(value_option):
        raise TypeError(&#39;`value_option` ({}) must be a str&#39;.format(type(value_option)))
    if value_option in [&#39;auto&#39;, &#39;all-off&#39;]:
        for interaction in self.obj:
            interaction.value = 0
            interaction.indicator = False
    elif value_option == &#39;all-on&#39;:
        for interaction in self.obj:
            interaction.value = 0
            interaction.indicator = True
    elif value_option == &#39;manual&#39;:
        if not np.all(pl.itercheck([value, indicators], pl.isarray)):
            raise TypeError(&#39;`value` ({}) and `indicators` ({}) must be arrays&#39;.format(
                type(value), type(indicators)))
        if len(value) != np.sum(indicators):
            raise ValueError(&#39;Length of `value` ({}) must equal the number of positive &#39; \
                &#39;values in `indicators` ({})&#39;.format(len(value), np.sum(indicators)))
        if len(indicators) != self.obj.size:
            raise ValueError(&#39;The length of `indicators` ({}) must be the same as the &#39; \
                &#39;number of possible interactions ({})&#39;.format(len(indicators), self.obj.size))
        ii = 0
        for i,interaction in enumerate(self.obj):
            interaction.indicator = indicators[i]
            if interaction.indicator:
                interaction.value = value[ii]
                ii += 1
    else:
        raise ValueError(&#39;`value_option` ({}) not recognized&#39;.format(value_option))

    self._strr = str(self.obj.get_values(use_indicators=True))
    self.value = self.obj.get_values(use_indicators=True)</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.ClusterInteractionValue.set_values"><code class="name flex">
<span>def <span class="ident">set_values</span></span>(<span>self, *args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Set the values from an array</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_values(self, *args, **kwargs):
    &#39;&#39;&#39;Set the values from an array
    &#39;&#39;&#39;
    self.obj.set_values(*args, **kwargs)</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.ClusterInteractionValue.update"><code class="name flex">
<span>def <span class="ident">update</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Update the values (where the indicators are positive) using a multivariate normal
distribution - call this from regress coeff if you want to update the interactions
conditional on all the other parameters.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update(self):
    &#39;&#39;&#39;Update the values (where the indicators are positive) using a multivariate normal
    distribution - call this from regress coeff if you want to update the interactions
    conditional on all the other parameters.
    &#39;&#39;&#39;
    if self.obj.sample_iter &lt; self.delay:
        return
    if self.obj.num_pos_indicators() == 0:
        # logging.info(&#39;No positive indicators, skipping&#39;)
        self._strr = &#39;[]&#39;
        return

    rhs = [
        STRNAMES.CLUSTER_INTERACTION_VALUE]
    lhs = [
        STRNAMES.GROWTH_VALUE,
        STRNAMES.SELF_INTERACTION_VALUE]
    X = self.G.data.construct_rhs(keys=rhs)
    y = self.G.data.construct_lhs(keys=lhs,
        kwargs_dict={STRNAMES.GROWTH_VALUE:{
            &#39;with_perturbations&#39;:self._there_are_perturbations}})
    process_prec = self.G[STRNAMES.PROCESSVAR].build_matrix(
        cov=False, sparse=True)
    prior_prec = build_prior_covariance(G=self.G, cov=False,
        order=rhs, sparse=True)

    pm = prior_prec @ (self.prior.mean.value * np.ones(prior_prec.shape[0]).reshape(-1,1))

    prec = X.T @ process_prec @ X + prior_prec
    cov = pinv(prec, self)
    mean = (cov @ (X.T @ process_prec.dot(y) + pm)).ravel()

    # print(np.hstack((y, self.G.data.lhs.vector.reshape(-1,1))))
    # for perturbation in self.G.perturbations:
    #     print()
    #     print(perturbation.magnitude.cluster_array())
    #     print(perturbation.indicator.cluster_array())

    self.mean.value = mean
    self.cov.value = cov
    value = self.sample()
    self.obj.set_values(arr=value, use_indicators=True)
    self.update_str()

    if np.any(np.isnan(self.value)):
        logging.critical(&#39;mean: {}&#39;.format(self.mean.value))
        logging.critical(&#39;nan in cov: {}&#39;.format(np.any(np.isnan(self.cov.value))))
        logging.critical(&#39;value: {}&#39;.format(self.value))
        raise ValueError(&#39;`Values in {} are nan: {}&#39;.format(self.name, self.value))</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.ClusterInteractionValue.update_str"><code class="name flex">
<span>def <span class="ident">update_str</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_str(self):
    self._strr = str(self.obj.get_values(use_indicators=True))</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.ClusterInteractionValue.visualize"><code class="name flex">
<span>def <span class="ident">visualize</span></span>(<span>self, basepath: str, section: str = 'posterior', taxa_formatter: str = '%(paperformat)s', yticklabels: str = '%(paperformat)s %(index)s', xticklabels: str = '%(index)s', fixed_clustering: bool = False)</span>
</code></dt>
<dd>
<div class="desc"><p>Render the interaction matrices in the folder <code>basepath</code></p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>basepath</code></strong> :&ensp;<code>str</code></dt>
<dd>This is the loction to write the files to</dd>
<dt><strong><code>section</code></strong> :&ensp;<code>str</code></dt>
<dd>Section of the trace to compute on. Options:
'posterior' : posterior samples
'burnin' : burn-in samples
'entire' : both burn-in and posterior samples</dd>
<dt><strong><code>taxa_formatter</code></strong> :&ensp;<code>str, None</code></dt>
<dd>This is the format of the label to return for each Taxa. If None, it will return
the taxon's name</dd>
<dt><strong><code>yticklabels</code></strong>, <strong><code>xticklabels</code></strong> :&ensp;<code>str</code></dt>
<dd>These are the formats to plot the y-axis and x0axis, respectively.</dd>
<dt><strong><code>fixed_clustering</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, plot the variable as if clustering was fixed. Since the
cluster assignments never change, all of the taxa within the cluster
have identical values, so we can choose any taxon within the cluster to
represent it</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def visualize(self, basepath: str, section: str=&#39;posterior&#39;, taxa_formatter: str=&#39;%(paperformat)s&#39;, 
    yticklabels: str=&#39;%(paperformat)s %(index)s&#39;, xticklabels: str=&#39;%(index)s&#39;,
    fixed_clustering: bool=False):
    &#39;&#39;&#39;Render the interaction matrices in the folder `basepath`

    Parameters
    ----------
    basepath : str
        This is the loction to write the files to
    section : str
        Section of the trace to compute on. Options:
            &#39;posterior&#39; : posterior samples
            &#39;burnin&#39; : burn-in samples
            &#39;entire&#39; : both burn-in and posterior samples
    taxa_formatter : str, None
        This is the format of the label to return for each Taxa. If None, it will return
        the taxon&#39;s name
    yticklabels, xticklabels : str
        These are the formats to plot the y-axis and x0axis, respectively.
    fixed_clustering : bool
        If True, plot the variable as if clustering was fixed. Since the
        cluster assignments never change, all of the taxa within the cluster
        have identical values, so we can choose any taxon within the cluster to
        represent it
    &#39;&#39;&#39;
    if not self.G.inference.tracer.is_being_traced(self.obj.name):
        logging.info(&#39;Interactions are not being learned&#39;)
    
    # Get cluster ordering
    taxa = self.G.data.taxa
    summ = pl.summary(self.obj, set_nan_to_0=True, section=section)
    clustering = self.G[STRNAMES.CLUSTERING_OBJ]

    if fixed_clustering:
        # Condense each matrix to cluster level. Make the labels clusters
        order = None
        labels = [&#39;Cluster {}&#39;.format(iii+1) for iii in range(len(clustering))]
        yticklabels = [&#39;{cname} {idx}&#39;.format(cname=labels[idx], idx=idx) for idx in range(len(clustering))]
        xticklabels = [&#39;{}&#39;.format(i) for i in range(len(clustering))]
        taxa = None

        for k in summ:
            M_taxa = summ[k]
            M_clus = []
            for cluster1 in clustering:
                temp = []
                aidx = list(cluster1.members)[0]
                for cluster2 in clustering:
                    if cluster1.id == cluster2.id:
                        temp.append(np.nan)
                        continue
                    bidx = list(cluster2.members)[0]
                    temp.append(M_taxa[aidx, bidx])
                M_clus.append(temp)
            summ[k] = np.asarray(M_clus)
    else:
        order = _make_cluster_order(graph=self.G, section=section)
        labels = [taxa[aidx].name for aidx in order]
        taxa = self.G.data.taxa

    for k,M in summ.items():

        visualization.render_interaction_strength(interaction_matrix=M,
            log_scale=True, taxa=taxa, yticklabels=yticklabels,
            xticklabels=xticklabels, order=order, 
            title=&#39;{} Interactions&#39;.format(k.capitalize()))
        fig = plt.gcf()
        fig.tight_layout()
        plt.savefig(os.path.join(basepath, &#39;{}_matrix.pdf&#39;.format(k)))
        plt.close()

        if not fixed_clustering:
            M = M[order, :]
            M = M[:, order]
        
        df = pd.DataFrame(M, index=labels, columns=labels)
        df.to_csv(os.path.join(basepath, &#39;{}_matrix.tsv&#39;.format(k)),
            sep=&#39;\t&#39;, index=True, header=True)</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="mdsine2.pylab.variables.MVN" href="pylab/variables.html#mdsine2.pylab.variables.MVN">MVN</a></b></code>:
<ul class="hlist">
<li><code><a title="mdsine2.pylab.variables.MVN.T" href="pylab/variables.html#mdsine2.pylab.variables.Variable.T">T</a></code></li>
<li><code><a title="mdsine2.pylab.variables.MVN.add_child" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_child">add_child</a></code></li>
<li><code><a title="mdsine2.pylab.variables.MVN.add_parent" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_parent">add_parent</a></code></li>
<li><code><a title="mdsine2.pylab.variables.MVN.add_prior" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_prior">add_prior</a></code></li>
<li><code><a title="mdsine2.pylab.variables.MVN.add_trace" href="pylab/base.html#mdsine2.pylab.base.Traceable.add_trace">add_trace</a></code></li>
<li><code><a title="mdsine2.pylab.variables.MVN.add_undirected" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_undirected">add_undirected</a></code></li>
<li><code><a title="mdsine2.pylab.variables.MVN.degree" href="pylab/graph.html#mdsine2.pylab.graph.Node.degree">degree</a></code></li>
<li><code><a title="mdsine2.pylab.variables.MVN.delete" href="pylab/graph.html#mdsine2.pylab.graph.BaseNode.delete">delete</a></code></li>
<li><code><a title="mdsine2.pylab.variables.MVN.get_adjacent_keys" href="pylab/graph.html#mdsine2.pylab.graph.Node.get_adjacent_keys">get_adjacent_keys</a></code></li>
<li><code><a title="mdsine2.pylab.variables.MVN.get_iter" href="pylab/base.html#mdsine2.pylab.base.Traceable.get_iter">get_iter</a></code></li>
<li><code><a title="mdsine2.pylab.variables.MVN.get_trace_from_disk" href="pylab/base.html#mdsine2.pylab.base.Traceable.get_trace_from_disk">get_trace_from_disk</a></code></li>
<li><code><a title="mdsine2.pylab.variables.MVN.load" href="pylab/base.html#mdsine2.pylab.base.Saveable.load">load</a></code></li>
<li><code><a title="mdsine2.pylab.variables.MVN.metropolis" href="pylab/graph.html#mdsine2.pylab.graph.Node.metropolis">metropolis</a></code></li>
<li><code><a title="mdsine2.pylab.variables.MVN.overwrite_entire_trace_on_disk" href="pylab/base.html#mdsine2.pylab.base.Traceable.overwrite_entire_trace_on_disk">overwrite_entire_trace_on_disk</a></code></li>
<li><code><a title="mdsine2.pylab.variables.MVN.remove_local_trace" href="pylab/variables.html#mdsine2.pylab.variables.Variable.remove_local_trace">remove_local_trace</a></code></li>
<li><code><a title="mdsine2.pylab.variables.MVN.sample" href="pylab/variables.html#mdsine2.pylab.variables.MVN.sample">sample</a></code></li>
<li><code><a title="mdsine2.pylab.variables.MVN.save" href="pylab/base.html#mdsine2.pylab.base.Saveable.save">save</a></code></li>
<li><code><a title="mdsine2.pylab.variables.MVN.set_save_location" href="pylab/base.html#mdsine2.pylab.base.Saveable.set_save_location">set_save_location</a></code></li>
<li><code><a title="mdsine2.pylab.variables.MVN.set_trace" href="pylab/base.html#mdsine2.pylab.base.Traceable.set_trace">set_trace</a></code></li>
<li><code><a title="mdsine2.pylab.variables.MVN.set_value_shape" href="pylab/variables.html#mdsine2.pylab.variables.Variable.set_value_shape">set_value_shape</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="mdsine2.posterior.Concentration"><code class="flex name class">
<span>class <span class="ident">Concentration</span></span>
<span>(</span><span>prior: <a title="mdsine2.pylab.variables.Gamma" href="pylab/variables.html#mdsine2.pylab.variables.Gamma">Gamma</a>, value: Union[float, int] = None, n_iter: int = None, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Defines the posterior for the concentration parameter that is used
in learning the cluster assignments.
The posterior is implemented as it is describes in 'Bayesian Inference
for Density Estimation' by M. D. Escobar and M. West, 1995.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>prior</code></strong> :&ensp;<code>mdsine2.variables.Gamma</code></dt>
<dd>Prior distribution</dd>
<dt><strong><code>value</code></strong> :&ensp;<code>float, int</code></dt>
<dd>
<ul>
<li>Initial value of the concentration</li>
<li>Default value is the mean of the prior</li>
</ul>
</dd>
<dt><strong><code>n_iter</code></strong> :&ensp;<code>int</code></dt>
<dd>How many times to resample</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Concentration(pl.variables.Gamma):
    &#39;&#39;&#39;Defines the posterior for the concentration parameter that is used
    in learning the cluster assignments.
    The posterior is implemented as it is describes in &#39;Bayesian Inference
    for Density Estimation&#39; by M. D. Escobar and M. West, 1995.

    Parameters
    ----------
    prior : mdsine2.variables.Gamma
        Prior distribution
    value : float, int
        - Initial value of the concentration
        - Default value is the mean of the prior
    n_iter : int
        How many times to resample
    &#39;&#39;&#39;
    def __init__(self, prior: variables.Gamma, value: Union[float, int]=None, 
        n_iter: int=None, **kwargs):
        kwargs[&#39;name&#39;] = STRNAMES.CONCENTRATION
        # initialize shape and scale as the same as the priors
        # we will be updating this later
        pl.variables.Gamma.__init__(self, shape=prior.shape.value, scale=prior.scale.value,
            dtype=float, **kwargs)
        self.add_prior(prior)

    def initialize(self, value_option: str, hyperparam_option: str, n_iter: int=None, 
        value: Union[int, float]=None, shape: Union[int, float]=None, scale: Union[int, float]=None, 
        delay: int=0):
        &#39;&#39;&#39;Initialize the hyperparameters of the beta prior

        Parameters
        ----------
        value_option : str
            - Options to initialize the value
            - &#39;manual&#39;
                - Set the value manually, `value` must also be specified
            - &#39;auto&#39;, &#39;prior-mean&#39;
                - Set to the mean of the prior
        hyperparam_option : str
            - Options ot initialize the hyperparameters
            - Options
                - &#39;manual&#39;
                    - Set the values manually. `shape` and `scale` must also be specified
                - &#39;auto&#39;, &#39;diffuse&#39;
                    - shape = 1e-5, scale= 1e5
        shape, scale : int, float
            - User specified values
            - Only necessary if `hyperparam_option` == &#39;manual&#39;
        &#39;&#39;&#39;
        if not pl.isint(delay):
            raise TypeError(&#39;`delay` ({}) must be an int&#39;.format(type(delay)))
        if delay &lt; 0:
            raise ValueError(&#39;`delay` ({}) must be &gt;= 0&#39;.format(delay))
        self.delay = delay

        if hyperparam_option == &#39;manual&#39;:
            self.prior.shape.override_value(shape)
            self.prior.scale.override_value(scale)
            self.n_iter = n_iter

        elif hyperparam_option in [&#39;diffuse&#39;, &#39;auto&#39;]:
            self.prior.shape.override_value(1e-5)
            self.prior.scale.override_value(1e5)
            self.n_iter = 20
        else:
            raise ValueError(&#39;hyperparam_option `{}` not recognized&#39;.format(hyperparam_option))

        if value_option == &#39;manual&#39;:
            self.value = value
        elif value_option in [&#39;auto&#39;, &#39;prior-mean&#39;]:
            self.value = self.prior.mean()
        else:
            raise ValueError(&#39;value_option `{}` not recognized&#39;.format(value_option))

        self.shape.value = self.prior.shape.value
        self.scale.value = self.prior.scale.value
        logging.info(&#39;Cluster Concentration initialization results:\n&#39; \
            &#39;\tprior shape: {}\n\tprior scale: {}\n\tvalue: {}&#39;.format(
                self.prior.shape.value, self.prior.scale.value, self.value))

    def update(self):
        &#39;&#39;&#39;Sample the posterior of the concentration parameter
        &#39;&#39;&#39;
        if self.sample_iter &lt; self.delay:
            return

        clustering = self.G[STRNAMES.CLUSTER_INTERACTION_VALUE].clustering
        k = len(clustering)
        n = self.G.data.n_taxa
        for _ in range(self.n_iter):
            #first sample eta from a beta distribution
            eta = npr.beta(self.value+1,n)
            #sample alpha from a mixture of gammas
            pi_eta = [0.0, n]
            pi_eta[0] = (self.prior.shape.value+k-1.)/(1/(self.prior.scale.value)-np.log(eta))
            self.scale.value =  1/(1/self.prior.scale.value - np.log(eta))
            self.shape.value = self.prior.shape.value + k
            if np.random.choice([0,1], p=pi_eta/np.sum(pi_eta)) != 0:
                self.shape.value -= 1
            self.sample()

    def visualize(self, path: str, f: IO, section: str=&#39;posterior&#39;) -&gt; IO:
        &#39;&#39;&#39;Render the traces in the folder `basepath` and write the 
        learned values to the file `f`.

        Parameters
        ----------
        path : str
            This is the path to write the files to
        f : _io.TextIOWrapper
            File that we are writing the values to
        section : str
            Section of the trace to compute on. Options:
                &#39;posterior&#39; : posterior samples
                &#39;burnin&#39; : burn-in samples
                &#39;entire&#39; : both burn-in and posterior samples

        Returns
        -------
        _io.TextIOWrapper
        &#39;&#39;&#39;
        return _scalar_visualize(self, path=path, f=f, section=section)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="mdsine2.pylab.variables.Gamma" href="pylab/variables.html#mdsine2.pylab.variables.Gamma">Gamma</a></li>
<li><a title="mdsine2.pylab.variables.Variable" href="pylab/variables.html#mdsine2.pylab.variables.Variable">Variable</a></li>
<li><a title="mdsine2.pylab.graph.Node" href="pylab/graph.html#mdsine2.pylab.graph.Node">Node</a></li>
<li><a title="mdsine2.pylab.graph.BaseNode" href="pylab/graph.html#mdsine2.pylab.graph.BaseNode">BaseNode</a></li>
<li><a title="mdsine2.pylab.base.Saveable" href="pylab/base.html#mdsine2.pylab.base.Saveable">Saveable</a></li>
<li>mdsine2.pylab.variables._BaseArithmeticClass</li>
<li><a title="mdsine2.pylab.base.Traceable" href="pylab/base.html#mdsine2.pylab.base.Traceable">Traceable</a></li>
<li>mdsine2.pylab.variables._RandomBase</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="mdsine2.posterior.Concentration.initialize"><code class="name flex">
<span>def <span class="ident">initialize</span></span>(<span>self, value_option: str, hyperparam_option: str, n_iter: int = None, value: Union[int, float] = None, shape: Union[int, float] = None, scale: Union[int, float] = None, delay: int = 0)</span>
</code></dt>
<dd>
<div class="desc"><p>Initialize the hyperparameters of the beta prior</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>value_option</code></strong> :&ensp;<code>str</code></dt>
<dd>
<ul>
<li>Options to initialize the value</li>
<li>'manual'<ul>
<li>Set the value manually, <code>value</code> must also be specified</li>
</ul>
</li>
<li>'auto', 'prior-mean'<ul>
<li>Set to the mean of the prior</li>
</ul>
</li>
</ul>
</dd>
<dt><strong><code>hyperparam_option</code></strong> :&ensp;<code>str</code></dt>
<dd>
<ul>
<li>Options ot initialize the hyperparameters</li>
<li>Options<ul>
<li>'manual'<ul>
<li>Set the values manually. <code>shape</code> and <code>scale</code> must also be specified</li>
</ul>
</li>
<li>'auto', 'diffuse'<ul>
<li>shape = 1e-5, scale= 1e5</li>
</ul>
</li>
</ul>
</li>
</ul>
</dd>
<dt><strong><code>shape</code></strong>, <strong><code>scale</code></strong> :&ensp;<code>int, float</code></dt>
<dd>
<ul>
<li>User specified values</li>
<li>Only necessary if <code>hyperparam_option</code> == 'manual'</li>
</ul>
</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def initialize(self, value_option: str, hyperparam_option: str, n_iter: int=None, 
    value: Union[int, float]=None, shape: Union[int, float]=None, scale: Union[int, float]=None, 
    delay: int=0):
    &#39;&#39;&#39;Initialize the hyperparameters of the beta prior

    Parameters
    ----------
    value_option : str
        - Options to initialize the value
        - &#39;manual&#39;
            - Set the value manually, `value` must also be specified
        - &#39;auto&#39;, &#39;prior-mean&#39;
            - Set to the mean of the prior
    hyperparam_option : str
        - Options ot initialize the hyperparameters
        - Options
            - &#39;manual&#39;
                - Set the values manually. `shape` and `scale` must also be specified
            - &#39;auto&#39;, &#39;diffuse&#39;
                - shape = 1e-5, scale= 1e5
    shape, scale : int, float
        - User specified values
        - Only necessary if `hyperparam_option` == &#39;manual&#39;
    &#39;&#39;&#39;
    if not pl.isint(delay):
        raise TypeError(&#39;`delay` ({}) must be an int&#39;.format(type(delay)))
    if delay &lt; 0:
        raise ValueError(&#39;`delay` ({}) must be &gt;= 0&#39;.format(delay))
    self.delay = delay

    if hyperparam_option == &#39;manual&#39;:
        self.prior.shape.override_value(shape)
        self.prior.scale.override_value(scale)
        self.n_iter = n_iter

    elif hyperparam_option in [&#39;diffuse&#39;, &#39;auto&#39;]:
        self.prior.shape.override_value(1e-5)
        self.prior.scale.override_value(1e5)
        self.n_iter = 20
    else:
        raise ValueError(&#39;hyperparam_option `{}` not recognized&#39;.format(hyperparam_option))

    if value_option == &#39;manual&#39;:
        self.value = value
    elif value_option in [&#39;auto&#39;, &#39;prior-mean&#39;]:
        self.value = self.prior.mean()
    else:
        raise ValueError(&#39;value_option `{}` not recognized&#39;.format(value_option))

    self.shape.value = self.prior.shape.value
    self.scale.value = self.prior.scale.value
    logging.info(&#39;Cluster Concentration initialization results:\n&#39; \
        &#39;\tprior shape: {}\n\tprior scale: {}\n\tvalue: {}&#39;.format(
            self.prior.shape.value, self.prior.scale.value, self.value))</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.Concentration.update"><code class="name flex">
<span>def <span class="ident">update</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Sample the posterior of the concentration parameter</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update(self):
    &#39;&#39;&#39;Sample the posterior of the concentration parameter
    &#39;&#39;&#39;
    if self.sample_iter &lt; self.delay:
        return

    clustering = self.G[STRNAMES.CLUSTER_INTERACTION_VALUE].clustering
    k = len(clustering)
    n = self.G.data.n_taxa
    for _ in range(self.n_iter):
        #first sample eta from a beta distribution
        eta = npr.beta(self.value+1,n)
        #sample alpha from a mixture of gammas
        pi_eta = [0.0, n]
        pi_eta[0] = (self.prior.shape.value+k-1.)/(1/(self.prior.scale.value)-np.log(eta))
        self.scale.value =  1/(1/self.prior.scale.value - np.log(eta))
        self.shape.value = self.prior.shape.value + k
        if np.random.choice([0,1], p=pi_eta/np.sum(pi_eta)) != 0:
            self.shape.value -= 1
        self.sample()</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.Concentration.visualize"><code class="name flex">
<span>def <span class="ident">visualize</span></span>(<span>self, path: str, f: <class 'IO'>, section: str = 'posterior') ‑> <class 'IO'></span>
</code></dt>
<dd>
<div class="desc"><p>Render the traces in the folder <code>basepath</code> and write the
learned values to the file <code>f</code>.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>path</code></strong> :&ensp;<code>str</code></dt>
<dd>This is the path to write the files to</dd>
<dt><strong><code>f</code></strong> :&ensp;<code>_io.TextIOWrapper</code></dt>
<dd>File that we are writing the values to</dd>
<dt><strong><code>section</code></strong> :&ensp;<code>str</code></dt>
<dd>Section of the trace to compute on. Options:
'posterior' : posterior samples
'burnin' : burn-in samples
'entire' : both burn-in and posterior samples</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>_io.TextIOWrapper</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def visualize(self, path: str, f: IO, section: str=&#39;posterior&#39;) -&gt; IO:
    &#39;&#39;&#39;Render the traces in the folder `basepath` and write the 
    learned values to the file `f`.

    Parameters
    ----------
    path : str
        This is the path to write the files to
    f : _io.TextIOWrapper
        File that we are writing the values to
    section : str
        Section of the trace to compute on. Options:
            &#39;posterior&#39; : posterior samples
            &#39;burnin&#39; : burn-in samples
            &#39;entire&#39; : both burn-in and posterior samples

    Returns
    -------
    _io.TextIOWrapper
    &#39;&#39;&#39;
    return _scalar_visualize(self, path=path, f=f, section=section)</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="mdsine2.pylab.variables.Gamma" href="pylab/variables.html#mdsine2.pylab.variables.Gamma">Gamma</a></b></code>:
<ul class="hlist">
<li><code><a title="mdsine2.pylab.variables.Gamma.T" href="pylab/variables.html#mdsine2.pylab.variables.Variable.T">T</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Gamma.add_child" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_child">add_child</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Gamma.add_parent" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_parent">add_parent</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Gamma.add_prior" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_prior">add_prior</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Gamma.add_trace" href="pylab/base.html#mdsine2.pylab.base.Traceable.add_trace">add_trace</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Gamma.add_undirected" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_undirected">add_undirected</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Gamma.degree" href="pylab/graph.html#mdsine2.pylab.graph.Node.degree">degree</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Gamma.delete" href="pylab/graph.html#mdsine2.pylab.graph.BaseNode.delete">delete</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Gamma.get_adjacent_keys" href="pylab/graph.html#mdsine2.pylab.graph.Node.get_adjacent_keys">get_adjacent_keys</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Gamma.get_iter" href="pylab/base.html#mdsine2.pylab.base.Traceable.get_iter">get_iter</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Gamma.get_trace_from_disk" href="pylab/base.html#mdsine2.pylab.base.Traceable.get_trace_from_disk">get_trace_from_disk</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Gamma.load" href="pylab/base.html#mdsine2.pylab.base.Saveable.load">load</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Gamma.metropolis" href="pylab/graph.html#mdsine2.pylab.graph.Node.metropolis">metropolis</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Gamma.overwrite_entire_trace_on_disk" href="pylab/base.html#mdsine2.pylab.base.Traceable.overwrite_entire_trace_on_disk">overwrite_entire_trace_on_disk</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Gamma.pdf" href="pylab/variables.html#mdsine2.pylab.variables.Gamma.pdf">pdf</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Gamma.remove_local_trace" href="pylab/variables.html#mdsine2.pylab.variables.Variable.remove_local_trace">remove_local_trace</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Gamma.sample" href="pylab/variables.html#mdsine2.pylab.variables.Gamma.sample">sample</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Gamma.save" href="pylab/base.html#mdsine2.pylab.base.Saveable.save">save</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Gamma.set_save_location" href="pylab/base.html#mdsine2.pylab.base.Saveable.set_save_location">set_save_location</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Gamma.set_trace" href="pylab/base.html#mdsine2.pylab.base.Traceable.set_trace">set_trace</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Gamma.set_value_shape" href="pylab/variables.html#mdsine2.pylab.variables.Variable.set_value_shape">set_value_shape</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="mdsine2.posterior.CondensedClustering"><code class="flex name class">
<span>class <span class="ident">CondensedClustering</span></span>
<span>(</span><span>oidx2cidx)</span>
</code></dt>
<dd>
<div class="desc"><p>Condensed clustering object that is not associated with the graph</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>oidx2cidx</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>Maps the cluster assignment to each taxon.
index -&gt; Taxa index
output -&gt; cluster index</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CondensedClustering:
    &#39;&#39;&#39;Condensed clustering object that is not associated with the graph

    Parameters
    ----------
    oidx2cidx : np.ndarray
        Maps the cluster assignment to each taxon.
        index -&gt; Taxa index
        output -&gt; cluster index

    &#39;&#39;&#39;
    def __init__(self, oidx2cidx):
        self.clusters = []
        self.oidx2cidx = oidx2cidx
        a = {}
        for oidx, cidx in enumerate(self.oidx2cidx):
            if cidx not in a:
                a[cidx] = [oidx]
            else:
                a[cidx].append(oidx)
        cidx = 0
        while cidx in a:
            self.clusters.append(np.asarray(a[cidx], dtype=int))
            cidx += 1

    def __len__(self):
        return len(self.clusters)</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.FilteringLogMP"><code class="flex name class">
<span>class <span class="ident">FilteringLogMP</span></span>
<span>(</span><span>mp: str, zero_inflation_transition_policy: Union[str, NoneType], **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>This is the posterior for the latent trajectory that are
sampled using a standard normal Metropolis-Hastings proposal.</p>
<p>This is the multiprocessing version of the class. All of the computation is
done on the subject level in parallel.</p>
<h2 id="parallelization-modes">Parallelization Modes</h2>
<p>'debug'
If this is selected, then we dont actually parallelize, but we go in
order of the objects in sequential order. We would do this if we want
to benchmark within each processor or do easier print statements
'full'
This is where each subject gets their own process</p>
<p>This assumes that we are using the log model</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>mp</code></strong> :&ensp;<code>str</code></dt>
<dd>
<ul>
<li>'debug'<ul>
<li>Does not actually parallelize, does it serially - we do this in case we
want to debug and/or benchmark</li>
</ul>
</li>
<li>'full'
Send each replicate to a processor each</li>
</ul>
</dd>
<dt><strong><code>zero_inflation_transition_policy</code></strong> :&ensp;<code>None, str</code></dt>
<dd>
<ul>
<li>Type of zero inflation to do. If None then there is no zero inflation</li>
</ul>
</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class FilteringLogMP(pl.graph.Node):
    &#39;&#39;&#39;This is the posterior for the latent trajectory that are
    sampled using a standard normal Metropolis-Hastings proposal.

    This is the multiprocessing version of the class. All of the computation is
    done on the subject level in parallel.

    Parallelization Modes
    ---------------------
    &#39;debug&#39;
        If this is selected, then we dont actually parallelize, but we go in
        order of the objects in sequential order. We would do this if we want
        to benchmark within each processor or do easier print statements
    &#39;full&#39;
        This is where each subject gets their own process

    This assumes that we are using the log model
    &#39;&#39;&#39;
    def __init__(self, mp: str, zero_inflation_transition_policy: Union[str,  type(None)],**kwargs):
        &#39;&#39;&#39;
        Parameters
        ----------
        mp : str
            - &#39;debug&#39;
                * Does not actually parallelize, does it serially - we do this in case we
                  want to debug and/or benchmark
            - &#39;full&#39;
                Send each replicate to a processor each
        zero_inflation_transition_policy : None, str
            - Type of zero inflation to do. If None then there is no zero inflation
        &#39;&#39;&#39;
        kwargs[&#39;name&#39;] = STRNAMES.FILTERING
        pl.graph.Node.__init__(self, **kwargs)
        self.x = TrajectorySet(G=self.G)
        self.mp = mp
        self.zero_inflation_transition_policy = zero_inflation_transition_policy

        self.print_vals = False
        self._strr = &#39;parallel&#39;

    def __str__(self) -&gt; str:
        return self._strr

    @property
    def sample_iter(self) -&gt; int:
        # It doesnt matter if we chose q or x because they are both the same
        return self.x.sample_iter

    def initialize(self, x_value_option: str, a0: Union[float, str], a1: Union[float, str], 
        v1: Union[float, int], v2: Union[float, int], essential_timepoints: Union[np.ndarray, str], 
        tune: Tuple[int, int], proposal_init_scale: Union[float, int],
        intermediate_step: Union[Tuple[str, Tuple], np.ndarray, List, type(None)], 
        intermediate_interpolation: str=None, delay: int=0, bandwidth: Union[float, int]=None, 
        window: int=None, target_acceptance_rate: Union[str, float]=0.44, 
        calculate_qpcr_loglik: bool=True):
        &#39;&#39;&#39;Initialize the values of the error model (values for the
        latent and the auxiliary trajectory). Additionally this sets
        the intermediate time points

        Initialize the values of the prior.

        Parameters
        ----------
        x_value_option : str
            - Option to initialize the value of the latent trajectory.
            - Options
                - &#39;coupling&#39;
                    - Sample the values around the data with extremely low variance.
                    - This also truncates the data so that it stays &gt; 0.
                - &#39;moving-avg&#39;
                    - Initialize the values using a moving average around the points.
                    - The bandwidth of the filter is by number of days, not the order
                      of timepoints. You must also provide the argument `bandwidth`.
                - &#39;loess&#39;, &#39;auto&#39;
                    - Implements the initialization of the values using LOESS (Locally
                    - Estimated Scatterplot Smoothing) algorithm. You must also provide
                      the `window` parameter
        tune : tuple(int, int)
            - This is how often to tune the individual covariances
            - The first element indicates which MCMC sample to stop the tuning
            - The second element is how often to update the proposal covariance
        a0, a1 : float, str
            These are the hyperparameters to calculate the dispersion of the
            negative binomial.
        v1, v2 : float, int, str
            These are the values used to calulcate the coupling variance between
            x and q
        intermediate_step : tuple(str, args), array, None
            - This is the type of interemediate timestep to intialize and the arguments
              for them. If this is None, then we do no intermediate timesteps.
            - Options:
                * &#39;step&#39;
                    - args: (stride (numeric), eps (numeric))
                    - We simulate at each timepoint every `stride` days.
                    - We do not set an intermediate time point if it is within `eps`
                      days of a given data point.
                * &#39;preserve-density&#39;
                    - args: (n (int), eps (numeric))
                    - We preserve the denisty of the given data by only simulating data
                    - `n` times between each essential datapoint. If a timepoint is within
                    - `eps` days of a given timepoint then we do not make an intermediate
                      point there.
                * &#39;manual;
                    - args: np.ndarray
                    - These are the points that we want to set. If these are not given times
                      then we set them as timepoints
        intermediate_interpolation : str
            - This is the type of interpolation to perform on the intermediate timepoints.
            - Options:
                - &#39;linear-interpolation&#39;, &#39;auto&#39;
                    - Perform linear interpolation between the two closest given timepoints
        essential_timepoints : np.ndarray, str, None
            - These are the timepoints that must be included in each subject. If one of the
              subjects has a missing timepoint there then we use an intermediate time point
              that this timepoint. It is initialized with linear interpolation. If all of the
              timepoints specified in this vector are included in a subject then nothing is
              done. If it is a str:
                - &#39;union&#39;, &#39;auto&#39;
                    * We take a union of all the timepoints in each subject and make sure
                      that all of the subjects have all those points.
        bandwidth : float
            This is the day bandwidth of the filter if the initialization method is
            done with &#39;moving-avg&#39;.
        window : int
            This is the window term for the LOESS initialization scheme. This is
            only used if value_initialization is done with &#39;loess&#39;
        target_acceptance_rate : numeric
            This is the target acceptance rate for each time point individually
        calculate_qpcr_loglik : bool
            If True, calculate the loglikelihood of the qPCR measurements during the 
            proposal
        &#39;&#39;&#39;
        if not pl.isint(delay):
            raise TypeError(&#39;`delay` ({}) must be an int&#39;.format(type(delay)))
        if delay &lt; 0:
            raise ValueError(&#39;`delay` ({}) must be &gt;= 0&#39;.format(delay))
        self.delay = delay
        self._there_are_perturbations = self.G.perturbations is not None

        # Set the hyperparameters
        if not pl.isfloat(target_acceptance_rate):
            raise TypeError(&#39;`target_acceptance_rate` must be a float&#39;.format(
                type(target_acceptance_rate)))
        if target_acceptance_rate &lt; 0 or target_acceptance_rate &gt; 1:
            raise ValueError(&#39;`target_acceptance_rate` ({}) must be in (0,1)&#39;.format(
                target_acceptance_rate))
        if not pl.istuple(tune):
            raise TypeError(&#39;`tune` ({}) must be a tuple&#39;.format(type(tune)))
        if len(tune) != 2:
            raise ValueError(&#39;`tune` ({}) must have 2 elements&#39;.format(len(tune)))
        if not pl.isint(tune[0]):
            raise TypeError(&#39;`tune` ({}) 1st parameter must be an int&#39;.format(type(tune[0])))
        if tune[0] &lt; 0:
            raise ValueError(&#39;`tune` ({}) 1st parameter must be &gt; 0&#39;.format(tune[0]))
        if not pl.isint(tune[1]):
            raise TypeError(&#39;`tune` ({}) 2nd parameter must be an int&#39;.format(type(tune[1])))
        if tune[1] &lt; 0:
            raise ValueError(&#39;`tune` ({}) 2nd parameter must be &gt; 0&#39;.format(tune[1]))
        
        if not pl.isnumeric(a0):
            raise TypeError(&#39;`a0` ({}) must be a numeric type&#39;.format(type(a0)))
        elif a0 &lt;= 0:
            raise ValueError(&#39;`a0` ({}) must be &gt; 0&#39;.format(a0))
        if not pl.isnumeric(a1):
            raise TypeError(&#39;`a1` ({}) must be a numeric type&#39;.format(type(a1)))
        elif a1 &lt;= 0:
            raise ValueError(&#39;`a1` ({}) must be &gt; 0&#39;.format(a1))

        if not pl.isnumeric(proposal_init_scale):
            raise TypeError(&#39;`proposal_init_scale` ({}) must be a numeric type (int, float)&#39;.format(
                type(proposal_init_scale)))
        if proposal_init_scale &lt; 0:
            raise ValueError(&#39;`proposal_init_scale` ({}) must be positive&#39;.format(
                proposal_init_scale))

        self.tune = tune
        self.a0 = a0
        self.a1 = a1
        self.target_acceptance_rate = target_acceptance_rate
        self.proposal_init_scale = proposal_init_scale
        self.v1 = v1
        self.v2 = v2

        # Set the essential timepoints (check to see if there is any missing data)
        if essential_timepoints is not None:
            logging.info(&#39;Setting up the essential timepoints&#39;)
            if pl.isstr(essential_timepoints):
                if essential_timepoints in [&#39;auto&#39;, &#39;union&#39;]:
                    essential_timepoints = OrderedSet()
                    for ts in self.G.data.times:
                        essential_timepoints = essential_timepoints.union(OrderedSet(list(ts)))
                    essential_timepoints = np.sort(list(essential_timepoints))
                else:
                    raise ValueError(&#39;`essential_timepoints` ({}) not recognized&#39;.format(
                        essential_timepoints))
            elif not pl.isarray(essential_timepoints):
                raise TypeError(&#39;`essential_timepoints` ({}) must be a str or an array&#39;.format(
                    type(essential_timepoints)))
            logging.info(&#39;Essential timepoints: {}&#39;.format(essential_timepoints))
            self.G.data.set_timepoints(times=essential_timepoints, eps=None, reset_timepoints=True)
            self.x.reset_value_size()

        # Set the intermediate timepoints if necessary
        if intermediate_step is not None:
            # Set the intermediate timepoints in the data
            if not pl.istuple(intermediate_step):
                raise TypeError(&#39;`intermediate_step` ({}) must be a tuple&#39;.format(
                    type(intermediate_step)))
            if len(intermediate_step) != 2:
                raise ValueError(&#39;`intermediate_step` ({}) must be length 2&#39;.format(
                    len(intermediate_step)))
            f, args = intermediate_step
            if not pl.isstr(f):
                raise TypeError(&#39;intermediate_step type ({}) must be a str&#39;.format(type(f)))
            if f == &#39;step&#39;:
                if not pl.istuple(args):
                    raise TypeError(&#39;`args` ({}) must be a tuple&#39;.format(type(args)))
                if len(args) != 2:
                    raise TypeError(&#39;`args` ({}) must have 2 arguments&#39;.format(len(args)))
                step, eps = args
                self.G.data.set_timepoints(timestep=step, eps=eps, reset_timepoints=False)
            elif f == &#39;preserve-density&#39;:
                if not pl.istuple(args):
                    raise TypeError(&#39;`args` ({}) must be a tuple&#39;.format(type(args)))
                if len(args) != 2:
                    raise TypeError(&#39;`args` ({}) must have 2 arguments&#39;.format(len(args)))
                n, eps = args
                if not pl.isint(n):
                    raise TypeError(&#39;`n` ({}) must be an int&#39;.format(type(n)))

                # For each timepoint, add `n` intermediate timepoints
                for ridx in range(self.G.data.n_replicates):
                    times = []
                    for i in range(len(self.G.data.times[ridx])-1):
                        t0 = self.G.data.times[ridx][i]
                        t1 = self.G.data.times[ridx][i+1]
                        step = (t1-t0)/(n+1)
                        times = np.append(times, np.arange(t0,t1,step=step))
                    times = np.sort(np.unique(times))
                    # print(&#39;\n\ntimes to put in&#39;, times)
                    self.G.data.set_timepoints(times=times, eps=eps, ridx=ridx, reset_timepoints=False)
                    # print(&#39;times for ridx {}&#39;.format(self.G.data.times[ridx]))
                    # print(&#39;len times&#39;, len(self.G.data.times[ridx]))
                    # print(&#39;data shape&#39;, self.G.data.data[ridx].shape)

                # sys.exit()
            elif f == &#39;manual&#39;:
                raise NotImplementedError(&#39;Not Implemented&#39;)
            else:
                raise ValueError(&#39;`intermediate_step type ({}) not recognized&#39;.format(f))
            self.x.reset_value_size()

        if intermediate_interpolation is not None:
            if intermediate_interpolation in [&#39;linear-interpolation&#39;, &#39;auto&#39;]:
                for ridx in range(self.G.data.n_replicates):
                    for tidx in range(self.G.data.n_timepoints_for_replicate[ridx]):
                        if tidx not in self.G.data.given_timeindices[ridx]:
                            # We need to interpolate this time point
                            # get the previous given and next given timepoint
                            prev_tidx = None
                            for ii in range(tidx-1,-1,-1):
                                if ii in self.G.data.given_timeindices[ridx]:
                                    prev_tidx = ii
                                    break
                            if prev_tidx is None:
                                # Set to the same as the closest forward timepoint then continue
                                next_idx = None
                                for ii in range(tidx+1, self.G.data.n_timepoints_for_replicate[ridx]):
                                    if ii in self.G.data.given_timeindices[ridx]:
                                        next_idx = ii
                                        break
                                self.G.data.data[ridx][:,tidx] = self.G.data.data[ridx][:,next_idx]
                                continue

                            next_tidx = None
                            for ii in range(tidx+1, self.G.data.n_timepoints_for_replicate[ridx]):
                                if ii in self.G.data.given_timeindices[ridx]:
                                    next_tidx = ii
                                    break
                            if next_tidx is None:
                                # Set to the previous timepoint then continue
                                self.G.data.data[ridx][:,tidx] = self.G.data.data[ridx][:,prev_tidx]
                                continue

                            # Interpolate from prev_tidx to next_tidx
                            x = self.G.data.times[ridx][tidx]
                            x0 = self.G.data.times[ridx][prev_tidx]
                            y0 = self.G.data.data[ridx][:,prev_tidx]
                            x1 = self.G.data.times[ridx][next_tidx]
                            y1 = self.G.data.data[ridx][:,next_tidx]
                            self.G.data.data[ridx][:,tidx] = y0 * (1-((x-x0)/(x1-x0))) + y1 * (1-((x1-x)/(x1-x0)))
            else:
                raise ValueError(&#39;`intermediate_interpolation` ({}) not recognized&#39;.format(intermediate_interpolation))

        # Initialize the latent trajectory
        if not pl.isstr(x_value_option):
            raise TypeError(&#39;`x_value_option` ({}) is not a str&#39;.format(type(x_value_option)))
        if x_value_option == &#39;coupling&#39;:
            self._init_coupling()
        elif x_value_option == &#39;moving-avg&#39;:
            if not pl.isnumeric(bandwidth):
                raise TypeError(&#39;`bandwidth` ({}) must be a numeric&#39;.format(type(bandwidth)))
            if bandwidth &lt;= 0:
                raise ValueError(&#39;`bandwidth` ({}) must be positive&#39;.format(bandwidth))
            self.bandwidth = bandwidth
            self._init_moving_avg()
        elif x_value_option in [&#39;loess&#39;, &#39;auto&#39;]:
            if window is None:
                raise TypeError(&#39;If `value_option` is loess, then `window` must be specified&#39;)
            if not pl.isint(window):
                raise TypeError(&#39;`window` ({}) must be an int&#39;.format(type(window)))
            if window &lt;= 0:
                raise ValueError(&#39;`window` ({}) must be &gt; 0&#39;.format(window))
            self.window = window
            self._init_loess()
        else:
            raise ValueError(&#39;`x_value_option` ({}) not recognized&#39;.format(x_value_option))

        # Get necessary data and set the parallel objects
        if self._there_are_perturbations:
            pert_starts = []
            pert_ends = []
            for perturbation in self.G.perturbations:
                pert_starts.append(perturbation.starts)
                pert_ends.append(perturbation.ends)
        else:
            pert_starts = None
            pert_ends = None

        if self.mp is None:
            self.mp = &#39;debug&#39;
        if not pl.isstr(self.mp):
            raise TypeError(&#39;`mp` ({}) must either be a string or None&#39;.format(type(self.mp)))
        if self.mp == &#39;debug&#39;:
            self.pool = []
        elif self.mp == &#39;full&#39;:
            self.pool = pl.multiprocessing.PersistentPool(G=self.G, ptype=&#39;sadw&#39;)
            self.worker_pids = []
        else:
            raise ValueError(&#39;`mp` ({}) not recognized&#39;.format(self.mp))

        for ridx, subj in enumerate(self.G.data.subjects):
            # Set up qPCR measurements and reads to send
            qpcr_log_measurements = {}
            for t in self.G.data.given_timepoints[ridx]:
                qpcr_log_measurements[t] = self.G.data.qpcr[ridx][t].log_data
            reads = self.G.data.subjects.iloc(ridx).reads

            worker = SubjectLogTrajectorySetMP()
            worker.initialize(
                zero_inflation_transition_policy=self.zero_inflation_transition_policy,
                times=self.G.data.times[ridx],
                qpcr_log_measurements=qpcr_log_measurements,
                reads=reads,
                there_are_intermediate_timepoints=True,
                there_are_perturbations=self._there_are_perturbations,
                pv_global=self.G[STRNAMES.PROCESSVAR].global_variance,
                x_prior_mean=np.log(1e7),
                x_prior_std=1e10,
                tune=tune[1],
                delay=delay,
                end_iter=tune[0],
                proposal_init_scale=proposal_init_scale,
                a0=a0,
                a1=a1,
                x=self.x[ridx].value,
                pert_starts=np.asarray(pert_starts),
                pert_ends=np.asarray(pert_ends),
                ridx=ridx,
                subjname=subj.name,
                calculate_qpcr_loglik=calculate_qpcr_loglik,
                h5py_xname=self.x[ridx].name,
                target_acceptance_rate=self.target_acceptance_rate)
            if self.mp == &#39;debug&#39;:
                self.pool.append(worker)
            elif self.mp == &#39;full&#39;:
                pid = self.pool.add_worker(worker)
                self.worker_pids.append(pid)

        # Set the data to the latent values
        self.set_latent_as_data(update_values=False)

        self.total_n_datapoints = 0
        for ridx in range(self.G.data.n_replicates):
            self.total_n_datapoints += self.x[ridx].value.shape[0] * self.x[ridx].value.shape[1]

    def _init_coupling(self):
        &#39;&#39;&#39;Initialize `x` by sampling around the data using a small
        variance using a truncated normal distribution
        &#39;&#39;&#39;
        for ridx, tidx, oidx in self.x.iter_indices():
            val = self.G.data.data[ridx][oidx,tidx]
            self.x[ridx][oidx,tidx] = pl.random.truncnormal.sample(
                loc=val, scale=math.sqrt(self.v1 * (val ** 2) + self.v2),
                low=0, high=float(&#39;inf&#39;))

    def _init_moving_avg(self):
        &#39;&#39;&#39;Initializes `x` by using a moving
        average over the data - using `self.bandwidth` as the bandwidth
        of number of days - it then samples around that point using the
        coupling variance.

        If there are no other points within the bandwidth around the point,
        then it just samples around the current timepoint with the coupling
        variance.
        &#39;&#39;&#39;
        for ridx in range(self.G.data.n_replicates):
            for tidx in range(self.G.data.n_timepoints_for_replicate[ridx]):
                tidx_low = np.searchsorted(
                    self.G.data.times[ridx], self.G.data.times[ridx][tidx]-self.bandwidth)
                tidx_high = np.searchsorted(
                    self.G.data.times[ridx], self.G.data.times[ridx][tidx]+self.bandwidth)

                for oidx in range(len(self.G.data.taxa)):
                    val = np.mean(self.G.data.data[ridx][oidx, tidx_low: tidx_high])
                    self.x[ridx][oidx,tidx] = pl.random.truncnormal.sample(
                        loc=val, scale=math.sqrt(self.v1 * (val ** 2) + self.v2),
                        low=0, high=float(&#39;inf&#39;))

    def _init_loess(self):
        &#39;&#39;&#39;Initialize the data using LOESS algorithm and then samples around that
        the coupling variance we implement the LOESS algorithm in the module
        `fit_loess.py`
        &#39;&#39;&#39;
        for ridx in range(self.G.data.n_replicates):
            xx = self.G.data.times[ridx]
            for oidx in range(len(self.G.data.taxa)):
                yy = self.G.data.data[ridx][oidx, :]
                loess = _Loess(xx, yy)

                for tidx, t in enumerate(self.G.data.times[ridx]):
                    val = loess.estimate(t, window=self.window)
                    self.x[ridx][oidx,tidx] = pl.random.truncnormal.sample(
                        loc=val, scale=math.sqrt(self.v1 * (val ** 2) + self.v2),
                        low=0, high=float(&#39;inf&#39;))

                    if np.isnan(self.x[ridx][oidx, tidx]):
                        print(&#39;crashed here&#39;, ridx, tidx, oidx)
                        print(&#39;mean&#39;, val)
                        print(&#39;t&#39;, t)
                        print(&#39;yy&#39;, yy)
                        print(&#39;std&#39;, math.sqrt(self.v1 * (val ** 2) + self.v2))
                        raise ValueError(&#39;&#39;)

    def set_latent_as_data(self, update_values: bool=True):
        &#39;&#39;&#39;Change the values in the data matrix so that it is the latent variables
        &#39;&#39;&#39;
        data = []
        for obj in self.x.value:
            data.append(obj.value)
        self.G.data.data = data
        if update_values:
            self.G.data.update_values()

    def add_trace(self):
        self.x.add_trace()

    def set_trace(self, *args, **kwargs):
        self.x.set_trace(*args, **kwargs)

    def kill(self):
        if self.mp == &#39;full&#39;:
            self.pool.kill()

    def update(self):
        &#39;&#39;&#39;Send out to each parallel object
        &#39;&#39;&#39;
        if self.sample_iter &lt; self.delay:
            return
        start_time = time.time()

        growth = self.G[STRNAMES.GROWTH_VALUE].value.ravel()
        self_interactions = self.G[STRNAMES.SELF_INTERACTION_VALUE].value.ravel()
        pv = self.G[STRNAMES.PROCESSVAR].value
        interactions = self.G[STRNAMES.INTERACTIONS_OBJ].get_datalevel_value_matrix(
            set_neg_indicators_to_nan=False)
        perts = None
        if self._there_are_perturbations:
            perts = []
            for perturbation in self.G.perturbations:
                perts.append(perturbation.item_array().reshape(-1,1))
            perts = np.hstack(perts)

        # zero_inflation = [self.G[STRNAMES.ZERO_INFLATION].value[ridx] for ridx in range(self.G.data.n_replicates)]
        qpcr_vars = []
        for aaa in self.G[STRNAMES.QPCR_VARIANCES].value:
            qpcr_vars.append(aaa.value)
        
        
        kwargs = {&#39;growth&#39;:growth, &#39;self_interactions&#39;:self_interactions,
            &#39;pv&#39;:pv, &#39;interactions&#39;:interactions, &#39;perturbations&#39;:perts, 
            &#39;zero_inflation_data&#39;: None, &#39;qpcr_variances&#39;:qpcr_vars}

        str_acc = [None]*self.G.data.n_replicates
        mpstr = None
        if self.mp == &#39;debug&#39;:

            for ridx in range(self.G.data.n_replicates):
                _, x, acc_rate = self.pool[ridx].persistent_run(**kwargs)
                self.x[ridx].value = x
                str_acc[ridx] = &#39;{:.3f}&#39;.format(acc_rate)
                mpstr = &#39;no-mp&#39;

        else:
            # raise NotImplementedError(&#39;Multiprocessing for filtering with zero inflation &#39; \
            #     &#39;is not implemented&#39;)
            ret = self.pool.map(func=&#39;persistent_run&#39;, args=kwargs)
            for ridx, x, acc_rate in ret:
                self.x[ridx].value = x
                str_acc[ridx] = &#39;{:.3f}&#39;.format(acc_rate)
            mpstr = &#39;mp&#39;

        self.set_latent_as_data()

        t = time.time() - start_time
        try:
            self._strr = &#39;{} - Time: {:.4f}, Acc: {}, data/sec: {:.2f}&#39;.format(mpstr, t,
                str(str_acc).replace(&#34;&#39;&#34;,&#39;&#39;), self.total_n_datapoints/t)
        except:
            self._strr = &#39;NA&#39;

    def visualize(self, basepath: str, section: str=&#39;posterior&#39;, taxa_formatter: str=&#39;%(name)s&#39;, 
        vmin: float=None, vmax: float=None):
        &#39;&#39;&#39;Plot the posterior for each filtering object

        Parameters
        ----------
        basepath : str
            This is the loction to write the files to
        section : str
            Section of the trace to compute on. Options:
                &#39;posterior&#39; : posterior samples
                &#39;burnin&#39; : burn-in samples
                &#39;entire&#39; : both burn-in and posterior samples
        taxa_formatter : str, None
            This is the format of the label to return for each . If None, it will return
            the taxon&#39;s name
        vmin, vmax : float
            These are the maximum and minimum values to plot the abundance of
        &#39;&#39;&#39;
        for ridx, replicate in enumerate(self.x.value):
            subj = self.G.data.subjects.iloc(ridx)
            path = os.path.join(basepath, &#39;Subject_{}&#39;.format(subj.name))
            os.makedirs(path, exist_ok=True)
            self.x.visualize(ridx=ridx, section=section, basepath=path, 
                taxa_formatter=taxa_formatter)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="mdsine2.pylab.graph.Node" href="pylab/graph.html#mdsine2.pylab.graph.Node">Node</a></li>
<li><a title="mdsine2.pylab.graph.BaseNode" href="pylab/graph.html#mdsine2.pylab.graph.BaseNode">BaseNode</a></li>
<li><a title="mdsine2.pylab.base.Saveable" href="pylab/base.html#mdsine2.pylab.base.Saveable">Saveable</a></li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="mdsine2.posterior.FilteringLogMP.sample_iter"><code class="name">var <span class="ident">sample_iter</span> : int</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def sample_iter(self) -&gt; int:
    # It doesnt matter if we chose q or x because they are both the same
    return self.x.sample_iter</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="mdsine2.posterior.FilteringLogMP.add_trace"><code class="name flex">
<span>def <span class="ident">add_trace</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add_trace(self):
    self.x.add_trace()</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.FilteringLogMP.initialize"><code class="name flex">
<span>def <span class="ident">initialize</span></span>(<span>self, x_value_option: str, a0: Union[float, str], a1: Union[float, str], v1: Union[float, int], v2: Union[float, int], essential_timepoints: Union[numpy.ndarray, str], tune: Tuple[int, int], proposal_init_scale: Union[float, int], intermediate_step: Union[Tuple[str, Tuple], numpy.ndarray, List, NoneType], intermediate_interpolation: str = None, delay: int = 0, bandwidth: Union[float, int] = None, window: int = None, target_acceptance_rate: Union[str, float] = 0.44, calculate_qpcr_loglik: bool = True)</span>
</code></dt>
<dd>
<div class="desc"><p>Initialize the values of the error model (values for the
latent and the auxiliary trajectory). Additionally this sets
the intermediate time points</p>
<p>Initialize the values of the prior.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>x_value_option</code></strong> :&ensp;<code>str</code></dt>
<dd>
<ul>
<li>Option to initialize the value of the latent trajectory.</li>
<li>Options<ul>
<li>'coupling'<ul>
<li>Sample the values around the data with extremely low variance.</li>
<li>This also truncates the data so that it stays &gt; 0.</li>
</ul>
</li>
<li>'moving-avg'<ul>
<li>Initialize the values using a moving average around the points.</li>
<li>The bandwidth of the filter is by number of days, not the order
of timepoints. You must also provide the argument <code>bandwidth</code>.</li>
</ul>
</li>
<li>'loess', 'auto'<ul>
<li>Implements the initialization of the values using LOESS (Locally</li>
<li>Estimated Scatterplot Smoothing) algorithm. You must also provide
the <code>window</code> parameter</li>
</ul>
</li>
</ul>
</li>
</ul>
</dd>
<dt><strong><code>tune</code></strong> :&ensp;<code>tuple(int, int)</code></dt>
<dd>
<ul>
<li>This is how often to tune the individual covariances</li>
<li>The first element indicates which MCMC sample to stop the tuning</li>
<li>The second element is how often to update the proposal covariance</li>
</ul>
</dd>
<dt><strong><code>a0</code></strong>, <strong><code>a1</code></strong> :&ensp;<code>float, str</code></dt>
<dd>These are the hyperparameters to calculate the dispersion of the
negative binomial.</dd>
<dt><strong><code>v1</code></strong>, <strong><code>v2</code></strong> :&ensp;<code>float, int, str</code></dt>
<dd>These are the values used to calulcate the coupling variance between
x and q</dd>
<dt><strong><code>intermediate_step</code></strong> :&ensp;<code>tuple(str, args), array, None</code></dt>
<dd>
<ul>
<li>This is the type of interemediate timestep to intialize and the arguments
for them. If this is None, then we do no intermediate timesteps.</li>
<li>Options:<ul>
<li>'step'<ul>
<li>args: (stride (numeric), eps (numeric))</li>
<li>We simulate at each timepoint every <code>stride</code> days.</li>
<li>We do not set an intermediate time point if it is within <code>eps</code>
days of a given data point.</li>
</ul>
</li>
<li>'preserve-density'<ul>
<li>args: (n (int), eps (numeric))</li>
<li>We preserve the denisty of the given data by only simulating data</li>
<li><code>n</code> times between each essential datapoint. If a timepoint is within</li>
<li><code>eps</code> days of a given timepoint then we do not make an intermediate
point there.</li>
</ul>
</li>
<li>'manual;<ul>
<li>args: np.ndarray</li>
<li>These are the points that we want to set. If these are not given times
then we set them as timepoints</li>
</ul>
</li>
</ul>
</li>
</ul>
</dd>
<dt><strong><code>intermediate_interpolation</code></strong> :&ensp;<code>str</code></dt>
<dd>
<ul>
<li>This is the type of interpolation to perform on the intermediate timepoints.</li>
<li>Options:<ul>
<li>'linear-interpolation', 'auto'<ul>
<li>Perform linear interpolation between the two closest given timepoints</li>
</ul>
</li>
</ul>
</li>
</ul>
</dd>
<dt><strong><code>essential_timepoints</code></strong> :&ensp;<code>np.ndarray, str, None</code></dt>
<dd>
<ul>
<li>These are the timepoints that must be included in each subject. If one of the
subjects has a missing timepoint there then we use an intermediate time point
that this timepoint. It is initialized with linear interpolation. If all of the
timepoints specified in this vector are included in a subject then nothing is
done. If it is a str:<ul>
<li>'union', 'auto'<ul>
<li>We take a union of all the timepoints in each subject and make sure
that all of the subjects have all those points.</li>
</ul>
</li>
</ul>
</li>
</ul>
</dd>
<dt><strong><code>bandwidth</code></strong> :&ensp;<code>float</code></dt>
<dd>This is the day bandwidth of the filter if the initialization method is
done with 'moving-avg'.</dd>
<dt><strong><code>window</code></strong> :&ensp;<code>int</code></dt>
<dd>This is the window term for the LOESS initialization scheme. This is
only used if value_initialization is done with 'loess'</dd>
<dt><strong><code>target_acceptance_rate</code></strong> :&ensp;<code>numeric</code></dt>
<dd>This is the target acceptance rate for each time point individually</dd>
<dt><strong><code>calculate_qpcr_loglik</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, calculate the loglikelihood of the qPCR measurements during the
proposal</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def initialize(self, x_value_option: str, a0: Union[float, str], a1: Union[float, str], 
    v1: Union[float, int], v2: Union[float, int], essential_timepoints: Union[np.ndarray, str], 
    tune: Tuple[int, int], proposal_init_scale: Union[float, int],
    intermediate_step: Union[Tuple[str, Tuple], np.ndarray, List, type(None)], 
    intermediate_interpolation: str=None, delay: int=0, bandwidth: Union[float, int]=None, 
    window: int=None, target_acceptance_rate: Union[str, float]=0.44, 
    calculate_qpcr_loglik: bool=True):
    &#39;&#39;&#39;Initialize the values of the error model (values for the
    latent and the auxiliary trajectory). Additionally this sets
    the intermediate time points

    Initialize the values of the prior.

    Parameters
    ----------
    x_value_option : str
        - Option to initialize the value of the latent trajectory.
        - Options
            - &#39;coupling&#39;
                - Sample the values around the data with extremely low variance.
                - This also truncates the data so that it stays &gt; 0.
            - &#39;moving-avg&#39;
                - Initialize the values using a moving average around the points.
                - The bandwidth of the filter is by number of days, not the order
                  of timepoints. You must also provide the argument `bandwidth`.
            - &#39;loess&#39;, &#39;auto&#39;
                - Implements the initialization of the values using LOESS (Locally
                - Estimated Scatterplot Smoothing) algorithm. You must also provide
                  the `window` parameter
    tune : tuple(int, int)
        - This is how often to tune the individual covariances
        - The first element indicates which MCMC sample to stop the tuning
        - The second element is how often to update the proposal covariance
    a0, a1 : float, str
        These are the hyperparameters to calculate the dispersion of the
        negative binomial.
    v1, v2 : float, int, str
        These are the values used to calulcate the coupling variance between
        x and q
    intermediate_step : tuple(str, args), array, None
        - This is the type of interemediate timestep to intialize and the arguments
          for them. If this is None, then we do no intermediate timesteps.
        - Options:
            * &#39;step&#39;
                - args: (stride (numeric), eps (numeric))
                - We simulate at each timepoint every `stride` days.
                - We do not set an intermediate time point if it is within `eps`
                  days of a given data point.
            * &#39;preserve-density&#39;
                - args: (n (int), eps (numeric))
                - We preserve the denisty of the given data by only simulating data
                - `n` times between each essential datapoint. If a timepoint is within
                - `eps` days of a given timepoint then we do not make an intermediate
                  point there.
            * &#39;manual;
                - args: np.ndarray
                - These are the points that we want to set. If these are not given times
                  then we set them as timepoints
    intermediate_interpolation : str
        - This is the type of interpolation to perform on the intermediate timepoints.
        - Options:
            - &#39;linear-interpolation&#39;, &#39;auto&#39;
                - Perform linear interpolation between the two closest given timepoints
    essential_timepoints : np.ndarray, str, None
        - These are the timepoints that must be included in each subject. If one of the
          subjects has a missing timepoint there then we use an intermediate time point
          that this timepoint. It is initialized with linear interpolation. If all of the
          timepoints specified in this vector are included in a subject then nothing is
          done. If it is a str:
            - &#39;union&#39;, &#39;auto&#39;
                * We take a union of all the timepoints in each subject and make sure
                  that all of the subjects have all those points.
    bandwidth : float
        This is the day bandwidth of the filter if the initialization method is
        done with &#39;moving-avg&#39;.
    window : int
        This is the window term for the LOESS initialization scheme. This is
        only used if value_initialization is done with &#39;loess&#39;
    target_acceptance_rate : numeric
        This is the target acceptance rate for each time point individually
    calculate_qpcr_loglik : bool
        If True, calculate the loglikelihood of the qPCR measurements during the 
        proposal
    &#39;&#39;&#39;
    if not pl.isint(delay):
        raise TypeError(&#39;`delay` ({}) must be an int&#39;.format(type(delay)))
    if delay &lt; 0:
        raise ValueError(&#39;`delay` ({}) must be &gt;= 0&#39;.format(delay))
    self.delay = delay
    self._there_are_perturbations = self.G.perturbations is not None

    # Set the hyperparameters
    if not pl.isfloat(target_acceptance_rate):
        raise TypeError(&#39;`target_acceptance_rate` must be a float&#39;.format(
            type(target_acceptance_rate)))
    if target_acceptance_rate &lt; 0 or target_acceptance_rate &gt; 1:
        raise ValueError(&#39;`target_acceptance_rate` ({}) must be in (0,1)&#39;.format(
            target_acceptance_rate))
    if not pl.istuple(tune):
        raise TypeError(&#39;`tune` ({}) must be a tuple&#39;.format(type(tune)))
    if len(tune) != 2:
        raise ValueError(&#39;`tune` ({}) must have 2 elements&#39;.format(len(tune)))
    if not pl.isint(tune[0]):
        raise TypeError(&#39;`tune` ({}) 1st parameter must be an int&#39;.format(type(tune[0])))
    if tune[0] &lt; 0:
        raise ValueError(&#39;`tune` ({}) 1st parameter must be &gt; 0&#39;.format(tune[0]))
    if not pl.isint(tune[1]):
        raise TypeError(&#39;`tune` ({}) 2nd parameter must be an int&#39;.format(type(tune[1])))
    if tune[1] &lt; 0:
        raise ValueError(&#39;`tune` ({}) 2nd parameter must be &gt; 0&#39;.format(tune[1]))
    
    if not pl.isnumeric(a0):
        raise TypeError(&#39;`a0` ({}) must be a numeric type&#39;.format(type(a0)))
    elif a0 &lt;= 0:
        raise ValueError(&#39;`a0` ({}) must be &gt; 0&#39;.format(a0))
    if not pl.isnumeric(a1):
        raise TypeError(&#39;`a1` ({}) must be a numeric type&#39;.format(type(a1)))
    elif a1 &lt;= 0:
        raise ValueError(&#39;`a1` ({}) must be &gt; 0&#39;.format(a1))

    if not pl.isnumeric(proposal_init_scale):
        raise TypeError(&#39;`proposal_init_scale` ({}) must be a numeric type (int, float)&#39;.format(
            type(proposal_init_scale)))
    if proposal_init_scale &lt; 0:
        raise ValueError(&#39;`proposal_init_scale` ({}) must be positive&#39;.format(
            proposal_init_scale))

    self.tune = tune
    self.a0 = a0
    self.a1 = a1
    self.target_acceptance_rate = target_acceptance_rate
    self.proposal_init_scale = proposal_init_scale
    self.v1 = v1
    self.v2 = v2

    # Set the essential timepoints (check to see if there is any missing data)
    if essential_timepoints is not None:
        logging.info(&#39;Setting up the essential timepoints&#39;)
        if pl.isstr(essential_timepoints):
            if essential_timepoints in [&#39;auto&#39;, &#39;union&#39;]:
                essential_timepoints = OrderedSet()
                for ts in self.G.data.times:
                    essential_timepoints = essential_timepoints.union(OrderedSet(list(ts)))
                essential_timepoints = np.sort(list(essential_timepoints))
            else:
                raise ValueError(&#39;`essential_timepoints` ({}) not recognized&#39;.format(
                    essential_timepoints))
        elif not pl.isarray(essential_timepoints):
            raise TypeError(&#39;`essential_timepoints` ({}) must be a str or an array&#39;.format(
                type(essential_timepoints)))
        logging.info(&#39;Essential timepoints: {}&#39;.format(essential_timepoints))
        self.G.data.set_timepoints(times=essential_timepoints, eps=None, reset_timepoints=True)
        self.x.reset_value_size()

    # Set the intermediate timepoints if necessary
    if intermediate_step is not None:
        # Set the intermediate timepoints in the data
        if not pl.istuple(intermediate_step):
            raise TypeError(&#39;`intermediate_step` ({}) must be a tuple&#39;.format(
                type(intermediate_step)))
        if len(intermediate_step) != 2:
            raise ValueError(&#39;`intermediate_step` ({}) must be length 2&#39;.format(
                len(intermediate_step)))
        f, args = intermediate_step
        if not pl.isstr(f):
            raise TypeError(&#39;intermediate_step type ({}) must be a str&#39;.format(type(f)))
        if f == &#39;step&#39;:
            if not pl.istuple(args):
                raise TypeError(&#39;`args` ({}) must be a tuple&#39;.format(type(args)))
            if len(args) != 2:
                raise TypeError(&#39;`args` ({}) must have 2 arguments&#39;.format(len(args)))
            step, eps = args
            self.G.data.set_timepoints(timestep=step, eps=eps, reset_timepoints=False)
        elif f == &#39;preserve-density&#39;:
            if not pl.istuple(args):
                raise TypeError(&#39;`args` ({}) must be a tuple&#39;.format(type(args)))
            if len(args) != 2:
                raise TypeError(&#39;`args` ({}) must have 2 arguments&#39;.format(len(args)))
            n, eps = args
            if not pl.isint(n):
                raise TypeError(&#39;`n` ({}) must be an int&#39;.format(type(n)))

            # For each timepoint, add `n` intermediate timepoints
            for ridx in range(self.G.data.n_replicates):
                times = []
                for i in range(len(self.G.data.times[ridx])-1):
                    t0 = self.G.data.times[ridx][i]
                    t1 = self.G.data.times[ridx][i+1]
                    step = (t1-t0)/(n+1)
                    times = np.append(times, np.arange(t0,t1,step=step))
                times = np.sort(np.unique(times))
                # print(&#39;\n\ntimes to put in&#39;, times)
                self.G.data.set_timepoints(times=times, eps=eps, ridx=ridx, reset_timepoints=False)
                # print(&#39;times for ridx {}&#39;.format(self.G.data.times[ridx]))
                # print(&#39;len times&#39;, len(self.G.data.times[ridx]))
                # print(&#39;data shape&#39;, self.G.data.data[ridx].shape)

            # sys.exit()
        elif f == &#39;manual&#39;:
            raise NotImplementedError(&#39;Not Implemented&#39;)
        else:
            raise ValueError(&#39;`intermediate_step type ({}) not recognized&#39;.format(f))
        self.x.reset_value_size()

    if intermediate_interpolation is not None:
        if intermediate_interpolation in [&#39;linear-interpolation&#39;, &#39;auto&#39;]:
            for ridx in range(self.G.data.n_replicates):
                for tidx in range(self.G.data.n_timepoints_for_replicate[ridx]):
                    if tidx not in self.G.data.given_timeindices[ridx]:
                        # We need to interpolate this time point
                        # get the previous given and next given timepoint
                        prev_tidx = None
                        for ii in range(tidx-1,-1,-1):
                            if ii in self.G.data.given_timeindices[ridx]:
                                prev_tidx = ii
                                break
                        if prev_tidx is None:
                            # Set to the same as the closest forward timepoint then continue
                            next_idx = None
                            for ii in range(tidx+1, self.G.data.n_timepoints_for_replicate[ridx]):
                                if ii in self.G.data.given_timeindices[ridx]:
                                    next_idx = ii
                                    break
                            self.G.data.data[ridx][:,tidx] = self.G.data.data[ridx][:,next_idx]
                            continue

                        next_tidx = None
                        for ii in range(tidx+1, self.G.data.n_timepoints_for_replicate[ridx]):
                            if ii in self.G.data.given_timeindices[ridx]:
                                next_tidx = ii
                                break
                        if next_tidx is None:
                            # Set to the previous timepoint then continue
                            self.G.data.data[ridx][:,tidx] = self.G.data.data[ridx][:,prev_tidx]
                            continue

                        # Interpolate from prev_tidx to next_tidx
                        x = self.G.data.times[ridx][tidx]
                        x0 = self.G.data.times[ridx][prev_tidx]
                        y0 = self.G.data.data[ridx][:,prev_tidx]
                        x1 = self.G.data.times[ridx][next_tidx]
                        y1 = self.G.data.data[ridx][:,next_tidx]
                        self.G.data.data[ridx][:,tidx] = y0 * (1-((x-x0)/(x1-x0))) + y1 * (1-((x1-x)/(x1-x0)))
        else:
            raise ValueError(&#39;`intermediate_interpolation` ({}) not recognized&#39;.format(intermediate_interpolation))

    # Initialize the latent trajectory
    if not pl.isstr(x_value_option):
        raise TypeError(&#39;`x_value_option` ({}) is not a str&#39;.format(type(x_value_option)))
    if x_value_option == &#39;coupling&#39;:
        self._init_coupling()
    elif x_value_option == &#39;moving-avg&#39;:
        if not pl.isnumeric(bandwidth):
            raise TypeError(&#39;`bandwidth` ({}) must be a numeric&#39;.format(type(bandwidth)))
        if bandwidth &lt;= 0:
            raise ValueError(&#39;`bandwidth` ({}) must be positive&#39;.format(bandwidth))
        self.bandwidth = bandwidth
        self._init_moving_avg()
    elif x_value_option in [&#39;loess&#39;, &#39;auto&#39;]:
        if window is None:
            raise TypeError(&#39;If `value_option` is loess, then `window` must be specified&#39;)
        if not pl.isint(window):
            raise TypeError(&#39;`window` ({}) must be an int&#39;.format(type(window)))
        if window &lt;= 0:
            raise ValueError(&#39;`window` ({}) must be &gt; 0&#39;.format(window))
        self.window = window
        self._init_loess()
    else:
        raise ValueError(&#39;`x_value_option` ({}) not recognized&#39;.format(x_value_option))

    # Get necessary data and set the parallel objects
    if self._there_are_perturbations:
        pert_starts = []
        pert_ends = []
        for perturbation in self.G.perturbations:
            pert_starts.append(perturbation.starts)
            pert_ends.append(perturbation.ends)
    else:
        pert_starts = None
        pert_ends = None

    if self.mp is None:
        self.mp = &#39;debug&#39;
    if not pl.isstr(self.mp):
        raise TypeError(&#39;`mp` ({}) must either be a string or None&#39;.format(type(self.mp)))
    if self.mp == &#39;debug&#39;:
        self.pool = []
    elif self.mp == &#39;full&#39;:
        self.pool = pl.multiprocessing.PersistentPool(G=self.G, ptype=&#39;sadw&#39;)
        self.worker_pids = []
    else:
        raise ValueError(&#39;`mp` ({}) not recognized&#39;.format(self.mp))

    for ridx, subj in enumerate(self.G.data.subjects):
        # Set up qPCR measurements and reads to send
        qpcr_log_measurements = {}
        for t in self.G.data.given_timepoints[ridx]:
            qpcr_log_measurements[t] = self.G.data.qpcr[ridx][t].log_data
        reads = self.G.data.subjects.iloc(ridx).reads

        worker = SubjectLogTrajectorySetMP()
        worker.initialize(
            zero_inflation_transition_policy=self.zero_inflation_transition_policy,
            times=self.G.data.times[ridx],
            qpcr_log_measurements=qpcr_log_measurements,
            reads=reads,
            there_are_intermediate_timepoints=True,
            there_are_perturbations=self._there_are_perturbations,
            pv_global=self.G[STRNAMES.PROCESSVAR].global_variance,
            x_prior_mean=np.log(1e7),
            x_prior_std=1e10,
            tune=tune[1],
            delay=delay,
            end_iter=tune[0],
            proposal_init_scale=proposal_init_scale,
            a0=a0,
            a1=a1,
            x=self.x[ridx].value,
            pert_starts=np.asarray(pert_starts),
            pert_ends=np.asarray(pert_ends),
            ridx=ridx,
            subjname=subj.name,
            calculate_qpcr_loglik=calculate_qpcr_loglik,
            h5py_xname=self.x[ridx].name,
            target_acceptance_rate=self.target_acceptance_rate)
        if self.mp == &#39;debug&#39;:
            self.pool.append(worker)
        elif self.mp == &#39;full&#39;:
            pid = self.pool.add_worker(worker)
            self.worker_pids.append(pid)

    # Set the data to the latent values
    self.set_latent_as_data(update_values=False)

    self.total_n_datapoints = 0
    for ridx in range(self.G.data.n_replicates):
        self.total_n_datapoints += self.x[ridx].value.shape[0] * self.x[ridx].value.shape[1]</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.FilteringLogMP.kill"><code class="name flex">
<span>def <span class="ident">kill</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def kill(self):
    if self.mp == &#39;full&#39;:
        self.pool.kill()</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.FilteringLogMP.set_latent_as_data"><code class="name flex">
<span>def <span class="ident">set_latent_as_data</span></span>(<span>self, update_values: bool = True)</span>
</code></dt>
<dd>
<div class="desc"><p>Change the values in the data matrix so that it is the latent variables</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_latent_as_data(self, update_values: bool=True):
    &#39;&#39;&#39;Change the values in the data matrix so that it is the latent variables
    &#39;&#39;&#39;
    data = []
    for obj in self.x.value:
        data.append(obj.value)
    self.G.data.data = data
    if update_values:
        self.G.data.update_values()</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.FilteringLogMP.set_trace"><code class="name flex">
<span>def <span class="ident">set_trace</span></span>(<span>self, *args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_trace(self, *args, **kwargs):
    self.x.set_trace(*args, **kwargs)</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.FilteringLogMP.update"><code class="name flex">
<span>def <span class="ident">update</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Send out to each parallel object</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update(self):
    &#39;&#39;&#39;Send out to each parallel object
    &#39;&#39;&#39;
    if self.sample_iter &lt; self.delay:
        return
    start_time = time.time()

    growth = self.G[STRNAMES.GROWTH_VALUE].value.ravel()
    self_interactions = self.G[STRNAMES.SELF_INTERACTION_VALUE].value.ravel()
    pv = self.G[STRNAMES.PROCESSVAR].value
    interactions = self.G[STRNAMES.INTERACTIONS_OBJ].get_datalevel_value_matrix(
        set_neg_indicators_to_nan=False)
    perts = None
    if self._there_are_perturbations:
        perts = []
        for perturbation in self.G.perturbations:
            perts.append(perturbation.item_array().reshape(-1,1))
        perts = np.hstack(perts)

    # zero_inflation = [self.G[STRNAMES.ZERO_INFLATION].value[ridx] for ridx in range(self.G.data.n_replicates)]
    qpcr_vars = []
    for aaa in self.G[STRNAMES.QPCR_VARIANCES].value:
        qpcr_vars.append(aaa.value)
    
    
    kwargs = {&#39;growth&#39;:growth, &#39;self_interactions&#39;:self_interactions,
        &#39;pv&#39;:pv, &#39;interactions&#39;:interactions, &#39;perturbations&#39;:perts, 
        &#39;zero_inflation_data&#39;: None, &#39;qpcr_variances&#39;:qpcr_vars}

    str_acc = [None]*self.G.data.n_replicates
    mpstr = None
    if self.mp == &#39;debug&#39;:

        for ridx in range(self.G.data.n_replicates):
            _, x, acc_rate = self.pool[ridx].persistent_run(**kwargs)
            self.x[ridx].value = x
            str_acc[ridx] = &#39;{:.3f}&#39;.format(acc_rate)
            mpstr = &#39;no-mp&#39;

    else:
        # raise NotImplementedError(&#39;Multiprocessing for filtering with zero inflation &#39; \
        #     &#39;is not implemented&#39;)
        ret = self.pool.map(func=&#39;persistent_run&#39;, args=kwargs)
        for ridx, x, acc_rate in ret:
            self.x[ridx].value = x
            str_acc[ridx] = &#39;{:.3f}&#39;.format(acc_rate)
        mpstr = &#39;mp&#39;

    self.set_latent_as_data()

    t = time.time() - start_time
    try:
        self._strr = &#39;{} - Time: {:.4f}, Acc: {}, data/sec: {:.2f}&#39;.format(mpstr, t,
            str(str_acc).replace(&#34;&#39;&#34;,&#39;&#39;), self.total_n_datapoints/t)
    except:
        self._strr = &#39;NA&#39;</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.FilteringLogMP.visualize"><code class="name flex">
<span>def <span class="ident">visualize</span></span>(<span>self, basepath: str, section: str = 'posterior', taxa_formatter: str = '%(name)s', vmin: float = None, vmax: float = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Plot the posterior for each filtering object</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>basepath</code></strong> :&ensp;<code>str</code></dt>
<dd>This is the loction to write the files to</dd>
<dt><strong><code>section</code></strong> :&ensp;<code>str</code></dt>
<dd>Section of the trace to compute on. Options:
'posterior' : posterior samples
'burnin' : burn-in samples
'entire' : both burn-in and posterior samples</dd>
<dt><strong><code>taxa_formatter</code></strong> :&ensp;<code>str, None</code></dt>
<dd>This is the format of the label to return for each . If None, it will return
the taxon's name</dd>
<dt><strong><code>vmin</code></strong>, <strong><code>vmax</code></strong> :&ensp;<code>float</code></dt>
<dd>These are the maximum and minimum values to plot the abundance of</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def visualize(self, basepath: str, section: str=&#39;posterior&#39;, taxa_formatter: str=&#39;%(name)s&#39;, 
    vmin: float=None, vmax: float=None):
    &#39;&#39;&#39;Plot the posterior for each filtering object

    Parameters
    ----------
    basepath : str
        This is the loction to write the files to
    section : str
        Section of the trace to compute on. Options:
            &#39;posterior&#39; : posterior samples
            &#39;burnin&#39; : burn-in samples
            &#39;entire&#39; : both burn-in and posterior samples
    taxa_formatter : str, None
        This is the format of the label to return for each . If None, it will return
        the taxon&#39;s name
    vmin, vmax : float
        These are the maximum and minimum values to plot the abundance of
    &#39;&#39;&#39;
    for ridx, replicate in enumerate(self.x.value):
        subj = self.G.data.subjects.iloc(ridx)
        path = os.path.join(basepath, &#39;Subject_{}&#39;.format(subj.name))
        os.makedirs(path, exist_ok=True)
        self.x.visualize(ridx=ridx, section=section, basepath=path, 
            taxa_formatter=taxa_formatter)</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="mdsine2.pylab.graph.Node" href="pylab/graph.html#mdsine2.pylab.graph.Node">Node</a></b></code>:
<ul class="hlist">
<li><code><a title="mdsine2.pylab.graph.Node.add_child" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_child">add_child</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.add_parent" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_parent">add_parent</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.add_prior" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_prior">add_prior</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.add_undirected" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_undirected">add_undirected</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.degree" href="pylab/graph.html#mdsine2.pylab.graph.Node.degree">degree</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.delete" href="pylab/graph.html#mdsine2.pylab.graph.BaseNode.delete">delete</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.get_adjacent_keys" href="pylab/graph.html#mdsine2.pylab.graph.Node.get_adjacent_keys">get_adjacent_keys</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.load" href="pylab/base.html#mdsine2.pylab.base.Saveable.load">load</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.metropolis" href="pylab/graph.html#mdsine2.pylab.graph.Node.metropolis">metropolis</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.save" href="pylab/base.html#mdsine2.pylab.base.Saveable.save">save</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.set_save_location" href="pylab/base.html#mdsine2.pylab.base.Saveable.set_save_location">set_save_location</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="mdsine2.posterior.GLVParameters"><code class="flex name class">
<span>class <span class="ident">GLVParameters</span></span>
<span>(</span><span>growth: <a title="mdsine2.posterior.Growth" href="#mdsine2.posterior.Growth">Growth</a>, self_interactions: <a title="mdsine2.posterior.SelfInteractions" href="#mdsine2.posterior.SelfInteractions">SelfInteractions</a>, interactions: <a title="mdsine2.posterior.ClusterInteractionValue" href="#mdsine2.posterior.ClusterInteractionValue">ClusterInteractionValue</a>, pert_mag: <a title="mdsine2.posterior.PerturbationMagnitudes" href="#mdsine2.posterior.PerturbationMagnitudes">PerturbationMagnitudes</a>, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>This is the posterior of the regression coefficients.
The current posterior assumes a prior mean of 0.</p>
<p>This class samples the growth, self-interactions, and cluster
interactions jointly.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>growth</code></strong> :&ensp;<code>posterior.Growth</code></dt>
<dd>This is the class that has the growth variables</dd>
<dt><strong><code>self_interactions</code></strong> :&ensp;<code>posterior.SelfInteractions</code></dt>
<dd>The self interaction terms for the Taxa</dd>
<dt><strong><code>interactions</code></strong> :&ensp;<code><a title="mdsine2.posterior.ClusterInteractionValue" href="#mdsine2.posterior.ClusterInteractionValue">ClusterInteractionValue</a></code></dt>
<dd>These are the cluster interaction values</dd>
<dt><strong><code>pert_mag</code></strong> :&ensp;<code><a title="mdsine2.posterior.PerturbationMagnitudes" href="#mdsine2.posterior.PerturbationMagnitudes">PerturbationMagnitudes</a>, None</code></dt>
<dd>These are the magnitudes of the perturbation parameters (per clsuter)
Set to None if there are no perturbations</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class GLVParameters(pl.variables.MVN):
    &#39;&#39;&#39;This is the posterior of the regression coefficients.
    The current posterior assumes a prior mean of 0.

    This class samples the growth, self-interactions, and cluster
    interactions jointly.

    Parameters
    ----------
    growth : posterior.Growth
        This is the class that has the growth variables
    self_interactions : posterior.SelfInteractions
        The self interaction terms for the Taxa
    interactions : ClusterInteractionValue
        These are the cluster interaction values
    pert_mag : PerturbationMagnitudes, None
        These are the magnitudes of the perturbation parameters (per clsuter)
        Set to None if there are no perturbations
    &#39;&#39;&#39;
    def __init__(self, growth: Growth, self_interactions: SelfInteractions, 
        interactions: ClusterInteractionValue, pert_mag: &#34;PerturbationMagnitudes&#34;, **kwargs):

        if not issubclass(growth.__class__, Growth):
            raise ValueError(&#39;`growth` ({}) must be a subclass of the Growth &#39; \
                &#39;class&#39;.format(type(growth)))
        if not issubclass(self_interactions.__class__, SelfInteractions):
            raise ValueError(&#39;`self_interactions` ({}) must be a subclass of the SelfInteractions &#39; \
                &#39;class&#39;.format(type(self_interactions)))
        if not issubclass(interactions.__class__, ClusterInteractionValue):
            raise ValueError(&#39;`interactions` ({}) must be a subclass of the Interactions &#39; \
                &#39;class&#39;.format(type(interactions)))
        if pert_mag is not None:
            if not issubclass(pert_mag.__class__, PerturbationMagnitudes):
                raise ValueError(&#39;`pert_mag` ({}) must be a subclass of the PerturbationMagnitudes &#39; \
                    &#39;class&#39;.format(type(pert_mag)))


        kwargs[&#39;name&#39;] = STRNAMES.GLV_PARAMETERS
        pl.variables.MVN.__init__(self, mean=None, cov=None, dtype=float, **kwargs)

        self.n_taxa = self.G.data.n_taxa
        self.growth = growth
        self.self_interactions = self_interactions
        self.interactions = interactions
        self.pert_mag = pert_mag
        self.clustering = interactions.clustering

        # These serve no functional purpose but we do it so that they are
        # connected in the graph structure. Each of these should have their
        # prior already initialized
        self.add_parent(self.growth)
        self.add_parent(self.self_interactions)
        self.add_parent(self.interactions)

    def __str__(self) -&gt; str:
        &#39;&#39;&#39;Make it more readable
        &#39;&#39;&#39;
        try:
            a = &#39;Growth:\n{}\nSelf Interactions:\n{}\nInteractions:\n{}\nPerturbations:\n{}\n&#39; \
                &#39;Acceptances:\n{}&#39;.format(
                self.growth.value, self.self_interactions.value,
                str(self.G[STRNAMES.CLUSTER_INTERACTION_VALUE]),
                str(self.pert_mag), np.mean(
                    self.acceptances[ np.max([self.sample_iter-50, 0]):self.sample_iter], axis=0))
        except:
            a = &#39;Growth:\n{}\nSelf Interactions:\n{}\nInteractions:\n{}\nPerturbations:\n{}&#39;.format(
                self.growth.value, self.self_interactions.value,
                str(self.G[STRNAMES.CLUSTER_INTERACTION_VALUE]),
                str(self.pert_mag))
        return a

    def initialize(self, update_jointly_pert_inter: bool):
        &#39;&#39;&#39;The interior objects are initialized by themselves. Define which variables
        get updated together.

        Note that the interactions and perturbations will always be updated before the
        growth rates and the self-interactions

        Interactions and perturbations
        ------------------------------
        These are conjugate and have a normal prior. If these are said to be updated
        jointly then we can sample directly with Gibbs sampling.

        Growths and self-interactions
        -----------------------------
        These are conjugate and have a truncated normal prior. If they are set to be 
        updated together, then we must do MH because we cannot sample from a truncated
        multivariate gaussian.

        Parameters
        ----------
        update_jointly_pert_inter : bool
            If True, update the interactions and the perturbations jointly.
            If False, update the interactions and perturbations separately - you
            randomly choose which one to update first.
        &#39;&#39;&#39;
        self._there_are_perturbations = self.G.perturbations is not None
        if not pl.isbool(update_jointly_pert_inter):
            raise TypeError(&#39;`update_jointly_pert_inter` ({}) must be a bool&#39;.format(
                type(update_jointly_pert_inter)))

        self.update_jointly_pert_inter = update_jointly_pert_inter
        self.sample_iter = 0
                
    # @profile
    def asarray(self) -&gt; np.ndarray:
        &#39;&#39;&#39;Builds the full regression coefficient vector.
        &#39;&#39;&#39;
        # build the entire thing
        a = np.append(self.growth.value, self.self_interactions.value)
        a = np.append(a, self.interactions.obj.get_values(use_indicators=True))
        return a

    def update(self):
        &#39;&#39;&#39;Either updated jointly using multivariate normal or update independently
        using truncated normal distributions for growth and self-interactions.

        Always update the one that the interactions is in first
        &#39;&#39;&#39;
        self._update_perts_and_inter()
        if self._there_are_perturbations:
            self.G.data.design_matrices[STRNAMES.GROWTH_VALUE].build_with_perturbations()
        
        # Update growth and self-interactions
        if pl.random.misc.fast_sample_standard_uniform():
            self.growth.update()
            self.self_interactions.update()
        else:
            self.self_interactions.update()
            self.growth.update()

        self.sample_iter += 1

        if self._there_are_perturbations:
            # If there are perturbations then we need to update their
            # matrix because the growths changed
            self.G.data.design_matrices[STRNAMES.PERT_VALUE].update_values()

    # @profile
    def _update_perts_and_inter(self):
        &#39;&#39;&#39;Update the with Gibbs sampling of a multivariate normal.

        Parameters
        ----------
        args : tuple
            This is a tuple of length &gt; 1 that holds the variables on what to update
            together
        &#39;&#39;&#39;
        if not self.update_jointly_pert_inter:
            # Update separately
            # -----------------
            if pl.random.misc.fast_sample_standard_uniform() &lt; 0.5:
                self.G[STRNAMES.CLUSTER_INTERACTION_VALUE].update()
                if self._there_are_perturbations:
                    self.G[STRNAMES.PERT_VALUE].update()
            else:
                if self._there_are_perturbations:
                    self.G[STRNAMES.PERT_VALUE].update()
                self.G[STRNAMES.CLUSTER_INTERACTION_VALUE].update()
        else:
            # Update jointly
            # --------------
            rhs = [] # Right hand side. This is what we are marginalizing over
            lhs = [] # Left hand side. This is what we are conditioning on
            if self.interactions.obj.sample_iter &gt;= self.interactions.delay:
                rhs.append(STRNAMES.CLUSTER_INTERACTION_VALUE)
            else:
                lhs.append(STRNAMES.CLUSTER_INTERACTION_VALUE)
            if self._there_are_perturbations:
                if self.pert_mag.sample_iter &gt;= self.pert_mag.delay:
                    rhs.append(STRNAMES.PERT_VALUE)
                else:
                    lhs.append(STRNAMES.PERT_VALUE)

            if len(rhs) == 0:
                return

            lhs += [STRNAMES.GROWTH_VALUE, STRNAMES.SELF_INTERACTION_VALUE]
            X = self.G.data.construct_rhs(keys=rhs)
            if X.shape[1] == 0:
                logging.info(&#39;No columns, skipping&#39;)
                return
            y = self.G.data.construct_lhs(keys=lhs,
                kwargs_dict={STRNAMES.GROWTH_VALUE:{&#39;with_perturbations&#39;: False}})

            process_prec = self.G[STRNAMES.PROCESSVAR].build_matrix(
                cov=False, sparse=True)
            prior_prec = build_prior_covariance(G=self.G, cov=False,
                order=rhs, sparse=True)
            prior_means = build_prior_mean(G=self.G,order=rhs).reshape(-1,1)

            # Make the prior covariance matrix and process varaince
            prec = X.T @ process_prec @ X + prior_prec
            self.cov.value = pinv(prec, self)
            self.mean.value = np.asarray(self.cov.value @ (X.T @ process_prec.dot(y) + \
                prior_prec @ prior_means)).ravel()

            # sample posterior jointly and then assign the values to each coefficient
            # type, respectfully
            try:
                value = self.sample()
            except:
                logging.critical(&#39;failed here, updating separately&#39;)
                self.pert_mag.update()
                self.interactions.update()
                return

            i = 0
            if STRNAMES.CLUSTER_INTERACTION_VALUE in rhs:
                l = self.interactions.obj.num_pos_indicators()
                self.interactions.value = value[:l]
                self.interactions.set_values(arr=value[:l], use_indicators=True)
                self.interactions.update_str()
                i += l
            if self._there_are_perturbations:
                if STRNAMES.PERT_VALUE in rhs:
                    self.pert_mag.value = value[i:]
                    self.pert_mag.set_values(arr=value[i:], use_indicators=True)
                    self.pert_mag.update_str()
                    self.G.data.design_matrices[STRNAMES.GROWTH_VALUE].update_value()
                    # self.G.data.design_matrices[STRNAMES.PERT_VALUE].build()

    def add_trace(self):
        &#39;&#39;&#39;Trace values for growth, self-interactions, and cluster interaction values
        &#39;&#39;&#39;
        self.growth.add_trace()
        self.self_interactions.add_trace()
        self.interactions.add_trace()
        if self._there_are_perturbations:
            self.pert_mag.add_trace()

    def set_trace(self):
        self.growth.set_trace()
        self.self_interactions.set_trace()
        self.interactions.set_trace()
        if self._there_are_perturbations:
            self.pert_mag.set_trace()</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="mdsine2.pylab.variables.MVN" href="pylab/variables.html#mdsine2.pylab.variables.MVN">MVN</a></li>
<li><a title="mdsine2.pylab.variables.Variable" href="pylab/variables.html#mdsine2.pylab.variables.Variable">Variable</a></li>
<li><a title="mdsine2.pylab.graph.Node" href="pylab/graph.html#mdsine2.pylab.graph.Node">Node</a></li>
<li><a title="mdsine2.pylab.graph.BaseNode" href="pylab/graph.html#mdsine2.pylab.graph.BaseNode">BaseNode</a></li>
<li><a title="mdsine2.pylab.base.Saveable" href="pylab/base.html#mdsine2.pylab.base.Saveable">Saveable</a></li>
<li>mdsine2.pylab.variables._BaseArithmeticClass</li>
<li><a title="mdsine2.pylab.base.Traceable" href="pylab/base.html#mdsine2.pylab.base.Traceable">Traceable</a></li>
<li>mdsine2.pylab.variables._RandomBase</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="mdsine2.posterior.GLVParameters.add_trace"><code class="name flex">
<span>def <span class="ident">add_trace</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Trace values for growth, self-interactions, and cluster interaction values</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add_trace(self):
    &#39;&#39;&#39;Trace values for growth, self-interactions, and cluster interaction values
    &#39;&#39;&#39;
    self.growth.add_trace()
    self.self_interactions.add_trace()
    self.interactions.add_trace()
    if self._there_are_perturbations:
        self.pert_mag.add_trace()</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.GLVParameters.asarray"><code class="name flex">
<span>def <span class="ident">asarray</span></span>(<span>self) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Builds the full regression coefficient vector.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def asarray(self) -&gt; np.ndarray:
    &#39;&#39;&#39;Builds the full regression coefficient vector.
    &#39;&#39;&#39;
    # build the entire thing
    a = np.append(self.growth.value, self.self_interactions.value)
    a = np.append(a, self.interactions.obj.get_values(use_indicators=True))
    return a</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.GLVParameters.initialize"><code class="name flex">
<span>def <span class="ident">initialize</span></span>(<span>self, update_jointly_pert_inter: bool)</span>
</code></dt>
<dd>
<div class="desc"><p>The interior objects are initialized by themselves. Define which variables
get updated together.</p>
<p>Note that the interactions and perturbations will always be updated before the
growth rates and the self-interactions</p>
<h2 id="interactions-and-perturbations">Interactions And Perturbations</h2>
<p>These are conjugate and have a normal prior. If these are said to be updated
jointly then we can sample directly with Gibbs sampling.</p>
<h2 id="growths-and-self-interactions">Growths and self-interactions</h2>
<p>These are conjugate and have a truncated normal prior. If they are set to be
updated together, then we must do MH because we cannot sample from a truncated
multivariate gaussian.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>update_jointly_pert_inter</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, update the interactions and the perturbations jointly.
If False, update the interactions and perturbations separately - you
randomly choose which one to update first.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def initialize(self, update_jointly_pert_inter: bool):
    &#39;&#39;&#39;The interior objects are initialized by themselves. Define which variables
    get updated together.

    Note that the interactions and perturbations will always be updated before the
    growth rates and the self-interactions

    Interactions and perturbations
    ------------------------------
    These are conjugate and have a normal prior. If these are said to be updated
    jointly then we can sample directly with Gibbs sampling.

    Growths and self-interactions
    -----------------------------
    These are conjugate and have a truncated normal prior. If they are set to be 
    updated together, then we must do MH because we cannot sample from a truncated
    multivariate gaussian.

    Parameters
    ----------
    update_jointly_pert_inter : bool
        If True, update the interactions and the perturbations jointly.
        If False, update the interactions and perturbations separately - you
        randomly choose which one to update first.
    &#39;&#39;&#39;
    self._there_are_perturbations = self.G.perturbations is not None
    if not pl.isbool(update_jointly_pert_inter):
        raise TypeError(&#39;`update_jointly_pert_inter` ({}) must be a bool&#39;.format(
            type(update_jointly_pert_inter)))

    self.update_jointly_pert_inter = update_jointly_pert_inter
    self.sample_iter = 0</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.GLVParameters.update"><code class="name flex">
<span>def <span class="ident">update</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Either updated jointly using multivariate normal or update independently
using truncated normal distributions for growth and self-interactions.</p>
<p>Always update the one that the interactions is in first</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update(self):
    &#39;&#39;&#39;Either updated jointly using multivariate normal or update independently
    using truncated normal distributions for growth and self-interactions.

    Always update the one that the interactions is in first
    &#39;&#39;&#39;
    self._update_perts_and_inter()
    if self._there_are_perturbations:
        self.G.data.design_matrices[STRNAMES.GROWTH_VALUE].build_with_perturbations()
    
    # Update growth and self-interactions
    if pl.random.misc.fast_sample_standard_uniform():
        self.growth.update()
        self.self_interactions.update()
    else:
        self.self_interactions.update()
        self.growth.update()

    self.sample_iter += 1

    if self._there_are_perturbations:
        # If there are perturbations then we need to update their
        # matrix because the growths changed
        self.G.data.design_matrices[STRNAMES.PERT_VALUE].update_values()</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="mdsine2.pylab.variables.MVN" href="pylab/variables.html#mdsine2.pylab.variables.MVN">MVN</a></b></code>:
<ul class="hlist">
<li><code><a title="mdsine2.pylab.variables.MVN.T" href="pylab/variables.html#mdsine2.pylab.variables.Variable.T">T</a></code></li>
<li><code><a title="mdsine2.pylab.variables.MVN.add_child" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_child">add_child</a></code></li>
<li><code><a title="mdsine2.pylab.variables.MVN.add_parent" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_parent">add_parent</a></code></li>
<li><code><a title="mdsine2.pylab.variables.MVN.add_prior" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_prior">add_prior</a></code></li>
<li><code><a title="mdsine2.pylab.variables.MVN.add_undirected" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_undirected">add_undirected</a></code></li>
<li><code><a title="mdsine2.pylab.variables.MVN.degree" href="pylab/graph.html#mdsine2.pylab.graph.Node.degree">degree</a></code></li>
<li><code><a title="mdsine2.pylab.variables.MVN.delete" href="pylab/graph.html#mdsine2.pylab.graph.BaseNode.delete">delete</a></code></li>
<li><code><a title="mdsine2.pylab.variables.MVN.get_adjacent_keys" href="pylab/graph.html#mdsine2.pylab.graph.Node.get_adjacent_keys">get_adjacent_keys</a></code></li>
<li><code><a title="mdsine2.pylab.variables.MVN.get_iter" href="pylab/base.html#mdsine2.pylab.base.Traceable.get_iter">get_iter</a></code></li>
<li><code><a title="mdsine2.pylab.variables.MVN.get_trace_from_disk" href="pylab/base.html#mdsine2.pylab.base.Traceable.get_trace_from_disk">get_trace_from_disk</a></code></li>
<li><code><a title="mdsine2.pylab.variables.MVN.load" href="pylab/base.html#mdsine2.pylab.base.Saveable.load">load</a></code></li>
<li><code><a title="mdsine2.pylab.variables.MVN.metropolis" href="pylab/graph.html#mdsine2.pylab.graph.Node.metropolis">metropolis</a></code></li>
<li><code><a title="mdsine2.pylab.variables.MVN.overwrite_entire_trace_on_disk" href="pylab/base.html#mdsine2.pylab.base.Traceable.overwrite_entire_trace_on_disk">overwrite_entire_trace_on_disk</a></code></li>
<li><code><a title="mdsine2.pylab.variables.MVN.remove_local_trace" href="pylab/variables.html#mdsine2.pylab.variables.Variable.remove_local_trace">remove_local_trace</a></code></li>
<li><code><a title="mdsine2.pylab.variables.MVN.sample" href="pylab/variables.html#mdsine2.pylab.variables.MVN.sample">sample</a></code></li>
<li><code><a title="mdsine2.pylab.variables.MVN.save" href="pylab/base.html#mdsine2.pylab.base.Saveable.save">save</a></code></li>
<li><code><a title="mdsine2.pylab.variables.MVN.set_save_location" href="pylab/base.html#mdsine2.pylab.base.Saveable.set_save_location">set_save_location</a></code></li>
<li><code><a title="mdsine2.pylab.variables.MVN.set_trace" href="pylab/base.html#mdsine2.pylab.base.Traceable.set_trace">set_trace</a></code></li>
<li><code><a title="mdsine2.pylab.variables.MVN.set_value_shape" href="pylab/variables.html#mdsine2.pylab.variables.Variable.set_value_shape">set_value_shape</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="mdsine2.posterior.Growth"><code class="flex name class">
<span>class <span class="ident">Growth</span></span>
<span>(</span><span>prior: <a title="mdsine2.pylab.variables.TruncatedNormal" href="pylab/variables.html#mdsine2.pylab.variables.TruncatedNormal">TruncatedNormal</a>, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Growth values of Lotka-Voltera</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>prior</code></strong> :&ensp;<code>mdsine2.variables.TruncatedNormal</code></dt>
<dd>Prior distribution</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Growth(pl.variables.TruncatedNormal):
    &#39;&#39;&#39;Growth values of Lotka-Voltera

    Parameters
    ----------
    prior : mdsine2.variables.TruncatedNormal
        Prior distribution
    &#39;&#39;&#39;
    def __init__(self, prior: variables.TruncatedNormal, **kwargs):
        kwargs[&#39;name&#39;] = STRNAMES.GROWTH_VALUE
        pl.variables.TruncatedNormal.__init__(self, loc=None, scale2=None, low=0.,
            high=float(&#39;inf&#39;), dtype=float, **kwargs)
        self.set_value_shape(shape=(len(self.G.data.taxa),))
        self.add_prior(prior)
        self.delay = 0
        self._initialized = False

    def __str__(self) -&gt; str:
        return str(self.value)

    def update_str(self):
        return

    def initialize(self, value_option: str, truncation_settings: Union[str, Tuple[float, float]],
        value: np.ndarray=None, delay: int=0):
        &#39;&#39;&#39;Initialize the growth values and hyperparamters

        Parameters
        ----------
        value_option : str
            - How to initialize the values. Options:
                - &#39;manual&#39;
                    - Set the values manually. `value` must also be specified.
                - &#39;linear regression&#39;
                    - Set the values of the growth using linear regression
                - &#39;ones&#39;
                    - Set all of the values to 1.
                - &#39;auto&#39;
                    - Alias for &#39;ones&#39;
                - &#39;prior-mean&#39;
                    - Set to the mean of the prior
        value : array
            Only necessary if `value_option` is &#39;manual&#39;
        delay : int
            How many MCMC iterations to delay starting to update
        truncation_settings : str, tuple, None
            - These are the settings of how you set the upper and lower limit of the
              truncated distribution. If it is None, it will default to &#39;standard&#39;. Options:
                - &#39;positive&#39;, None
                    - Only constrains the values to being positive
                    - low=0., high=float(&#39;inf&#39;)
                - &#39;in-vivo&#39;, &#39;auto&#39;
                    - Tighter constraint on the growth values.
                    - low=0.1, high=ln(10)
                    - These values have the following meaning
                        - The slowest growing microbe will grow an order of magnitude in ~10 days
                        - The fastest growing microbe will grow an order of magnitude in 1 day
                - tuple(low, high)
                    - These are manually specified values for the low and high
        &#39;&#39;&#39;
        self._initialized = True
        self._there_are_perturbations = self.G.perturbations is not None
        if not pl.isint(delay):
            raise TypeError(&#39;`delay` ({}) must be an int&#39;.format(type(delay)))
        if delay &lt; 0:
            raise ValueError(&#39;`delay` ({}) must be &gt;= 0&#39;.format(delay))
        self.delay = delay

        # Truncation settings
        if truncation_settings is None:
            truncation_settings = &#39;positive&#39;
        if pl.isstr(truncation_settings):
            if truncation_settings == &#39;positive&#39;:
                self.low = 0.
                self.high = float(&#39;inf&#39;)
            elif truncation_settings in [&#39;in-vivo&#39;, &#39;auto&#39;]:
                self.low = 0.1
                self.high = math.log(10)
            else:
                raise ValueError(&#39;`truncation_settings` ({}) not recognized&#39;.format(
                    truncation_settings))
        elif pl.istuple(truncation_settings):
            if len(truncation_settings) != 2:
                raise ValueError(&#39;If `truncation_settings` is a tuple, it must have a &#39; \
                    &#39;length of 2 ({})&#39;.format(len(truncation_settings)))
            l,h = truncation_settings

            if (not pl.isnumeric(l)) or (not pl.isnumeric(h)):
                raise TypeError(&#39;`low` ({}) and `high` ({}) must be numerics&#39;.format(
                    type(l), type(h)))
            if l &lt; 0 or h &lt; 0:
                raise ValueError(&#39;`low` ({}) and `high` ({}) must be &gt;= 0&#39;.format(l,h))
            if h &lt;= l:
                raise ValueError(&#39;`low` ({}) must be strictly less than high ({})&#39;.format(l,h))
            self.high = h
            self.low = l
        else:
            raise TypeError(&#39;`truncation_settings` ({}) type not recognized&#39;)

        # Setting the value
        if not pl.isstr(value_option):
            raise TypeError(&#39;`value_option` ({}) must be a str&#39;.format(type(value_option)))
        if value_option == &#39;manual&#39;:
            if not pl.isarray(value):
                value = np.ones(len(self.G.data.taxa))*value
            if len(value) != self.G.data.n_taxa:
                raise ValueError(&#39;`value` ({}) must be ({}) long&#39;.format(
                    len(value), len(self.G.data.taxa)))
            self.value = value
        elif value_option == &#39;linear-regression&#39;:
            rhs = [
                STRNAMES.GROWTH_VALUE,
                STRNAMES.SELF_INTERACTION_VALUE
            ]
            lhs = []
            X = self.G.data.construct_rhs(
                keys=rhs, kwargs_dict={STRNAMES.GROWTH_VALUE:{&#39;with_perturbations&#39;:False}},
                index_out_perturbations=True)
            y = self.G.data.construct_lhs(keys=lhs, index_out_perturbations=True)

            prec = X.T @ X
            cov = pinv(prec, self)
            mean = (cov @ X.transpose().dot(y)).ravel()
            self.value = np.absolute(mean[:len(self.G.data.taxa)])
        elif value_option in [&#39;auto&#39;, &#39;ones&#39;]:
            self.value = np.ones(len(self.G.data.taxa), dtype=float)
        elif value_option == &#39;prior-mean&#39;:
            self.value = self.prior.mean() * np.ones(self.G.data.n_taxa)
        else:
            raise ValueError(&#39;`value_option` ({}) not recognized&#39;.format(value_option))

        logging.info(&#39;Growth value initialization: {}&#39;.format(self.value))
        logging.info(&#39;Growth prior mean: {}&#39;.format(self.prior.loc.value))
        logging.info(&#39;Growth truncation settings: {}&#39;.format((self.low, self.high)))

    def update(self):
        &#39;&#39;&#39;Update the values using a truncated normal
        &#39;&#39;&#39;
        if self.sample_iter &lt; self.delay:
            return

        self.calculate_posterior()
        self.sample()

        if not pl.isarray(self.value):
            # This will happen if there is 1 Taxa
            self.value = np.array([self.value])

        if np.any(np.isnan(self.value)):
            logging.critical(&#39;mean: {}&#39;.format(self.loc.value))
            logging.critical(&#39;var: {}&#39;.format(self.scale2.value))
            logging.critical(&#39;value: {}&#39;.format(self.value))
            raise ValueError(&#39;`Values in {} are nan: {}&#39;.format(self.name, self.value))

        if self._there_are_perturbations:
            # If there are perturbations then we need to update their
            # matrix because the growths changed
            self.G.data.design_matrices[STRNAMES.PERT_VALUE].update_values()

    def calculate_posterior(self):
        rhs = [STRNAMES.GROWTH_VALUE]
        if self._there_are_perturbations:
            lhs = [
                STRNAMES.SELF_INTERACTION_VALUE,
                STRNAMES.CLUSTER_INTERACTION_VALUE]
        else:
            lhs = [
                STRNAMES.SELF_INTERACTION_VALUE,
                STRNAMES.CLUSTER_INTERACTION_VALUE]
        X = self.G.data.construct_rhs(keys=rhs,
            kwargs_dict={STRNAMES.GROWTH_VALUE:{
                &#39;with_perturbations&#39;:self._there_are_perturbations}})
        y = self.G.data.construct_lhs(keys=lhs)
        # X = X.toarray()

        process_prec = self.G[STRNAMES.PROCESSVAR].build_matrix(
            cov=False, sparse=True)

        prior_prec = build_prior_covariance(G=self.G, cov=False,
            order=rhs, sparse=True)
        prior_mean = build_prior_mean(G=self.G, order=rhs).reshape(-1,1)
        pm = prior_prec @ prior_mean

        prec = X.T @ process_prec @ X + prior_prec
        cov = pinv(prec, self)

        self.loc.value = np.asarray(cov @ (X.T @ process_prec.dot(y) + pm)).ravel()
        self.scale2.value = np.diag(cov)

    def visualize(self, basepath: str, section: str=&#39;posterior&#39;, taxa_formatter: str=&#39;%(name)s&#39;, 
        true_value: np.ndarray=None) -&gt; pd.DataFrame:
        &#39;&#39;&#39;Render the traces in the folder `basepath`. Makes a `pandas.DataFrame` table
        where the index is the Taxa name in `taxa_formatter` and the columns are
        `mean`, `median`, `25th percentile`, `75th` percentile`.

        Parameters
        ----------
        basepath : str
            This is the loction to write the files to
        section : str
            Section of the trace to compute on. Options:
                &#39;posterior&#39; : posterior samples
                &#39;burnin&#39; : burn-in samples
                &#39;entire&#39; : both burn-in and posterior samples
        taxa_formatter : str, None
            This is the format of the label to return for each Taxa. If None, it will return
            the taxa name
        true_value : np.ndarray
            Ground truth values of the variable

        Returns
        -------
        pandas.DataFrame
        &#39;&#39;&#39;
        if not self.G.inference.tracer.is_being_traced(self):
            logging.info(&#39;`{}` not learned\n\tValue: {}\n&#39;.format(self.name, self.value))
            return pd.DataFrame()

        taxa = self.G.data.subjects.taxa
        summ = pl.summary(self, section=section)
        data = []
        index = []
        columns = [&#39;name&#39;] + [k for k in summ]

        for idx in range(len(taxa)):
            if taxa_formatter is not None:
                prefix = pl.taxaname_formatter(format=taxa_formatter, taxon=taxa[idx], taxa=taxa)
            else:
                prefix = taxa[idx].name
            index.append(taxa[idx].name)
            temp = [prefix]
            for _,v in summ.items():
                temp.append(v[idx])
            data.append(temp)
        
        df = pd.DataFrame(data, columns=columns, index=index)

        if section == &#39;posterior&#39;:
            len_posterior = self.G.inference.sample_iter + 1 - self.G.inference.burnin
        elif section == &#39;burnin&#39;:
            len_posterior = self.G.inference.burnin
        else:
            len_posterior = self.G.inference.sample_iter + 1

        print(self.G.inference.sample_iter)
        print(self.G.inference.burnin)
        print(len_posterior)

        # Plot the prior on top of the posterior
        if self.G.tracer.is_being_traced(STRNAMES.PRIOR_MEAN_GROWTH):
            prior_mean_trace = self.G[STRNAMES.PRIOR_MEAN_GROWTH].get_trace_from_disk(
                    section=section)
        else:
            prior_mean_trace = self.prior.loc.value * np.ones(len_posterior, dtype=float)
        if self.G.tracer.is_being_traced(STRNAMES.PRIOR_VAR_GROWTH):
            prior_std_trace = np.sqrt(
                self.G[STRNAMES.PRIOR_VAR_GROWTH].get_trace_from_disk(section=section))
        else:
            prior_std_trace = np.sqrt(self.prior.scale2.value) * np.ones(len_posterior, dtype=float)

        for idx in range(len(taxa)):
            fig = plt.figure()
            ax_posterior = fig.add_subplot(1,2,1)
            visualization.render_trace(var=self, idx=idx, plt_type=&#39;hist&#39;,
                label=section, color=&#39;blue&#39;, ax=ax_posterior, section=section,
                include_burnin=True, rasterized=True)

            # Get the limits and only look at the posterior within 20% range +- of
            # this number
            low_x, high_x = ax_posterior.get_xlim()

            arr = np.zeros(len(prior_std_trace), dtype=float)
            for i in range(len(prior_std_trace)):
                arr[i] = pl.random.truncnormal.sample(loc=prior_mean_trace[i], scale=prior_std_trace[i], 
                    low=self.low, high=self.high)
            visualization.render_trace(var=arr, plt_type=&#39;hist&#39;, 
                label=&#39;prior&#39;, color=&#39;red&#39;, ax=ax_posterior, rasterized=True)

            if true_value is not None:
                ax_posterior.axvline(x=true_value[idx], color=&#39;red&#39;, alpha=0.65, 
                    label=&#39;True Value&#39;)

            ax_posterior.legend()
            ax_posterior.set_xlim(left=low_x*.8, right=high_x*1.2)

            # plot the trace
            ax_trace = fig.add_subplot(1,2,2)
            visualization.render_trace(var=self, idx=idx, plt_type=&#39;trace&#39;, 
                ax=ax_trace, section=section, include_burnin=True, rasterized=True)

            if true_value is not None:
                ax_trace.axhline(y=true_value[idx], color=&#39;red&#39;, alpha=0.65, 
                    label=&#39;True Value&#39;)
                ax_trace.legend()

            fig.suptitle(&#39;{}&#39;.format(index[idx]))
            fig.tight_layout()
            fig.subplots_adjust(top=0.85)
            plt.savefig(os.path.join(basepath, &#39;{}.pdf&#39;.format(taxa[idx].name)))
            plt.close()

        return df</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="mdsine2.pylab.variables.TruncatedNormal" href="pylab/variables.html#mdsine2.pylab.variables.TruncatedNormal">TruncatedNormal</a></li>
<li><a title="mdsine2.pylab.variables.Variable" href="pylab/variables.html#mdsine2.pylab.variables.Variable">Variable</a></li>
<li><a title="mdsine2.pylab.graph.Node" href="pylab/graph.html#mdsine2.pylab.graph.Node">Node</a></li>
<li><a title="mdsine2.pylab.graph.BaseNode" href="pylab/graph.html#mdsine2.pylab.graph.BaseNode">BaseNode</a></li>
<li><a title="mdsine2.pylab.base.Saveable" href="pylab/base.html#mdsine2.pylab.base.Saveable">Saveable</a></li>
<li>mdsine2.pylab.variables._BaseArithmeticClass</li>
<li><a title="mdsine2.pylab.base.Traceable" href="pylab/base.html#mdsine2.pylab.base.Traceable">Traceable</a></li>
<li>mdsine2.pylab.variables._RandomBase</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="mdsine2.posterior.Growth.calculate_posterior"><code class="name flex">
<span>def <span class="ident">calculate_posterior</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def calculate_posterior(self):
    rhs = [STRNAMES.GROWTH_VALUE]
    if self._there_are_perturbations:
        lhs = [
            STRNAMES.SELF_INTERACTION_VALUE,
            STRNAMES.CLUSTER_INTERACTION_VALUE]
    else:
        lhs = [
            STRNAMES.SELF_INTERACTION_VALUE,
            STRNAMES.CLUSTER_INTERACTION_VALUE]
    X = self.G.data.construct_rhs(keys=rhs,
        kwargs_dict={STRNAMES.GROWTH_VALUE:{
            &#39;with_perturbations&#39;:self._there_are_perturbations}})
    y = self.G.data.construct_lhs(keys=lhs)
    # X = X.toarray()

    process_prec = self.G[STRNAMES.PROCESSVAR].build_matrix(
        cov=False, sparse=True)

    prior_prec = build_prior_covariance(G=self.G, cov=False,
        order=rhs, sparse=True)
    prior_mean = build_prior_mean(G=self.G, order=rhs).reshape(-1,1)
    pm = prior_prec @ prior_mean

    prec = X.T @ process_prec @ X + prior_prec
    cov = pinv(prec, self)

    self.loc.value = np.asarray(cov @ (X.T @ process_prec.dot(y) + pm)).ravel()
    self.scale2.value = np.diag(cov)</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.Growth.initialize"><code class="name flex">
<span>def <span class="ident">initialize</span></span>(<span>self, value_option: str, truncation_settings: Union[str, Tuple[float, float]], value: numpy.ndarray = None, delay: int = 0)</span>
</code></dt>
<dd>
<div class="desc"><p>Initialize the growth values and hyperparamters</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>value_option</code></strong> :&ensp;<code>str</code></dt>
<dd>
<ul>
<li>How to initialize the values. Options:<ul>
<li>'manual'<ul>
<li>Set the values manually. <code>value</code> must also be specified.</li>
</ul>
</li>
<li>'linear regression'<ul>
<li>Set the values of the growth using linear regression</li>
</ul>
</li>
<li>'ones'<ul>
<li>Set all of the values to 1.</li>
</ul>
</li>
<li>'auto'<ul>
<li>Alias for 'ones'</li>
</ul>
</li>
<li>'prior-mean'<ul>
<li>Set to the mean of the prior</li>
</ul>
</li>
</ul>
</li>
</ul>
</dd>
<dt><strong><code>value</code></strong> :&ensp;<code>array</code></dt>
<dd>Only necessary if <code>value_option</code> is 'manual'</dd>
<dt><strong><code>delay</code></strong> :&ensp;<code>int</code></dt>
<dd>How many MCMC iterations to delay starting to update</dd>
<dt><strong><code>truncation_settings</code></strong> :&ensp;<code>str, tuple, None</code></dt>
<dd>
<ul>
<li>These are the settings of how you set the upper and lower limit of the
truncated distribution. If it is None, it will default to 'standard'. Options:<ul>
<li>'positive', None<ul>
<li>Only constrains the values to being positive</li>
<li>low=0., high=float('inf')</li>
</ul>
</li>
<li>'in-vivo', 'auto'<ul>
<li>Tighter constraint on the growth values.</li>
<li>low=0.1, high=ln(10)</li>
<li>These values have the following meaning<ul>
<li>The slowest growing microbe will grow an order of magnitude in ~10 days</li>
<li>The fastest growing microbe will grow an order of magnitude in 1 day</li>
</ul>
</li>
</ul>
</li>
<li>tuple(low, high)<ul>
<li>These are manually specified values for the low and high</li>
</ul>
</li>
</ul>
</li>
</ul>
</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def initialize(self, value_option: str, truncation_settings: Union[str, Tuple[float, float]],
    value: np.ndarray=None, delay: int=0):
    &#39;&#39;&#39;Initialize the growth values and hyperparamters

    Parameters
    ----------
    value_option : str
        - How to initialize the values. Options:
            - &#39;manual&#39;
                - Set the values manually. `value` must also be specified.
            - &#39;linear regression&#39;
                - Set the values of the growth using linear regression
            - &#39;ones&#39;
                - Set all of the values to 1.
            - &#39;auto&#39;
                - Alias for &#39;ones&#39;
            - &#39;prior-mean&#39;
                - Set to the mean of the prior
    value : array
        Only necessary if `value_option` is &#39;manual&#39;
    delay : int
        How many MCMC iterations to delay starting to update
    truncation_settings : str, tuple, None
        - These are the settings of how you set the upper and lower limit of the
          truncated distribution. If it is None, it will default to &#39;standard&#39;. Options:
            - &#39;positive&#39;, None
                - Only constrains the values to being positive
                - low=0., high=float(&#39;inf&#39;)
            - &#39;in-vivo&#39;, &#39;auto&#39;
                - Tighter constraint on the growth values.
                - low=0.1, high=ln(10)
                - These values have the following meaning
                    - The slowest growing microbe will grow an order of magnitude in ~10 days
                    - The fastest growing microbe will grow an order of magnitude in 1 day
            - tuple(low, high)
                - These are manually specified values for the low and high
    &#39;&#39;&#39;
    self._initialized = True
    self._there_are_perturbations = self.G.perturbations is not None
    if not pl.isint(delay):
        raise TypeError(&#39;`delay` ({}) must be an int&#39;.format(type(delay)))
    if delay &lt; 0:
        raise ValueError(&#39;`delay` ({}) must be &gt;= 0&#39;.format(delay))
    self.delay = delay

    # Truncation settings
    if truncation_settings is None:
        truncation_settings = &#39;positive&#39;
    if pl.isstr(truncation_settings):
        if truncation_settings == &#39;positive&#39;:
            self.low = 0.
            self.high = float(&#39;inf&#39;)
        elif truncation_settings in [&#39;in-vivo&#39;, &#39;auto&#39;]:
            self.low = 0.1
            self.high = math.log(10)
        else:
            raise ValueError(&#39;`truncation_settings` ({}) not recognized&#39;.format(
                truncation_settings))
    elif pl.istuple(truncation_settings):
        if len(truncation_settings) != 2:
            raise ValueError(&#39;If `truncation_settings` is a tuple, it must have a &#39; \
                &#39;length of 2 ({})&#39;.format(len(truncation_settings)))
        l,h = truncation_settings

        if (not pl.isnumeric(l)) or (not pl.isnumeric(h)):
            raise TypeError(&#39;`low` ({}) and `high` ({}) must be numerics&#39;.format(
                type(l), type(h)))
        if l &lt; 0 or h &lt; 0:
            raise ValueError(&#39;`low` ({}) and `high` ({}) must be &gt;= 0&#39;.format(l,h))
        if h &lt;= l:
            raise ValueError(&#39;`low` ({}) must be strictly less than high ({})&#39;.format(l,h))
        self.high = h
        self.low = l
    else:
        raise TypeError(&#39;`truncation_settings` ({}) type not recognized&#39;)

    # Setting the value
    if not pl.isstr(value_option):
        raise TypeError(&#39;`value_option` ({}) must be a str&#39;.format(type(value_option)))
    if value_option == &#39;manual&#39;:
        if not pl.isarray(value):
            value = np.ones(len(self.G.data.taxa))*value
        if len(value) != self.G.data.n_taxa:
            raise ValueError(&#39;`value` ({}) must be ({}) long&#39;.format(
                len(value), len(self.G.data.taxa)))
        self.value = value
    elif value_option == &#39;linear-regression&#39;:
        rhs = [
            STRNAMES.GROWTH_VALUE,
            STRNAMES.SELF_INTERACTION_VALUE
        ]
        lhs = []
        X = self.G.data.construct_rhs(
            keys=rhs, kwargs_dict={STRNAMES.GROWTH_VALUE:{&#39;with_perturbations&#39;:False}},
            index_out_perturbations=True)
        y = self.G.data.construct_lhs(keys=lhs, index_out_perturbations=True)

        prec = X.T @ X
        cov = pinv(prec, self)
        mean = (cov @ X.transpose().dot(y)).ravel()
        self.value = np.absolute(mean[:len(self.G.data.taxa)])
    elif value_option in [&#39;auto&#39;, &#39;ones&#39;]:
        self.value = np.ones(len(self.G.data.taxa), dtype=float)
    elif value_option == &#39;prior-mean&#39;:
        self.value = self.prior.mean() * np.ones(self.G.data.n_taxa)
    else:
        raise ValueError(&#39;`value_option` ({}) not recognized&#39;.format(value_option))

    logging.info(&#39;Growth value initialization: {}&#39;.format(self.value))
    logging.info(&#39;Growth prior mean: {}&#39;.format(self.prior.loc.value))
    logging.info(&#39;Growth truncation settings: {}&#39;.format((self.low, self.high)))</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.Growth.update"><code class="name flex">
<span>def <span class="ident">update</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Update the values using a truncated normal</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update(self):
    &#39;&#39;&#39;Update the values using a truncated normal
    &#39;&#39;&#39;
    if self.sample_iter &lt; self.delay:
        return

    self.calculate_posterior()
    self.sample()

    if not pl.isarray(self.value):
        # This will happen if there is 1 Taxa
        self.value = np.array([self.value])

    if np.any(np.isnan(self.value)):
        logging.critical(&#39;mean: {}&#39;.format(self.loc.value))
        logging.critical(&#39;var: {}&#39;.format(self.scale2.value))
        logging.critical(&#39;value: {}&#39;.format(self.value))
        raise ValueError(&#39;`Values in {} are nan: {}&#39;.format(self.name, self.value))

    if self._there_are_perturbations:
        # If there are perturbations then we need to update their
        # matrix because the growths changed
        self.G.data.design_matrices[STRNAMES.PERT_VALUE].update_values()</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.Growth.update_str"><code class="name flex">
<span>def <span class="ident">update_str</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_str(self):
    return</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.Growth.visualize"><code class="name flex">
<span>def <span class="ident">visualize</span></span>(<span>self, basepath: str, section: str = 'posterior', taxa_formatter: str = '%(name)s', true_value: numpy.ndarray = None) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Render the traces in the folder <code>basepath</code>. Makes a <code>pandas.DataFrame</code> table
where the index is the Taxa name in <code>taxa_formatter</code> and the columns are
<code>mean</code>, <code>median</code>, <code>25th percentile</code>, <code>75th</code> percentile`.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>basepath</code></strong> :&ensp;<code>str</code></dt>
<dd>This is the loction to write the files to</dd>
<dt><strong><code>section</code></strong> :&ensp;<code>str</code></dt>
<dd>Section of the trace to compute on. Options:
'posterior' : posterior samples
'burnin' : burn-in samples
'entire' : both burn-in and posterior samples</dd>
<dt><strong><code>taxa_formatter</code></strong> :&ensp;<code>str, None</code></dt>
<dd>This is the format of the label to return for each Taxa. If None, it will return
the taxa name</dd>
<dt><strong><code>true_value</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>Ground truth values of the variable</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pandas.DataFrame</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def visualize(self, basepath: str, section: str=&#39;posterior&#39;, taxa_formatter: str=&#39;%(name)s&#39;, 
    true_value: np.ndarray=None) -&gt; pd.DataFrame:
    &#39;&#39;&#39;Render the traces in the folder `basepath`. Makes a `pandas.DataFrame` table
    where the index is the Taxa name in `taxa_formatter` and the columns are
    `mean`, `median`, `25th percentile`, `75th` percentile`.

    Parameters
    ----------
    basepath : str
        This is the loction to write the files to
    section : str
        Section of the trace to compute on. Options:
            &#39;posterior&#39; : posterior samples
            &#39;burnin&#39; : burn-in samples
            &#39;entire&#39; : both burn-in and posterior samples
    taxa_formatter : str, None
        This is the format of the label to return for each Taxa. If None, it will return
        the taxa name
    true_value : np.ndarray
        Ground truth values of the variable

    Returns
    -------
    pandas.DataFrame
    &#39;&#39;&#39;
    if not self.G.inference.tracer.is_being_traced(self):
        logging.info(&#39;`{}` not learned\n\tValue: {}\n&#39;.format(self.name, self.value))
        return pd.DataFrame()

    taxa = self.G.data.subjects.taxa
    summ = pl.summary(self, section=section)
    data = []
    index = []
    columns = [&#39;name&#39;] + [k for k in summ]

    for idx in range(len(taxa)):
        if taxa_formatter is not None:
            prefix = pl.taxaname_formatter(format=taxa_formatter, taxon=taxa[idx], taxa=taxa)
        else:
            prefix = taxa[idx].name
        index.append(taxa[idx].name)
        temp = [prefix]
        for _,v in summ.items():
            temp.append(v[idx])
        data.append(temp)
    
    df = pd.DataFrame(data, columns=columns, index=index)

    if section == &#39;posterior&#39;:
        len_posterior = self.G.inference.sample_iter + 1 - self.G.inference.burnin
    elif section == &#39;burnin&#39;:
        len_posterior = self.G.inference.burnin
    else:
        len_posterior = self.G.inference.sample_iter + 1

    print(self.G.inference.sample_iter)
    print(self.G.inference.burnin)
    print(len_posterior)

    # Plot the prior on top of the posterior
    if self.G.tracer.is_being_traced(STRNAMES.PRIOR_MEAN_GROWTH):
        prior_mean_trace = self.G[STRNAMES.PRIOR_MEAN_GROWTH].get_trace_from_disk(
                section=section)
    else:
        prior_mean_trace = self.prior.loc.value * np.ones(len_posterior, dtype=float)
    if self.G.tracer.is_being_traced(STRNAMES.PRIOR_VAR_GROWTH):
        prior_std_trace = np.sqrt(
            self.G[STRNAMES.PRIOR_VAR_GROWTH].get_trace_from_disk(section=section))
    else:
        prior_std_trace = np.sqrt(self.prior.scale2.value) * np.ones(len_posterior, dtype=float)

    for idx in range(len(taxa)):
        fig = plt.figure()
        ax_posterior = fig.add_subplot(1,2,1)
        visualization.render_trace(var=self, idx=idx, plt_type=&#39;hist&#39;,
            label=section, color=&#39;blue&#39;, ax=ax_posterior, section=section,
            include_burnin=True, rasterized=True)

        # Get the limits and only look at the posterior within 20% range +- of
        # this number
        low_x, high_x = ax_posterior.get_xlim()

        arr = np.zeros(len(prior_std_trace), dtype=float)
        for i in range(len(prior_std_trace)):
            arr[i] = pl.random.truncnormal.sample(loc=prior_mean_trace[i], scale=prior_std_trace[i], 
                low=self.low, high=self.high)
        visualization.render_trace(var=arr, plt_type=&#39;hist&#39;, 
            label=&#39;prior&#39;, color=&#39;red&#39;, ax=ax_posterior, rasterized=True)

        if true_value is not None:
            ax_posterior.axvline(x=true_value[idx], color=&#39;red&#39;, alpha=0.65, 
                label=&#39;True Value&#39;)

        ax_posterior.legend()
        ax_posterior.set_xlim(left=low_x*.8, right=high_x*1.2)

        # plot the trace
        ax_trace = fig.add_subplot(1,2,2)
        visualization.render_trace(var=self, idx=idx, plt_type=&#39;trace&#39;, 
            ax=ax_trace, section=section, include_burnin=True, rasterized=True)

        if true_value is not None:
            ax_trace.axhline(y=true_value[idx], color=&#39;red&#39;, alpha=0.65, 
                label=&#39;True Value&#39;)
            ax_trace.legend()

        fig.suptitle(&#39;{}&#39;.format(index[idx]))
        fig.tight_layout()
        fig.subplots_adjust(top=0.85)
        plt.savefig(os.path.join(basepath, &#39;{}.pdf&#39;.format(taxa[idx].name)))
        plt.close()

    return df</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="mdsine2.pylab.variables.TruncatedNormal" href="pylab/variables.html#mdsine2.pylab.variables.TruncatedNormal">TruncatedNormal</a></b></code>:
<ul class="hlist">
<li><code><a title="mdsine2.pylab.variables.TruncatedNormal.T" href="pylab/variables.html#mdsine2.pylab.variables.Variable.T">T</a></code></li>
<li><code><a title="mdsine2.pylab.variables.TruncatedNormal.add_child" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_child">add_child</a></code></li>
<li><code><a title="mdsine2.pylab.variables.TruncatedNormal.add_parent" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_parent">add_parent</a></code></li>
<li><code><a title="mdsine2.pylab.variables.TruncatedNormal.add_prior" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_prior">add_prior</a></code></li>
<li><code><a title="mdsine2.pylab.variables.TruncatedNormal.add_trace" href="pylab/base.html#mdsine2.pylab.base.Traceable.add_trace">add_trace</a></code></li>
<li><code><a title="mdsine2.pylab.variables.TruncatedNormal.add_undirected" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_undirected">add_undirected</a></code></li>
<li><code><a title="mdsine2.pylab.variables.TruncatedNormal.cdf" href="pylab/variables.html#mdsine2.pylab.variables.TruncatedNormal.cdf">cdf</a></code></li>
<li><code><a title="mdsine2.pylab.variables.TruncatedNormal.degree" href="pylab/graph.html#mdsine2.pylab.graph.Node.degree">degree</a></code></li>
<li><code><a title="mdsine2.pylab.variables.TruncatedNormal.delete" href="pylab/graph.html#mdsine2.pylab.graph.BaseNode.delete">delete</a></code></li>
<li><code><a title="mdsine2.pylab.variables.TruncatedNormal.get_adjacent_keys" href="pylab/graph.html#mdsine2.pylab.graph.Node.get_adjacent_keys">get_adjacent_keys</a></code></li>
<li><code><a title="mdsine2.pylab.variables.TruncatedNormal.get_iter" href="pylab/base.html#mdsine2.pylab.base.Traceable.get_iter">get_iter</a></code></li>
<li><code><a title="mdsine2.pylab.variables.TruncatedNormal.get_trace_from_disk" href="pylab/base.html#mdsine2.pylab.base.Traceable.get_trace_from_disk">get_trace_from_disk</a></code></li>
<li><code><a title="mdsine2.pylab.variables.TruncatedNormal.load" href="pylab/base.html#mdsine2.pylab.base.Saveable.load">load</a></code></li>
<li><code><a title="mdsine2.pylab.variables.TruncatedNormal.logcdf" href="pylab/variables.html#mdsine2.pylab.variables.TruncatedNormal.logcdf">logcdf</a></code></li>
<li><code><a title="mdsine2.pylab.variables.TruncatedNormal.logpdf" href="pylab/variables.html#mdsine2.pylab.variables.TruncatedNormal.logpdf">logpdf</a></code></li>
<li><code><a title="mdsine2.pylab.variables.TruncatedNormal.metropolis" href="pylab/graph.html#mdsine2.pylab.graph.Node.metropolis">metropolis</a></code></li>
<li><code><a title="mdsine2.pylab.variables.TruncatedNormal.overwrite_entire_trace_on_disk" href="pylab/base.html#mdsine2.pylab.base.Traceable.overwrite_entire_trace_on_disk">overwrite_entire_trace_on_disk</a></code></li>
<li><code><a title="mdsine2.pylab.variables.TruncatedNormal.pdf" href="pylab/variables.html#mdsine2.pylab.variables.TruncatedNormal.pdf">pdf</a></code></li>
<li><code><a title="mdsine2.pylab.variables.TruncatedNormal.remove_local_trace" href="pylab/variables.html#mdsine2.pylab.variables.Variable.remove_local_trace">remove_local_trace</a></code></li>
<li><code><a title="mdsine2.pylab.variables.TruncatedNormal.sample" href="pylab/variables.html#mdsine2.pylab.variables.TruncatedNormal.sample">sample</a></code></li>
<li><code><a title="mdsine2.pylab.variables.TruncatedNormal.save" href="pylab/base.html#mdsine2.pylab.base.Saveable.save">save</a></code></li>
<li><code><a title="mdsine2.pylab.variables.TruncatedNormal.set_save_location" href="pylab/base.html#mdsine2.pylab.base.Saveable.set_save_location">set_save_location</a></code></li>
<li><code><a title="mdsine2.pylab.variables.TruncatedNormal.set_trace" href="pylab/base.html#mdsine2.pylab.base.Traceable.set_trace">set_trace</a></code></li>
<li><code><a title="mdsine2.pylab.variables.TruncatedNormal.set_value_shape" href="pylab/variables.html#mdsine2.pylab.variables.Variable.set_value_shape">set_value_shape</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="mdsine2.posterior.PerturbationIndicators"><code class="flex name class">
<span>class <span class="ident">PerturbationIndicators</span></span>
<span>(</span><span>need_to_trace: bool, relative: bool, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>This is the indicator for a perturbation</p>
<p>We only need to trace once for the perturbations. Our default is to only
trace from the magnitudes. Thus, we only trace the indicators (here) if
we are learning here and not learning the magnitudes.</p>
<p>Parameters</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PerturbationIndicators(pl.Node):
    &#39;&#39;&#39;This is the indicator for a perturbation

    We only need to trace once for the perturbations. Our default is to only
    trace from the magnitudes. Thus, we only trace the indicators (here) if
    we are learning here and not learning the magnitudes.
    &#39;&#39;&#39;
    def __init__(self, need_to_trace: bool, relative: bool, **kwargs):
        &#39;&#39;&#39;Parameters
        &#39;&#39;&#39;
        kwargs[&#39;name&#39;] = STRNAMES.PERT_INDICATOR
        pl.Node.__init__(self, **kwargs)
        self.need_to_trace = need_to_trace
        self.perturbations = self.G.perturbations # mdsine2.pyab.base.Perturbations
        self.clustering = None # mdsine2.pylab.cluster.Clustering
        self._time_taken = None
        if relative:
            self.update = self.update_relative
        else:
            self.update = self.update_slow

    def __str__(self) -&gt; str:
        s = &#39;Perturbation Indicators - time: {}s&#39;.format(self._time_taken)
        for perturbation in self.perturbations:
            arr = perturbation.indicator.cluster_bool_array()
            s += &#39;\nperturbation {} ({}/{}): {}&#39;.format(perturbation.name,
                np.sum(arr), len(arr), arr)
        return s

    @property
    def sample_iter(self) -&gt; int:
        return self.perturbations[0].sample_iter

    def add_trace(self):
        &#39;&#39;&#39;Only trace if perturbation indicators are being learned and the
        perturbation value is not being learned
        &#39;&#39;&#39;
        if self.need_to_trace:
            for perturbation in self.perturbations:
                perturbation.add_trace()

    def set_trace(self, *args, **kwargs):
        &#39;&#39;&#39;Only trace if perturbation indicators are being learned and the
        perturbation value is not being learned
        &#39;&#39;&#39;
        if self.need_to_trace:
            for perturbation in self.perturbations:
                perturbation.set_trace(*args, **kwargs)

    def initialize(self, value_option: str, p: float=None, delay: int=0):
        &#39;&#39;&#39;Initialize the based on the passed in option.

        Parameters
        ----------
        value_option : str
            - Different ways to initialize the values. Options:
                - &#39;auto&#39;, &#39;all-off&#39;
                    - Turn all of the indicators off
                - &#39;all-on&#39;
                    - Turn all the indicators on
                - &#39;random&#39;
                    - Randomly assign the indicator with probability `p`
        p : float
            Only required if `value_option` == &#39;random&#39;
        delay : int
            How many Gibbs steps to delay updating the values
        &#39;&#39;&#39;
        # print(&#39;in pert ind&#39;)
        if not pl.isint(delay):
            raise TypeError(&#39;`delay` ({}) must be an int&#39;.format(type(delay)))
        if delay &lt; 0:
            raise ValueError(&#39;`delay` ({}) must be &gt;= 0&#39;.format(delay))
        self.delay = delay

        for perturbation in self.perturbations:
            perturbation.indicator.set_signal_when_clusters_change(True)
        self.clustering = self.G.perturbations[0].indicator.clustering # mdsine2.pylab.cluster.Clustering

        # Set the value
        if not pl.isstr(value_option):
            raise ValueError(&#39;`value_option` ({}) must be a str&#39;.format(type(value_option)))
        if value_option in [&#39;all-off&#39;, &#39;auto&#39;]:
            value = False
        elif value_option == &#39;all-on&#39;:
            value = True
        elif value_option == &#39;random&#39;:
            if not pl.isfloat(p):
                raise TypeError(&#39;`p` ({}) must be a float&#39;.format(type(p)))
            if p &lt; 0 or p &gt; 1:
                raise ValueError(&#39;`p` ({}) must be [0,1]&#39;.format(p))
        else:
            raise ValueError(&#39;`value_option` ({}) not recognized&#39;.format(value_option))
        for perturbation in self.perturbations:
            for cid in perturbation.clustering.clusters:
                if value_option == &#39;random&#39;:
                    perturbation.indicator.value[cid] = bool(pl.random.bernoulli.sample(p))
                else:
                    perturbation.indicator.value[cid] = value

        # These are for the function `self._make_idx_for_clusters`
        self.ndts_bias = []
        self.n_replicates = self.G.data.n_replicates
        self.n_perturbations = len(self.G.perturbations)
        self.n_dts_for_replicate = self.G.data.n_dts_for_replicate
        self.total_dts = np.sum(self.n_dts_for_replicate)
        self.replicate_bias = np.zeros(self.n_replicates, dtype=int)
        self.n_taxa = len(self.G.data.taxa)
        for ridx in range(1, self.n_replicates):
            self.replicate_bias[ridx] = self.replicate_bias[ridx-1] + \
                self.n_taxa * self.n_dts_for_replicate[ridx - 1]
        for ridx in range(self.G.data.n_replicates):
            self.ndts_bias.append(
                np.arange(0, self.G.data.n_dts_for_replicate[ridx] * self.n_taxa, self.n_taxa))

        s = &#39;Perturbation indicator initialization results:\n&#39;
        for i, perturbation in enumerate(self.perturbations):
            s += &#39;\tPerturbation {}:\n&#39; \
                &#39;\t\tindicator: {}\n&#39;.format(i, perturbation.indicator.cluster_bool_array())
        logging.info(s)

    def _make_idx_for_clusters(self) -&gt; Dict[int, np.ndarray]:
        &#39;&#39;&#39;Creates a dictionary that maps the cluster id to the
        rows that correspond to each Taxa in the cluster.

        We cannot cast this with numba because it does not support Fortran style
        raveling :(.

        Returns
        -------
        dict: int -&gt; np.ndarray
            Maps the cluster ID to the row indices corresponding to it
        &#39;&#39;&#39;
        clusters = [np.asarray(oidxs, dtype=int).reshape(-1,1) \
            for oidxs in self.clustering.tolistoflists()]
        n_dts=self.G.data.n_dts_for_replicate

        d = {}
        cids = self.clustering.order

        for cidx,cid in enumerate(cids):
            a = np.zeros(len(clusters[cidx]) * self.total_dts, dtype=int)
            i = 0
            for ridx in range(self.n_replicates):
                idxs = np.zeros(
                    (len(clusters[cidx]),
                    self.n_dts_for_replicate[ridx]), int)
                idxs = idxs + clusters[cidx]
                idxs = idxs + self.ndts_bias[ridx]
                idxs = idxs + self.replicate_bias[ridx]
                idxs = idxs.ravel(&#39;F&#39;)
                l = len(idxs)
                a[i:i+l] = idxs
                i += l

            d[cid] = a
        
        if self.G.data.zero_inflation_transition_policy is not None:
            # We need to convert the indices that are meant from no zero inflation to 
            # ones that take into account zero inflation - use the array from 
            # `data.Data._setrows_to_include_zero_inflation`. If the index should be
            # included, then we subtract the number of indexes that are previously off
            # before that index. If it should not be included then we exclude it
            prevoff_arr = self.G.data.off_previously_arr_zero_inflation
            rows_to_include = self.G.data.zero_inflation_transition_policy
            for cid in d:
                arr = d[cid]
                new_arr = np.zeros(len(arr), dtype=int)
                n = 0
                for i, idx in enumerate(arr):
                    if rows_to_include[idx]:
                        new_arr[n] = idx - prevoff_arr[i]
                        n += 1
                new_arr = new_arr[:n]
        return d

    # @profile
    def make_rel_params(self):
        &#39;&#39;&#39;We make the parameters needed to update the relative log-likelihod.
        This function is called once at the beginning of the update.

        THIS ASSUMES THAT EACH PERTURBATION CLUSTERS ARE DEFINED BY THE SAME CLUSTERS
            - To make this separate, make a higher level list for each perturbation index
              for each individual perturbation

        Parameters that we create with this function
        --------------------------------------------
        - ys : dict (int -&gt; np.ndarray)
            - Maps the target cluster id to the observation matrix that it
              corresponds to (only the Taxa in the target cluster). This 
              array already has the growth and self-interactions subtracted
              out:
                - $ \frac{log(x_{k+1}) - log(x_{k})}{dt} - a_{1,k} - a_{2,k}x_{k} $
        - process_precs : dict (int -&gt; np.ndarray)
            - Maps the target cluster id to the vector of the process precision
              that corresponds to the target cluster (only the Taxa in the target
              cluster). This is a 1D array that corresponds to the diagonal of what
              would be the precision matrix.
        - interactionXs : dict (int -&gt; np.ndarray)
            - Maps the target cluster id to the design matrix for the interactions
              going into that cluster. We pre-index it with the rows and columns
        - prior_prec_interaction : dict (int -&gt; np.ndarray)
            - Maps the target cluster id to to the diagonal of the prior precision 
              for the interaction values.
        - prior_mean_interaction : dict (int -&gt; np.ndarray)
            - Maps the target cluster id to to the diagonal of the prior mean 
              for the interaction values.
        - prior_ll_ons : np.ndarray
            - Prior log likelihood of a positive indicator. These are separate for each
              perturbation.
        - prior_ll_offs : np.ndarray
            - Prior log likelihood of the negative indicator. These are separate for each
              perturbation.
        - priorvar_logdet_diffs : np.ndarray
            - This is the prior variance log determinant that we add when the indicator
              is positive. This is different for each perturbation.
        - perturbationsXs : dict (int -&gt; np.ndarray)
            - Maps the target cluster id to the design matrix that corresponds to 
              the on perturbations of the target clusters. This is preindexed by the 
              rows but not the columns - the columns assume that all of the perturbations
              are on and we index the ones that we want.
        - prior_prec_perturbations : np.ndarray
            - This is the prior precision of the magnitude for each of the perturbations. Use
              the perturbation index to get the value
        - prior_mean_perturbations : np.ndarray
            - This is the prior mean of the magnitude for each one of the perturbations. Use
              the perturbation index to get the value
        &#39;&#39;&#39;
        row_idxs = self._make_idx_for_clusters()

        # Create ys
        self.ys = {}
        y = self.G.data.construct_lhs(keys=[
            STRNAMES.SELF_INTERACTION_VALUE, STRNAMES.GROWTH_VALUE],
            kwargs_dict={STRNAMES.GROWTH_VALUE:{&#39;with_perturbations&#39;: False}})
        for tcid in self.clustering.order:
            self.ys[tcid] = y[row_idxs[tcid], :]

        # Create process_precs
        self.process_precs = {}
        process_prec_diag = self.G[STRNAMES.PROCESSVAR].prec
        for tcid in self.clustering.order:
            self.process_precs[tcid] = process_prec_diag[row_idxs[tcid]]

        # Make interactionXs
        self.interactionXs = {}
        interactions = self.G[STRNAMES.INTERACTIONS_OBJ]
        XM_master = self.G.data.design_matrices[STRNAMES.CLUSTER_INTERACTION_VALUE].toarray()
        for tcid in self.clustering.order:
            cols = []
            for i, interaction in enumerate(interactions.iter_valid()):
                if interaction.target_cid == tcid:
                    if interaction.indicator:
                        cols.append(i)
            cols = np.asarray(cols, dtype=int)
            self.interactionXs[tcid] = pl.util.fast_index(M=XM_master, 
                rows=row_idxs[tcid], cols=cols)

        # Make prior parameters for interactions
        self.prior_prec_interaction = 1/self.G[STRNAMES.PRIOR_VAR_INTERACTIONS].value
        self.prior_mean_interaction = self.G[STRNAMES.PRIOR_MEAN_INTERACTIONS].value

        # Make the perturbation parameters
        self.prior_ll_ons = []
        self.prior_ll_offs = []
        self.priorvar_logdet_diffs = []
        self.prior_prec_perturbations = []
        self.prior_mean_perturbations = []

        for perturbation in self.G.perturbations:
            prob_on = perturbation.probability.value
            self.prior_ll_ons.append(np.log(prob_on))
            self.prior_ll_offs.append(np.log(1 - prob_on))
            
            self.priorvar_logdet_diffs.append(
                np.log(perturbation.magnitude.prior.scale2.value))

            self.prior_prec_perturbations.append( 
                1/perturbation.magnitude.prior.scale2.value)

            self.prior_mean_perturbations.append(
                perturbation.magnitude.prior.loc.value)

        # Make perturbation matrices
        self.perturbationsXs = {}
        self.G.data.design_matrices[STRNAMES.PERT_VALUE].M.build(build=True, 
            build_for_neg_ind=True)
        Xpert_master = self.G.data.design_matrices[STRNAMES.PERT_VALUE].toarray()
        for tcid in self.clustering.order:
            self.perturbationsXs[tcid] = Xpert_master[row_idxs[tcid], :]

        self.n_clusters = len(self.clustering.order)
        self.clustering_order = self.clustering.order

        self.col2pidxcidx = []
        for pidx in range(len(self.perturbations)):
            for cidx in range(len(self.clustering.order)):
                self.col2pidxcidx.append((pidx, cidx))

        self.arr = []
        for perturbation in self.perturbations:
            self.arr = np.append(
                self.arr, 
                perturbation.indicator.cluster_bool_array())
        self.arr = np.asarray(self.arr, dtype=bool)

    # @profile
    def update_relative(self):
        &#39;&#39;&#39;Update each perturbation indicator for the given cluster by
        calculating the realtive loglikelihoods of it being on/off as
        supposed to as is. Because this is a relative loglikelihood, we
        only need to take into account the following parameters of the
        model:
            - Only the Taxa in the cluster in question
            - Only the perturbations for that cluster
            - Only the interactions going into the cluster

        Because these matrices are considerably smaller and considered &#39;dense&#39;, we
        do the operations in numpy instead of scipy sparse.

        We permute the order that the indices are updated for more robust mixing
        &#39;&#39;&#39;
        if self.sample_iter &lt; self.delay:
            return
        start_time = time.time()

        self.make_rel_params()

        # Iterate over each perturbation indicator variable
        iidxs = npr.permutation(len(self.arr))
        for iidx in iidxs:
            self.update_single_idx_fast(idx=iidx)

        # Set the perturbation indicators from arr
        i = 0
        for perturbation in self.perturbations:
            for cid in self.clustering.order:
                perturbation.indicator.value[cid] = self.arr[i]
                i += 1

        # rebuild the growth design matrix
        self.G.data.design_matrices[STRNAMES.PERT_VALUE].M.build()
        self.G.data.design_matrices[STRNAMES.GROWTH_VALUE].build_with_perturbations()
        self._time_taken = time.time() - start_time

    # @profile
    def update_single_idx_fast(self, idx: int):
        &#39;&#39;&#39;Do a Gibbs step for a single cluster
        &#39;&#39;&#39;
        pidx, cidx = self.col2pidxcidx[idx]

        prior_ll_on = self.prior_ll_ons[pidx]
        prior_ll_off = self.prior_ll_offs[pidx]

        d_on = self.calculate_relative_marginal_loglikelihood(idx=idx, val=True)
        d_off = self.calculate_relative_marginal_loglikelihood(idx=idx, val=False)

        ll_on = d_on + prior_ll_on
        ll_off = d_off + prior_ll_off
        dd = [ll_off, ll_on]

        # print(&#39;\nindicator&#39;, idx)
        # print(&#39;fast\n\ttotal: {}\n\tbeta_logdet_diff: {}\n\t&#39; \
        #     &#39;priorvar_logdet_diff: {}\n\tbEb_diff: {}\n\t&#39; \
        #     &#39;bEbprior_diff: {}&#39;.format(
        #         ll_on - ll_off,
        #         d_on[&#39;beta_logdet&#39;] - d_off[&#39;beta_logdet&#39;],
        #         d_on[&#39;priorvar_logdet&#39;] - d_off[&#39;priorvar_logdet&#39;],
        #         d_on[&#39;bEb&#39;] - d_off[&#39;bEb&#39;],
        #         d_on[&#39;bEbprior&#39;] - d_off[&#39;bEbprior&#39;]))
        # self.update_single_idx_slow(idx)

        res = bool(sample_categorical_log(dd))
        self.arr[idx] = res

    # @profile
    def calculate_relative_marginal_loglikelihood(self, idx: int, val: bool) -&gt; float:
        &#39;&#39;&#39;Calculate the relative marginal loglikelihood of switching the `idx`&#39;th index
        of the perturbation matrix to `val`

        Parameters
        ----------
        idx : int
            This is the index of the indicator we are sampling
        val : bool
            This is the value we are testing it at.

        Returns
        -------
        float
        &#39;&#39;&#39;
        # Create and get the data
        self.arr[idx] = val
        pidx, cidx = self.col2pidxcidx[idx]
        tcid = self.clustering_order[cidx]

        y = self.ys[tcid]
        process_prec = self.process_precs[tcid]
        X = self.interactionXs[tcid]

        prior_mean = []
        prior_prec_diag = []
        cols = []
        for temp_pidx in range(len(self.perturbations)):
            col = int(cidx + temp_pidx * self.n_clusters)
            if self.arr[col]:
                cols.append(col)
                prior_mean.append(self.prior_mean_perturbations[temp_pidx])
                prior_prec_diag.append(self.prior_prec_perturbations[temp_pidx])
        Xpert = self.perturbationsXs[tcid][:, cols]

        if Xpert.shape[1] + X.shape[1] == 0:
            # return {
            #     &#39;ret&#39;: 0,
            #     &#39;beta_logdet&#39;: 0,
            #     &#39;priorvar_logdet&#39;: 0,
            #     &#39;bEb&#39;: 0,
            #     &#39;bEbprior&#39;: 0}
            return 0

        prior_mean = np.append(
            prior_mean,
            np.full(X.shape[1], self.prior_mean_interaction))
        prior_prec_diag = np.append(
            prior_prec_diag,
            np.full(X.shape[1], self.prior_prec_interaction))
        X = np.hstack((Xpert, X))
        prior_prec = np.diag(prior_prec_diag)
        pm = (prior_prec_diag * prior_mean).reshape(-1,1)

        # Do the marginalization
        a = X.T * process_prec
        beta_prec = (a @ X) + prior_prec
        beta_cov = pinv(beta_prec, self)
        beta_mean = beta_cov @ ((a @ y) + pm )

        bEb = (beta_mean.T @ beta_prec @ beta_mean)[0,0]
        try:
            beta_logdet = log_det(beta_cov, self)
        except:
            logging.critical(&#39;Crashed in log_det&#39;)
            logging.critical(&#39;beta_cov:\n{}&#39;.format(beta_cov))
            logging.critical(&#39;prior_prec\n{}&#39;.format(prior_prec))
            raise
        
        if val:
            bEbprior = (self.prior_mean_perturbations[pidx]**2) * \
                self.prior_prec_perturbations[pidx]
            priorvar_logdet = self.priorvar_logdet_diffs[pidx]
        else:
            bEbprior = 0
            priorvar_logdet = 0

        ll2 = 0.5 * (beta_logdet - priorvar_logdet)
        ll3 = 0.5 * (bEb - bEbprior)

        # return {
        #     &#39;ret&#39;: ll2+ll3,
        #     &#39;beta_logdet&#39;: beta_logdet,
        #     &#39;priorvar_logdet&#39;: priorvar_logdet,
        #     &#39;bEb&#39;: bEb,
        #     &#39;bEbprior&#39;: bEbprior}
        return ll2 + ll3        

    # @profile
    def update_slow(self):
        &#39;&#39;&#39;Update each cluster indicator variable for the perturbation
        &#39;&#39;&#39;
        start_time = time.time()

        if self.sample_iter &lt; self.delay:
            return

        n_clusters = len(self.clustering.order)
        n_perturbations = len(self.perturbations)
        idxs = npr.permutation(int(n_clusters*n_perturbations))
        for idx in idxs:
            self.update_single_idx_slow(idx=idx)

        # rebuild the growth design matrix
        self.G.data.design_matrices[STRNAMES.PERT_VALUE].M.build()
        self.G.data.design_matrices[STRNAMES.GROWTH_VALUE].build_with_perturbations()
        self._time_taken = time.time() - start_time

    # @profile
    def update_single_idx_slow(self, idx: int):
        &#39;&#39;&#39;Do a Gibbs step for a single cluster and perturbation

        Parameters
        ----------
        idx : int
            This is the index of the indicator in vectorized form
        &#39;&#39;&#39;
        cidx = idx % self.G.data.n_taxa
        cid = self.clustering.order[cidx]
        
        pidx = idx // self.G.data.n_taxa
        perturbation = self.perturbations[pidx]

        d_on = self.calculate_marginal_loglikelihood(cid=cid, val=True,
            perturbation=perturbation)
        d_off = self.calculate_marginal_loglikelihood(cid=cid, val=False,
            perturbation=perturbation)

        prior_ll_on = np.log(perturbation.probability.value)
        prior_ll_off = np.log(1 - perturbation.probability.value)

        ll_on = d_on[&#39;ret&#39;] + prior_ll_on
        ll_off = d_off[&#39;ret&#39;] + prior_ll_off
        dd = [ll_off, ll_on]

        res = bool(sample_categorical_log(dd))
        if perturbation.indicator.value[cid] != res:
            perturbation.indicator.value[cid] = res
            self.G.data.design_matrices[STRNAMES.PERT_VALUE].build()

    # @profile
    def calculate_marginal_loglikelihood(self, cid: int, val: bool, 
        perturbation: pl.contrib.ClusterPerturbationEffect) -&gt; Dict[str, float]:
        &#39;&#39;&#39;Calculate the log marginal likelihood with the perturbations integrated
        out
        &#39;&#39;&#39;
        # Set parameters
        perturbation.indicator.value[cid] = val
        self.G.data.design_matrices[STRNAMES.PERT_VALUE].M.build()

        # Make matrices
        rhs = [STRNAMES.PERT_VALUE, STRNAMES.CLUSTER_INTERACTION_VALUE]
        lhs = [STRNAMES.GROWTH_VALUE, STRNAMES.SELF_INTERACTION_VALUE]
        X = self.G.data.construct_rhs(keys=rhs)
        y = self.G.data.construct_lhs(keys=lhs, 
            kwargs_dict={STRNAMES.GROWTH_VALUE:{&#39;with_perturbations&#39;: False}})
        
        if X.shape[1] == 0:
            return {
            &#39;ret&#39;: 0,
            &#39;beta_logdet&#39;: 0,
            &#39;priorvar_logdet&#39;: 0,
            &#39;bEb&#39;: 0,
            &#39;bEbprior&#39;: 0}

        process_prec = self.G[STRNAMES.PROCESSVAR].build_matrix(cov=False, sparse=False)
        prior_prec = build_prior_covariance(G=self.G, cov=False, order=rhs, sparse=False)
        prior_var = build_prior_covariance(G=self.G, cov=True, order=rhs, sparse=False)
        prior_mean = build_prior_mean(G=self.G, order=rhs, shape=(-1,1))

        # Calculate the posterior
        beta_prec = X.T @ process_prec @ X + prior_prec
        beta_cov = pinv(beta_prec, self)
        beta_mean = beta_cov @ ( X.T @ process_prec @ y + prior_prec @ prior_mean )
        beta_mean = np.asarray(beta_mean).reshape(-1,1)

        # Perform the marginalization
        try:
            beta_logdet = log_det(beta_cov, self)
        except:
            logging.critical(&#39;Crashed in log_det&#39;)
            logging.critical(&#39;beta_cov:\n{}&#39;.format(beta_cov))
            logging.critical(&#39;prior_prec\n{}&#39;.format(prior_prec))
            raise
        priorvar_logdet = log_det(prior_var, self)
        ll2 = 0.5 * (beta_logdet - priorvar_logdet)

        a = np.asarray(prior_mean.T @ prior_prec @ prior_mean)[0,0]
        b = np.asarray(beta_mean.T @ beta_prec @ beta_mean)[0,0]
        ll3 = -0.5 * (a  - b)

        return {
            &#39;ret&#39;: ll2+ll3,
            &#39;beta_logdet&#39;: beta_logdet,
            &#39;priorvar_logdet&#39;: priorvar_logdet,
            &#39;bEb&#39;: b,
            &#39;bEbprior&#39;: a}

    def total_on(self) -&gt; int:
        n = 0
        for perturbation in self.perturbations:
            n += perturbation.indicator.num_on_clusters()
        return n

    def visualize(self, path: str, pidx: int, section: str=&#39;posterior&#39;, fixed_clustering: bool=False):
        &#39;&#39;&#39;Make a table of the `pidx`th perturbation bayes factors

        Parameters
        ----------
        path : str
            This is the path to write the bayes factors
        f : _io.TextIOWrapper
            File that we are writing the values to
        section : str
            Section of the trace to compute on. Options:
                &#39;posterior&#39; : posterior samples
                &#39;burnin&#39; : burn-in samples
                &#39;entire&#39; : both burn-in and posterior samples
        fixed_clustering : bool
            If True, plot the variable as if clustering was fixed. Since the
            cluster assignments never change, all of the taxa within the cluster
            have identical values, so we can choose any taxon within the cluster to
            represent it
        &#39;&#39;&#39;
        from .util import generate_perturbation_bayes_factors_posthoc

        perturbation = self.perturbations[pidx]
        bayes_factors = generate_perturbation_bayes_factors_posthoc(
            mcmc=self.G.inference, perturbation=perturbation, section=section)
        if fixed_clustering:
            # Condense the taxa level bayes factors to cluster level bayes factors
            clustering = self.G[STRNAMES.CLUSTERING_OBJ]
            bf_clusters = []
            for cluster in clustering:
                aidx = list(cluster.members)[0]
                bf_clusters.append(bayes_factors[aidx])
            bf_clusters = np.asarray(bf_clusters)
            df = pd.DataFrame([bf_clusters], index=[perturbation.name],
                columns=[&#39;Cluster {}&#39;.format(cidx+1) for cidx in range(len(clustering))])
        else:
            df = pd.DataFrame([bayes_factors], index=[perturbation.name], 
                columns=[taxon.name for taxon in self.G.data.taxa])
        df.to_csv(path, sep=&#39;\t&#39;, index=True, header=True)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="mdsine2.pylab.graph.Node" href="pylab/graph.html#mdsine2.pylab.graph.Node">Node</a></li>
<li><a title="mdsine2.pylab.graph.BaseNode" href="pylab/graph.html#mdsine2.pylab.graph.BaseNode">BaseNode</a></li>
<li><a title="mdsine2.pylab.base.Saveable" href="pylab/base.html#mdsine2.pylab.base.Saveable">Saveable</a></li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="mdsine2.posterior.PerturbationIndicators.sample_iter"><code class="name">var <span class="ident">sample_iter</span> : int</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def sample_iter(self) -&gt; int:
    return self.perturbations[0].sample_iter</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="mdsine2.posterior.PerturbationIndicators.add_trace"><code class="name flex">
<span>def <span class="ident">add_trace</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Only trace if perturbation indicators are being learned and the
perturbation value is not being learned</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add_trace(self):
    &#39;&#39;&#39;Only trace if perturbation indicators are being learned and the
    perturbation value is not being learned
    &#39;&#39;&#39;
    if self.need_to_trace:
        for perturbation in self.perturbations:
            perturbation.add_trace()</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.PerturbationIndicators.calculate_marginal_loglikelihood"><code class="name flex">
<span>def <span class="ident">calculate_marginal_loglikelihood</span></span>(<span>self, cid: int, val: bool, perturbation: <a title="mdsine2.pylab.contrib.ClusterPerturbationEffect" href="pylab/contrib.html#mdsine2.pylab.contrib.ClusterPerturbationEffect">ClusterPerturbationEffect</a>) ‑> Dict[str, float]</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate the log marginal likelihood with the perturbations integrated
out</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def calculate_marginal_loglikelihood(self, cid: int, val: bool, 
    perturbation: pl.contrib.ClusterPerturbationEffect) -&gt; Dict[str, float]:
    &#39;&#39;&#39;Calculate the log marginal likelihood with the perturbations integrated
    out
    &#39;&#39;&#39;
    # Set parameters
    perturbation.indicator.value[cid] = val
    self.G.data.design_matrices[STRNAMES.PERT_VALUE].M.build()

    # Make matrices
    rhs = [STRNAMES.PERT_VALUE, STRNAMES.CLUSTER_INTERACTION_VALUE]
    lhs = [STRNAMES.GROWTH_VALUE, STRNAMES.SELF_INTERACTION_VALUE]
    X = self.G.data.construct_rhs(keys=rhs)
    y = self.G.data.construct_lhs(keys=lhs, 
        kwargs_dict={STRNAMES.GROWTH_VALUE:{&#39;with_perturbations&#39;: False}})
    
    if X.shape[1] == 0:
        return {
        &#39;ret&#39;: 0,
        &#39;beta_logdet&#39;: 0,
        &#39;priorvar_logdet&#39;: 0,
        &#39;bEb&#39;: 0,
        &#39;bEbprior&#39;: 0}

    process_prec = self.G[STRNAMES.PROCESSVAR].build_matrix(cov=False, sparse=False)
    prior_prec = build_prior_covariance(G=self.G, cov=False, order=rhs, sparse=False)
    prior_var = build_prior_covariance(G=self.G, cov=True, order=rhs, sparse=False)
    prior_mean = build_prior_mean(G=self.G, order=rhs, shape=(-1,1))

    # Calculate the posterior
    beta_prec = X.T @ process_prec @ X + prior_prec
    beta_cov = pinv(beta_prec, self)
    beta_mean = beta_cov @ ( X.T @ process_prec @ y + prior_prec @ prior_mean )
    beta_mean = np.asarray(beta_mean).reshape(-1,1)

    # Perform the marginalization
    try:
        beta_logdet = log_det(beta_cov, self)
    except:
        logging.critical(&#39;Crashed in log_det&#39;)
        logging.critical(&#39;beta_cov:\n{}&#39;.format(beta_cov))
        logging.critical(&#39;prior_prec\n{}&#39;.format(prior_prec))
        raise
    priorvar_logdet = log_det(prior_var, self)
    ll2 = 0.5 * (beta_logdet - priorvar_logdet)

    a = np.asarray(prior_mean.T @ prior_prec @ prior_mean)[0,0]
    b = np.asarray(beta_mean.T @ beta_prec @ beta_mean)[0,0]
    ll3 = -0.5 * (a  - b)

    return {
        &#39;ret&#39;: ll2+ll3,
        &#39;beta_logdet&#39;: beta_logdet,
        &#39;priorvar_logdet&#39;: priorvar_logdet,
        &#39;bEb&#39;: b,
        &#39;bEbprior&#39;: a}</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.PerturbationIndicators.calculate_relative_marginal_loglikelihood"><code class="name flex">
<span>def <span class="ident">calculate_relative_marginal_loglikelihood</span></span>(<span>self, idx: int, val: bool) ‑> float</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate the relative marginal loglikelihood of switching the <code>idx</code>'th index
of the perturbation matrix to <code>val</code></p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>idx</code></strong> :&ensp;<code>int</code></dt>
<dd>This is the index of the indicator we are sampling</dd>
<dt><strong><code>val</code></strong> :&ensp;<code>bool</code></dt>
<dd>This is the value we are testing it at.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>float</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def calculate_relative_marginal_loglikelihood(self, idx: int, val: bool) -&gt; float:
    &#39;&#39;&#39;Calculate the relative marginal loglikelihood of switching the `idx`&#39;th index
    of the perturbation matrix to `val`

    Parameters
    ----------
    idx : int
        This is the index of the indicator we are sampling
    val : bool
        This is the value we are testing it at.

    Returns
    -------
    float
    &#39;&#39;&#39;
    # Create and get the data
    self.arr[idx] = val
    pidx, cidx = self.col2pidxcidx[idx]
    tcid = self.clustering_order[cidx]

    y = self.ys[tcid]
    process_prec = self.process_precs[tcid]
    X = self.interactionXs[tcid]

    prior_mean = []
    prior_prec_diag = []
    cols = []
    for temp_pidx in range(len(self.perturbations)):
        col = int(cidx + temp_pidx * self.n_clusters)
        if self.arr[col]:
            cols.append(col)
            prior_mean.append(self.prior_mean_perturbations[temp_pidx])
            prior_prec_diag.append(self.prior_prec_perturbations[temp_pidx])
    Xpert = self.perturbationsXs[tcid][:, cols]

    if Xpert.shape[1] + X.shape[1] == 0:
        # return {
        #     &#39;ret&#39;: 0,
        #     &#39;beta_logdet&#39;: 0,
        #     &#39;priorvar_logdet&#39;: 0,
        #     &#39;bEb&#39;: 0,
        #     &#39;bEbprior&#39;: 0}
        return 0

    prior_mean = np.append(
        prior_mean,
        np.full(X.shape[1], self.prior_mean_interaction))
    prior_prec_diag = np.append(
        prior_prec_diag,
        np.full(X.shape[1], self.prior_prec_interaction))
    X = np.hstack((Xpert, X))
    prior_prec = np.diag(prior_prec_diag)
    pm = (prior_prec_diag * prior_mean).reshape(-1,1)

    # Do the marginalization
    a = X.T * process_prec
    beta_prec = (a @ X) + prior_prec
    beta_cov = pinv(beta_prec, self)
    beta_mean = beta_cov @ ((a @ y) + pm )

    bEb = (beta_mean.T @ beta_prec @ beta_mean)[0,0]
    try:
        beta_logdet = log_det(beta_cov, self)
    except:
        logging.critical(&#39;Crashed in log_det&#39;)
        logging.critical(&#39;beta_cov:\n{}&#39;.format(beta_cov))
        logging.critical(&#39;prior_prec\n{}&#39;.format(prior_prec))
        raise
    
    if val:
        bEbprior = (self.prior_mean_perturbations[pidx]**2) * \
            self.prior_prec_perturbations[pidx]
        priorvar_logdet = self.priorvar_logdet_diffs[pidx]
    else:
        bEbprior = 0
        priorvar_logdet = 0

    ll2 = 0.5 * (beta_logdet - priorvar_logdet)
    ll3 = 0.5 * (bEb - bEbprior)

    # return {
    #     &#39;ret&#39;: ll2+ll3,
    #     &#39;beta_logdet&#39;: beta_logdet,
    #     &#39;priorvar_logdet&#39;: priorvar_logdet,
    #     &#39;bEb&#39;: bEb,
    #     &#39;bEbprior&#39;: bEbprior}
    return ll2 + ll3        </code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.PerturbationIndicators.initialize"><code class="name flex">
<span>def <span class="ident">initialize</span></span>(<span>self, value_option: str, p: float = None, delay: int = 0)</span>
</code></dt>
<dd>
<div class="desc"><p>Initialize the based on the passed in option.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>value_option</code></strong> :&ensp;<code>str</code></dt>
<dd>
<ul>
<li>Different ways to initialize the values. Options:<ul>
<li>'auto', 'all-off'<ul>
<li>Turn all of the indicators off</li>
</ul>
</li>
<li>'all-on'<ul>
<li>Turn all the indicators on</li>
</ul>
</li>
<li>'random'<ul>
<li>Randomly assign the indicator with probability <code>p</code></li>
</ul>
</li>
</ul>
</li>
</ul>
</dd>
<dt><strong><code>p</code></strong> :&ensp;<code>float</code></dt>
<dd>Only required if <code>value_option</code> == 'random'</dd>
<dt><strong><code>delay</code></strong> :&ensp;<code>int</code></dt>
<dd>How many Gibbs steps to delay updating the values</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def initialize(self, value_option: str, p: float=None, delay: int=0):
    &#39;&#39;&#39;Initialize the based on the passed in option.

    Parameters
    ----------
    value_option : str
        - Different ways to initialize the values. Options:
            - &#39;auto&#39;, &#39;all-off&#39;
                - Turn all of the indicators off
            - &#39;all-on&#39;
                - Turn all the indicators on
            - &#39;random&#39;
                - Randomly assign the indicator with probability `p`
    p : float
        Only required if `value_option` == &#39;random&#39;
    delay : int
        How many Gibbs steps to delay updating the values
    &#39;&#39;&#39;
    # print(&#39;in pert ind&#39;)
    if not pl.isint(delay):
        raise TypeError(&#39;`delay` ({}) must be an int&#39;.format(type(delay)))
    if delay &lt; 0:
        raise ValueError(&#39;`delay` ({}) must be &gt;= 0&#39;.format(delay))
    self.delay = delay

    for perturbation in self.perturbations:
        perturbation.indicator.set_signal_when_clusters_change(True)
    self.clustering = self.G.perturbations[0].indicator.clustering # mdsine2.pylab.cluster.Clustering

    # Set the value
    if not pl.isstr(value_option):
        raise ValueError(&#39;`value_option` ({}) must be a str&#39;.format(type(value_option)))
    if value_option in [&#39;all-off&#39;, &#39;auto&#39;]:
        value = False
    elif value_option == &#39;all-on&#39;:
        value = True
    elif value_option == &#39;random&#39;:
        if not pl.isfloat(p):
            raise TypeError(&#39;`p` ({}) must be a float&#39;.format(type(p)))
        if p &lt; 0 or p &gt; 1:
            raise ValueError(&#39;`p` ({}) must be [0,1]&#39;.format(p))
    else:
        raise ValueError(&#39;`value_option` ({}) not recognized&#39;.format(value_option))
    for perturbation in self.perturbations:
        for cid in perturbation.clustering.clusters:
            if value_option == &#39;random&#39;:
                perturbation.indicator.value[cid] = bool(pl.random.bernoulli.sample(p))
            else:
                perturbation.indicator.value[cid] = value

    # These are for the function `self._make_idx_for_clusters`
    self.ndts_bias = []
    self.n_replicates = self.G.data.n_replicates
    self.n_perturbations = len(self.G.perturbations)
    self.n_dts_for_replicate = self.G.data.n_dts_for_replicate
    self.total_dts = np.sum(self.n_dts_for_replicate)
    self.replicate_bias = np.zeros(self.n_replicates, dtype=int)
    self.n_taxa = len(self.G.data.taxa)
    for ridx in range(1, self.n_replicates):
        self.replicate_bias[ridx] = self.replicate_bias[ridx-1] + \
            self.n_taxa * self.n_dts_for_replicate[ridx - 1]
    for ridx in range(self.G.data.n_replicates):
        self.ndts_bias.append(
            np.arange(0, self.G.data.n_dts_for_replicate[ridx] * self.n_taxa, self.n_taxa))

    s = &#39;Perturbation indicator initialization results:\n&#39;
    for i, perturbation in enumerate(self.perturbations):
        s += &#39;\tPerturbation {}:\n&#39; \
            &#39;\t\tindicator: {}\n&#39;.format(i, perturbation.indicator.cluster_bool_array())
    logging.info(s)</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.PerturbationIndicators.make_rel_params"><code class="name flex">
<span>def <span class="ident">make_rel_params</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>We make the parameters needed to update the relative log-likelihod.
This function is called once at the beginning of the update.</p>
<p>THIS ASSUMES THAT EACH PERTURBATION CLUSTERS ARE DEFINED BY THE SAME CLUSTERS
- To make this separate, make a higher level list for each perturbation index
for each individual perturbation</p>
<h2 id="parameters-that-we-create-with-this-function">Parameters That We Create With This Function</h2>
<ul>
<li>ys : dict (int -&gt; np.ndarray)<ul>
<li>Maps the target cluster id to the observation matrix that it
corresponds to (only the Taxa in the target cluster). This
array already has the growth and self-interactions subtracted
out:<ul>
<li>$
rac{log(x_{k+1}) - log(x_{k})}{dt} - a_{1,k} - a_{2,k}x_{k} $</li>
</ul>
</li>
</ul>
</li>
<li>process_precs : dict (int -&gt; np.ndarray)<ul>
<li>Maps the target cluster id to the vector of the process precision
that corresponds to the target cluster (only the Taxa in the target
cluster). This is a 1D array that corresponds to the diagonal of what
would be the precision matrix.</li>
</ul>
</li>
<li>interactionXs : dict (int -&gt; np.ndarray)<ul>
<li>Maps the target cluster id to the design matrix for the interactions
going into that cluster. We pre-index it with the rows and columns</li>
</ul>
</li>
<li>prior_prec_interaction : dict (int -&gt; np.ndarray)<ul>
<li>Maps the target cluster id to to the diagonal of the prior precision
for the interaction values.</li>
</ul>
</li>
<li>prior_mean_interaction : dict (int -&gt; np.ndarray)<ul>
<li>Maps the target cluster id to to the diagonal of the prior mean
for the interaction values.</li>
</ul>
</li>
<li>prior_ll_ons : np.ndarray<ul>
<li>Prior log likelihood of a positive indicator. These are separate for each
perturbation.</li>
</ul>
</li>
<li>prior_ll_offs : np.ndarray<ul>
<li>Prior log likelihood of the negative indicator. These are separate for each
perturbation.</li>
</ul>
</li>
<li>priorvar_logdet_diffs : np.ndarray<ul>
<li>This is the prior variance log determinant that we add when the indicator
is positive. This is different for each perturbation.</li>
</ul>
</li>
<li>perturbationsXs : dict (int -&gt; np.ndarray)<ul>
<li>Maps the target cluster id to the design matrix that corresponds to
the on perturbations of the target clusters. This is preindexed by the
rows but not the columns - the columns assume that all of the perturbations
are on and we index the ones that we want.</li>
</ul>
</li>
<li>prior_prec_perturbations : np.ndarray<ul>
<li>This is the prior precision of the magnitude for each of the perturbations. Use
the perturbation index to get the value</li>
</ul>
</li>
<li>prior_mean_perturbations : np.ndarray<ul>
<li>This is the prior mean of the magnitude for each one of the perturbations. Use
the perturbation index to get the value</li>
</ul>
</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def make_rel_params(self):
    &#39;&#39;&#39;We make the parameters needed to update the relative log-likelihod.
    This function is called once at the beginning of the update.

    THIS ASSUMES THAT EACH PERTURBATION CLUSTERS ARE DEFINED BY THE SAME CLUSTERS
        - To make this separate, make a higher level list for each perturbation index
          for each individual perturbation

    Parameters that we create with this function
    --------------------------------------------
    - ys : dict (int -&gt; np.ndarray)
        - Maps the target cluster id to the observation matrix that it
          corresponds to (only the Taxa in the target cluster). This 
          array already has the growth and self-interactions subtracted
          out:
            - $ \frac{log(x_{k+1}) - log(x_{k})}{dt} - a_{1,k} - a_{2,k}x_{k} $
    - process_precs : dict (int -&gt; np.ndarray)
        - Maps the target cluster id to the vector of the process precision
          that corresponds to the target cluster (only the Taxa in the target
          cluster). This is a 1D array that corresponds to the diagonal of what
          would be the precision matrix.
    - interactionXs : dict (int -&gt; np.ndarray)
        - Maps the target cluster id to the design matrix for the interactions
          going into that cluster. We pre-index it with the rows and columns
    - prior_prec_interaction : dict (int -&gt; np.ndarray)
        - Maps the target cluster id to to the diagonal of the prior precision 
          for the interaction values.
    - prior_mean_interaction : dict (int -&gt; np.ndarray)
        - Maps the target cluster id to to the diagonal of the prior mean 
          for the interaction values.
    - prior_ll_ons : np.ndarray
        - Prior log likelihood of a positive indicator. These are separate for each
          perturbation.
    - prior_ll_offs : np.ndarray
        - Prior log likelihood of the negative indicator. These are separate for each
          perturbation.
    - priorvar_logdet_diffs : np.ndarray
        - This is the prior variance log determinant that we add when the indicator
          is positive. This is different for each perturbation.
    - perturbationsXs : dict (int -&gt; np.ndarray)
        - Maps the target cluster id to the design matrix that corresponds to 
          the on perturbations of the target clusters. This is preindexed by the 
          rows but not the columns - the columns assume that all of the perturbations
          are on and we index the ones that we want.
    - prior_prec_perturbations : np.ndarray
        - This is the prior precision of the magnitude for each of the perturbations. Use
          the perturbation index to get the value
    - prior_mean_perturbations : np.ndarray
        - This is the prior mean of the magnitude for each one of the perturbations. Use
          the perturbation index to get the value
    &#39;&#39;&#39;
    row_idxs = self._make_idx_for_clusters()

    # Create ys
    self.ys = {}
    y = self.G.data.construct_lhs(keys=[
        STRNAMES.SELF_INTERACTION_VALUE, STRNAMES.GROWTH_VALUE],
        kwargs_dict={STRNAMES.GROWTH_VALUE:{&#39;with_perturbations&#39;: False}})
    for tcid in self.clustering.order:
        self.ys[tcid] = y[row_idxs[tcid], :]

    # Create process_precs
    self.process_precs = {}
    process_prec_diag = self.G[STRNAMES.PROCESSVAR].prec
    for tcid in self.clustering.order:
        self.process_precs[tcid] = process_prec_diag[row_idxs[tcid]]

    # Make interactionXs
    self.interactionXs = {}
    interactions = self.G[STRNAMES.INTERACTIONS_OBJ]
    XM_master = self.G.data.design_matrices[STRNAMES.CLUSTER_INTERACTION_VALUE].toarray()
    for tcid in self.clustering.order:
        cols = []
        for i, interaction in enumerate(interactions.iter_valid()):
            if interaction.target_cid == tcid:
                if interaction.indicator:
                    cols.append(i)
        cols = np.asarray(cols, dtype=int)
        self.interactionXs[tcid] = pl.util.fast_index(M=XM_master, 
            rows=row_idxs[tcid], cols=cols)

    # Make prior parameters for interactions
    self.prior_prec_interaction = 1/self.G[STRNAMES.PRIOR_VAR_INTERACTIONS].value
    self.prior_mean_interaction = self.G[STRNAMES.PRIOR_MEAN_INTERACTIONS].value

    # Make the perturbation parameters
    self.prior_ll_ons = []
    self.prior_ll_offs = []
    self.priorvar_logdet_diffs = []
    self.prior_prec_perturbations = []
    self.prior_mean_perturbations = []

    for perturbation in self.G.perturbations:
        prob_on = perturbation.probability.value
        self.prior_ll_ons.append(np.log(prob_on))
        self.prior_ll_offs.append(np.log(1 - prob_on))
        
        self.priorvar_logdet_diffs.append(
            np.log(perturbation.magnitude.prior.scale2.value))

        self.prior_prec_perturbations.append( 
            1/perturbation.magnitude.prior.scale2.value)

        self.prior_mean_perturbations.append(
            perturbation.magnitude.prior.loc.value)

    # Make perturbation matrices
    self.perturbationsXs = {}
    self.G.data.design_matrices[STRNAMES.PERT_VALUE].M.build(build=True, 
        build_for_neg_ind=True)
    Xpert_master = self.G.data.design_matrices[STRNAMES.PERT_VALUE].toarray()
    for tcid in self.clustering.order:
        self.perturbationsXs[tcid] = Xpert_master[row_idxs[tcid], :]

    self.n_clusters = len(self.clustering.order)
    self.clustering_order = self.clustering.order

    self.col2pidxcidx = []
    for pidx in range(len(self.perturbations)):
        for cidx in range(len(self.clustering.order)):
            self.col2pidxcidx.append((pidx, cidx))

    self.arr = []
    for perturbation in self.perturbations:
        self.arr = np.append(
            self.arr, 
            perturbation.indicator.cluster_bool_array())
    self.arr = np.asarray(self.arr, dtype=bool)</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.PerturbationIndicators.set_trace"><code class="name flex">
<span>def <span class="ident">set_trace</span></span>(<span>self, *args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Only trace if perturbation indicators are being learned and the
perturbation value is not being learned</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_trace(self, *args, **kwargs):
    &#39;&#39;&#39;Only trace if perturbation indicators are being learned and the
    perturbation value is not being learned
    &#39;&#39;&#39;
    if self.need_to_trace:
        for perturbation in self.perturbations:
            perturbation.set_trace(*args, **kwargs)</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.PerturbationIndicators.total_on"><code class="name flex">
<span>def <span class="ident">total_on</span></span>(<span>self) ‑> int</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def total_on(self) -&gt; int:
    n = 0
    for perturbation in self.perturbations:
        n += perturbation.indicator.num_on_clusters()
    return n</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.PerturbationIndicators.update_relative"><code class="name flex">
<span>def <span class="ident">update_relative</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Update each perturbation indicator for the given cluster by
calculating the realtive loglikelihoods of it being on/off as
supposed to as is. Because this is a relative loglikelihood, we
only need to take into account the following parameters of the
model:
- Only the Taxa in the cluster in question
- Only the perturbations for that cluster
- Only the interactions going into the cluster</p>
<p>Because these matrices are considerably smaller and considered 'dense', we
do the operations in numpy instead of scipy sparse.</p>
<p>We permute the order that the indices are updated for more robust mixing</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_relative(self):
    &#39;&#39;&#39;Update each perturbation indicator for the given cluster by
    calculating the realtive loglikelihoods of it being on/off as
    supposed to as is. Because this is a relative loglikelihood, we
    only need to take into account the following parameters of the
    model:
        - Only the Taxa in the cluster in question
        - Only the perturbations for that cluster
        - Only the interactions going into the cluster

    Because these matrices are considerably smaller and considered &#39;dense&#39;, we
    do the operations in numpy instead of scipy sparse.

    We permute the order that the indices are updated for more robust mixing
    &#39;&#39;&#39;
    if self.sample_iter &lt; self.delay:
        return
    start_time = time.time()

    self.make_rel_params()

    # Iterate over each perturbation indicator variable
    iidxs = npr.permutation(len(self.arr))
    for iidx in iidxs:
        self.update_single_idx_fast(idx=iidx)

    # Set the perturbation indicators from arr
    i = 0
    for perturbation in self.perturbations:
        for cid in self.clustering.order:
            perturbation.indicator.value[cid] = self.arr[i]
            i += 1

    # rebuild the growth design matrix
    self.G.data.design_matrices[STRNAMES.PERT_VALUE].M.build()
    self.G.data.design_matrices[STRNAMES.GROWTH_VALUE].build_with_perturbations()
    self._time_taken = time.time() - start_time</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.PerturbationIndicators.update_single_idx_fast"><code class="name flex">
<span>def <span class="ident">update_single_idx_fast</span></span>(<span>self, idx: int)</span>
</code></dt>
<dd>
<div class="desc"><p>Do a Gibbs step for a single cluster</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_single_idx_fast(self, idx: int):
    &#39;&#39;&#39;Do a Gibbs step for a single cluster
    &#39;&#39;&#39;
    pidx, cidx = self.col2pidxcidx[idx]

    prior_ll_on = self.prior_ll_ons[pidx]
    prior_ll_off = self.prior_ll_offs[pidx]

    d_on = self.calculate_relative_marginal_loglikelihood(idx=idx, val=True)
    d_off = self.calculate_relative_marginal_loglikelihood(idx=idx, val=False)

    ll_on = d_on + prior_ll_on
    ll_off = d_off + prior_ll_off
    dd = [ll_off, ll_on]

    # print(&#39;\nindicator&#39;, idx)
    # print(&#39;fast\n\ttotal: {}\n\tbeta_logdet_diff: {}\n\t&#39; \
    #     &#39;priorvar_logdet_diff: {}\n\tbEb_diff: {}\n\t&#39; \
    #     &#39;bEbprior_diff: {}&#39;.format(
    #         ll_on - ll_off,
    #         d_on[&#39;beta_logdet&#39;] - d_off[&#39;beta_logdet&#39;],
    #         d_on[&#39;priorvar_logdet&#39;] - d_off[&#39;priorvar_logdet&#39;],
    #         d_on[&#39;bEb&#39;] - d_off[&#39;bEb&#39;],
    #         d_on[&#39;bEbprior&#39;] - d_off[&#39;bEbprior&#39;]))
    # self.update_single_idx_slow(idx)

    res = bool(sample_categorical_log(dd))
    self.arr[idx] = res</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.PerturbationIndicators.update_single_idx_slow"><code class="name flex">
<span>def <span class="ident">update_single_idx_slow</span></span>(<span>self, idx: int)</span>
</code></dt>
<dd>
<div class="desc"><p>Do a Gibbs step for a single cluster and perturbation</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>idx</code></strong> :&ensp;<code>int</code></dt>
<dd>This is the index of the indicator in vectorized form</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_single_idx_slow(self, idx: int):
    &#39;&#39;&#39;Do a Gibbs step for a single cluster and perturbation

    Parameters
    ----------
    idx : int
        This is the index of the indicator in vectorized form
    &#39;&#39;&#39;
    cidx = idx % self.G.data.n_taxa
    cid = self.clustering.order[cidx]
    
    pidx = idx // self.G.data.n_taxa
    perturbation = self.perturbations[pidx]

    d_on = self.calculate_marginal_loglikelihood(cid=cid, val=True,
        perturbation=perturbation)
    d_off = self.calculate_marginal_loglikelihood(cid=cid, val=False,
        perturbation=perturbation)

    prior_ll_on = np.log(perturbation.probability.value)
    prior_ll_off = np.log(1 - perturbation.probability.value)

    ll_on = d_on[&#39;ret&#39;] + prior_ll_on
    ll_off = d_off[&#39;ret&#39;] + prior_ll_off
    dd = [ll_off, ll_on]

    res = bool(sample_categorical_log(dd))
    if perturbation.indicator.value[cid] != res:
        perturbation.indicator.value[cid] = res
        self.G.data.design_matrices[STRNAMES.PERT_VALUE].build()</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.PerturbationIndicators.update_slow"><code class="name flex">
<span>def <span class="ident">update_slow</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Update each cluster indicator variable for the perturbation</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_slow(self):
    &#39;&#39;&#39;Update each cluster indicator variable for the perturbation
    &#39;&#39;&#39;
    start_time = time.time()

    if self.sample_iter &lt; self.delay:
        return

    n_clusters = len(self.clustering.order)
    n_perturbations = len(self.perturbations)
    idxs = npr.permutation(int(n_clusters*n_perturbations))
    for idx in idxs:
        self.update_single_idx_slow(idx=idx)

    # rebuild the growth design matrix
    self.G.data.design_matrices[STRNAMES.PERT_VALUE].M.build()
    self.G.data.design_matrices[STRNAMES.GROWTH_VALUE].build_with_perturbations()
    self._time_taken = time.time() - start_time</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.PerturbationIndicators.visualize"><code class="name flex">
<span>def <span class="ident">visualize</span></span>(<span>self, path: str, pidx: int, section: str = 'posterior', fixed_clustering: bool = False)</span>
</code></dt>
<dd>
<div class="desc"><p>Make a table of the <code>pidx</code>th perturbation bayes factors</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>path</code></strong> :&ensp;<code>str</code></dt>
<dd>This is the path to write the bayes factors</dd>
<dt><strong><code>f</code></strong> :&ensp;<code>_io.TextIOWrapper</code></dt>
<dd>File that we are writing the values to</dd>
<dt><strong><code>section</code></strong> :&ensp;<code>str</code></dt>
<dd>Section of the trace to compute on. Options:
'posterior' : posterior samples
'burnin' : burn-in samples
'entire' : both burn-in and posterior samples</dd>
<dt><strong><code>fixed_clustering</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, plot the variable as if clustering was fixed. Since the
cluster assignments never change, all of the taxa within the cluster
have identical values, so we can choose any taxon within the cluster to
represent it</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def visualize(self, path: str, pidx: int, section: str=&#39;posterior&#39;, fixed_clustering: bool=False):
    &#39;&#39;&#39;Make a table of the `pidx`th perturbation bayes factors

    Parameters
    ----------
    path : str
        This is the path to write the bayes factors
    f : _io.TextIOWrapper
        File that we are writing the values to
    section : str
        Section of the trace to compute on. Options:
            &#39;posterior&#39; : posterior samples
            &#39;burnin&#39; : burn-in samples
            &#39;entire&#39; : both burn-in and posterior samples
    fixed_clustering : bool
        If True, plot the variable as if clustering was fixed. Since the
        cluster assignments never change, all of the taxa within the cluster
        have identical values, so we can choose any taxon within the cluster to
        represent it
    &#39;&#39;&#39;
    from .util import generate_perturbation_bayes_factors_posthoc

    perturbation = self.perturbations[pidx]
    bayes_factors = generate_perturbation_bayes_factors_posthoc(
        mcmc=self.G.inference, perturbation=perturbation, section=section)
    if fixed_clustering:
        # Condense the taxa level bayes factors to cluster level bayes factors
        clustering = self.G[STRNAMES.CLUSTERING_OBJ]
        bf_clusters = []
        for cluster in clustering:
            aidx = list(cluster.members)[0]
            bf_clusters.append(bayes_factors[aidx])
        bf_clusters = np.asarray(bf_clusters)
        df = pd.DataFrame([bf_clusters], index=[perturbation.name],
            columns=[&#39;Cluster {}&#39;.format(cidx+1) for cidx in range(len(clustering))])
    else:
        df = pd.DataFrame([bayes_factors], index=[perturbation.name], 
            columns=[taxon.name for taxon in self.G.data.taxa])
    df.to_csv(path, sep=&#39;\t&#39;, index=True, header=True)</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="mdsine2.pylab.graph.Node" href="pylab/graph.html#mdsine2.pylab.graph.Node">Node</a></b></code>:
<ul class="hlist">
<li><code><a title="mdsine2.pylab.graph.Node.add_child" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_child">add_child</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.add_parent" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_parent">add_parent</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.add_prior" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_prior">add_prior</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.add_undirected" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_undirected">add_undirected</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.degree" href="pylab/graph.html#mdsine2.pylab.graph.Node.degree">degree</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.delete" href="pylab/graph.html#mdsine2.pylab.graph.BaseNode.delete">delete</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.get_adjacent_keys" href="pylab/graph.html#mdsine2.pylab.graph.Node.get_adjacent_keys">get_adjacent_keys</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.load" href="pylab/base.html#mdsine2.pylab.base.Saveable.load">load</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.metropolis" href="pylab/graph.html#mdsine2.pylab.graph.Node.metropolis">metropolis</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.save" href="pylab/base.html#mdsine2.pylab.base.Saveable.save">save</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.set_save_location" href="pylab/base.html#mdsine2.pylab.base.Saveable.set_save_location">set_save_location</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="mdsine2.posterior.PerturbationMagnitudes"><code class="flex name class">
<span>class <span class="ident">PerturbationMagnitudes</span></span>
<span>(</span><span>**kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>These update the perturbation values jointly.</p>
<p>Parameters</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PerturbationMagnitudes(pl.variables.Normal):
    &#39;&#39;&#39;These update the perturbation values jointly.
    &#39;&#39;&#39;
    def __init__(self, **kwargs):
        &#39;&#39;&#39;Parameters
        &#39;&#39;&#39;

        kwargs[&#39;name&#39;] = STRNAMES.PERT_VALUE
        pl.variables.Normal.__init__(self, loc=None, scale2=None, dtype=float, **kwargs)
        self.perturbations = self.G.perturbations # mdsine2.pylab.base.Perturbations 

    def __str__(self) -&gt; str:
        s = &#39;Perturbation Magnitudes (multiplicative)&#39;
        for perturbation in self.perturbations:
            s += &#39;\n\t perturbation {}: {}&#39;.format(
                perturbation.name, perturbation.cluster_array(only_pos_ind=True))
        return s

    def __len__(self) -&gt; int:
        &#39;&#39;&#39;Return the number of on indicators
        &#39;&#39;&#39;
        n = 0
        for perturbation in self.perturbations:
            n += perturbation.indicator.num_on_clusters()
        return n

    def set_values(self, arr: np.ndarray, use_indicators: bool=True):
        &#39;&#39;&#39;Set the values of the perturbation of them stacked one on top of each other

        Parameters
        ----------
        arr : np.ndarray
            Values for all of the perturbations in order
        use_indicators : bool
            If True, the values only refer to the on indicators
        &#39;&#39;&#39;
        i = 0
        for perturbation in self.perturbations:
            l = perturbation.indicator.num_on_clusters()
            perturbation.set_values_from_array(values=arr[i:i+l],
                use_indicators=use_indicators)
            i += l

    def update_str(self):
        return

    @property
    def sample_iter(self):
        return self.perturbations[0].sample_iter

    def initialize(self, value_option: str, value: np.ndarray=None, 
        loc: Union[float, int]=None, delay: int=0):
        &#39;&#39;&#39;Initialize the prior and the value of the perturbation. We assume that
        each perturbation has the same hyperparameters for the prior

        Parameters
        ----------
        value_option : str
            How to initialize the values. Options:
                &#39;manual&#39;
                    Set the value manually, `value` must also be specified
                &#39;zero&#39;
                    Set all the values to zero.
                &#39;auto&#39;, &#39;prior-mean&#39;
                    Initialize to the same value as the prior mean
        delay : int, None
            How many MCMC iterations to delay the update of the values.
        loc, value : int, float, array
            - Only necessary if any of the options are &#39;manual&#39;
        &#39;&#39;&#39;
        if delay is None:
            delay = 0
        if not pl.isint(delay):
            raise TypeError(&#39;`delay` ({}) must be an int&#39;.format(type(delay)))
        if delay &lt; 0:
            raise ValueError(&#39;`delay` ({}) must be &gt;= 0&#39;.format(delay))
        self.delay = delay
        for perturbation in self.perturbations:
            perturbation.magnitude.set_signal_when_clusters_change(True)

        # Set the value of the perturbations
        if not pl.isstr(value_option):
            raise TypeError(&#39;`value_option` ({}) must be a str&#39;.format(type(value_option)))
        if value_option == &#39;manual&#39;:
            for pidx, perturbation in enumerate(self.perturbations):
                v = value[pidx]
                for cidx, val in enumerate(v):
                    cid = perturbation.clustering.order[cidx]
                    perturbation.indicator.value[cid] = not np.isnan(val)
                    perturbation.magnitude.value[cid] = val if not np.isnan(val) else 0

        elif value_option == &#39;zero&#39;:
            for perturbation in self.perturbations:
                for cid in perturbation.clustering.order:
                    perturbation.magnitude.value[cid] = 0
        elif value_option in [&#39;auto&#39;, &#39;prior-mean&#39;]:
            for perturbation in self.perturbations:
                loc = perturbation.magnitude.prior.loc.value
                for cid in perturbation.clustering.order:
                    perturbation.magnitude.value[cid] = loc
        else:
            raise ValueError(&#39;`value_option` ({}) not recognized&#39;.format(value_option))


        s = &#39;Perturbation magnitude initialization results:\n&#39;
        for perturbation in self.perturbations:
            if perturbation.name is not None:
                a = perturbation.name
            s += &#39;\tPerturbation {}:\n&#39; \
                &#39;\t\tvalue: {}\n&#39;.format(a, perturbation.magnitude.cluster_array())
        logging.info(s)

    def update(self):
        &#39;&#39;&#39;Update with a gibbs step jointly
        &#39;&#39;&#39;
        if self.sample_iter &lt; self.delay:
            return

        n_on = [perturbation.indicator.num_on_clusters() for perturbation in \
            self.perturbations]
        
        if n_on == 0:
            return

        rhs = [STRNAMES.PERT_VALUE]
        lhs = [
            STRNAMES.GROWTH_VALUE,
            STRNAMES.SELF_INTERACTION_VALUE,
            STRNAMES.CLUSTER_INTERACTION_VALUE]
        X = self.G.data.construct_rhs(keys=rhs, toarray=True)
        y = self.G.data.construct_lhs(keys=lhs,
            kwargs_dict={STRNAMES.GROWTH_VALUE:{
                &#39;with_perturbations&#39;:False}})

        process_prec = self.G[STRNAMES.PROCESSVAR].prec
        prior_prec = build_prior_covariance(G=self.G, cov=False, order=rhs, sparse=False)

        prior_mean = build_prior_mean(G=self.G, order=rhs).reshape(-1,1)

        a = X.T * process_prec
        prec = a @ X + prior_prec
        cov = pinv(prec, self)
        mean = np.asarray(cov @ (a @ y + prior_prec @ prior_mean)).ravel()

        # print(&#39;\n\ny\n&#39;,np.hstack((y, self.G.data.lhs.vector.reshape(-1,1))))
        # print(self.G[STRNAMES.CLUSTER_INTERACTION_VALUE].value)
        # print(self.G[STRNAMES.GROWTH_VALUE].value)
        # print(self.G[STRNAMES.SELF_INTERACTION_VALUE].value)

        self.loc.value = mean
        self.scale2.value = np.diag(cov)
        value = self.sample()

        if np.any(np.isnan(value)):
            logging.critical(&#39;mean: {}&#39;.format(self.loc.value))
            logging.critical(&#39;var: {}&#39;.format(self.scale2.value))
            logging.critical(&#39;value: {}&#39;.format(self.value))
            logging.critical(&#39;prior mean: {}&#39;.format(prior_mean.ravel()))
            raise ValueError(&#39;`Values in {} are nan: {}&#39;.format(self.name, self.value))

        i = 0
        for pidx, perturbation in enumerate(self.perturbations):
            perturbation.set_values_from_array(value[i:i+n_on[pidx]], use_indicators=True)
            i += n_on[pidx]

        # Rebuild the design matrix
        self.G.data.design_matrices[STRNAMES.GROWTH_VALUE].build_with_perturbations()

    def set_trace(self, *args, **kwargs):
        for perturbation in self.perturbations:
            perturbation.set_trace(*args, **kwargs)

    def add_trace(self, *args, **kwargs):
        for perturbation in self.perturbations:
            perturbation.add_trace(*args, **kwargs)

    def asarray(self) -&gt; np.ndarray:
        &#39;&#39;&#39;Get an array of the perturbation magnitudes
        &#39;&#39;&#39;
        a = []
        for perturbation in self.perturbations:
            a.append(perturbation.cluster_array(only_pos_ind=True))
        return np.asarray(list(itertools.chain.from_iterable(a)))

    def toarray(self) -&gt; np.ndarray:
        &#39;&#39;&#39;Alias for asarray
        &#39;&#39;&#39;
        return self.asarray()

    def visualize(self, basepath: str, pidx: int, section: str=&#39;posterior&#39;, taxa_formatter: str=&#39;%(name)s&#39;,
        fixed_clustering: bool=False):
        &#39;&#39;&#39;Render the traces in the folder `basepath`. Makes a `pandas.DataFrame` table
        where the index is the Taxa name in `taxa_formatter` and the columns are
        `mean`, `median`, `25th percentile`, `75th` percentile`.

        Parameters
        ----------
        basepath : str
            This is the loction to write the files to
        section : str
            Section of the trace to compute on. Options:
                &#39;posterior&#39; : posterior samples
                &#39;burnin&#39; : burn-in samples
                &#39;entire&#39; : both burn-in and posterior samples
        taxa_formatter : str, None
            This is the format of the label to return for each Taxa. If None, it will return
            the taxon&#39;s name
        fixed_clustering : bool
            If True, plot the variable as if clustering was fixed. Since the
            cluster assignments never change, all of the taxa within the cluster
            have identical values, so we can choose any taxon within the cluster to
            represent it
        &#39;&#39;&#39;
        perturbation = self.perturbations[pidx]
        taxa = self.G.data.taxa
        clustering = self.G[STRNAMES.CLUSTERING_OBJ]

        summ = pl.summary(perturbation, set_nan_to_0=True, section=section)
        data = np.asarray([arr for _,arr in summ.items()]).T
        if fixed_clustering:
            data_clustering = []
            for cluster in clustering:
                aidx = list(cluster.members)[0]
                data_clustering.append(data[aidx, :])
            index = [&#39;Cluster {}&#39;.format(cidx + 1) for cidx in range(len(clustering))]
            data = data_clustering
        else:
            index = [taxon.name for taxon in self.G.data.taxa]
        df = pd.DataFrame(data, columns=[k for k in summ], 
            index=index)
        df.to_csv(os.path.join(basepath, &#39;values.tsv&#39;), sep=&#39;\t&#39;, index=True, header=True)

        if section == &#39;posterior&#39;:
            len_posterior = self.G.inference.sample_iter + 1 - self.G.inference.burnin
        elif section == &#39;burnin&#39;:
            len_posterior = self.G.inference.burnin
        else:
            len_posterior = self.G.inference.sample_iter + 1

        # Make the prior
        if self.G.inference.tracer.is_being_traced(STRNAMES.PRIOR_MEAN_PERT):
            prior_mean_trace = perturbation.magnitude.prior.loc.get_trace_from_disk(section=section)
        else:
            prior_mean_trace = np.ones(len_posterior) + perturbation.magnitude.prior.loc.value
        if self.G.inference.tracer.is_being_traced(STRNAMES.PRIOR_VAR_PERT):
            prior_std_trace = np.sqrt(perturbation.magnitude.prior.scale2.get_trace_from_disk(section=section))
        else:
            prior_std_trace = np.ones(len_posterior) + np.sqrt(perturbation.magnitude.prior.scale2.value)
        
        if fixed_clustering:
            rang = len(clustering)
        else:
            rang = len(self.G.data.taxa)

        for iii in range(rang):
            if fixed_clustering:
                cluster = clustering[iii]
                oidx = list(cluster.members)[0]
            else:
                oidx = iii
            fig = plt.figure()
            ax_posterior = fig.add_subplot(1,2,1)
            visualization.render_trace(var=perturbation, idx=oidx, plt_type=&#39;hist&#39;,
                label=section, color=&#39;blue&#39;, ax=ax_posterior, section=section,
                include_burnin=True, rasterized=True)

            low_x, high_x = ax_posterior.get_xlim()
            arr = np.zeros(len(prior_std_trace), dtype=float)
            for i in range(len(prior_std_trace)):
                arr[i] = pl.random.normal.sample(loc=prior_mean_trace[i], scale=prior_std_trace[i])
            visualization.render_trace(var=arr, plt_type=&#39;hist&#39;, 
                label=&#39;prior&#39;, color=&#39;red&#39;, ax=ax_posterior, rasterized=True)

            ax_posterior.legend()
            ax_posterior.set_xlim(left=low_x*.8, right=high_x*1.2)

            # plot the trace
            ax_trace = fig.add_subplot(1,2,2)
            visualization.render_trace(var=perturbation, idx=oidx, plt_type=&#39;trace&#39;, 
                ax=ax_trace, section=section, include_burnin=True, rasterized=True)

            if fixed_clustering:
                fig.suptitle(&#39;Cluster {}&#39;.format(iii+1))
            else:
                fig.suptitle(&#39;{}&#39;.format(pl.taxaname_formatter(
                    format=taxa_formatter, taxon=oidx, taxa=self.G.data.taxa)))
            fig.tight_layout()
            fig.subplots_adjust(top=0.85)
            if fixed_clustering:
                plt.savefig(os.path.join(basepath, &#39;cluster{}.pdf&#39;.format(iii+1)))
            else:
                plt.savefig(os.path.join(basepath, &#39;{}.pdf&#39;.format(taxa[oidx].name)))
            plt.close()</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="mdsine2.pylab.variables.Normal" href="pylab/variables.html#mdsine2.pylab.variables.Normal">Normal</a></li>
<li><a title="mdsine2.pylab.variables.Variable" href="pylab/variables.html#mdsine2.pylab.variables.Variable">Variable</a></li>
<li><a title="mdsine2.pylab.graph.Node" href="pylab/graph.html#mdsine2.pylab.graph.Node">Node</a></li>
<li><a title="mdsine2.pylab.graph.BaseNode" href="pylab/graph.html#mdsine2.pylab.graph.BaseNode">BaseNode</a></li>
<li><a title="mdsine2.pylab.base.Saveable" href="pylab/base.html#mdsine2.pylab.base.Saveable">Saveable</a></li>
<li>mdsine2.pylab.variables._BaseArithmeticClass</li>
<li><a title="mdsine2.pylab.base.Traceable" href="pylab/base.html#mdsine2.pylab.base.Traceable">Traceable</a></li>
<li>mdsine2.pylab.variables._RandomBase</li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="mdsine2.posterior.PerturbationMagnitudes.sample_iter"><code class="name">var <span class="ident">sample_iter</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def sample_iter(self):
    return self.perturbations[0].sample_iter</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="mdsine2.posterior.PerturbationMagnitudes.asarray"><code class="name flex">
<span>def <span class="ident">asarray</span></span>(<span>self) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Get an array of the perturbation magnitudes</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def asarray(self) -&gt; np.ndarray:
    &#39;&#39;&#39;Get an array of the perturbation magnitudes
    &#39;&#39;&#39;
    a = []
    for perturbation in self.perturbations:
        a.append(perturbation.cluster_array(only_pos_ind=True))
    return np.asarray(list(itertools.chain.from_iterable(a)))</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.PerturbationMagnitudes.initialize"><code class="name flex">
<span>def <span class="ident">initialize</span></span>(<span>self, value_option: str, value: numpy.ndarray = None, loc: Union[float, int] = None, delay: int = 0)</span>
</code></dt>
<dd>
<div class="desc"><p>Initialize the prior and the value of the perturbation. We assume that
each perturbation has the same hyperparameters for the prior</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>value_option</code></strong> :&ensp;<code>str</code></dt>
<dd>How to initialize the values. Options:
'manual'
Set the value manually, <code>value</code> must also be specified
'zero'
Set all the values to zero.
'auto', 'prior-mean'
Initialize to the same value as the prior mean</dd>
<dt><strong><code>delay</code></strong> :&ensp;<code>int, None</code></dt>
<dd>How many MCMC iterations to delay the update of the values.</dd>
<dt><strong><code>loc</code></strong>, <strong><code>value</code></strong> :&ensp;<code>int, float, array</code></dt>
<dd>
<ul>
<li>Only necessary if any of the options are 'manual'</li>
</ul>
</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def initialize(self, value_option: str, value: np.ndarray=None, 
    loc: Union[float, int]=None, delay: int=0):
    &#39;&#39;&#39;Initialize the prior and the value of the perturbation. We assume that
    each perturbation has the same hyperparameters for the prior

    Parameters
    ----------
    value_option : str
        How to initialize the values. Options:
            &#39;manual&#39;
                Set the value manually, `value` must also be specified
            &#39;zero&#39;
                Set all the values to zero.
            &#39;auto&#39;, &#39;prior-mean&#39;
                Initialize to the same value as the prior mean
    delay : int, None
        How many MCMC iterations to delay the update of the values.
    loc, value : int, float, array
        - Only necessary if any of the options are &#39;manual&#39;
    &#39;&#39;&#39;
    if delay is None:
        delay = 0
    if not pl.isint(delay):
        raise TypeError(&#39;`delay` ({}) must be an int&#39;.format(type(delay)))
    if delay &lt; 0:
        raise ValueError(&#39;`delay` ({}) must be &gt;= 0&#39;.format(delay))
    self.delay = delay
    for perturbation in self.perturbations:
        perturbation.magnitude.set_signal_when_clusters_change(True)

    # Set the value of the perturbations
    if not pl.isstr(value_option):
        raise TypeError(&#39;`value_option` ({}) must be a str&#39;.format(type(value_option)))
    if value_option == &#39;manual&#39;:
        for pidx, perturbation in enumerate(self.perturbations):
            v = value[pidx]
            for cidx, val in enumerate(v):
                cid = perturbation.clustering.order[cidx]
                perturbation.indicator.value[cid] = not np.isnan(val)
                perturbation.magnitude.value[cid] = val if not np.isnan(val) else 0

    elif value_option == &#39;zero&#39;:
        for perturbation in self.perturbations:
            for cid in perturbation.clustering.order:
                perturbation.magnitude.value[cid] = 0
    elif value_option in [&#39;auto&#39;, &#39;prior-mean&#39;]:
        for perturbation in self.perturbations:
            loc = perturbation.magnitude.prior.loc.value
            for cid in perturbation.clustering.order:
                perturbation.magnitude.value[cid] = loc
    else:
        raise ValueError(&#39;`value_option` ({}) not recognized&#39;.format(value_option))


    s = &#39;Perturbation magnitude initialization results:\n&#39;
    for perturbation in self.perturbations:
        if perturbation.name is not None:
            a = perturbation.name
        s += &#39;\tPerturbation {}:\n&#39; \
            &#39;\t\tvalue: {}\n&#39;.format(a, perturbation.magnitude.cluster_array())
    logging.info(s)</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.PerturbationMagnitudes.set_values"><code class="name flex">
<span>def <span class="ident">set_values</span></span>(<span>self, arr: numpy.ndarray, use_indicators: bool = True)</span>
</code></dt>
<dd>
<div class="desc"><p>Set the values of the perturbation of them stacked one on top of each other</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>arr</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>Values for all of the perturbations in order</dd>
<dt><strong><code>use_indicators</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, the values only refer to the on indicators</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_values(self, arr: np.ndarray, use_indicators: bool=True):
    &#39;&#39;&#39;Set the values of the perturbation of them stacked one on top of each other

    Parameters
    ----------
    arr : np.ndarray
        Values for all of the perturbations in order
    use_indicators : bool
        If True, the values only refer to the on indicators
    &#39;&#39;&#39;
    i = 0
    for perturbation in self.perturbations:
        l = perturbation.indicator.num_on_clusters()
        perturbation.set_values_from_array(values=arr[i:i+l],
            use_indicators=use_indicators)
        i += l</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.PerturbationMagnitudes.toarray"><code class="name flex">
<span>def <span class="ident">toarray</span></span>(<span>self) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Alias for asarray</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def toarray(self) -&gt; np.ndarray:
    &#39;&#39;&#39;Alias for asarray
    &#39;&#39;&#39;
    return self.asarray()</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.PerturbationMagnitudes.update"><code class="name flex">
<span>def <span class="ident">update</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Update with a gibbs step jointly</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update(self):
    &#39;&#39;&#39;Update with a gibbs step jointly
    &#39;&#39;&#39;
    if self.sample_iter &lt; self.delay:
        return

    n_on = [perturbation.indicator.num_on_clusters() for perturbation in \
        self.perturbations]
    
    if n_on == 0:
        return

    rhs = [STRNAMES.PERT_VALUE]
    lhs = [
        STRNAMES.GROWTH_VALUE,
        STRNAMES.SELF_INTERACTION_VALUE,
        STRNAMES.CLUSTER_INTERACTION_VALUE]
    X = self.G.data.construct_rhs(keys=rhs, toarray=True)
    y = self.G.data.construct_lhs(keys=lhs,
        kwargs_dict={STRNAMES.GROWTH_VALUE:{
            &#39;with_perturbations&#39;:False}})

    process_prec = self.G[STRNAMES.PROCESSVAR].prec
    prior_prec = build_prior_covariance(G=self.G, cov=False, order=rhs, sparse=False)

    prior_mean = build_prior_mean(G=self.G, order=rhs).reshape(-1,1)

    a = X.T * process_prec
    prec = a @ X + prior_prec
    cov = pinv(prec, self)
    mean = np.asarray(cov @ (a @ y + prior_prec @ prior_mean)).ravel()

    # print(&#39;\n\ny\n&#39;,np.hstack((y, self.G.data.lhs.vector.reshape(-1,1))))
    # print(self.G[STRNAMES.CLUSTER_INTERACTION_VALUE].value)
    # print(self.G[STRNAMES.GROWTH_VALUE].value)
    # print(self.G[STRNAMES.SELF_INTERACTION_VALUE].value)

    self.loc.value = mean
    self.scale2.value = np.diag(cov)
    value = self.sample()

    if np.any(np.isnan(value)):
        logging.critical(&#39;mean: {}&#39;.format(self.loc.value))
        logging.critical(&#39;var: {}&#39;.format(self.scale2.value))
        logging.critical(&#39;value: {}&#39;.format(self.value))
        logging.critical(&#39;prior mean: {}&#39;.format(prior_mean.ravel()))
        raise ValueError(&#39;`Values in {} are nan: {}&#39;.format(self.name, self.value))

    i = 0
    for pidx, perturbation in enumerate(self.perturbations):
        perturbation.set_values_from_array(value[i:i+n_on[pidx]], use_indicators=True)
        i += n_on[pidx]

    # Rebuild the design matrix
    self.G.data.design_matrices[STRNAMES.GROWTH_VALUE].build_with_perturbations()</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.PerturbationMagnitudes.update_str"><code class="name flex">
<span>def <span class="ident">update_str</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_str(self):
    return</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.PerturbationMagnitudes.visualize"><code class="name flex">
<span>def <span class="ident">visualize</span></span>(<span>self, basepath: str, pidx: int, section: str = 'posterior', taxa_formatter: str = '%(name)s', fixed_clustering: bool = False)</span>
</code></dt>
<dd>
<div class="desc"><p>Render the traces in the folder <code>basepath</code>. Makes a <code>pandas.DataFrame</code> table
where the index is the Taxa name in <code>taxa_formatter</code> and the columns are
<code>mean</code>, <code>median</code>, <code>25th percentile</code>, <code>75th</code> percentile`.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>basepath</code></strong> :&ensp;<code>str</code></dt>
<dd>This is the loction to write the files to</dd>
<dt><strong><code>section</code></strong> :&ensp;<code>str</code></dt>
<dd>Section of the trace to compute on. Options:
'posterior' : posterior samples
'burnin' : burn-in samples
'entire' : both burn-in and posterior samples</dd>
<dt><strong><code>taxa_formatter</code></strong> :&ensp;<code>str, None</code></dt>
<dd>This is the format of the label to return for each Taxa. If None, it will return
the taxon's name</dd>
<dt><strong><code>fixed_clustering</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, plot the variable as if clustering was fixed. Since the
cluster assignments never change, all of the taxa within the cluster
have identical values, so we can choose any taxon within the cluster to
represent it</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def visualize(self, basepath: str, pidx: int, section: str=&#39;posterior&#39;, taxa_formatter: str=&#39;%(name)s&#39;,
    fixed_clustering: bool=False):
    &#39;&#39;&#39;Render the traces in the folder `basepath`. Makes a `pandas.DataFrame` table
    where the index is the Taxa name in `taxa_formatter` and the columns are
    `mean`, `median`, `25th percentile`, `75th` percentile`.

    Parameters
    ----------
    basepath : str
        This is the loction to write the files to
    section : str
        Section of the trace to compute on. Options:
            &#39;posterior&#39; : posterior samples
            &#39;burnin&#39; : burn-in samples
            &#39;entire&#39; : both burn-in and posterior samples
    taxa_formatter : str, None
        This is the format of the label to return for each Taxa. If None, it will return
        the taxon&#39;s name
    fixed_clustering : bool
        If True, plot the variable as if clustering was fixed. Since the
        cluster assignments never change, all of the taxa within the cluster
        have identical values, so we can choose any taxon within the cluster to
        represent it
    &#39;&#39;&#39;
    perturbation = self.perturbations[pidx]
    taxa = self.G.data.taxa
    clustering = self.G[STRNAMES.CLUSTERING_OBJ]

    summ = pl.summary(perturbation, set_nan_to_0=True, section=section)
    data = np.asarray([arr for _,arr in summ.items()]).T
    if fixed_clustering:
        data_clustering = []
        for cluster in clustering:
            aidx = list(cluster.members)[0]
            data_clustering.append(data[aidx, :])
        index = [&#39;Cluster {}&#39;.format(cidx + 1) for cidx in range(len(clustering))]
        data = data_clustering
    else:
        index = [taxon.name for taxon in self.G.data.taxa]
    df = pd.DataFrame(data, columns=[k for k in summ], 
        index=index)
    df.to_csv(os.path.join(basepath, &#39;values.tsv&#39;), sep=&#39;\t&#39;, index=True, header=True)

    if section == &#39;posterior&#39;:
        len_posterior = self.G.inference.sample_iter + 1 - self.G.inference.burnin
    elif section == &#39;burnin&#39;:
        len_posterior = self.G.inference.burnin
    else:
        len_posterior = self.G.inference.sample_iter + 1

    # Make the prior
    if self.G.inference.tracer.is_being_traced(STRNAMES.PRIOR_MEAN_PERT):
        prior_mean_trace = perturbation.magnitude.prior.loc.get_trace_from_disk(section=section)
    else:
        prior_mean_trace = np.ones(len_posterior) + perturbation.magnitude.prior.loc.value
    if self.G.inference.tracer.is_being_traced(STRNAMES.PRIOR_VAR_PERT):
        prior_std_trace = np.sqrt(perturbation.magnitude.prior.scale2.get_trace_from_disk(section=section))
    else:
        prior_std_trace = np.ones(len_posterior) + np.sqrt(perturbation.magnitude.prior.scale2.value)
    
    if fixed_clustering:
        rang = len(clustering)
    else:
        rang = len(self.G.data.taxa)

    for iii in range(rang):
        if fixed_clustering:
            cluster = clustering[iii]
            oidx = list(cluster.members)[0]
        else:
            oidx = iii
        fig = plt.figure()
        ax_posterior = fig.add_subplot(1,2,1)
        visualization.render_trace(var=perturbation, idx=oidx, plt_type=&#39;hist&#39;,
            label=section, color=&#39;blue&#39;, ax=ax_posterior, section=section,
            include_burnin=True, rasterized=True)

        low_x, high_x = ax_posterior.get_xlim()
        arr = np.zeros(len(prior_std_trace), dtype=float)
        for i in range(len(prior_std_trace)):
            arr[i] = pl.random.normal.sample(loc=prior_mean_trace[i], scale=prior_std_trace[i])
        visualization.render_trace(var=arr, plt_type=&#39;hist&#39;, 
            label=&#39;prior&#39;, color=&#39;red&#39;, ax=ax_posterior, rasterized=True)

        ax_posterior.legend()
        ax_posterior.set_xlim(left=low_x*.8, right=high_x*1.2)

        # plot the trace
        ax_trace = fig.add_subplot(1,2,2)
        visualization.render_trace(var=perturbation, idx=oidx, plt_type=&#39;trace&#39;, 
            ax=ax_trace, section=section, include_burnin=True, rasterized=True)

        if fixed_clustering:
            fig.suptitle(&#39;Cluster {}&#39;.format(iii+1))
        else:
            fig.suptitle(&#39;{}&#39;.format(pl.taxaname_formatter(
                format=taxa_formatter, taxon=oidx, taxa=self.G.data.taxa)))
        fig.tight_layout()
        fig.subplots_adjust(top=0.85)
        if fixed_clustering:
            plt.savefig(os.path.join(basepath, &#39;cluster{}.pdf&#39;.format(iii+1)))
        else:
            plt.savefig(os.path.join(basepath, &#39;{}.pdf&#39;.format(taxa[oidx].name)))
        plt.close()</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="mdsine2.pylab.variables.Normal" href="pylab/variables.html#mdsine2.pylab.variables.Normal">Normal</a></b></code>:
<ul class="hlist">
<li><code><a title="mdsine2.pylab.variables.Normal.T" href="pylab/variables.html#mdsine2.pylab.variables.Variable.T">T</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Normal.add_child" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_child">add_child</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Normal.add_parent" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_parent">add_parent</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Normal.add_prior" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_prior">add_prior</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Normal.add_trace" href="pylab/base.html#mdsine2.pylab.base.Traceable.add_trace">add_trace</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Normal.add_undirected" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_undirected">add_undirected</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Normal.cdf" href="pylab/variables.html#mdsine2.pylab.variables.Normal.cdf">cdf</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Normal.degree" href="pylab/graph.html#mdsine2.pylab.graph.Node.degree">degree</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Normal.delete" href="pylab/graph.html#mdsine2.pylab.graph.BaseNode.delete">delete</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Normal.get_adjacent_keys" href="pylab/graph.html#mdsine2.pylab.graph.Node.get_adjacent_keys">get_adjacent_keys</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Normal.get_iter" href="pylab/base.html#mdsine2.pylab.base.Traceable.get_iter">get_iter</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Normal.get_trace_from_disk" href="pylab/base.html#mdsine2.pylab.base.Traceable.get_trace_from_disk">get_trace_from_disk</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Normal.load" href="pylab/base.html#mdsine2.pylab.base.Saveable.load">load</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Normal.logcdf" href="pylab/variables.html#mdsine2.pylab.variables.Normal.logcdf">logcdf</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Normal.logpdf" href="pylab/variables.html#mdsine2.pylab.variables.Normal.logpdf">logpdf</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Normal.metropolis" href="pylab/graph.html#mdsine2.pylab.graph.Node.metropolis">metropolis</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Normal.overwrite_entire_trace_on_disk" href="pylab/base.html#mdsine2.pylab.base.Traceable.overwrite_entire_trace_on_disk">overwrite_entire_trace_on_disk</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Normal.pdf" href="pylab/variables.html#mdsine2.pylab.variables.Normal.pdf">pdf</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Normal.remove_local_trace" href="pylab/variables.html#mdsine2.pylab.variables.Variable.remove_local_trace">remove_local_trace</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Normal.sample" href="pylab/variables.html#mdsine2.pylab.variables.Normal.sample">sample</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Normal.save" href="pylab/base.html#mdsine2.pylab.base.Saveable.save">save</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Normal.set_save_location" href="pylab/base.html#mdsine2.pylab.base.Saveable.set_save_location">set_save_location</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Normal.set_trace" href="pylab/base.html#mdsine2.pylab.base.Traceable.set_trace">set_trace</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Normal.set_value_shape" href="pylab/variables.html#mdsine2.pylab.variables.Variable.set_value_shape">set_value_shape</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="mdsine2.posterior.PerturbationProbabilities"><code class="flex name class">
<span>class <span class="ident">PerturbationProbabilities</span></span>
<span>(</span><span>**kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>This is the probability for a positive interaction for a perturbation</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PerturbationProbabilities(pl.Node):
    &#39;&#39;&#39;This is the probability for a positive interaction for a perturbation
    &#39;&#39;&#39;
    def __init__(self, **kwargs):
        kwargs[&#39;name&#39;] = STRNAMES.PERT_INDICATOR_PROB
        pl.Node.__init__(self, **kwargs)
        self.perturbations = self.G.perturbations # mdsine2.pylab.base.Perturbations

    def __str__(self) -&gt; str:
        s = &#39;Perturbation Indicator probabilities&#39;
        for perturbation in self.perturbations:
            s += &#39;\n\tperturbation {}: {}&#39;.format(
                perturbation.name,
                perturbation.probability.value)
        return s

    @property
    def sample_iter(self) -&gt; int:
        return self.perturbations[0].probability.sample_iter

    def initialize(self, value_option: str, hyperparam_option: str, a: float=None, 
        b: float=None, value: Union[float, int]=None, N: Union[str, int]=&#39;auto&#39;, delay: int=0):
        &#39;&#39;&#39;Initialize the hyperparameters of the prior and the value. Each
        perturbation has the same prior.

        Parameters
        ----------
        value_option : str
            How to initialize the values. Options:
                &#39;manual&#39;
                    Set the value manually, `value` must also be specified
                &#39;auto&#39;, &#39;prior-mean&#39;
                    Initialize the value as the prior mean
        hyperparam_option : str
            How to initialize `a` and `b`. Options:
                &#39;manual&#39;
                    Set the value manually. `a` and `b` must also be specified
                &#39;weak-agnostic&#39; or &#39;auto&#39;
                    a=b=0.5
                &#39;strong-dense&#39;
                    a = N, N are the expected number of clusters
                    b = 0.5
                &#39;strong-sparse&#39;
                    a = 0.5
                    b = N, N are the expected number of clusters
                &#39;mult-sparse-M&#39;
                    a = 0.5
                    b = N*M(N*M-1), N are the expected number of clusters
        N : str, int
            This is the number of clusters to set the hyperparam options to 
            (if they are dependent on the number of cluster). If &#39;auto&#39;, set to the expected number
            of clusters from a dirichlet process. Else use this number (must be an int).
        delay : int
            How many MCMC iterations to delay starting the update of the variable
        value, a, b : int, float
            User specified values
            Only necessary if `hyperparam_option` == &#39;manual&#39;
        &#39;&#39;&#39;
        if not pl.isint(delay):
            raise TypeError(&#39;`delay` ({}) must be an int&#39;.format(type(delay)))
        if delay &lt; 0:
            raise ValueError(&#39;`delay` ({}) must be &gt;= 0&#39;.format(delay))
        self.delay = delay

        # Set the hyper-parameters
        if not pl.isstr(hyperparam_option):
            raise ValueError(&#39;`hyperparam_option` ({}) must be a str&#39;.format(type(hyperparam_option)))
        if hyperparam_option == &#39;manual&#39;:
            if (not pl.isnumeric(a)) or (not pl.isnumeric(b)):
                raise TypeError(&#39;If `hyperparam_option` is &#34;manual&#34; then `a` ({})&#39; \
                    &#39; and `b` ({}) must be numerics&#39;.format(type(a), type(b)))
        elif hyperparam_option in [&#39;auto&#39;, &#39;weak-agnostic&#39;]:
            a = 0.5
            b = 0.5

        # Set N
        if pl.isstr(N):
            if N == &#39;auto&#39;:
                N = expected_n_clusters(G=self.G)
            elif N == &#39;fixed-clustering&#39;:
                N = len(self.G[STRNAMES.CLUSTERING_OBJ])
            else:
                raise ValueError(&#39;`N` ({}) nto recognized&#39;.format(N))
        elif pl.isint(N):
            if N &lt; 0:
                raise ValueError(&#39;`N` ({}) must be positive&#39;.format(N))
        else:
            raise TypeError(&#39;`N` ({}) type not recognized&#39;.format(type(N)))

        
        if hyperparam_option == &#39;strong-dense&#39;:
            a = N
            b = 0.5
        elif hyperparam_option == &#39;strong-sparse&#39;:
            a = 0.5
            b = N
        elif &#39;mult-sparse&#39; in hyperparam_option:
            try:
                M = float(hyperparam_option.replace(&#39;mult-sparse-&#39;, &#39;&#39;))
            except:
                raise ValueError(&#39;`mult-sparse` in the wrong format ({})&#39;.format(
                    hyperparam_option))
            N = N * M
            a = 0.5
            b = N

        for perturbation in self.perturbations:
            perturbation.probability.prior.a.override_value(a)
            perturbation.probability.prior.b.override_value(b)

        # Set the value
        if not pl.isstr(value_option):
            raise TypeError(&#39;`value_option` ({}) must be a str&#39;.format(type(value_option)))
        if value_option == &#39;manual&#39;:
            if not pl.isnumeric(value):
                raise TypeError(&#39;If `value_option` is &#34;manual&#34; then `value` ({})&#39; \
                    &#39; must be a numeric&#39;.format(type(value)))
            for perturbation in self.perturbations:
                perturbation.probability.value = value
        elif value_option in [&#39;auto&#39;, &#39;prior-mean&#39;]:
            for perturbation in self.perturbations:
                perturbation.probability.value = perturbation.probability.prior.mean()
        else:
            raise ValueError(&#39;`value_option` ({}) not recognized&#39;.format(value_option))

        s = &#39;Perturbation indicator probability initialization results:\n&#39;
        for i, perturbation in enumerate(self.perturbations):
            s += &#39;\tPerturbation {}:\n&#39; \
                &#39;\t\tprior a: {}\n&#39; \
                &#39;\t\tprior b: {}\n&#39; \
                &#39;\t\tvalue: {}\n&#39;.format(i,
                    perturbation.probability.prior.a.value,
                    perturbation.probability.prior.b.value,
                    perturbation.probability.value)
        logging.info(s)

    def update(self):
        &#39;&#39;&#39;Update according to how many positive and negative indicators there
        are
        &#39;&#39;&#39;
        if self.sample_iter &lt; self.delay:
            return
        for perturbation in self.perturbations:
            num_pos = perturbation.indicator.num_on_clusters()
            num_neg = len(perturbation.clustering.clusters) - num_pos
            perturbation.probability.a.value = perturbation.probability.prior.a.value + num_pos
            perturbation.probability.b.value = perturbation.probability.prior.b.value + num_neg
            perturbation.probability.sample()

    def set_trace(self, *args, **kwargs):
        for perturbation in self.perturbations:
            perturbation.probability.set_trace(*args, **kwargs)

    def add_trace(self, *args, **kwargs):
        for perturbation in self.perturbations:
            perturbation.probability.add_trace(*args, **kwargs)

    def visualize(self, path: str, f: IO, pidx: int, section: str=&#39;posterior&#39;) -&gt; IO:
        &#39;&#39;&#39;Visualize the `pidx`th perturbation prior magnitude

        Parameters
        ----------
        obj : mdsine2.Variable
        path : str
            This is the path to write the files to
        f : _io.TextIOWrapper
            File that we are writing the values to
        section : str
            Section of the trace to compute on. Options:
                &#39;posterior&#39; : posterior samples
                &#39;burnin&#39; : burn-in samples
                &#39;entire&#39; : both burn-in and posterior samples

        Returns
        -------
        _io.TextIOWrapper
        &#39;&#39;&#39;
        if not self.G.inference.is_in_inference_order(self.name):
            return f
        return _scalar_visualize(path=path, f=f, section=section,
            obj=self.perturbations[pidx].probability, log_scale=False)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="mdsine2.pylab.graph.Node" href="pylab/graph.html#mdsine2.pylab.graph.Node">Node</a></li>
<li><a title="mdsine2.pylab.graph.BaseNode" href="pylab/graph.html#mdsine2.pylab.graph.BaseNode">BaseNode</a></li>
<li><a title="mdsine2.pylab.base.Saveable" href="pylab/base.html#mdsine2.pylab.base.Saveable">Saveable</a></li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="mdsine2.posterior.PerturbationProbabilities.sample_iter"><code class="name">var <span class="ident">sample_iter</span> : int</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def sample_iter(self) -&gt; int:
    return self.perturbations[0].probability.sample_iter</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="mdsine2.posterior.PerturbationProbabilities.add_trace"><code class="name flex">
<span>def <span class="ident">add_trace</span></span>(<span>self, *args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add_trace(self, *args, **kwargs):
    for perturbation in self.perturbations:
        perturbation.probability.add_trace(*args, **kwargs)</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.PerturbationProbabilities.initialize"><code class="name flex">
<span>def <span class="ident">initialize</span></span>(<span>self, value_option: str, hyperparam_option: str, a: float = None, b: float = None, value: Union[float, int] = None, N: Union[str, int] = 'auto', delay: int = 0)</span>
</code></dt>
<dd>
<div class="desc"><p>Initialize the hyperparameters of the prior and the value. Each
perturbation has the same prior.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>value_option</code></strong> :&ensp;<code>str</code></dt>
<dd>How to initialize the values. Options:
'manual'
Set the value manually, <code>value</code> must also be specified
'auto', 'prior-mean'
Initialize the value as the prior mean</dd>
<dt><strong><code>hyperparam_option</code></strong> :&ensp;<code>str</code></dt>
<dd>How to initialize <code>a</code> and <code>b</code>. Options:
'manual'
Set the value manually. <code>a</code> and <code>b</code> must also be specified
'weak-agnostic' or 'auto'
a=b=0.5
'strong-dense'
a = N, N are the expected number of clusters
b = 0.5
'strong-sparse'
a = 0.5
b = N, N are the expected number of clusters
'mult-sparse-M'
a = 0.5
b = N<em>M(N</em>M-1), N are the expected number of clusters</dd>
<dt><strong><code>N</code></strong> :&ensp;<code>str, int</code></dt>
<dd>This is the number of clusters to set the hyperparam options to
(if they are dependent on the number of cluster). If 'auto', set to the expected number
of clusters from a dirichlet process. Else use this number (must be an int).</dd>
<dt><strong><code>delay</code></strong> :&ensp;<code>int</code></dt>
<dd>How many MCMC iterations to delay starting the update of the variable</dd>
<dt><strong><code>value</code></strong>, <strong><code>a</code></strong>, <strong><code>b</code></strong> :&ensp;<code>int, float</code></dt>
<dd>User specified values
Only necessary if <code>hyperparam_option</code> == 'manual'</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def initialize(self, value_option: str, hyperparam_option: str, a: float=None, 
    b: float=None, value: Union[float, int]=None, N: Union[str, int]=&#39;auto&#39;, delay: int=0):
    &#39;&#39;&#39;Initialize the hyperparameters of the prior and the value. Each
    perturbation has the same prior.

    Parameters
    ----------
    value_option : str
        How to initialize the values. Options:
            &#39;manual&#39;
                Set the value manually, `value` must also be specified
            &#39;auto&#39;, &#39;prior-mean&#39;
                Initialize the value as the prior mean
    hyperparam_option : str
        How to initialize `a` and `b`. Options:
            &#39;manual&#39;
                Set the value manually. `a` and `b` must also be specified
            &#39;weak-agnostic&#39; or &#39;auto&#39;
                a=b=0.5
            &#39;strong-dense&#39;
                a = N, N are the expected number of clusters
                b = 0.5
            &#39;strong-sparse&#39;
                a = 0.5
                b = N, N are the expected number of clusters
            &#39;mult-sparse-M&#39;
                a = 0.5
                b = N*M(N*M-1), N are the expected number of clusters
    N : str, int
        This is the number of clusters to set the hyperparam options to 
        (if they are dependent on the number of cluster). If &#39;auto&#39;, set to the expected number
        of clusters from a dirichlet process. Else use this number (must be an int).
    delay : int
        How many MCMC iterations to delay starting the update of the variable
    value, a, b : int, float
        User specified values
        Only necessary if `hyperparam_option` == &#39;manual&#39;
    &#39;&#39;&#39;
    if not pl.isint(delay):
        raise TypeError(&#39;`delay` ({}) must be an int&#39;.format(type(delay)))
    if delay &lt; 0:
        raise ValueError(&#39;`delay` ({}) must be &gt;= 0&#39;.format(delay))
    self.delay = delay

    # Set the hyper-parameters
    if not pl.isstr(hyperparam_option):
        raise ValueError(&#39;`hyperparam_option` ({}) must be a str&#39;.format(type(hyperparam_option)))
    if hyperparam_option == &#39;manual&#39;:
        if (not pl.isnumeric(a)) or (not pl.isnumeric(b)):
            raise TypeError(&#39;If `hyperparam_option` is &#34;manual&#34; then `a` ({})&#39; \
                &#39; and `b` ({}) must be numerics&#39;.format(type(a), type(b)))
    elif hyperparam_option in [&#39;auto&#39;, &#39;weak-agnostic&#39;]:
        a = 0.5
        b = 0.5

    # Set N
    if pl.isstr(N):
        if N == &#39;auto&#39;:
            N = expected_n_clusters(G=self.G)
        elif N == &#39;fixed-clustering&#39;:
            N = len(self.G[STRNAMES.CLUSTERING_OBJ])
        else:
            raise ValueError(&#39;`N` ({}) nto recognized&#39;.format(N))
    elif pl.isint(N):
        if N &lt; 0:
            raise ValueError(&#39;`N` ({}) must be positive&#39;.format(N))
    else:
        raise TypeError(&#39;`N` ({}) type not recognized&#39;.format(type(N)))

    
    if hyperparam_option == &#39;strong-dense&#39;:
        a = N
        b = 0.5
    elif hyperparam_option == &#39;strong-sparse&#39;:
        a = 0.5
        b = N
    elif &#39;mult-sparse&#39; in hyperparam_option:
        try:
            M = float(hyperparam_option.replace(&#39;mult-sparse-&#39;, &#39;&#39;))
        except:
            raise ValueError(&#39;`mult-sparse` in the wrong format ({})&#39;.format(
                hyperparam_option))
        N = N * M
        a = 0.5
        b = N

    for perturbation in self.perturbations:
        perturbation.probability.prior.a.override_value(a)
        perturbation.probability.prior.b.override_value(b)

    # Set the value
    if not pl.isstr(value_option):
        raise TypeError(&#39;`value_option` ({}) must be a str&#39;.format(type(value_option)))
    if value_option == &#39;manual&#39;:
        if not pl.isnumeric(value):
            raise TypeError(&#39;If `value_option` is &#34;manual&#34; then `value` ({})&#39; \
                &#39; must be a numeric&#39;.format(type(value)))
        for perturbation in self.perturbations:
            perturbation.probability.value = value
    elif value_option in [&#39;auto&#39;, &#39;prior-mean&#39;]:
        for perturbation in self.perturbations:
            perturbation.probability.value = perturbation.probability.prior.mean()
    else:
        raise ValueError(&#39;`value_option` ({}) not recognized&#39;.format(value_option))

    s = &#39;Perturbation indicator probability initialization results:\n&#39;
    for i, perturbation in enumerate(self.perturbations):
        s += &#39;\tPerturbation {}:\n&#39; \
            &#39;\t\tprior a: {}\n&#39; \
            &#39;\t\tprior b: {}\n&#39; \
            &#39;\t\tvalue: {}\n&#39;.format(i,
                perturbation.probability.prior.a.value,
                perturbation.probability.prior.b.value,
                perturbation.probability.value)
    logging.info(s)</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.PerturbationProbabilities.set_trace"><code class="name flex">
<span>def <span class="ident">set_trace</span></span>(<span>self, *args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_trace(self, *args, **kwargs):
    for perturbation in self.perturbations:
        perturbation.probability.set_trace(*args, **kwargs)</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.PerturbationProbabilities.update"><code class="name flex">
<span>def <span class="ident">update</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Update according to how many positive and negative indicators there
are</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update(self):
    &#39;&#39;&#39;Update according to how many positive and negative indicators there
    are
    &#39;&#39;&#39;
    if self.sample_iter &lt; self.delay:
        return
    for perturbation in self.perturbations:
        num_pos = perturbation.indicator.num_on_clusters()
        num_neg = len(perturbation.clustering.clusters) - num_pos
        perturbation.probability.a.value = perturbation.probability.prior.a.value + num_pos
        perturbation.probability.b.value = perturbation.probability.prior.b.value + num_neg
        perturbation.probability.sample()</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.PerturbationProbabilities.visualize"><code class="name flex">
<span>def <span class="ident">visualize</span></span>(<span>self, path: str, f: <class 'IO'>, pidx: int, section: str = 'posterior') ‑> <class 'IO'></span>
</code></dt>
<dd>
<div class="desc"><p>Visualize the <code>pidx</code>th perturbation prior magnitude</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>obj</code></strong> :&ensp;<code>mdsine2.Variable</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>path</code></strong> :&ensp;<code>str</code></dt>
<dd>This is the path to write the files to</dd>
<dt><strong><code>f</code></strong> :&ensp;<code>_io.TextIOWrapper</code></dt>
<dd>File that we are writing the values to</dd>
<dt><strong><code>section</code></strong> :&ensp;<code>str</code></dt>
<dd>Section of the trace to compute on. Options:
'posterior' : posterior samples
'burnin' : burn-in samples
'entire' : both burn-in and posterior samples</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>_io.TextIOWrapper</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def visualize(self, path: str, f: IO, pidx: int, section: str=&#39;posterior&#39;) -&gt; IO:
    &#39;&#39;&#39;Visualize the `pidx`th perturbation prior magnitude

    Parameters
    ----------
    obj : mdsine2.Variable
    path : str
        This is the path to write the files to
    f : _io.TextIOWrapper
        File that we are writing the values to
    section : str
        Section of the trace to compute on. Options:
            &#39;posterior&#39; : posterior samples
            &#39;burnin&#39; : burn-in samples
            &#39;entire&#39; : both burn-in and posterior samples

    Returns
    -------
    _io.TextIOWrapper
    &#39;&#39;&#39;
    if not self.G.inference.is_in_inference_order(self.name):
        return f
    return _scalar_visualize(path=path, f=f, section=section,
        obj=self.perturbations[pidx].probability, log_scale=False)</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="mdsine2.pylab.graph.Node" href="pylab/graph.html#mdsine2.pylab.graph.Node">Node</a></b></code>:
<ul class="hlist">
<li><code><a title="mdsine2.pylab.graph.Node.add_child" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_child">add_child</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.add_parent" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_parent">add_parent</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.add_prior" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_prior">add_prior</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.add_undirected" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_undirected">add_undirected</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.degree" href="pylab/graph.html#mdsine2.pylab.graph.Node.degree">degree</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.delete" href="pylab/graph.html#mdsine2.pylab.graph.BaseNode.delete">delete</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.get_adjacent_keys" href="pylab/graph.html#mdsine2.pylab.graph.Node.get_adjacent_keys">get_adjacent_keys</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.load" href="pylab/base.html#mdsine2.pylab.base.Saveable.load">load</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.metropolis" href="pylab/graph.html#mdsine2.pylab.graph.Node.metropolis">metropolis</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.save" href="pylab/base.html#mdsine2.pylab.base.Saveable.save">save</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.set_save_location" href="pylab/base.html#mdsine2.pylab.base.Saveable.set_save_location">set_save_location</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="mdsine2.posterior.PriorMeanInteractions"><code class="flex name class">
<span>class <span class="ident">PriorMeanInteractions</span></span>
<span>(</span><span>prior: <a title="mdsine2.pylab.variables.Normal" href="pylab/variables.html#mdsine2.pylab.variables.Normal">Normal</a>, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>This is the posterior mean for the interactions</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PriorMeanInteractions(pl.variables.Normal):
    &#39;&#39;&#39;This is the posterior mean for the interactions
    &#39;&#39;&#39;

    def __init__(self, prior: variables.Normal, **kwargs):
        kwargs[&#39;name&#39;] = STRNAMES.PRIOR_MEAN_INTERACTIONS
        pl.variables.Normal.__init__(self, loc=None, scale2=None, dtype=float, **kwargs)
        self.add_prior(prior)

    def __str__(self) -&gt; str:
        # If this fails, it is because we are dividing by 0 sampler_iter
        # If which case we just return the value 
        s = str(self.value)
        return s

    def initialize(self, value_option: str, loc_option: str, scale2_option: str, 
        value: Union[float, int]=None, loc: Union[float, int]=None, 
        scale2: Union[float, int]=None, delay: int=0):
        &#39;&#39;&#39;Initialize the hyperparameters

        Parameters
        ----------
        value_option : str
            How to set the value. Options:
                &#39;zero&#39;
                    Set to zero
                &#39;prior-mean&#39;, &#39;auto&#39;
                    Set to the mean of the prior
                &#39;manual&#39;
                    Specify with the `value` parameter
        loc_option : str
            How to set the loc of the prior
                &#39;zero&#39;, &#39;auto&#39;
                    Set to zero
                &#39;manual&#39;
                    Set with the `loc` parameter
        scale2_option : str
            &#39;same-as-aii&#39;, &#39;auto&#39;
                Set as the same variance as the self-interactions
            &#39;manual&#39;
                Set with the `scale2` parameter
        value, loc, scale2 : float
            These are only necessary if we specify manual for any of the other 
            options
        delay : int
            How much to delay the start of the update during inference
        &#39;&#39;&#39;
        if not pl.isint(delay):
            raise TypeError(&#39;`delay` ({}) must be an int&#39;.format(type(delay)))
        if delay &lt; 0:
            raise ValueError(&#39;`delay` ({}) must be &gt;= 0&#39;.format(delay))
        self.delay = delay

        # Set the location parameter
        if not pl.isstr(loc_option):
            raise TypeError(&#39;`loc_option` ({}) must be a str&#39;.format(type(loc_option)))
        if loc_option == &#39;manual&#39;:
            if not pl.isnumeric(loc):
                raise TypeError(&#39;`loc` ({}) must be a numeric&#39;.format(type(loc)))
        elif loc_option in [&#39;zero&#39;, &#39;auto&#39;]:
            loc = 0
        else:
            raise ValueError(&#39;`loc_option` ({}) not recognized&#39;.format(loc_option))
        self.prior.loc.override_value(loc)

        # Set the scale2 parameter
        if not pl.isstr(scale2_option):
            raise TypeError(&#39;`scale2_option` ({}) must be a str&#39;.format(type(scale2_option)))
        if scale2_option == &#39;manual&#39;:
            if not pl.isnumeric(scale2):
                raise TypeError(&#39;`scale2` ({}) must be a numeric&#39;.format(type(scale2)))
            if scale2 &lt;= 0:
                raise ValueError(&#39;`scale2` ({}) must be positive&#39;.format(scale2))
        elif scale2_option in [&#39;same-as-aii&#39;, &#39;auto&#39;]:
            scale2 = self.G[STRNAMES.PRIOR_VAR_SELF_INTERACTIONS].value
        else:
            raise ValueError(&#39;`scale2_option` ({}) not recognized&#39;.format(scale2_option))
        self.prior.scale2.override_value(scale2)

        # Set the value
        if not pl.isstr(value_option):
            raise TypeError(&#39;`value_option` ({}) must be a str&#39;.format(type(value_option)))
        if value_option == &#39;manual&#39;:
            if not pl.isnumeric(value):
                raise TypeError(&#39;`value` ({}) must be a numeric&#39;.format(type(value)))
        elif value_option in [&#39;prior-mean&#39;, &#39;auto&#39;]:
            value = self.prior.loc.value
        elif value_option == &#39;zero&#39;:
            value = 0
        else:
            raise ValueError(&#39;`value_option` ({}) not recognized&#39;.format(value_option))
        self.value = value

    def update(self):
        &#39;&#39;&#39;Update using Gibbs sampling
        &#39;&#39;&#39;
        if self.sample_iter &lt; self.delay:
            return

        if self.G[STRNAMES.CLUSTER_INTERACTION_INDICATOR].num_pos_indicators == 0:
            # sample from the prior
            self.value = self.prior.sample()
            return

        x = self.G[STRNAMES.CLUSTER_INTERACTION_VALUE].value
        prec = 1/self.G[STRNAMES.PRIOR_VAR_INTERACTIONS].value

        prior_prec = 1/self.prior.scale2.value
        prior_mean = self.prior.loc.value

        self.scale2.value = 1/(prior_prec + (len(x)*prec))
        self.loc.value = self.scale2.value * ((prior_mean * prior_prec) + (np.sum(x)*prec))
        self.sample()

    def visualize(self, path: str, f: IO, section: str=&#39;posterior&#39;) -&gt; IO:
        return _scalar_visualize(self, path=path, f=f, section=section)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="mdsine2.pylab.variables.Normal" href="pylab/variables.html#mdsine2.pylab.variables.Normal">Normal</a></li>
<li><a title="mdsine2.pylab.variables.Variable" href="pylab/variables.html#mdsine2.pylab.variables.Variable">Variable</a></li>
<li><a title="mdsine2.pylab.graph.Node" href="pylab/graph.html#mdsine2.pylab.graph.Node">Node</a></li>
<li><a title="mdsine2.pylab.graph.BaseNode" href="pylab/graph.html#mdsine2.pylab.graph.BaseNode">BaseNode</a></li>
<li><a title="mdsine2.pylab.base.Saveable" href="pylab/base.html#mdsine2.pylab.base.Saveable">Saveable</a></li>
<li>mdsine2.pylab.variables._BaseArithmeticClass</li>
<li><a title="mdsine2.pylab.base.Traceable" href="pylab/base.html#mdsine2.pylab.base.Traceable">Traceable</a></li>
<li>mdsine2.pylab.variables._RandomBase</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="mdsine2.posterior.PriorMeanInteractions.initialize"><code class="name flex">
<span>def <span class="ident">initialize</span></span>(<span>self, value_option: str, loc_option: str, scale2_option: str, value: Union[float, int] = None, loc: Union[float, int] = None, scale2: Union[float, int] = None, delay: int = 0)</span>
</code></dt>
<dd>
<div class="desc"><p>Initialize the hyperparameters</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>value_option</code></strong> :&ensp;<code>str</code></dt>
<dd>How to set the value. Options:
'zero'
Set to zero
'prior-mean', 'auto'
Set to the mean of the prior
'manual'
Specify with the <code>value</code> parameter</dd>
<dt><strong><code>loc_option</code></strong> :&ensp;<code>str</code></dt>
<dd>How to set the loc of the prior
'zero', 'auto'
Set to zero
'manual'
Set with the <code>loc</code> parameter</dd>
<dt><strong><code>scale2_option</code></strong> :&ensp;<code>str</code></dt>
<dd>'same-as-aii', 'auto'
Set as the same variance as the self-interactions
'manual'
Set with the <code>scale2</code> parameter</dd>
<dt><strong><code>value</code></strong>, <strong><code>loc</code></strong>, <strong><code>scale2</code></strong> :&ensp;<code>float</code></dt>
<dd>These are only necessary if we specify manual for any of the other
options</dd>
<dt><strong><code>delay</code></strong> :&ensp;<code>int</code></dt>
<dd>How much to delay the start of the update during inference</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def initialize(self, value_option: str, loc_option: str, scale2_option: str, 
    value: Union[float, int]=None, loc: Union[float, int]=None, 
    scale2: Union[float, int]=None, delay: int=0):
    &#39;&#39;&#39;Initialize the hyperparameters

    Parameters
    ----------
    value_option : str
        How to set the value. Options:
            &#39;zero&#39;
                Set to zero
            &#39;prior-mean&#39;, &#39;auto&#39;
                Set to the mean of the prior
            &#39;manual&#39;
                Specify with the `value` parameter
    loc_option : str
        How to set the loc of the prior
            &#39;zero&#39;, &#39;auto&#39;
                Set to zero
            &#39;manual&#39;
                Set with the `loc` parameter
    scale2_option : str
        &#39;same-as-aii&#39;, &#39;auto&#39;
            Set as the same variance as the self-interactions
        &#39;manual&#39;
            Set with the `scale2` parameter
    value, loc, scale2 : float
        These are only necessary if we specify manual for any of the other 
        options
    delay : int
        How much to delay the start of the update during inference
    &#39;&#39;&#39;
    if not pl.isint(delay):
        raise TypeError(&#39;`delay` ({}) must be an int&#39;.format(type(delay)))
    if delay &lt; 0:
        raise ValueError(&#39;`delay` ({}) must be &gt;= 0&#39;.format(delay))
    self.delay = delay

    # Set the location parameter
    if not pl.isstr(loc_option):
        raise TypeError(&#39;`loc_option` ({}) must be a str&#39;.format(type(loc_option)))
    if loc_option == &#39;manual&#39;:
        if not pl.isnumeric(loc):
            raise TypeError(&#39;`loc` ({}) must be a numeric&#39;.format(type(loc)))
    elif loc_option in [&#39;zero&#39;, &#39;auto&#39;]:
        loc = 0
    else:
        raise ValueError(&#39;`loc_option` ({}) not recognized&#39;.format(loc_option))
    self.prior.loc.override_value(loc)

    # Set the scale2 parameter
    if not pl.isstr(scale2_option):
        raise TypeError(&#39;`scale2_option` ({}) must be a str&#39;.format(type(scale2_option)))
    if scale2_option == &#39;manual&#39;:
        if not pl.isnumeric(scale2):
            raise TypeError(&#39;`scale2` ({}) must be a numeric&#39;.format(type(scale2)))
        if scale2 &lt;= 0:
            raise ValueError(&#39;`scale2` ({}) must be positive&#39;.format(scale2))
    elif scale2_option in [&#39;same-as-aii&#39;, &#39;auto&#39;]:
        scale2 = self.G[STRNAMES.PRIOR_VAR_SELF_INTERACTIONS].value
    else:
        raise ValueError(&#39;`scale2_option` ({}) not recognized&#39;.format(scale2_option))
    self.prior.scale2.override_value(scale2)

    # Set the value
    if not pl.isstr(value_option):
        raise TypeError(&#39;`value_option` ({}) must be a str&#39;.format(type(value_option)))
    if value_option == &#39;manual&#39;:
        if not pl.isnumeric(value):
            raise TypeError(&#39;`value` ({}) must be a numeric&#39;.format(type(value)))
    elif value_option in [&#39;prior-mean&#39;, &#39;auto&#39;]:
        value = self.prior.loc.value
    elif value_option == &#39;zero&#39;:
        value = 0
    else:
        raise ValueError(&#39;`value_option` ({}) not recognized&#39;.format(value_option))
    self.value = value</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.PriorMeanInteractions.update"><code class="name flex">
<span>def <span class="ident">update</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Update using Gibbs sampling</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update(self):
    &#39;&#39;&#39;Update using Gibbs sampling
    &#39;&#39;&#39;
    if self.sample_iter &lt; self.delay:
        return

    if self.G[STRNAMES.CLUSTER_INTERACTION_INDICATOR].num_pos_indicators == 0:
        # sample from the prior
        self.value = self.prior.sample()
        return

    x = self.G[STRNAMES.CLUSTER_INTERACTION_VALUE].value
    prec = 1/self.G[STRNAMES.PRIOR_VAR_INTERACTIONS].value

    prior_prec = 1/self.prior.scale2.value
    prior_mean = self.prior.loc.value

    self.scale2.value = 1/(prior_prec + (len(x)*prec))
    self.loc.value = self.scale2.value * ((prior_mean * prior_prec) + (np.sum(x)*prec))
    self.sample()</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.PriorMeanInteractions.visualize"><code class="name flex">
<span>def <span class="ident">visualize</span></span>(<span>self, path: str, f: <class 'IO'>, section: str = 'posterior') ‑> <class 'IO'></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def visualize(self, path: str, f: IO, section: str=&#39;posterior&#39;) -&gt; IO:
    return _scalar_visualize(self, path=path, f=f, section=section)</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="mdsine2.pylab.variables.Normal" href="pylab/variables.html#mdsine2.pylab.variables.Normal">Normal</a></b></code>:
<ul class="hlist">
<li><code><a title="mdsine2.pylab.variables.Normal.T" href="pylab/variables.html#mdsine2.pylab.variables.Variable.T">T</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Normal.add_child" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_child">add_child</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Normal.add_parent" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_parent">add_parent</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Normal.add_prior" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_prior">add_prior</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Normal.add_trace" href="pylab/base.html#mdsine2.pylab.base.Traceable.add_trace">add_trace</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Normal.add_undirected" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_undirected">add_undirected</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Normal.cdf" href="pylab/variables.html#mdsine2.pylab.variables.Normal.cdf">cdf</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Normal.degree" href="pylab/graph.html#mdsine2.pylab.graph.Node.degree">degree</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Normal.delete" href="pylab/graph.html#mdsine2.pylab.graph.BaseNode.delete">delete</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Normal.get_adjacent_keys" href="pylab/graph.html#mdsine2.pylab.graph.Node.get_adjacent_keys">get_adjacent_keys</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Normal.get_iter" href="pylab/base.html#mdsine2.pylab.base.Traceable.get_iter">get_iter</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Normal.get_trace_from_disk" href="pylab/base.html#mdsine2.pylab.base.Traceable.get_trace_from_disk">get_trace_from_disk</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Normal.load" href="pylab/base.html#mdsine2.pylab.base.Saveable.load">load</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Normal.logcdf" href="pylab/variables.html#mdsine2.pylab.variables.Normal.logcdf">logcdf</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Normal.logpdf" href="pylab/variables.html#mdsine2.pylab.variables.Normal.logpdf">logpdf</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Normal.metropolis" href="pylab/graph.html#mdsine2.pylab.graph.Node.metropolis">metropolis</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Normal.overwrite_entire_trace_on_disk" href="pylab/base.html#mdsine2.pylab.base.Traceable.overwrite_entire_trace_on_disk">overwrite_entire_trace_on_disk</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Normal.pdf" href="pylab/variables.html#mdsine2.pylab.variables.Normal.pdf">pdf</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Normal.remove_local_trace" href="pylab/variables.html#mdsine2.pylab.variables.Variable.remove_local_trace">remove_local_trace</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Normal.sample" href="pylab/variables.html#mdsine2.pylab.variables.Normal.sample">sample</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Normal.save" href="pylab/base.html#mdsine2.pylab.base.Saveable.save">save</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Normal.set_save_location" href="pylab/base.html#mdsine2.pylab.base.Saveable.set_save_location">set_save_location</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Normal.set_trace" href="pylab/base.html#mdsine2.pylab.base.Traceable.set_trace">set_trace</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Normal.set_value_shape" href="pylab/variables.html#mdsine2.pylab.variables.Variable.set_value_shape">set_value_shape</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="mdsine2.posterior.PriorMeanMH"><code class="flex name class">
<span>class <span class="ident">PriorMeanMH</span></span>
<span>(</span><span>prior: <a title="mdsine2.pylab.variables.TruncatedNormal" href="pylab/variables.html#mdsine2.pylab.variables.TruncatedNormal">TruncatedNormal</a>, child_name: str, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>This implements the posterior for the prior mean of the either
the growths or the self-interactions</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>prior</code></strong> :&ensp;<code>variables.TruncatedNormal</code></dt>
<dd>Prior distribution</dd>
<dt><strong><code>child_name</code></strong> :&ensp;<code>str</code></dt>
<dd>Name of the child</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PriorMeanMH(pl.variables.TruncatedNormal):
    &#39;&#39;&#39;This implements the posterior for the prior mean of the either
    the growths or the self-interactions

    Parameters
    ----------
    prior : variables.TruncatedNormal
        Prior distribution
    child_name : str
        Name of the child
    &#39;&#39;&#39;
    def __init__(self, prior: variables.TruncatedNormal, child_name: str, **kwargs):
        if child_name == STRNAMES.GROWTH_VALUE:
            kwargs[&#39;name&#39;] = STRNAMES.PRIOR_MEAN_GROWTH
        elif child_name == STRNAMES.SELF_INTERACTION_VALUE:
            kwargs[&#39;name&#39;] = STRNAMES.PRIOR_MEAN_SELF_INTERACTIONS
        else:
            raise ValueError(&#39;`child_name` ({}) not recognized&#39;.format(child_name))
        pl.variables.TruncatedNormal.__init__(self, loc=None, scale2=None, dtype=float, **kwargs)
        self.child_name = child_name
        self.add_prior(prior)
        self.proposal = pl.variables.TruncatedNormal(loc=None, scale2=None, value=None)

    def __str__(self) -&gt; str:
        # If this fails, it is because we are dividing by 0 sampler_iter
        # If which case we just return the value 
        try:
            s = &#39;Value: {}, Acceptance rate: {}&#39;.format(
                self.value, np.mean(self.acceptances[
                    np.max([self.sample_iter-50, 0]):self.sample_iter]))
        except:
            s = str(self.value)
        return s

    def initialize(self, value_option: str, loc_option: str, scale2_option: str,
        truncation_settings: Union[str, Tuple[float, float]], proposal_option: str, 
        target_acceptance_rate: Union[str, float], tune: Union[str, int], 
        end_tune: Union[str, int], value: float=None, loc: float=None, scale: float=None, 
        proposal_var: float=None, delay: int=0):
        &#39;&#39;&#39;These are the parameters to initialize the parameters
        of the class. Depending whether it is a self-interaction
        or a growth, it does it differently.

        Parameters
        ----------
        value_option : str
            How to initialize the value. Options:
                &#39;auto&#39;, &#39;prior-mean&#39;
                    Set to the prior mean
                &#39;linear-regression&#39;
                    Set the values from an unregularized linear regression
                &#39;manual&#39;
                    `value` must also be specified
        truncation_settings: str, tuple
            How to set the truncation parameters. The proposal trucation will
            be set the same way.
                tuple - (low,high)
                    These are the truncation parameters
                &#39;auto&#39;
                    If self-interactions, &#39;negative&#39;. If growths, &#39;positive&#39;
                &#39;positive&#39;
                    (0, \infty)
                &#39;negative&#39;
                    (-\infty, 0)
                &#39;in-vivo&#39;
                    Not implemented
        loc_option : str
            How to set the mean
                &#39;auto&#39;, &#39;median-linear-regression&#39;
                    Set the mean to the median of the values from an
                    unregularized linear-regression
                &#39;manual&#39;
                    `loc` must also be specified
        scale2_option : str
            How to set the standard deviation
                &#39;auto&#39;, &#39;diffuse-linear-regression&#39;
                    Set the variance to 10^4 * median(a_l)
                &#39;manaul&#39;
                    `scale2` must also be specified.
        proposal_option : str
            How to initialize the proposal variance:
                &#39;auto&#39;
                    loc**2 / 100
                &#39;manual&#39;
                    `proposal_var` must also be supplied
        target_acceptance_rate : str, float
            If float, this is the target acceptance rate
            If str: 
                &#39;optimal&#39;, &#39;auto&#39;: 0.44
        tune : str, int
            How often to tune the proposal. If str:
                &#39;auto&#39;: 50
        end_tune : str, int
            When to stop tuning the proposal. If str:
                &#39;auto&#39;, &#39;half-burnin&#39;: Half of burnin
        &#39;&#39;&#39;
        self._there_are_perturbations = self.G.perturbations is not None
        if not pl.isint(delay):
            raise TypeError(&#39;`delay` ({}) must be an int&#39;.format(type(delay)))
        if delay &lt; 0:
            raise ValueError(&#39;`delay` ({}) must be &gt;= 0&#39;.format(delay))
        self.delay = delay

        # Set the propsal parameters
        if pl.isstr(target_acceptance_rate):
            if target_acceptance_rate in [&#39;optimal&#39;, &#39;auto&#39;]:
                target_acceptance_rate = 0.44
            else:
                raise ValueError(&#39;`target_acceptance_rate` ({}) not recognized&#39;.format(
                    target_acceptance_rate))
        elif pl.isfloat(target_acceptance_rate):
            if target_acceptance_rate &lt; 0 or target_acceptance_rate &gt; 1:
                raise ValueError(&#39;`target_acceptance_rate` ({}) out of range&#39;.format(
                    target_acceptance_rate))
        else:
            raise TypeError(&#39;`target_acceptance_rate` ({}) type not recognized&#39;.format(
                type(target_acceptance_rate)))
        self.target_acceptance_rate = target_acceptance_rate

        if pl.isstr(tune):
            if tune in [&#39;auto&#39;]:
                tune = 50
            else:
                raise ValueError(&#39;`tune` ({}) not recognized&#39;.format(tune))
        elif pl.isint(tune):
            if tune &lt; 0:
                raise ValueError(&#39;`tune` ({}) must be &gt; 0&#39;.format(
                    tune))
        else:
            raise TypeError(&#39;`tune` ({}) type not recognized&#39;.format(type(tune)))
        self.tune = tune

        if pl.isstr(end_tune):
            if end_tune in [&#39;auto&#39;, &#39;half-burnin&#39;]:
                end_tune = int(self.G.inference.burnin/2)
            else:
                raise ValueError(&#39;`tune` ({}) not recognized&#39;.format(end_tune))
        elif pl.isint(end_tune):
            if end_tune &lt; 0 or end_tune &gt; self.G.inference.burnin:
                raise ValueError(&#39;`end_tune` ({}) out of range (0, {})&#39;.format(
                    end_tune, self.G.inference.burnin))
        else:
            raise TypeError(&#39;`end_tune` ({}) type not recognized&#39;.format(type(end_tune)))
        self.end_tune = end_tune

        # Set the truncation settings
        if truncation_settings is None:
            truncation_settings = &#39;positive&#39;
        if pl.isstr(truncation_settings):
            if truncation_settings == &#39;positive&#39;:
                self.low = 0.
                self.high = float(&#39;inf&#39;)
            elif truncation_settings == &#39;in-vivo&#39;:
                self.low = 0.1
                self.high = np.log(10)
            else:
                raise ValueError(&#39;`truncation_settings` ({}) not recognized&#39;.format(
                    truncation_settings))
        elif pl.istuple(truncation_settings):
            if len(truncation_settings) != 2:
                raise ValueError(&#39;If `truncation_settings` is a tuple, it must have a &#39; \
                    &#39;length of 2 ({})&#39;.format(len(truncation_settings)))
            l,h = truncation_settings

            if (not pl.isnumeric(l)) or (not pl.isnumeric(h)):
                raise TypeError(&#39;`low` ({}) and `high` ({}) must be numerics&#39;.format(
                    type(l), type(h)))
            if l &lt; 0 or h &lt; 0:
                raise ValueError(&#39;`low` ({}) and `high` ({}) must be &gt;= 0&#39;.format(l,h))
            if h &lt;= l:
                raise ValueError(&#39;`low` ({}) must be strictly less than high ({})&#39;.format(l,h))
            self.high = h
            self.low = l
        else:
            raise TypeError(&#39;`truncation_settings` ({}) type not recognized&#39;)
        self.proposal.high = self.high
        self.proposal.low = self.low

        # Set the loc
        if not pl.isstr(loc_option):
            raise TypeError(&#39;`loc_option` ({}) must be a str&#39;.format(type(loc_option)))
        if loc_option == &#39;manual&#39;:
            if not pl.isnumeric(loc):
                raise TypeError(&#39;`loc` ({}) must be a numeric&#39;.format(type(loc)))
        elif loc_option in [&#39;auto&#39;, &#39;median-linear-regression&#39;]:
            # Perform linear regression
            if self.child_name == STRNAMES.GROWTH_VALUE:
                rhs = [STRNAMES.GROWTH_VALUE, STRNAMES.SELF_INTERACTION_VALUE]
                lhs = []
            else:
                rhs = [STRNAMES.SELF_INTERACTION_VALUE]
                lhs = [STRNAMES.GROWTH_VALUE]
            X = self.G.data.construct_rhs(keys=rhs,
                kwargs_dict={STRNAMES.GROWTH_VALUE:{&#39;with_perturbations&#39;:False}},
                index_out_perturbations=True)
            y = self.G.data.construct_lhs(keys=lhs, 
                kwargs_dict={STRNAMES.GROWTH_VALUE:{&#39;with_perturbations&#39;:False}},
                index_out_perturbations=True)

            prec = X.T @ X
            cov = pinv(prec, self)
            loc = cov @ X.T @ y

            if self.child_name == STRNAMES.GROWTH_VALUE:
                loc = np.median(loc[:self.G.data.n_taxa])
            else:
                loc = np.median(loc)
        else:
            raise ValueError(&#39;`loc_option` ({}) not recognized&#39;.format(loc_option))
        self.prior.loc.override_value(loc)

        # Set the scale
        if not pl.isstr(scale2_option):
            raise TypeError(&#39;`scale2_option` ({}) must be a str&#39;.format(type(scale2_option)))
        if scale2_option == &#39;manual&#39;:
            if not pl.isnumeric(scale):
                raise TypeError(&#39;`scale` ({}) must be a numeric&#39;.format(type(scale)))
        elif scale2_option in [&#39;auto&#39;, &#39;diffuse-linear-regression&#39;]:
            # Perform linear regression
            if self.child_name == STRNAMES.GROWTH_VALUE:
                rhs = [STRNAMES.GROWTH_VALUE, STRNAMES.SELF_INTERACTION_VALUE]
                lhs = []
            else:
                rhs = [STRNAMES.SELF_INTERACTION_VALUE]
                lhs = [STRNAMES.GROWTH_VALUE]
            X = self.G.data.construct_rhs(keys=rhs,
                kwargs_dict={STRNAMES.GROWTH_VALUE:{&#39;with_perturbations&#39;:False}},
                index_out_perturbations=True)
            y = self.G.data.construct_lhs(keys=lhs, 
                kwargs_dict={STRNAMES.GROWTH_VALUE:{&#39;with_perturbations&#39;:False}},
                index_out_perturbations=True)

            prec = X.T @ X
            cov = pinv(prec, self)
            mean = cov @ X.T @ y

            if self.child_name == STRNAMES.GROWTH_VALUE:
                mean = np.median(mean[:self.G.data.n_taxa])
            else:
                mean = np.median(mean)
            scale2 = 1e4 * (mean**2)
        else:
            raise ValueError(&#39;`scale2_option` ({}) not recognized&#39;.format(scale2_option))
        self.prior.scale2.override_value(scale2)

        # Set the value
        if not pl.isstr(value_option):
            raise TypeError(&#39;`value_option` ({}) must be a str&#39;.format(type(value_option)))
        if value_option == &#39;manual&#39;:
            if not pl.isnumeric(value):
                raise TypeError(&#39;`value` ({}) must be a numeric&#39;.format(type(value)))
        elif value_option in [&#39;linear-regression&#39;]:
            # Perform linear regression
            if self.child_name == STRNAMES.GROWTH_VALUE:
                rhs = [STRNAMES.GROWTH_VALUE, STRNAMES.SELF_INTERACTION_VALUE]
                lhs = []
            else:
                rhs = [STRNAMES.SELF_INTERACTION_VALUE]
                lhs = [STRNAMES.GROWTH_VALUE]
            X = self.G.data.construct_rhs(keys=rhs,
                kwargs_dict={STRNAMES.GROWTH_VALUE:{&#39;with_perturbations&#39;:False}},
                index_out_perturbations=True)
            y = self.G.data.construct_lhs(keys=lhs, 
                kwargs_dict={STRNAMES.GROWTH_VALUE:{&#39;with_perturbations&#39;:False}},
                index_out_perturbations=True)

            prec = X.T @ X
            cov = pinv(prec, self)
            mean = cov @ X.T @ y

            if self.child_name == STRNAMES.GROWTH_VALUE:
                value = mean[:self.G.data.n_taxa]
            else:
                value = mean
        elif value_option in [&#39;auto&#39;, &#39;prior-mean&#39;]:
            value = self.prior.loc.value
        else:
            raise ValueError(&#39;`value_option` ({}) not recognized&#39;.format(value_option))
        self.value = value

        # Set the proposal variance
        if not pl.isstr(proposal_option):
            raise TypeError(&#39;`proposal_option` ({}) must be a str&#39;.format(
                type(proposal_option)))
        elif proposal_option == &#39;manual&#39;:
            if not pl.isnumeric(proposal_var):
                raise TypeError(&#39;`proposal_var` ({}) must be a numeric&#39;.format(
                    type(proposal_var)))
            if proposal_var &lt;= 0:
                raise ValueError(&#39;`proposal_var` ({}) not proper&#39;.format(proposal_var))
        elif proposal_option in [&#39;auto&#39;]:
            proposal_var = (self.value ** 2)/10
        else:
            raise ValueError(&#39;`proposal_option` ({}) not recognized&#39;.format(
                proposal_option))
        self.proposal.scale2.value = proposal_var

    def update_var(self):
        &#39;&#39;&#39;Update the `var` parameter so that we adjust the acceptance
        rate to `target_acceptance_rate`
        &#39;&#39;&#39;
        if self.sample_iter == 0:
            self.temp_acceptances = 0
            self.acceptances = np.zeros(self.G.inference.n_samples, dtype=bool)
        
        elif self.sample_iter &gt; self.end_tune:
            # Don&#39;t do any more updates
            return
        
        elif self.sample_iter % self.tune == 0:
            # Update var
            acceptance_rate = self.temp_acceptances / self.tune
            if acceptance_rate &gt; self.target_acceptance_rate:
                self.proposal.scale2.value *= 1.5
            else:
                self.proposal.scale2.value /= 1.5
            self.temp_acceptances = 0

    def update(self):
        &#39;&#39;&#39;First we check if we need to tune the var, which we do during
        the first half of burnin. We calculate the likelihoods in logspace
        &#39;&#39;&#39;
        if self.sample_iter &lt; self.delay:
            return
        self.update_var()
        proposal_std = np.sqrt(self.proposal.scale2.value)

        # Get necessary data of the respective parameter
        variable = self.G[self.child_name]
        x = variable.value.ravel()
        std = np.sqrt(variable.prior.scale2.value)

        low = variable.low
        high = variable.high

        # propose a new value for the mean
        prev_mean = self.value
        self.proposal.loc.value = self.value
        new_mean = self.proposal.sample() # Sample a new value

        # Calculate the target distribution ll
        prev_target_ll = pl.random.truncnormal.logpdf( 
            value=prev_mean, loc=self.prior.loc.value, 
            scale=np.sqrt(self.prior.scale2.value), low=self.low,
            high=self.high)
        for i in range(len(x)):
            prev_target_ll += pl.random.truncnormal.logpdf(
                value=x[i], loc=prev_mean, scale=std,
                low=low, high=high)
        new_target_ll = pl.random.truncnormal.logpdf( 
            value=new_mean, loc=self.prior.loc.value, 
            scale=np.sqrt(self.prior.scale2.value), low=self.low,
            high=self.high)
        for i in range(len(x)):
            new_target_ll += pl.random.truncnormal.logpdf(
                value=x[i], loc=new_mean, scale=std,
                low=low, high=high)

        # Normalize by the ll of the proposal
        prev_prop_ll = pl.random.truncnormal.logpdf(
            value=prev_mean, loc=new_mean, scale=proposal_std,
            low=low, high=high)
        
        new_prop_ll = pl.random.truncnormal.logpdf(
            value=new_mean, loc=prev_mean, scale=proposal_std,
            low=low, high=high)

        # Accept or reject
        r = (new_target_ll - prev_prop_ll) - \
            (prev_target_ll - new_prop_ll)
        u = np.log(pl.random.misc.fast_sample_standard_uniform())

        if r &gt;= u:
            self.acceptances[self.sample_iter] = True
            self.value = new_mean
            self.temp_acceptances += 1
        else:
            self.value = prev_mean

    def visualize(self, path: str, section: str=&#39;posterior&#39;) -&gt; pd.DataFrame:
        &#39;&#39;&#39;Render the traces in the folder `basepath` and returns a
        `pandas.DataFrame` with summary statistics

        Parameters
        ----------
        path : str
            This is the path to write the files to
        f : _io.TextIOWrapper
            File that we are writing the values to
        section : str
            Section of the trace to compute on. Options:
                &#39;posterior&#39; : posterior samples
                &#39;burnin&#39; : burn-in samples
                &#39;entire&#39; : both burn-in and posterior samples

        Returns
        -------
        pandas.DataFrame
        &#39;&#39;&#39;
        if not self.G.inference.tracer.is_being_traced(self):
            logging.info(&#39;`{}` not learned\n\tValue: {}\n&#39;.format(self.name, self.value))
            return pd.DataFrame()
        summ = pl.summary(self, section=section)
        data = [[self.name] + [v for _,v in summ.items()]]
        columns = [&#39;name&#39;] + [k for k in summ]
        index = [&#39;mean&#39;]
        df = pd.DataFrame(data, columns=columns, index=index)

        # Plot the traces
        ax1, ax2 = visualization.render_trace(var=self, plt_type=&#39;both&#39;, section=section,
            include_burnin=True, log_scale=True, rasterized=True)

        # Plot the prior over the posterior
        l,h = ax1.get_xlim()
        xs = np.arange(l,h,step=(h-l)/100) 
        ys = []
        for x in xs:
            ys.append(pl.random.normal.pdf(value=x, 
                loc=self.prior.loc.value,
                scale=np.sqrt(self.prior.scale2.value)))
        ax1.plot(xs, ys, label=&#39;prior&#39;, alpha=0.5, color=&#39;red&#39;, rasterized=True)
        ax1.legend()

        # Plot the acceptance rate over the trace
        ax3 = ax2.twinx()
        ax3 = visualization.render_acceptance_rate_trace(var=self, ax=ax3, 
            label=&#39;Acceptance Rate&#39;, color=&#39;red&#39;, scatter=False, rasterized=True)
        ax3.legend()
        fig = plt.gcf()
        fig.tight_layout()
        fig.suptitle(self.name)
        plt.savefig(path)
        plt.close()

        return df</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="mdsine2.pylab.variables.TruncatedNormal" href="pylab/variables.html#mdsine2.pylab.variables.TruncatedNormal">TruncatedNormal</a></li>
<li><a title="mdsine2.pylab.variables.Variable" href="pylab/variables.html#mdsine2.pylab.variables.Variable">Variable</a></li>
<li><a title="mdsine2.pylab.graph.Node" href="pylab/graph.html#mdsine2.pylab.graph.Node">Node</a></li>
<li><a title="mdsine2.pylab.graph.BaseNode" href="pylab/graph.html#mdsine2.pylab.graph.BaseNode">BaseNode</a></li>
<li><a title="mdsine2.pylab.base.Saveable" href="pylab/base.html#mdsine2.pylab.base.Saveable">Saveable</a></li>
<li>mdsine2.pylab.variables._BaseArithmeticClass</li>
<li><a title="mdsine2.pylab.base.Traceable" href="pylab/base.html#mdsine2.pylab.base.Traceable">Traceable</a></li>
<li>mdsine2.pylab.variables._RandomBase</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="mdsine2.posterior.PriorMeanMH.initialize"><code class="name flex">
<span>def <span class="ident">initialize</span></span>(<span>self, value_option: str, loc_option: str, scale2_option: str, truncation_settings: Union[str, Tuple[float, float]], proposal_option: str, target_acceptance_rate: Union[str, float], tune: Union[str, int], end_tune: Union[str, int], value: float = None, loc: float = None, scale: float = None, proposal_var: float = None, delay: int = 0)</span>
</code></dt>
<dd>
<div class="desc"><p>These are the parameters to initialize the parameters
of the class. Depending whether it is a self-interaction
or a growth, it does it differently.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>value_option</code></strong> :&ensp;<code>str</code></dt>
<dd>How to initialize the value. Options:
'auto', 'prior-mean'
Set to the prior mean
'linear-regression'
Set the values from an unregularized linear regression
'manual'
<code>value</code> must also be specified</dd>
<dt><strong><code>truncation_settings</code></strong> :&ensp;<code>str, tuple</code></dt>
<dd>How to set the truncation parameters. The proposal trucation will
be set the same way.
tuple - (low,high)
These are the truncation parameters
'auto'
If self-interactions, 'negative'. If growths, 'positive'
'positive'
(0, \infty)
'negative'
(-\infty, 0)
'in-vivo'
Not implemented</dd>
<dt><strong><code>loc_option</code></strong> :&ensp;<code>str</code></dt>
<dd>How to set the mean
'auto', 'median-linear-regression'
Set the mean to the median of the values from an
unregularized linear-regression
'manual'
<code>loc</code> must also be specified</dd>
<dt><strong><code>scale2_option</code></strong> :&ensp;<code>str</code></dt>
<dd>How to set the standard deviation
'auto', 'diffuse-linear-regression'
Set the variance to 10^4 * median(a_l)
'manaul'
<code>scale2</code> must also be specified.</dd>
<dt><strong><code>proposal_option</code></strong> :&ensp;<code>str</code></dt>
<dd>How to initialize the proposal variance:
'auto'
loc**2 / 100
'manual'
<code>proposal_var</code> must also be supplied</dd>
<dt><strong><code>target_acceptance_rate</code></strong> :&ensp;<code>str, float</code></dt>
<dd>If float, this is the target acceptance rate
If str:
'optimal', 'auto': 0.44</dd>
<dt><strong><code>tune</code></strong> :&ensp;<code>str, int</code></dt>
<dd>How often to tune the proposal. If str:
'auto': 50</dd>
<dt><strong><code>end_tune</code></strong> :&ensp;<code>str, int</code></dt>
<dd>When to stop tuning the proposal. If str:
'auto', 'half-burnin': Half of burnin</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def initialize(self, value_option: str, loc_option: str, scale2_option: str,
    truncation_settings: Union[str, Tuple[float, float]], proposal_option: str, 
    target_acceptance_rate: Union[str, float], tune: Union[str, int], 
    end_tune: Union[str, int], value: float=None, loc: float=None, scale: float=None, 
    proposal_var: float=None, delay: int=0):
    &#39;&#39;&#39;These are the parameters to initialize the parameters
    of the class. Depending whether it is a self-interaction
    or a growth, it does it differently.

    Parameters
    ----------
    value_option : str
        How to initialize the value. Options:
            &#39;auto&#39;, &#39;prior-mean&#39;
                Set to the prior mean
            &#39;linear-regression&#39;
                Set the values from an unregularized linear regression
            &#39;manual&#39;
                `value` must also be specified
    truncation_settings: str, tuple
        How to set the truncation parameters. The proposal trucation will
        be set the same way.
            tuple - (low,high)
                These are the truncation parameters
            &#39;auto&#39;
                If self-interactions, &#39;negative&#39;. If growths, &#39;positive&#39;
            &#39;positive&#39;
                (0, \infty)
            &#39;negative&#39;
                (-\infty, 0)
            &#39;in-vivo&#39;
                Not implemented
    loc_option : str
        How to set the mean
            &#39;auto&#39;, &#39;median-linear-regression&#39;
                Set the mean to the median of the values from an
                unregularized linear-regression
            &#39;manual&#39;
                `loc` must also be specified
    scale2_option : str
        How to set the standard deviation
            &#39;auto&#39;, &#39;diffuse-linear-regression&#39;
                Set the variance to 10^4 * median(a_l)
            &#39;manaul&#39;
                `scale2` must also be specified.
    proposal_option : str
        How to initialize the proposal variance:
            &#39;auto&#39;
                loc**2 / 100
            &#39;manual&#39;
                `proposal_var` must also be supplied
    target_acceptance_rate : str, float
        If float, this is the target acceptance rate
        If str: 
            &#39;optimal&#39;, &#39;auto&#39;: 0.44
    tune : str, int
        How often to tune the proposal. If str:
            &#39;auto&#39;: 50
    end_tune : str, int
        When to stop tuning the proposal. If str:
            &#39;auto&#39;, &#39;half-burnin&#39;: Half of burnin
    &#39;&#39;&#39;
    self._there_are_perturbations = self.G.perturbations is not None
    if not pl.isint(delay):
        raise TypeError(&#39;`delay` ({}) must be an int&#39;.format(type(delay)))
    if delay &lt; 0:
        raise ValueError(&#39;`delay` ({}) must be &gt;= 0&#39;.format(delay))
    self.delay = delay

    # Set the propsal parameters
    if pl.isstr(target_acceptance_rate):
        if target_acceptance_rate in [&#39;optimal&#39;, &#39;auto&#39;]:
            target_acceptance_rate = 0.44
        else:
            raise ValueError(&#39;`target_acceptance_rate` ({}) not recognized&#39;.format(
                target_acceptance_rate))
    elif pl.isfloat(target_acceptance_rate):
        if target_acceptance_rate &lt; 0 or target_acceptance_rate &gt; 1:
            raise ValueError(&#39;`target_acceptance_rate` ({}) out of range&#39;.format(
                target_acceptance_rate))
    else:
        raise TypeError(&#39;`target_acceptance_rate` ({}) type not recognized&#39;.format(
            type(target_acceptance_rate)))
    self.target_acceptance_rate = target_acceptance_rate

    if pl.isstr(tune):
        if tune in [&#39;auto&#39;]:
            tune = 50
        else:
            raise ValueError(&#39;`tune` ({}) not recognized&#39;.format(tune))
    elif pl.isint(tune):
        if tune &lt; 0:
            raise ValueError(&#39;`tune` ({}) must be &gt; 0&#39;.format(
                tune))
    else:
        raise TypeError(&#39;`tune` ({}) type not recognized&#39;.format(type(tune)))
    self.tune = tune

    if pl.isstr(end_tune):
        if end_tune in [&#39;auto&#39;, &#39;half-burnin&#39;]:
            end_tune = int(self.G.inference.burnin/2)
        else:
            raise ValueError(&#39;`tune` ({}) not recognized&#39;.format(end_tune))
    elif pl.isint(end_tune):
        if end_tune &lt; 0 or end_tune &gt; self.G.inference.burnin:
            raise ValueError(&#39;`end_tune` ({}) out of range (0, {})&#39;.format(
                end_tune, self.G.inference.burnin))
    else:
        raise TypeError(&#39;`end_tune` ({}) type not recognized&#39;.format(type(end_tune)))
    self.end_tune = end_tune

    # Set the truncation settings
    if truncation_settings is None:
        truncation_settings = &#39;positive&#39;
    if pl.isstr(truncation_settings):
        if truncation_settings == &#39;positive&#39;:
            self.low = 0.
            self.high = float(&#39;inf&#39;)
        elif truncation_settings == &#39;in-vivo&#39;:
            self.low = 0.1
            self.high = np.log(10)
        else:
            raise ValueError(&#39;`truncation_settings` ({}) not recognized&#39;.format(
                truncation_settings))
    elif pl.istuple(truncation_settings):
        if len(truncation_settings) != 2:
            raise ValueError(&#39;If `truncation_settings` is a tuple, it must have a &#39; \
                &#39;length of 2 ({})&#39;.format(len(truncation_settings)))
        l,h = truncation_settings

        if (not pl.isnumeric(l)) or (not pl.isnumeric(h)):
            raise TypeError(&#39;`low` ({}) and `high` ({}) must be numerics&#39;.format(
                type(l), type(h)))
        if l &lt; 0 or h &lt; 0:
            raise ValueError(&#39;`low` ({}) and `high` ({}) must be &gt;= 0&#39;.format(l,h))
        if h &lt;= l:
            raise ValueError(&#39;`low` ({}) must be strictly less than high ({})&#39;.format(l,h))
        self.high = h
        self.low = l
    else:
        raise TypeError(&#39;`truncation_settings` ({}) type not recognized&#39;)
    self.proposal.high = self.high
    self.proposal.low = self.low

    # Set the loc
    if not pl.isstr(loc_option):
        raise TypeError(&#39;`loc_option` ({}) must be a str&#39;.format(type(loc_option)))
    if loc_option == &#39;manual&#39;:
        if not pl.isnumeric(loc):
            raise TypeError(&#39;`loc` ({}) must be a numeric&#39;.format(type(loc)))
    elif loc_option in [&#39;auto&#39;, &#39;median-linear-regression&#39;]:
        # Perform linear regression
        if self.child_name == STRNAMES.GROWTH_VALUE:
            rhs = [STRNAMES.GROWTH_VALUE, STRNAMES.SELF_INTERACTION_VALUE]
            lhs = []
        else:
            rhs = [STRNAMES.SELF_INTERACTION_VALUE]
            lhs = [STRNAMES.GROWTH_VALUE]
        X = self.G.data.construct_rhs(keys=rhs,
            kwargs_dict={STRNAMES.GROWTH_VALUE:{&#39;with_perturbations&#39;:False}},
            index_out_perturbations=True)
        y = self.G.data.construct_lhs(keys=lhs, 
            kwargs_dict={STRNAMES.GROWTH_VALUE:{&#39;with_perturbations&#39;:False}},
            index_out_perturbations=True)

        prec = X.T @ X
        cov = pinv(prec, self)
        loc = cov @ X.T @ y

        if self.child_name == STRNAMES.GROWTH_VALUE:
            loc = np.median(loc[:self.G.data.n_taxa])
        else:
            loc = np.median(loc)
    else:
        raise ValueError(&#39;`loc_option` ({}) not recognized&#39;.format(loc_option))
    self.prior.loc.override_value(loc)

    # Set the scale
    if not pl.isstr(scale2_option):
        raise TypeError(&#39;`scale2_option` ({}) must be a str&#39;.format(type(scale2_option)))
    if scale2_option == &#39;manual&#39;:
        if not pl.isnumeric(scale):
            raise TypeError(&#39;`scale` ({}) must be a numeric&#39;.format(type(scale)))
    elif scale2_option in [&#39;auto&#39;, &#39;diffuse-linear-regression&#39;]:
        # Perform linear regression
        if self.child_name == STRNAMES.GROWTH_VALUE:
            rhs = [STRNAMES.GROWTH_VALUE, STRNAMES.SELF_INTERACTION_VALUE]
            lhs = []
        else:
            rhs = [STRNAMES.SELF_INTERACTION_VALUE]
            lhs = [STRNAMES.GROWTH_VALUE]
        X = self.G.data.construct_rhs(keys=rhs,
            kwargs_dict={STRNAMES.GROWTH_VALUE:{&#39;with_perturbations&#39;:False}},
            index_out_perturbations=True)
        y = self.G.data.construct_lhs(keys=lhs, 
            kwargs_dict={STRNAMES.GROWTH_VALUE:{&#39;with_perturbations&#39;:False}},
            index_out_perturbations=True)

        prec = X.T @ X
        cov = pinv(prec, self)
        mean = cov @ X.T @ y

        if self.child_name == STRNAMES.GROWTH_VALUE:
            mean = np.median(mean[:self.G.data.n_taxa])
        else:
            mean = np.median(mean)
        scale2 = 1e4 * (mean**2)
    else:
        raise ValueError(&#39;`scale2_option` ({}) not recognized&#39;.format(scale2_option))
    self.prior.scale2.override_value(scale2)

    # Set the value
    if not pl.isstr(value_option):
        raise TypeError(&#39;`value_option` ({}) must be a str&#39;.format(type(value_option)))
    if value_option == &#39;manual&#39;:
        if not pl.isnumeric(value):
            raise TypeError(&#39;`value` ({}) must be a numeric&#39;.format(type(value)))
    elif value_option in [&#39;linear-regression&#39;]:
        # Perform linear regression
        if self.child_name == STRNAMES.GROWTH_VALUE:
            rhs = [STRNAMES.GROWTH_VALUE, STRNAMES.SELF_INTERACTION_VALUE]
            lhs = []
        else:
            rhs = [STRNAMES.SELF_INTERACTION_VALUE]
            lhs = [STRNAMES.GROWTH_VALUE]
        X = self.G.data.construct_rhs(keys=rhs,
            kwargs_dict={STRNAMES.GROWTH_VALUE:{&#39;with_perturbations&#39;:False}},
            index_out_perturbations=True)
        y = self.G.data.construct_lhs(keys=lhs, 
            kwargs_dict={STRNAMES.GROWTH_VALUE:{&#39;with_perturbations&#39;:False}},
            index_out_perturbations=True)

        prec = X.T @ X
        cov = pinv(prec, self)
        mean = cov @ X.T @ y

        if self.child_name == STRNAMES.GROWTH_VALUE:
            value = mean[:self.G.data.n_taxa]
        else:
            value = mean
    elif value_option in [&#39;auto&#39;, &#39;prior-mean&#39;]:
        value = self.prior.loc.value
    else:
        raise ValueError(&#39;`value_option` ({}) not recognized&#39;.format(value_option))
    self.value = value

    # Set the proposal variance
    if not pl.isstr(proposal_option):
        raise TypeError(&#39;`proposal_option` ({}) must be a str&#39;.format(
            type(proposal_option)))
    elif proposal_option == &#39;manual&#39;:
        if not pl.isnumeric(proposal_var):
            raise TypeError(&#39;`proposal_var` ({}) must be a numeric&#39;.format(
                type(proposal_var)))
        if proposal_var &lt;= 0:
            raise ValueError(&#39;`proposal_var` ({}) not proper&#39;.format(proposal_var))
    elif proposal_option in [&#39;auto&#39;]:
        proposal_var = (self.value ** 2)/10
    else:
        raise ValueError(&#39;`proposal_option` ({}) not recognized&#39;.format(
            proposal_option))
    self.proposal.scale2.value = proposal_var</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.PriorMeanMH.update"><code class="name flex">
<span>def <span class="ident">update</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>First we check if we need to tune the var, which we do during
the first half of burnin. We calculate the likelihoods in logspace</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update(self):
    &#39;&#39;&#39;First we check if we need to tune the var, which we do during
    the first half of burnin. We calculate the likelihoods in logspace
    &#39;&#39;&#39;
    if self.sample_iter &lt; self.delay:
        return
    self.update_var()
    proposal_std = np.sqrt(self.proposal.scale2.value)

    # Get necessary data of the respective parameter
    variable = self.G[self.child_name]
    x = variable.value.ravel()
    std = np.sqrt(variable.prior.scale2.value)

    low = variable.low
    high = variable.high

    # propose a new value for the mean
    prev_mean = self.value
    self.proposal.loc.value = self.value
    new_mean = self.proposal.sample() # Sample a new value

    # Calculate the target distribution ll
    prev_target_ll = pl.random.truncnormal.logpdf( 
        value=prev_mean, loc=self.prior.loc.value, 
        scale=np.sqrt(self.prior.scale2.value), low=self.low,
        high=self.high)
    for i in range(len(x)):
        prev_target_ll += pl.random.truncnormal.logpdf(
            value=x[i], loc=prev_mean, scale=std,
            low=low, high=high)
    new_target_ll = pl.random.truncnormal.logpdf( 
        value=new_mean, loc=self.prior.loc.value, 
        scale=np.sqrt(self.prior.scale2.value), low=self.low,
        high=self.high)
    for i in range(len(x)):
        new_target_ll += pl.random.truncnormal.logpdf(
            value=x[i], loc=new_mean, scale=std,
            low=low, high=high)

    # Normalize by the ll of the proposal
    prev_prop_ll = pl.random.truncnormal.logpdf(
        value=prev_mean, loc=new_mean, scale=proposal_std,
        low=low, high=high)
    
    new_prop_ll = pl.random.truncnormal.logpdf(
        value=new_mean, loc=prev_mean, scale=proposal_std,
        low=low, high=high)

    # Accept or reject
    r = (new_target_ll - prev_prop_ll) - \
        (prev_target_ll - new_prop_ll)
    u = np.log(pl.random.misc.fast_sample_standard_uniform())

    if r &gt;= u:
        self.acceptances[self.sample_iter] = True
        self.value = new_mean
        self.temp_acceptances += 1
    else:
        self.value = prev_mean</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.PriorMeanMH.update_var"><code class="name flex">
<span>def <span class="ident">update_var</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Update the <code>var</code> parameter so that we adjust the acceptance
rate to <code>target_acceptance_rate</code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_var(self):
    &#39;&#39;&#39;Update the `var` parameter so that we adjust the acceptance
    rate to `target_acceptance_rate`
    &#39;&#39;&#39;
    if self.sample_iter == 0:
        self.temp_acceptances = 0
        self.acceptances = np.zeros(self.G.inference.n_samples, dtype=bool)
    
    elif self.sample_iter &gt; self.end_tune:
        # Don&#39;t do any more updates
        return
    
    elif self.sample_iter % self.tune == 0:
        # Update var
        acceptance_rate = self.temp_acceptances / self.tune
        if acceptance_rate &gt; self.target_acceptance_rate:
            self.proposal.scale2.value *= 1.5
        else:
            self.proposal.scale2.value /= 1.5
        self.temp_acceptances = 0</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.PriorMeanMH.visualize"><code class="name flex">
<span>def <span class="ident">visualize</span></span>(<span>self, path: str, section: str = 'posterior') ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Render the traces in the folder <code>basepath</code> and returns a
<code>pandas.DataFrame</code> with summary statistics</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>path</code></strong> :&ensp;<code>str</code></dt>
<dd>This is the path to write the files to</dd>
<dt><strong><code>f</code></strong> :&ensp;<code>_io.TextIOWrapper</code></dt>
<dd>File that we are writing the values to</dd>
<dt><strong><code>section</code></strong> :&ensp;<code>str</code></dt>
<dd>Section of the trace to compute on. Options:
'posterior' : posterior samples
'burnin' : burn-in samples
'entire' : both burn-in and posterior samples</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pandas.DataFrame</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def visualize(self, path: str, section: str=&#39;posterior&#39;) -&gt; pd.DataFrame:
    &#39;&#39;&#39;Render the traces in the folder `basepath` and returns a
    `pandas.DataFrame` with summary statistics

    Parameters
    ----------
    path : str
        This is the path to write the files to
    f : _io.TextIOWrapper
        File that we are writing the values to
    section : str
        Section of the trace to compute on. Options:
            &#39;posterior&#39; : posterior samples
            &#39;burnin&#39; : burn-in samples
            &#39;entire&#39; : both burn-in and posterior samples

    Returns
    -------
    pandas.DataFrame
    &#39;&#39;&#39;
    if not self.G.inference.tracer.is_being_traced(self):
        logging.info(&#39;`{}` not learned\n\tValue: {}\n&#39;.format(self.name, self.value))
        return pd.DataFrame()
    summ = pl.summary(self, section=section)
    data = [[self.name] + [v for _,v in summ.items()]]
    columns = [&#39;name&#39;] + [k for k in summ]
    index = [&#39;mean&#39;]
    df = pd.DataFrame(data, columns=columns, index=index)

    # Plot the traces
    ax1, ax2 = visualization.render_trace(var=self, plt_type=&#39;both&#39;, section=section,
        include_burnin=True, log_scale=True, rasterized=True)

    # Plot the prior over the posterior
    l,h = ax1.get_xlim()
    xs = np.arange(l,h,step=(h-l)/100) 
    ys = []
    for x in xs:
        ys.append(pl.random.normal.pdf(value=x, 
            loc=self.prior.loc.value,
            scale=np.sqrt(self.prior.scale2.value)))
    ax1.plot(xs, ys, label=&#39;prior&#39;, alpha=0.5, color=&#39;red&#39;, rasterized=True)
    ax1.legend()

    # Plot the acceptance rate over the trace
    ax3 = ax2.twinx()
    ax3 = visualization.render_acceptance_rate_trace(var=self, ax=ax3, 
        label=&#39;Acceptance Rate&#39;, color=&#39;red&#39;, scatter=False, rasterized=True)
    ax3.legend()
    fig = plt.gcf()
    fig.tight_layout()
    fig.suptitle(self.name)
    plt.savefig(path)
    plt.close()

    return df</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="mdsine2.pylab.variables.TruncatedNormal" href="pylab/variables.html#mdsine2.pylab.variables.TruncatedNormal">TruncatedNormal</a></b></code>:
<ul class="hlist">
<li><code><a title="mdsine2.pylab.variables.TruncatedNormal.T" href="pylab/variables.html#mdsine2.pylab.variables.Variable.T">T</a></code></li>
<li><code><a title="mdsine2.pylab.variables.TruncatedNormal.add_child" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_child">add_child</a></code></li>
<li><code><a title="mdsine2.pylab.variables.TruncatedNormal.add_parent" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_parent">add_parent</a></code></li>
<li><code><a title="mdsine2.pylab.variables.TruncatedNormal.add_prior" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_prior">add_prior</a></code></li>
<li><code><a title="mdsine2.pylab.variables.TruncatedNormal.add_trace" href="pylab/base.html#mdsine2.pylab.base.Traceable.add_trace">add_trace</a></code></li>
<li><code><a title="mdsine2.pylab.variables.TruncatedNormal.add_undirected" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_undirected">add_undirected</a></code></li>
<li><code><a title="mdsine2.pylab.variables.TruncatedNormal.cdf" href="pylab/variables.html#mdsine2.pylab.variables.TruncatedNormal.cdf">cdf</a></code></li>
<li><code><a title="mdsine2.pylab.variables.TruncatedNormal.degree" href="pylab/graph.html#mdsine2.pylab.graph.Node.degree">degree</a></code></li>
<li><code><a title="mdsine2.pylab.variables.TruncatedNormal.delete" href="pylab/graph.html#mdsine2.pylab.graph.BaseNode.delete">delete</a></code></li>
<li><code><a title="mdsine2.pylab.variables.TruncatedNormal.get_adjacent_keys" href="pylab/graph.html#mdsine2.pylab.graph.Node.get_adjacent_keys">get_adjacent_keys</a></code></li>
<li><code><a title="mdsine2.pylab.variables.TruncatedNormal.get_iter" href="pylab/base.html#mdsine2.pylab.base.Traceable.get_iter">get_iter</a></code></li>
<li><code><a title="mdsine2.pylab.variables.TruncatedNormal.get_trace_from_disk" href="pylab/base.html#mdsine2.pylab.base.Traceable.get_trace_from_disk">get_trace_from_disk</a></code></li>
<li><code><a title="mdsine2.pylab.variables.TruncatedNormal.load" href="pylab/base.html#mdsine2.pylab.base.Saveable.load">load</a></code></li>
<li><code><a title="mdsine2.pylab.variables.TruncatedNormal.logcdf" href="pylab/variables.html#mdsine2.pylab.variables.TruncatedNormal.logcdf">logcdf</a></code></li>
<li><code><a title="mdsine2.pylab.variables.TruncatedNormal.logpdf" href="pylab/variables.html#mdsine2.pylab.variables.TruncatedNormal.logpdf">logpdf</a></code></li>
<li><code><a title="mdsine2.pylab.variables.TruncatedNormal.metropolis" href="pylab/graph.html#mdsine2.pylab.graph.Node.metropolis">metropolis</a></code></li>
<li><code><a title="mdsine2.pylab.variables.TruncatedNormal.overwrite_entire_trace_on_disk" href="pylab/base.html#mdsine2.pylab.base.Traceable.overwrite_entire_trace_on_disk">overwrite_entire_trace_on_disk</a></code></li>
<li><code><a title="mdsine2.pylab.variables.TruncatedNormal.pdf" href="pylab/variables.html#mdsine2.pylab.variables.TruncatedNormal.pdf">pdf</a></code></li>
<li><code><a title="mdsine2.pylab.variables.TruncatedNormal.remove_local_trace" href="pylab/variables.html#mdsine2.pylab.variables.Variable.remove_local_trace">remove_local_trace</a></code></li>
<li><code><a title="mdsine2.pylab.variables.TruncatedNormal.sample" href="pylab/variables.html#mdsine2.pylab.variables.TruncatedNormal.sample">sample</a></code></li>
<li><code><a title="mdsine2.pylab.variables.TruncatedNormal.save" href="pylab/base.html#mdsine2.pylab.base.Saveable.save">save</a></code></li>
<li><code><a title="mdsine2.pylab.variables.TruncatedNormal.set_save_location" href="pylab/base.html#mdsine2.pylab.base.Saveable.set_save_location">set_save_location</a></code></li>
<li><code><a title="mdsine2.pylab.variables.TruncatedNormal.set_trace" href="pylab/base.html#mdsine2.pylab.base.Traceable.set_trace">set_trace</a></code></li>
<li><code><a title="mdsine2.pylab.variables.TruncatedNormal.set_value_shape" href="pylab/variables.html#mdsine2.pylab.variables.Variable.set_value_shape">set_value_shape</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="mdsine2.posterior.PriorMeanPerturbationSingle"><code class="flex name class">
<span>class <span class="ident">PriorMeanPerturbationSingle</span></span>
<span>(</span><span>prior: <a title="mdsine2.pylab.variables.Normal" href="pylab/variables.html#mdsine2.pylab.variables.Normal">Normal</a>, perturbation: <a title="mdsine2.pylab.contrib.ClusterPerturbationEffect" href="pylab/contrib.html#mdsine2.pylab.contrib.ClusterPerturbationEffect">ClusterPerturbationEffect</a>, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Scalar normal variable parameterized by the loc (mean) and scale2
(variance)</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>loc</code></strong> :&ensp;<code>float, int, Variable</code></dt>
<dd>This is the mean of the distribution</dd>
<dt><strong><code>scale2</code></strong> :&ensp;<code>float, int, Variable</code></dt>
<dd>This is the variance of the distribution</dd>
<dt><strong><code>kwargs</code></strong> :&ensp;<code>dict</code></dt>
<dd>These are extra parameters for the Node class</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PriorMeanPerturbationSingle(pl.variables.Normal):
    
    def __init__(self, prior: variables.Normal, perturbation: pl.ClusterPerturbationEffect, **kwargs):

        kwargs[&#39;name&#39;] = STRNAMES.PRIOR_MEAN_PERT + &#39;_&#39; + perturbation.name
        pl.variables.Normal.__init__(self, loc=None, scale2=None, dtype=float, **kwargs)
        self.add_prior(prior)
        self.perturbation = perturbation

    def initialize(self, value_option: str, loc_option: str, scale2_option: str, 
        value: float=None, loc: float=None, scale2: float=None, delay: int=0):
        &#39;&#39;&#39;Initialize the hyperparameters

        Parameters
        ----------
        value_option : str
            How to set the value. Options:
                &#39;zero&#39;
                    Set to zero
                &#39;prior-mean&#39;, &#39;auto&#39;
                    Set to the mean of the prior
                &#39;manual&#39;
                    Specify with the `value` parameter
        loc_option : str
            How to set the mean of the prior
                &#39;zero&#39;, &#39;auto&#39;
                    Set to zero
                &#39;manual&#39;
                    Set with the `loc` parameter
        scale2_option : str
            &#39;diffuse&#39;, &#39;auto&#39;
                Variance is set to 10e4
            &#39;tight&#39;
                Variance is set to 1e2
            &#39;manual&#39;
                Set with the `scale2` parameter
        value, loc, scale2 : float
            These are only necessary if we specify manual for any of the other 
            options
        delay : int
            How much to delay the start of the update during inference
        &#39;&#39;&#39;
        if not pl.isint(delay):
            raise TypeError(&#39;`delay` ({}) must be an int&#39;.format(type(delay)))
        if delay &lt; 0:
            raise ValueError(&#39;`delay` ({}) must be &gt;= 0&#39;.format(delay))
        self.delay = delay

        # Set the loc
        if not pl.isstr(loc_option):
            raise TypeError(&#39;`loc_option` ({}) must be a str&#39;.format(type(loc_option)))
        if loc_option == &#39;manual&#39;:
            if not pl.isnumeric(loc):
                raise TypeError(&#39;`loc` ({}) must be a numeric&#39;.format(type(loc)))
        elif loc_option in [&#39;zero&#39;, &#39;auto&#39;]:
            loc = 0
        else:
            raise ValueError(&#39;`loc_option` ({}) not recognized&#39;.format(loc_option))
        self.prior.loc.override_value(loc)

        # Set the scale2
        if not pl.isstr(scale2_option):
            raise TypeError(&#39;`scale2_option` ({}) must be a str&#39;.format(type(scale2_option)))
        if scale2_option == &#39;manual&#39;:
            if not pl.isnumeric(scale2):
                raise TypeError(&#39;`scale2` ({}) must be a numeric&#39;.format(type(scale2)))
            if scale2 &lt;= 0:
                raise ValueError(&#39;`scale2` ({}) must be positive&#39;.format(scale2))
        elif scale2_option in [&#39;diffuse&#39;, &#39;auto&#39;]:
            scale2 = 1e4
        elif scale2_option == &#39;tight&#39;:
            scale2 = 1e2
        else:
            raise ValueError(&#39;`scale2_option` ({}) not recognized&#39;.format(scale2_option))
        self.prior.scale2.override_value(scale2)

        # Set the value
        if not pl.isstr(value_option):
            raise TypeError(&#39;`value_option` ({}) must be a str&#39;.format(type(value_option)))
        if value_option == &#39;manual&#39;:
            if not pl.isnumeric(value):
                raise TypeError(&#39;`value` ({}) must be a numeric&#39;.format(type(value)))
        elif value_option in [&#39;prior-mean&#39;, &#39;auto&#39;]:
            value = self.prior.loc.value
        elif value_option == &#39;zero&#39;:
            value = 0
        else:
            raise ValueError(&#39;`value_option` ({}) not recognized&#39;.format(value_option))
        self.value = value
        
    def update(self):
        &#39;&#39;&#39;Update using a Gibbs update
        &#39;&#39;&#39;
        if self.sample_iter &lt; self.delay:
            return

        x = self.perturbation.cluster_array(only_pos_ind=True)
        prec = 1/self.perturbation.magnitude.prior.scale2.value

        prior_prec = 1/self.prior.scale2.value
        prior_mean = self.prior.loc.value

        self.scale2.value = 1/(prior_prec + (len(x)*prec))
        self.loc.value = self.scale2.value * ((prior_mean * prior_prec) + (np.sum(x)*prec))
        self.sample()</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="mdsine2.pylab.variables.Normal" href="pylab/variables.html#mdsine2.pylab.variables.Normal">Normal</a></li>
<li><a title="mdsine2.pylab.variables.Variable" href="pylab/variables.html#mdsine2.pylab.variables.Variable">Variable</a></li>
<li><a title="mdsine2.pylab.graph.Node" href="pylab/graph.html#mdsine2.pylab.graph.Node">Node</a></li>
<li><a title="mdsine2.pylab.graph.BaseNode" href="pylab/graph.html#mdsine2.pylab.graph.BaseNode">BaseNode</a></li>
<li><a title="mdsine2.pylab.base.Saveable" href="pylab/base.html#mdsine2.pylab.base.Saveable">Saveable</a></li>
<li>mdsine2.pylab.variables._BaseArithmeticClass</li>
<li><a title="mdsine2.pylab.base.Traceable" href="pylab/base.html#mdsine2.pylab.base.Traceable">Traceable</a></li>
<li>mdsine2.pylab.variables._RandomBase</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="mdsine2.posterior.PriorMeanPerturbationSingle.initialize"><code class="name flex">
<span>def <span class="ident">initialize</span></span>(<span>self, value_option: str, loc_option: str, scale2_option: str, value: float = None, loc: float = None, scale2: float = None, delay: int = 0)</span>
</code></dt>
<dd>
<div class="desc"><p>Initialize the hyperparameters</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>value_option</code></strong> :&ensp;<code>str</code></dt>
<dd>How to set the value. Options:
'zero'
Set to zero
'prior-mean', 'auto'
Set to the mean of the prior
'manual'
Specify with the <code>value</code> parameter</dd>
<dt><strong><code>loc_option</code></strong> :&ensp;<code>str</code></dt>
<dd>How to set the mean of the prior
'zero', 'auto'
Set to zero
'manual'
Set with the <code>loc</code> parameter</dd>
<dt><strong><code>scale2_option</code></strong> :&ensp;<code>str</code></dt>
<dd>'diffuse', 'auto'
Variance is set to 10e4
'tight'
Variance is set to 1e2
'manual'
Set with the <code>scale2</code> parameter</dd>
<dt><strong><code>value</code></strong>, <strong><code>loc</code></strong>, <strong><code>scale2</code></strong> :&ensp;<code>float</code></dt>
<dd>These are only necessary if we specify manual for any of the other
options</dd>
<dt><strong><code>delay</code></strong> :&ensp;<code>int</code></dt>
<dd>How much to delay the start of the update during inference</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def initialize(self, value_option: str, loc_option: str, scale2_option: str, 
    value: float=None, loc: float=None, scale2: float=None, delay: int=0):
    &#39;&#39;&#39;Initialize the hyperparameters

    Parameters
    ----------
    value_option : str
        How to set the value. Options:
            &#39;zero&#39;
                Set to zero
            &#39;prior-mean&#39;, &#39;auto&#39;
                Set to the mean of the prior
            &#39;manual&#39;
                Specify with the `value` parameter
    loc_option : str
        How to set the mean of the prior
            &#39;zero&#39;, &#39;auto&#39;
                Set to zero
            &#39;manual&#39;
                Set with the `loc` parameter
    scale2_option : str
        &#39;diffuse&#39;, &#39;auto&#39;
            Variance is set to 10e4
        &#39;tight&#39;
            Variance is set to 1e2
        &#39;manual&#39;
            Set with the `scale2` parameter
    value, loc, scale2 : float
        These are only necessary if we specify manual for any of the other 
        options
    delay : int
        How much to delay the start of the update during inference
    &#39;&#39;&#39;
    if not pl.isint(delay):
        raise TypeError(&#39;`delay` ({}) must be an int&#39;.format(type(delay)))
    if delay &lt; 0:
        raise ValueError(&#39;`delay` ({}) must be &gt;= 0&#39;.format(delay))
    self.delay = delay

    # Set the loc
    if not pl.isstr(loc_option):
        raise TypeError(&#39;`loc_option` ({}) must be a str&#39;.format(type(loc_option)))
    if loc_option == &#39;manual&#39;:
        if not pl.isnumeric(loc):
            raise TypeError(&#39;`loc` ({}) must be a numeric&#39;.format(type(loc)))
    elif loc_option in [&#39;zero&#39;, &#39;auto&#39;]:
        loc = 0
    else:
        raise ValueError(&#39;`loc_option` ({}) not recognized&#39;.format(loc_option))
    self.prior.loc.override_value(loc)

    # Set the scale2
    if not pl.isstr(scale2_option):
        raise TypeError(&#39;`scale2_option` ({}) must be a str&#39;.format(type(scale2_option)))
    if scale2_option == &#39;manual&#39;:
        if not pl.isnumeric(scale2):
            raise TypeError(&#39;`scale2` ({}) must be a numeric&#39;.format(type(scale2)))
        if scale2 &lt;= 0:
            raise ValueError(&#39;`scale2` ({}) must be positive&#39;.format(scale2))
    elif scale2_option in [&#39;diffuse&#39;, &#39;auto&#39;]:
        scale2 = 1e4
    elif scale2_option == &#39;tight&#39;:
        scale2 = 1e2
    else:
        raise ValueError(&#39;`scale2_option` ({}) not recognized&#39;.format(scale2_option))
    self.prior.scale2.override_value(scale2)

    # Set the value
    if not pl.isstr(value_option):
        raise TypeError(&#39;`value_option` ({}) must be a str&#39;.format(type(value_option)))
    if value_option == &#39;manual&#39;:
        if not pl.isnumeric(value):
            raise TypeError(&#39;`value` ({}) must be a numeric&#39;.format(type(value)))
    elif value_option in [&#39;prior-mean&#39;, &#39;auto&#39;]:
        value = self.prior.loc.value
    elif value_option == &#39;zero&#39;:
        value = 0
    else:
        raise ValueError(&#39;`value_option` ({}) not recognized&#39;.format(value_option))
    self.value = value</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.PriorMeanPerturbationSingle.update"><code class="name flex">
<span>def <span class="ident">update</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Update using a Gibbs update</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update(self):
    &#39;&#39;&#39;Update using a Gibbs update
    &#39;&#39;&#39;
    if self.sample_iter &lt; self.delay:
        return

    x = self.perturbation.cluster_array(only_pos_ind=True)
    prec = 1/self.perturbation.magnitude.prior.scale2.value

    prior_prec = 1/self.prior.scale2.value
    prior_mean = self.prior.loc.value

    self.scale2.value = 1/(prior_prec + (len(x)*prec))
    self.loc.value = self.scale2.value * ((prior_mean * prior_prec) + (np.sum(x)*prec))
    self.sample()</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="mdsine2.pylab.variables.Normal" href="pylab/variables.html#mdsine2.pylab.variables.Normal">Normal</a></b></code>:
<ul class="hlist">
<li><code><a title="mdsine2.pylab.variables.Normal.T" href="pylab/variables.html#mdsine2.pylab.variables.Variable.T">T</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Normal.add_child" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_child">add_child</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Normal.add_parent" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_parent">add_parent</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Normal.add_prior" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_prior">add_prior</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Normal.add_trace" href="pylab/base.html#mdsine2.pylab.base.Traceable.add_trace">add_trace</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Normal.add_undirected" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_undirected">add_undirected</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Normal.cdf" href="pylab/variables.html#mdsine2.pylab.variables.Normal.cdf">cdf</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Normal.degree" href="pylab/graph.html#mdsine2.pylab.graph.Node.degree">degree</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Normal.delete" href="pylab/graph.html#mdsine2.pylab.graph.BaseNode.delete">delete</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Normal.get_adjacent_keys" href="pylab/graph.html#mdsine2.pylab.graph.Node.get_adjacent_keys">get_adjacent_keys</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Normal.get_iter" href="pylab/base.html#mdsine2.pylab.base.Traceable.get_iter">get_iter</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Normal.get_trace_from_disk" href="pylab/base.html#mdsine2.pylab.base.Traceable.get_trace_from_disk">get_trace_from_disk</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Normal.load" href="pylab/base.html#mdsine2.pylab.base.Saveable.load">load</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Normal.logcdf" href="pylab/variables.html#mdsine2.pylab.variables.Normal.logcdf">logcdf</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Normal.logpdf" href="pylab/variables.html#mdsine2.pylab.variables.Normal.logpdf">logpdf</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Normal.metropolis" href="pylab/graph.html#mdsine2.pylab.graph.Node.metropolis">metropolis</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Normal.overwrite_entire_trace_on_disk" href="pylab/base.html#mdsine2.pylab.base.Traceable.overwrite_entire_trace_on_disk">overwrite_entire_trace_on_disk</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Normal.pdf" href="pylab/variables.html#mdsine2.pylab.variables.Normal.pdf">pdf</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Normal.remove_local_trace" href="pylab/variables.html#mdsine2.pylab.variables.Variable.remove_local_trace">remove_local_trace</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Normal.sample" href="pylab/variables.html#mdsine2.pylab.variables.Normal.sample">sample</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Normal.save" href="pylab/base.html#mdsine2.pylab.base.Saveable.save">save</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Normal.set_save_location" href="pylab/base.html#mdsine2.pylab.base.Saveable.set_save_location">set_save_location</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Normal.set_trace" href="pylab/base.html#mdsine2.pylab.base.Traceable.set_trace">set_trace</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Normal.set_value_shape" href="pylab/variables.html#mdsine2.pylab.variables.Variable.set_value_shape">set_value_shape</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="mdsine2.posterior.PriorMeanPerturbations"><code class="flex name class">
<span>class <span class="ident">PriorMeanPerturbations</span></span>
<span>(</span><span>**kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Class iterates over the perturbations for updating the mean of the prior
for each perturbation</p>
<p>All perturbations get the same hyperparameters</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PriorMeanPerturbations(pl.Variable):
    &#39;&#39;&#39;Class iterates over the perturbations for updating the mean of the prior
    for each perturbation

    All perturbations get the same hyperparameters
    &#39;&#39;&#39;
    def __init__(self, **kwargs):
        kwargs[&#39;name&#39;] = STRNAMES.PRIOR_MEAN_PERT
        pl.Variable.__init__(self, **kwargs)
        self.perturbations = self.G.perturbations # mdsine.pylab.base.Perturbations

        if self.perturbations is None:
            raise TypeError(&#39;Only instantiate this object if there are perturbations&#39;)

    def __str__(self) -&gt; str:
        s = &#39;Perturbation Magnitude Prior Means&#39;
        for perturbation in self.perturbations:
            s += &#39;\n\tperturbation {}: {}&#39;.format(
                perturbation.name,
                perturbation.magnitude.prior.loc.value)
        return s

    @property
    def sample_iter(self) -&gt; int:
        return self.perturbations[0].magnitude.prior.loc.sample_iter

    def initialize(self, **kwargs):
        &#39;&#39;&#39;Every prior variance on the perturbations gets the same hyperparameters
        &#39;&#39;&#39;
        for perturbation in self.perturbations:
            perturbation.magnitude.prior.loc.initialize(**kwargs)

    def update(self):
        for perturbation in self.perturbations:
            perturbation.magnitude.prior.loc.update()

    def set_trace(self, *args, **kwargs):
        for perturbation in self.perturbations:
            perturbation.magnitude.prior.loc.set_trace(*args, **kwargs)

    def add_trace(self, *args, **kwargs):
        for perturbation in self.perturbations:
            perturbation.magnitude.prior.loc.add_trace(*args, **kwargs)

    def get_single_value_of_perts(self) -&gt; np.ndarray:
        &#39;&#39;&#39;Get the variance for each perturbation
        &#39;&#39;&#39;
        return np.asarray([p.magnitude.prior.loc.value for p in self.perturbations])

    def toarray(self, only_pos_ind: bool=True) -&gt; np.ndarray:
        &#39;&#39;&#39;Return the diagonal of the prior variances stacked up in order

        Parameters
        ----------
        only_pos_ind : bool
            If True, only put in the values for the positively indicated clusters
            for each perturbation

        Returns
        -------
        np.ndarray
        &#39;&#39;&#39;
        ret = []
        for perturbation in self.perturbations:
            if only_pos_ind:
                n = perturbation.indicator.num_on_clusters()
            else:
                n = len(perturbation.clustering)
            ret = np.append(
                ret, np.zeros(n, dtype=float)+perturbation.magnitude.prior.loc.value)
        return ret

    def visualize(self, path: str, f: IO, pidx: int, section: str=&#39;posterior&#39;) -&gt; IO:
        &#39;&#39;&#39;Visualize the `pidx`th perturbation prior magnitude

        Parameters
        ----------
        obj : mdsine2.Variable
        path : str
            This is the path to write the files to
        f : _io.TextIOWrapper
            File that we are writing the values to
        section : str
            Section of the trace to compute on. Options:
                &#39;posterior&#39; : posterior samples
                &#39;burnin&#39; : burn-in samples
                &#39;entire&#39; : both burn-in and posterior samples

        Returns
        -------
        _io.TextIOWrapper
        &#39;&#39;&#39;
        perturbation = self.perturbations[pidx]
        if not self.G.inference.is_in_inference_order(self.name):
            return f
        return _scalar_visualize(path=path, f=f, section=section,
            obj=perturbation.magnitude.prior.loc,
            log_scale=False)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="mdsine2.pylab.variables.Variable" href="pylab/variables.html#mdsine2.pylab.variables.Variable">Variable</a></li>
<li><a title="mdsine2.pylab.graph.Node" href="pylab/graph.html#mdsine2.pylab.graph.Node">Node</a></li>
<li><a title="mdsine2.pylab.graph.BaseNode" href="pylab/graph.html#mdsine2.pylab.graph.BaseNode">BaseNode</a></li>
<li><a title="mdsine2.pylab.base.Saveable" href="pylab/base.html#mdsine2.pylab.base.Saveable">Saveable</a></li>
<li>mdsine2.pylab.variables._BaseArithmeticClass</li>
<li><a title="mdsine2.pylab.base.Traceable" href="pylab/base.html#mdsine2.pylab.base.Traceable">Traceable</a></li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="mdsine2.posterior.PriorMeanPerturbations.sample_iter"><code class="name">var <span class="ident">sample_iter</span> : int</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def sample_iter(self) -&gt; int:
    return self.perturbations[0].magnitude.prior.loc.sample_iter</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="mdsine2.posterior.PriorMeanPerturbations.get_single_value_of_perts"><code class="name flex">
<span>def <span class="ident">get_single_value_of_perts</span></span>(<span>self) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Get the variance for each perturbation</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_single_value_of_perts(self) -&gt; np.ndarray:
    &#39;&#39;&#39;Get the variance for each perturbation
    &#39;&#39;&#39;
    return np.asarray([p.magnitude.prior.loc.value for p in self.perturbations])</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.PriorMeanPerturbations.initialize"><code class="name flex">
<span>def <span class="ident">initialize</span></span>(<span>self, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Every prior variance on the perturbations gets the same hyperparameters</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def initialize(self, **kwargs):
    &#39;&#39;&#39;Every prior variance on the perturbations gets the same hyperparameters
    &#39;&#39;&#39;
    for perturbation in self.perturbations:
        perturbation.magnitude.prior.loc.initialize(**kwargs)</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.PriorMeanPerturbations.toarray"><code class="name flex">
<span>def <span class="ident">toarray</span></span>(<span>self, only_pos_ind: bool = True) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Return the diagonal of the prior variances stacked up in order</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>only_pos_ind</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, only put in the values for the positively indicated clusters
for each perturbation</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.ndarray</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def toarray(self, only_pos_ind: bool=True) -&gt; np.ndarray:
    &#39;&#39;&#39;Return the diagonal of the prior variances stacked up in order

    Parameters
    ----------
    only_pos_ind : bool
        If True, only put in the values for the positively indicated clusters
        for each perturbation

    Returns
    -------
    np.ndarray
    &#39;&#39;&#39;
    ret = []
    for perturbation in self.perturbations:
        if only_pos_ind:
            n = perturbation.indicator.num_on_clusters()
        else:
            n = len(perturbation.clustering)
        ret = np.append(
            ret, np.zeros(n, dtype=float)+perturbation.magnitude.prior.loc.value)
    return ret</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.PriorMeanPerturbations.update"><code class="name flex">
<span>def <span class="ident">update</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update(self):
    for perturbation in self.perturbations:
        perturbation.magnitude.prior.loc.update()</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.PriorMeanPerturbations.visualize"><code class="name flex">
<span>def <span class="ident">visualize</span></span>(<span>self, path: str, f: <class 'IO'>, pidx: int, section: str = 'posterior') ‑> <class 'IO'></span>
</code></dt>
<dd>
<div class="desc"><p>Visualize the <code>pidx</code>th perturbation prior magnitude</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>obj</code></strong> :&ensp;<code>mdsine2.Variable</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>path</code></strong> :&ensp;<code>str</code></dt>
<dd>This is the path to write the files to</dd>
<dt><strong><code>f</code></strong> :&ensp;<code>_io.TextIOWrapper</code></dt>
<dd>File that we are writing the values to</dd>
<dt><strong><code>section</code></strong> :&ensp;<code>str</code></dt>
<dd>Section of the trace to compute on. Options:
'posterior' : posterior samples
'burnin' : burn-in samples
'entire' : both burn-in and posterior samples</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>_io.TextIOWrapper</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def visualize(self, path: str, f: IO, pidx: int, section: str=&#39;posterior&#39;) -&gt; IO:
    &#39;&#39;&#39;Visualize the `pidx`th perturbation prior magnitude

    Parameters
    ----------
    obj : mdsine2.Variable
    path : str
        This is the path to write the files to
    f : _io.TextIOWrapper
        File that we are writing the values to
    section : str
        Section of the trace to compute on. Options:
            &#39;posterior&#39; : posterior samples
            &#39;burnin&#39; : burn-in samples
            &#39;entire&#39; : both burn-in and posterior samples

    Returns
    -------
    _io.TextIOWrapper
    &#39;&#39;&#39;
    perturbation = self.perturbations[pidx]
    if not self.G.inference.is_in_inference_order(self.name):
        return f
    return _scalar_visualize(path=path, f=f, section=section,
        obj=perturbation.magnitude.prior.loc,
        log_scale=False)</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="mdsine2.pylab.variables.Variable" href="pylab/variables.html#mdsine2.pylab.variables.Variable">Variable</a></b></code>:
<ul class="hlist">
<li><code><a title="mdsine2.pylab.variables.Variable.T" href="pylab/variables.html#mdsine2.pylab.variables.Variable.T">T</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.add_child" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_child">add_child</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.add_parent" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_parent">add_parent</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.add_prior" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_prior">add_prior</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.add_trace" href="pylab/base.html#mdsine2.pylab.base.Traceable.add_trace">add_trace</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.add_undirected" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_undirected">add_undirected</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.degree" href="pylab/graph.html#mdsine2.pylab.graph.Node.degree">degree</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.delete" href="pylab/graph.html#mdsine2.pylab.graph.BaseNode.delete">delete</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.get_adjacent_keys" href="pylab/graph.html#mdsine2.pylab.graph.Node.get_adjacent_keys">get_adjacent_keys</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.get_iter" href="pylab/base.html#mdsine2.pylab.base.Traceable.get_iter">get_iter</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.get_trace_from_disk" href="pylab/base.html#mdsine2.pylab.base.Traceable.get_trace_from_disk">get_trace_from_disk</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.load" href="pylab/base.html#mdsine2.pylab.base.Saveable.load">load</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.metropolis" href="pylab/graph.html#mdsine2.pylab.graph.Node.metropolis">metropolis</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.overwrite_entire_trace_on_disk" href="pylab/base.html#mdsine2.pylab.base.Traceable.overwrite_entire_trace_on_disk">overwrite_entire_trace_on_disk</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.remove_local_trace" href="pylab/variables.html#mdsine2.pylab.variables.Variable.remove_local_trace">remove_local_trace</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.save" href="pylab/base.html#mdsine2.pylab.base.Saveable.save">save</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.set_save_location" href="pylab/base.html#mdsine2.pylab.base.Saveable.set_save_location">set_save_location</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.set_trace" href="pylab/base.html#mdsine2.pylab.base.Traceable.set_trace">set_trace</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.set_value_shape" href="pylab/variables.html#mdsine2.pylab.variables.Variable.set_value_shape">set_value_shape</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="mdsine2.posterior.PriorVarInteractions"><code class="flex name class">
<span>class <span class="ident">PriorVarInteractions</span></span>
<span>(</span><span>prior: <a title="mdsine2.pylab.variables.SICS" href="pylab/variables.html#mdsine2.pylab.variables.SICS">SICS</a>, value: Union[float, int] = None, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>This is the posterior of the prior variance of regression coefficients
for the interaction (off diagonal) variables</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PriorVarInteractions(pl.variables.SICS):
    &#39;&#39;&#39;This is the posterior of the prior variance of regression coefficients
    for the interaction (off diagonal) variables
    &#39;&#39;&#39;
    def __init__(self, prior: variables.SICS, value: Union[float, int]=None, **kwargs):

        kwargs[&#39;name&#39;] = STRNAMES.PRIOR_VAR_INTERACTIONS
        pl.variables.SICS.__init__(self, value=value,
            dtype=float, **kwargs)
        self.add_prior(prior)

    def initialize(self, value_option: str, dof_option: str, scale_option: str, value: float=None,
        mean_scaling_factor: Union[float, int]=None, dof: Union[float, int]=None, 
        scale: Union[float, int]=None, delay: int=0):
        &#39;&#39;&#39;Initialize the hyperparameters of the self interaction variance based on the
        passed in option

        Parameters
        ----------
        value_option : str
            - Initialize the value based on the specified option
            - Options
                - &#39;manual&#39;
                    - Set the value manually, `value` must also be specified
                - &#39;auto&#39;, &#39;prior-mean&#39;
                    - Set the value to the mean of the prior
        scale_option : str
            - Initialize the scale of the prior
            - Options
                - &#39;manual&#39;
                    - Set the value manually, `scale` must also be specified
                - &#39;auto&#39;, &#39;same-as-aii&#39;
                    - Set the mean the same as the self-interactions
        dof_option : str
            Initialize the dof of the parameter
            Options:
                &#39;manual&#39;: Set the value with the parameter `dof`
                &#39;diffuse&#39;: Set the value to 2.01
                &#39;strong&#39;: Set the valuye to the expected number of interactions
                &#39;auto&#39;: Set to diffuse
        dof, scale : int, float
            - User specified values
            - Only necessary if `hyperparam_option` == &#39;manual&#39;
        &#39;&#39;&#39;
        if not pl.isint(delay):
            raise TypeError(&#39;`delay` ({}) must be an int&#39;.format(type(delay)))
        if delay &lt; 0:
            raise ValueError(&#39;`delay` ({}) must be &gt;= 0&#39;.format(delay))
        self.delay = delay

        self.interactions = self.G[STRNAMES.CLUSTER_INTERACTION_VALUE]

        if not pl.isstr(dof_option):
            raise TypeError(&#39;`dof_option` ({}) must be a str&#39;.format(type(dof_option)))
        if dof_option == &#39;manual&#39;:
            if not pl.isnumeric(dof):
                raise TypeError(&#39;`dof` ({}) must be a numeric&#39;.format(type(dof)))
            if dof &lt; 0:
                raise ValueError(&#39;`dof` ({}) must be &gt; 0 for it to be a valid prior&#39;.format(dof))
        elif dof_option in [&#39;diffuse&#39;, &#39;auto&#39;]:
            dof = 2.01
        elif dof_option == &#39;strong&#39;:
            N = expected_n_clusters(G=self.G)
            dof = N * (N - 1)
        else:
            raise ValueError(&#39;`dof_option` ({}) not recognized&#39;.format(dof_option))
        self.prior.dof.override_value(dof)

        if not pl.isstr(scale_option):
            raise TypeError(&#39;`scale_option` ({}) must be a str&#39;.format(type(scale_option)))
        if scale_option == &#39;manual&#39;:
            if not pl.isnumeric(scale):
                raise TypeError(&#39;`scale` ({}) must be a numeric&#39;.format(type(scale)))
            if scale &lt; 0:
                raise ValueError(&#39;`scale` ({}) must be &gt; 0 for it to be a valid prior&#39;.format(scale))
        elif scale_option in [&#39;auto&#39;, &#39;same-as-aii&#39;]:
            mean = self.G[STRNAMES.PRIOR_VAR_SELF_INTERACTIONS].prior.mean()
            scale = mean * (self.prior.dof.value - 2) /(self.prior.dof.value)
        else:
            raise ValueError(&#39;`scale_option` ({}) not recognized&#39;.format(scale_option))
        self.prior.scale.override_value(scale)

        if not pl.isstr(value_option):
            raise TypeError(&#39;`value_option` ({}) must be a str&#39;.format(type(value_option)))
        if value_option == &#39;manual&#39;:
            if not pl.isnumeric(value):
                raise ValueError(&#39;`value` ({}) must be numeric (float,int)&#39;.format(value.__class__))
            self.value = value
        elif value_option in [&#39;auto&#39;, &#39;prior-mean&#39;]:
            if not pl.isnumeric(mean_scaling_factor):
                raise ValueError(&#39;`mean_scaling_factor` ({}) must be a numeric type &#39; \
                    &#39;(float,int)&#39;.format(mean_scaling_factor.__class__))
            self.value = self.prior.mean()
        else:
            raise ValueError(&#39;`value_option` ({}) not recognized&#39;.format(value_option))

        logging.info(&#39;Prior Variance Interactions initialization results:\n&#39; \
            &#39;\tprior dof: {}\n&#39; \
            &#39;\tprior scale: {}\n&#39; \
            &#39;\tvalue: {}&#39;.format(
                self.prior.dof.value, self.prior.scale.value, self.value))

    # @profile
    def update(self):
        &#39;&#39;&#39;Calculate the posterior of the prior variance
        &#39;&#39;&#39;
        if self.sample_iter &lt; self.delay:
            return

        x = self.interactions.obj.get_values(use_indicators=True)
        mu = self.G[STRNAMES.PRIOR_MEAN_INTERACTIONS].value

        se = np.sum(np.square(x - mu))
        n = len(x)

        self.dof.value = self.prior.dof.value + n
        self.scale.value = ((self.prior.scale.value * self.prior.dof.value) + \
           se)/self.dof.value
        self.sample()

    def visualize(self, path: str, f: IO, section: str=&#39;posterior&#39;) -&gt; IO:
        return _scalar_visualize(self, path=path, f=f, section=section)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="mdsine2.pylab.variables.SICS" href="pylab/variables.html#mdsine2.pylab.variables.SICS">SICS</a></li>
<li><a title="mdsine2.pylab.variables.Variable" href="pylab/variables.html#mdsine2.pylab.variables.Variable">Variable</a></li>
<li><a title="mdsine2.pylab.graph.Node" href="pylab/graph.html#mdsine2.pylab.graph.Node">Node</a></li>
<li><a title="mdsine2.pylab.graph.BaseNode" href="pylab/graph.html#mdsine2.pylab.graph.BaseNode">BaseNode</a></li>
<li><a title="mdsine2.pylab.base.Saveable" href="pylab/base.html#mdsine2.pylab.base.Saveable">Saveable</a></li>
<li>mdsine2.pylab.variables._BaseArithmeticClass</li>
<li><a title="mdsine2.pylab.base.Traceable" href="pylab/base.html#mdsine2.pylab.base.Traceable">Traceable</a></li>
<li>mdsine2.pylab.variables._RandomBase</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="mdsine2.posterior.PriorVarInteractions.initialize"><code class="name flex">
<span>def <span class="ident">initialize</span></span>(<span>self, value_option: str, dof_option: str, scale_option: str, value: float = None, mean_scaling_factor: Union[float, int] = None, dof: Union[float, int] = None, scale: Union[float, int] = None, delay: int = 0)</span>
</code></dt>
<dd>
<div class="desc"><p>Initialize the hyperparameters of the self interaction variance based on the
passed in option</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>value_option</code></strong> :&ensp;<code>str</code></dt>
<dd>
<ul>
<li>Initialize the value based on the specified option</li>
<li>Options<ul>
<li>'manual'<ul>
<li>Set the value manually, <code>value</code> must also be specified</li>
</ul>
</li>
<li>'auto', 'prior-mean'<ul>
<li>Set the value to the mean of the prior</li>
</ul>
</li>
</ul>
</li>
</ul>
</dd>
<dt><strong><code>scale_option</code></strong> :&ensp;<code>str</code></dt>
<dd>
<ul>
<li>Initialize the scale of the prior</li>
<li>Options<ul>
<li>'manual'<ul>
<li>Set the value manually, <code>scale</code> must also be specified</li>
</ul>
</li>
<li>'auto', 'same-as-aii'<ul>
<li>Set the mean the same as the self-interactions</li>
</ul>
</li>
</ul>
</li>
</ul>
</dd>
<dt><strong><code>dof_option</code></strong> :&ensp;<code>str</code></dt>
<dd>Initialize the dof of the parameter
Options:
'manual': Set the value with the parameter <code>dof</code>
'diffuse': Set the value to 2.01
'strong': Set the valuye to the expected number of interactions
'auto': Set to diffuse</dd>
<dt><strong><code>dof</code></strong>, <strong><code>scale</code></strong> :&ensp;<code>int, float</code></dt>
<dd>
<ul>
<li>User specified values</li>
<li>Only necessary if <code>hyperparam_option</code> == 'manual'</li>
</ul>
</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def initialize(self, value_option: str, dof_option: str, scale_option: str, value: float=None,
    mean_scaling_factor: Union[float, int]=None, dof: Union[float, int]=None, 
    scale: Union[float, int]=None, delay: int=0):
    &#39;&#39;&#39;Initialize the hyperparameters of the self interaction variance based on the
    passed in option

    Parameters
    ----------
    value_option : str
        - Initialize the value based on the specified option
        - Options
            - &#39;manual&#39;
                - Set the value manually, `value` must also be specified
            - &#39;auto&#39;, &#39;prior-mean&#39;
                - Set the value to the mean of the prior
    scale_option : str
        - Initialize the scale of the prior
        - Options
            - &#39;manual&#39;
                - Set the value manually, `scale` must also be specified
            - &#39;auto&#39;, &#39;same-as-aii&#39;
                - Set the mean the same as the self-interactions
    dof_option : str
        Initialize the dof of the parameter
        Options:
            &#39;manual&#39;: Set the value with the parameter `dof`
            &#39;diffuse&#39;: Set the value to 2.01
            &#39;strong&#39;: Set the valuye to the expected number of interactions
            &#39;auto&#39;: Set to diffuse
    dof, scale : int, float
        - User specified values
        - Only necessary if `hyperparam_option` == &#39;manual&#39;
    &#39;&#39;&#39;
    if not pl.isint(delay):
        raise TypeError(&#39;`delay` ({}) must be an int&#39;.format(type(delay)))
    if delay &lt; 0:
        raise ValueError(&#39;`delay` ({}) must be &gt;= 0&#39;.format(delay))
    self.delay = delay

    self.interactions = self.G[STRNAMES.CLUSTER_INTERACTION_VALUE]

    if not pl.isstr(dof_option):
        raise TypeError(&#39;`dof_option` ({}) must be a str&#39;.format(type(dof_option)))
    if dof_option == &#39;manual&#39;:
        if not pl.isnumeric(dof):
            raise TypeError(&#39;`dof` ({}) must be a numeric&#39;.format(type(dof)))
        if dof &lt; 0:
            raise ValueError(&#39;`dof` ({}) must be &gt; 0 for it to be a valid prior&#39;.format(dof))
    elif dof_option in [&#39;diffuse&#39;, &#39;auto&#39;]:
        dof = 2.01
    elif dof_option == &#39;strong&#39;:
        N = expected_n_clusters(G=self.G)
        dof = N * (N - 1)
    else:
        raise ValueError(&#39;`dof_option` ({}) not recognized&#39;.format(dof_option))
    self.prior.dof.override_value(dof)

    if not pl.isstr(scale_option):
        raise TypeError(&#39;`scale_option` ({}) must be a str&#39;.format(type(scale_option)))
    if scale_option == &#39;manual&#39;:
        if not pl.isnumeric(scale):
            raise TypeError(&#39;`scale` ({}) must be a numeric&#39;.format(type(scale)))
        if scale &lt; 0:
            raise ValueError(&#39;`scale` ({}) must be &gt; 0 for it to be a valid prior&#39;.format(scale))
    elif scale_option in [&#39;auto&#39;, &#39;same-as-aii&#39;]:
        mean = self.G[STRNAMES.PRIOR_VAR_SELF_INTERACTIONS].prior.mean()
        scale = mean * (self.prior.dof.value - 2) /(self.prior.dof.value)
    else:
        raise ValueError(&#39;`scale_option` ({}) not recognized&#39;.format(scale_option))
    self.prior.scale.override_value(scale)

    if not pl.isstr(value_option):
        raise TypeError(&#39;`value_option` ({}) must be a str&#39;.format(type(value_option)))
    if value_option == &#39;manual&#39;:
        if not pl.isnumeric(value):
            raise ValueError(&#39;`value` ({}) must be numeric (float,int)&#39;.format(value.__class__))
        self.value = value
    elif value_option in [&#39;auto&#39;, &#39;prior-mean&#39;]:
        if not pl.isnumeric(mean_scaling_factor):
            raise ValueError(&#39;`mean_scaling_factor` ({}) must be a numeric type &#39; \
                &#39;(float,int)&#39;.format(mean_scaling_factor.__class__))
        self.value = self.prior.mean()
    else:
        raise ValueError(&#39;`value_option` ({}) not recognized&#39;.format(value_option))

    logging.info(&#39;Prior Variance Interactions initialization results:\n&#39; \
        &#39;\tprior dof: {}\n&#39; \
        &#39;\tprior scale: {}\n&#39; \
        &#39;\tvalue: {}&#39;.format(
            self.prior.dof.value, self.prior.scale.value, self.value))</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.PriorVarInteractions.update"><code class="name flex">
<span>def <span class="ident">update</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate the posterior of the prior variance</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update(self):
    &#39;&#39;&#39;Calculate the posterior of the prior variance
    &#39;&#39;&#39;
    if self.sample_iter &lt; self.delay:
        return

    x = self.interactions.obj.get_values(use_indicators=True)
    mu = self.G[STRNAMES.PRIOR_MEAN_INTERACTIONS].value

    se = np.sum(np.square(x - mu))
    n = len(x)

    self.dof.value = self.prior.dof.value + n
    self.scale.value = ((self.prior.scale.value * self.prior.dof.value) + \
       se)/self.dof.value
    self.sample()</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.PriorVarInteractions.visualize"><code class="name flex">
<span>def <span class="ident">visualize</span></span>(<span>self, path: str, f: <class 'IO'>, section: str = 'posterior') ‑> <class 'IO'></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def visualize(self, path: str, f: IO, section: str=&#39;posterior&#39;) -&gt; IO:
    return _scalar_visualize(self, path=path, f=f, section=section)</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="mdsine2.pylab.variables.SICS" href="pylab/variables.html#mdsine2.pylab.variables.SICS">SICS</a></b></code>:
<ul class="hlist">
<li><code><a title="mdsine2.pylab.variables.SICS.T" href="pylab/variables.html#mdsine2.pylab.variables.Variable.T">T</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.add_child" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_child">add_child</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.add_parent" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_parent">add_parent</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.add_prior" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_prior">add_prior</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.add_trace" href="pylab/base.html#mdsine2.pylab.base.Traceable.add_trace">add_trace</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.add_undirected" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_undirected">add_undirected</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.degree" href="pylab/graph.html#mdsine2.pylab.graph.Node.degree">degree</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.delete" href="pylab/graph.html#mdsine2.pylab.graph.BaseNode.delete">delete</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.get_adjacent_keys" href="pylab/graph.html#mdsine2.pylab.graph.Node.get_adjacent_keys">get_adjacent_keys</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.get_iter" href="pylab/base.html#mdsine2.pylab.base.Traceable.get_iter">get_iter</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.get_trace_from_disk" href="pylab/base.html#mdsine2.pylab.base.Traceable.get_trace_from_disk">get_trace_from_disk</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.load" href="pylab/base.html#mdsine2.pylab.base.Saveable.load">load</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.logpdf" href="pylab/variables.html#mdsine2.pylab.variables.SICS.logpdf">logpdf</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.metropolis" href="pylab/graph.html#mdsine2.pylab.graph.Node.metropolis">metropolis</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.overwrite_entire_trace_on_disk" href="pylab/base.html#mdsine2.pylab.base.Traceable.overwrite_entire_trace_on_disk">overwrite_entire_trace_on_disk</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.pdf" href="pylab/variables.html#mdsine2.pylab.variables.SICS.pdf">pdf</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.remove_local_trace" href="pylab/variables.html#mdsine2.pylab.variables.Variable.remove_local_trace">remove_local_trace</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.sample" href="pylab/variables.html#mdsine2.pylab.variables.SICS.sample">sample</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.save" href="pylab/base.html#mdsine2.pylab.base.Saveable.save">save</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.set_save_location" href="pylab/base.html#mdsine2.pylab.base.Saveable.set_save_location">set_save_location</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.set_trace" href="pylab/base.html#mdsine2.pylab.base.Traceable.set_trace">set_trace</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.set_value_shape" href="pylab/variables.html#mdsine2.pylab.variables.Variable.set_value_shape">set_value_shape</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="mdsine2.posterior.PriorVarMH"><code class="flex name class">
<span>class <span class="ident">PriorVarMH</span></span>
<span>(</span><span>prior: <a title="mdsine2.pylab.variables.SICS" href="pylab/variables.html#mdsine2.pylab.variables.SICS">SICS</a>, child_name: str, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>This is the posterior for the prior variance of either the growth
or self-interaction parameter. We update with a MH update since this
prior is not conjugate.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>prior</code></strong> :&ensp;<code>pl.variables.SICS</code></dt>
<dd>This is the prior of this distribution - which is a Squared
Inverse Chi Squared (SICS) distribution</dd>
<dt><strong><code>child_name</code></strong> :&ensp;<code>str</code></dt>
<dd>This is the name of the variable that this is a prior variance
for. This is either the name of the growth parameter or the
self-interactions parameter</dd>
<dt><strong><code>kwargs</code></strong> :&ensp;<code>dict</code></dt>
<dd>These are the other parameters for the initialization.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PriorVarMH(pl.variables.SICS):
    &#39;&#39;&#39;This is the posterior for the prior variance of either the growth
    or self-interaction parameter. We update with a MH update since this 
    prior is not conjugate.

    Parameters
    ----------
    prior : pl.variables.SICS
        This is the prior of this distribution - which is a Squared
        Inverse Chi Squared (SICS) distribution
    child_name : str
        This is the name of the variable that this is a prior variance
        for. This is either the name of the growth parameter or the 
        self-interactions parameter
    kwargs : dict
        These are the other parameters for the initialization.
    &#39;&#39;&#39;

    def __init__(self, prior: variables.SICS, child_name: str, **kwargs):
        if child_name == STRNAMES.GROWTH_VALUE:
            kwargs[&#39;name&#39;] = STRNAMES.PRIOR_VAR_GROWTH
        elif child_name == STRNAMES.SELF_INTERACTION_VALUE:
            kwargs[&#39;name&#39;] = STRNAMES.PRIOR_VAR_SELF_INTERACTIONS
        else:
            raise ValueError(&#39;`child_name` ({}) not recognized&#39;.format(child_name))
        pl.variables.SICS.__init__(self, dtype=float, **kwargs)
        self.child_name = child_name
        self.add_prior(prior)
        self.proposal = pl.variables.SICS(dof=None, scale=None, value=None)

    def __str__(self) -&gt; str:
        # If this fails, it is because we are dividing by 0 sampler_iter
        # If which case we just return the value 
        try:
            s = &#39;Value: {}, Acceptance rate: {}&#39;.format(
                self.value, np.mean(self.acceptances[
                    np.max([self.sample_iter-50, 0]):self.sample_iter]))
        except:
            s = str(self.value)
        return s

    def initialize(self, value_option: str, dof_option: str, scale_option: str, 
        proposal_option: str, target_acceptance_rate: Union[float,str], tune: Union[float, str], 
        end_tune: Union[str, int], value: float=None, dof: float=None, scale: float=None, 
        proposal_dof: float=None, delay: int=0):
        &#39;&#39;&#39;Initialize the parameters of the distribution and the 
        proposal distribution

        Parameters
        ----------
        value_option : str
            Different ways to initialize the values
            Options
                &#39;manual&#39;
                    Set the value manually, `value` must also be specified
                &#39;unregularized&#39;
                    Do unregularized regression and the value is set to the
                    variance of the growth values
                &#39;prior-mean&#39;, &#39;auto&#39;
                    Set the value to the prior of the mean
        scale_option : str
            Different ways to initialize the scale of the prior
            Options
                &#39;manual&#39;
                    Set the value manually, `scale` must also be specified
                &#39;auto&#39;, &#39;inflated-median&#39;
                    We set the scale such that the mean of the prior is
                    equal to the median growth values calculated
                    with linear regression squared and inflated by 100.
        dof_option : str
            How informative the prior should be (setting the dof)
                &#39;diffuse&#39;: set to the mimumum value (2)
                &#39;weak&#39;: set so that 10% of the posterior comes from the prior
                &#39;strong&#39;: set so that 50% of the posterior comes from the prior
                &#39;manual&#39;: set to the value provided in the parameter `shape`
                &#39;auto&#39;: Set to &#39;weak&#39;
        proposal_option : str
            How to set the initial dof of the proposal - this will get adjusted with
            tuning
                &#39;tight&#39;, &#39;auto&#39;
                    Set the dof to be 15, relatively strong initially
                &#39;diffuse&#39;
                    Set the dof to be 2.5, relatively diffuse initially
                &#39;manual&#39;
                    Set the dof with the parameter `proposal_dof&#39;
        target_acceptance_rate : float, str
            This is the target_acceptance rate. Options:
                &#39;auto&#39;, &#39;optimal&#39;
                    Set to 0.44
                float
                    This is the value you want
        tune : str, int
            This is how often you want to update the proposal dof
                int
                &#39;auto&#39;
                    Set to every 50 iterations
        end_tune : str, int
            This is when to stop the tuning
                &#39;half-burnin&#39;, &#39;auto&#39;
                    Half of burnin, rounded down
                int
        delay : int
            How many iterations to delay updating the value of the variance
        &#39;&#39;&#39;
        if not pl.isint(delay):
            raise TypeError(&#39;`delay` ({}) must be an int&#39;.format(type(delay)))
        if delay &lt; 0:
            raise ValueError(&#39;`delay` ({}) must be &gt;= 0&#39;.format(delay))
        self.delay = delay

        # Set the proposal dof
        if not pl.isstr(proposal_option):
            raise TypeError(&#39;`proposal_option` ({}) must be a str&#39;.format(
                type(proposal_option)))
        elif proposal_option == &#39;manual&#39;:
            if not pl.isnumeric(proposal_dof):
                raise TypeError(&#39;`proposal_dof` ({}) must be a numeric&#39;.format(
                    type(proposal_dof)))
            if proposal_dof &lt; 2:
                raise ValueError(&#39;`proposal_dof` ({}) not proper&#39;.format(proposal_dof))
        elif proposal_option in [&#39;tight&#39;, &#39;auto&#39;]:
            proposal_dof = 15
        elif proposal_option == &#39;diffuse&#39;:
            proposal_dof = 2.5
        else:
            raise ValueError(&#39;`proposal_option` ({}) not recognized&#39;.format(
                proposal_option))
        self.proposal.dof.value = proposal_dof

        # Set the propsal parameters
        if pl.isstr(target_acceptance_rate):
            if target_acceptance_rate in [&#39;optimal&#39;, &#39;auto&#39;]:
                target_acceptance_rate = 0.44
            else:
                raise ValueError(&#39;`target_acceptance_rate` ({}) not recognized&#39;.format(
                    target_acceptance_rate))
        elif pl.isfloat(target_acceptance_rate):
            if target_acceptance_rate &lt; 0 or target_acceptance_rate &gt; 1:
                raise ValueError(&#39;`target_acceptance_rate` ({}) out of range&#39;.format(
                    target_acceptance_rate))
        else:
            raise TypeError(&#39;`target_acceptance_rate` ({}) type not recognized&#39;.format(
                type(target_acceptance_rate)))
        self.target_acceptance_rate = target_acceptance_rate

        if pl.isstr(tune):
            if tune in [&#39;auto&#39;]:
                tune = 50
            else:
                raise ValueError(&#39;`tune` ({}) not recognized&#39;.format(tune))
        elif pl.isint(tune):
            if tune &lt; 0:
                raise ValueError(&#39;`tune` ({}) must be &gt; 0&#39;.format(
                    tune))
        else:
            raise TypeError(&#39;`tune` ({}) type not recognized&#39;.format(type(tune)))
        self.tune = tune

        if pl.isstr(end_tune):
            if end_tune in [&#39;auto&#39;, &#39;half-burnin&#39;]:
                end_tune = int(self.G.inference.burnin/2)
            else:
                raise ValueError(&#39;`tune` ({}) not recognized&#39;.format(end_tune))
        elif pl.isint(end_tune):
            if end_tune &lt; 0 or end_tune &gt; self.G.inference.burnin:
                raise ValueError(&#39;`end_tune` ({}) out of range (0, {})&#39;.format(
                    end_tune, self.G.inference.burnin))
        else:
            raise TypeError(&#39;`end_tune` ({}) type not recognized&#39;.format(type(end_tune)))
        self.end_tune = end_tune

        # Set the prior dof
        if not pl.isstr(dof_option):
            raise TypeError(&#39;`dof_option` ({}) must be a str&#39;.format(type(dof_option)))
        if dof_option == &#39;manual&#39;:
            if not pl.isnumeric(dof):
                raise TypeError(&#39;`dof` ({}) must be a numeric&#39;.format(type(dof)))
            if dof &lt; 2:
                raise ValueError(&#39;`dof` ({}) must be &gt;= 2&#39;.format(dof))
        elif dof_option == &#39;diffuse&#39;:
            dof = 2.5
        elif dof_option in [&#39;weak&#39;, &#39;auto&#39;]:
            dof = len(self.G.data.taxa)/9
        elif dof_option == &#39;strong&#39;:
            dof = len(self.G.data.taxa)/2
        else:
            raise ValueError(&#39;`dof_option` ({}) not recognized&#39;.format(dof_option))
        if dof &lt; 2:
            raise ValueError(&#39;`dof` ({}) must be strictly larger than 2 to be a proper&#39; \
                &#39; prior&#39;.format(dof))
        self.prior.dof.override_value(dof)

        # Set the prior scale
        if not pl.isstr(scale_option):
            raise TypeError(&#39;`scale_option` ({}) must be a str&#39;.format(type(scale_option)))
        if scale_option == &#39;manual&#39;:
            if not pl.isnumeric(scale):
                raise TypeError(&#39;`scale` ({}) must be a numeric&#39;.format(type(scale)))
            if scale &lt;= 0:
                raise ValueError(&#39;`scale` ({}) must be positive&#39;.format(scale))
        elif scale_option in [&#39;auto&#39;, &#39;inflated-median&#39;]:
            # Perform linear regression
            rhs = [STRNAMES.GROWTH_VALUE, STRNAMES.SELF_INTERACTION_VALUE]
            X = self.G.data.construct_rhs(keys=rhs,
                kwargs_dict={STRNAMES.GROWTH_VALUE:{&#39;with_perturbations&#39;:False}},
                index_out_perturbations=True)
            y = self.G.data.construct_lhs(index_out_perturbations=True)

            prec = X.T @ X
            cov = pinv(prec, self)
            mean = cov @ X.T @ y
            if self.child_name == STRNAMES.GROWTH_VALUE:
                mean = 1e4*(np.median(mean[:self.G.data.n_taxa]) ** 2)
            else:
                mean = 1e4*(np.median(mean[self.G.data.n_taxa:]) ** 2)

            # Calculate the scale
            scale = mean * (self.prior.dof.value - 2) / self.prior.dof.value
        else:
            raise ValueError(&#39;`scale_option` ({}) not recognized&#39;.format(scale_option))
        self.prior.scale.override_value(scale)

        # Set the initial value of the prior
        if not pl.isstr(value_option):
            raise TypeError(&#39;`value_option` ({}) must be a str&#39;.format(type(value_option)))
        if value_option == &#39;manual&#39;:
            if not pl.isnumeric(value):
                raise ValueError(&#39;If `value_option` == &#34;manual&#34;, value ({}) &#39; \
                    &#39;must be a numeric (float, int)&#39;.format(value.__class__))
        elif value_option in [&#39;inflated-median&#39;]:
            # No interactions
            rhs = [
                STRNAMES.GROWTH_VALUE,
                STRNAMES.SELF_INTERACTION_VALUE]
            X = self.G.data.construct_rhs(keys=rhs,
                kwargs_dict={STRNAMES.GROWTH_VALUE:{&#39;with_perturbations&#39;:False}},
                index_out_perturbations=True)
            y = self.G.data.construct_lhs(index_out_perturbations=True)

            prec = X.T @ X
            cov = pinv(prec, self)
            mean = cov @ X.T @ y
            if self.child_name == STRNAMES.GROWTH_VALUE:
                value = 1e4*(np.median(mean[:self.G.data.n_taxa]) ** 2)
            else:
                value = 1e4*(np.median(mean[self.G.data.n_taxa:]) ** 2)
        elif value_option in [&#39;prior-mean&#39;, &#39;auto&#39;]:
            value = self.prior.mean()
        else:
            raise ValueError(&#39;`value_option` &#34;{}&#34; not recognized&#39;.format(value_option))
        self.value = value

    def update_dof(self):
        &#39;&#39;&#39;Updat the `dof` parameter so that we adjust the acceptance
        rate to `target_acceptance_rate`
        &#39;&#39;&#39;
        if self.sample_iter == 0:
            self.temp_acceptances = 0
            self.acceptances = np.zeros(self.G.inference.n_samples, dtype=bool)
        
        elif self.sample_iter &gt; self.end_tune:
            # Don&#39;t do any more updates
            return
        
        elif self.sample_iter % self.tune == 0:
            # Update dof
            acceptance_rate = self.temp_acceptances / self.tune
            if acceptance_rate &gt; self.target_acceptance_rate:
                self.proposal.dof.value = self.proposal.dof.value * 1.5
            else:
                self.proposal.dof.value = self.proposal.dof.value / 1.5
            self.temp_acceptances = 0

    def update(self):
        &#39;&#39;&#39;First we check if we need to tune the dof, which we do during
        the first half of burnin. We calculate the likelihoods in logspace
        &#39;&#39;&#39;
        if self.sample_iter &lt; self.delay:
            return
        self.update_dof()

        # Get necessary data of the respective parameter
        var = self.G[self.child_name]
        x = var.value.ravel()
        mu = var.prior.loc.value
        low = var.low
        high = var.high

        # propose a new value
        prev_value = self.value
        prev_value_std = math.sqrt(prev_value)
        self.proposal.scale.value = self.value
        new_value = self.proposal.sample() # Sample a new value
        new_value_std = math.sqrt(new_value)

        # Calculate the target distribution ll
        prev_target_ll = 0
        for i in range(len(x)):
            prev_target_ll += pl.random.truncnormal.logpdf(
                value=x[i], loc=mu, scale=prev_value_std,
                low=low, high=high)
        new_target_ll = 0
        for i in range(len(x)):
            new_target_ll += pl.random.truncnormal.logpdf(
                value=x[i], loc=mu, scale=new_value_std,
                low=low, high=high)

        # Normalize by the ll of the proposal
        prev_prop_ll = self.proposal.logpdf(value=prev_value)
        new_prop_ll = self.proposal.logpdf(value=new_value)

        # Accept or reject
        r = (new_target_ll - prev_prop_ll) - \
            (prev_target_ll - new_prop_ll)
        u = np.log(pl.random.misc.fast_sample_standard_uniform())

        if r &gt;= u:
            self.acceptances[self.sample_iter] = True
            self.value = new_value
            self.temp_acceptances += 1
        else:
            self.value = prev_value

    def visualize(self, path: str, section: str=&#39;posterior&#39;) -&gt; pd.DataFrame:
        &#39;&#39;&#39;Render the traces in the folder `basepath` and returns a
        `pandas.DataFrame` with summary statistics

        Parameters
        ----------
        path : str
            This is the path to write the files to
        f : _io.TextIOWrapper
            File that we are writing the values to
        section : str
            Section of the trace to compute on. Options:
                &#39;posterior&#39; : posterior samples
                &#39;burnin&#39; : burn-in samples
                &#39;entire&#39; : both burn-in and posterior samples

        Returns
        -------
        pandas.DataFrame
        &#39;&#39;&#39;
        if not self.G.inference.tracer.is_being_traced(self):
            logging.info(&#39;`{}` not learned\n\tValue: {}\n&#39;.format(self.name, self.value))
            return pd.DataFrame()
        summ = pl.summary(self, section=section)
        data = [[self.name] + [v for _,v in summ.items()]]
        columns = [&#39;name&#39;] + [k for k in summ]
        index = [&#39;variance&#39;]
        df = pd.DataFrame(data, columns=columns, index=index)

        # Plot the traces
        ax1, ax2 = visualization.render_trace(var=self, plt_type=&#39;both&#39;, section=section,
            include_burnin=True, log_scale=True, rasterized=True)

        # Plot the prior over the posterior
        l,h = ax1.get_xlim()
        xs = np.arange(l,h,step=(h-l)/100) 
        ys = []
        for x in xs:
            ys.append(pl.random.sics.pdf(value=x, 
                dof=self.prior.dof.value,
                scale=self.prior.scale.value))
        ax1.plot(xs, ys, label=&#39;prior&#39;, alpha=0.5, color=&#39;red&#39;, rasterized=True)
        ax1.legend()

        # Plot the acceptance rate over the trace
        ax3 = ax2.twinx()
        ax3 = visualization.render_acceptance_rate_trace(var=self, ax=ax3, 
            label=&#39;Acceptance Rate&#39;, color=&#39;red&#39;, scatter=False, rasterized=True)
        ax3.legend()
        fig = plt.gcf()
        fig.tight_layout()
        fig.suptitle(self.name)
        plt.savefig(path)
        plt.close()

        return df</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="mdsine2.pylab.variables.SICS" href="pylab/variables.html#mdsine2.pylab.variables.SICS">SICS</a></li>
<li><a title="mdsine2.pylab.variables.Variable" href="pylab/variables.html#mdsine2.pylab.variables.Variable">Variable</a></li>
<li><a title="mdsine2.pylab.graph.Node" href="pylab/graph.html#mdsine2.pylab.graph.Node">Node</a></li>
<li><a title="mdsine2.pylab.graph.BaseNode" href="pylab/graph.html#mdsine2.pylab.graph.BaseNode">BaseNode</a></li>
<li><a title="mdsine2.pylab.base.Saveable" href="pylab/base.html#mdsine2.pylab.base.Saveable">Saveable</a></li>
<li>mdsine2.pylab.variables._BaseArithmeticClass</li>
<li><a title="mdsine2.pylab.base.Traceable" href="pylab/base.html#mdsine2.pylab.base.Traceable">Traceable</a></li>
<li>mdsine2.pylab.variables._RandomBase</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="mdsine2.posterior.PriorVarMH.initialize"><code class="name flex">
<span>def <span class="ident">initialize</span></span>(<span>self, value_option: str, dof_option: str, scale_option: str, proposal_option: str, target_acceptance_rate: Union[float, str], tune: Union[float, str], end_tune: Union[str, int], value: float = None, dof: float = None, scale: float = None, proposal_dof: float = None, delay: int = 0)</span>
</code></dt>
<dd>
<div class="desc"><p>Initialize the parameters of the distribution and the
proposal distribution</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>value_option</code></strong> :&ensp;<code>str</code></dt>
<dd>Different ways to initialize the values
Options
'manual'
Set the value manually, <code>value</code> must also be specified
'unregularized'
Do unregularized regression and the value is set to the
variance of the growth values
'prior-mean', 'auto'
Set the value to the prior of the mean</dd>
<dt><strong><code>scale_option</code></strong> :&ensp;<code>str</code></dt>
<dd>Different ways to initialize the scale of the prior
Options
'manual'
Set the value manually, <code>scale</code> must also be specified
'auto', 'inflated-median'
We set the scale such that the mean of the prior is
equal to the median growth values calculated
with linear regression squared and inflated by 100.</dd>
<dt><strong><code>dof_option</code></strong> :&ensp;<code>str</code></dt>
<dd>How informative the prior should be (setting the dof)
'diffuse': set to the mimumum value (2)
'weak': set so that 10% of the posterior comes from the prior
'strong': set so that 50% of the posterior comes from the prior
'manual': set to the value provided in the parameter <code>shape</code>
'auto': Set to 'weak'</dd>
<dt><strong><code>proposal_option</code></strong> :&ensp;<code>str</code></dt>
<dd>How to set the initial dof of the proposal - this will get adjusted with
tuning
'tight', 'auto'
Set the dof to be 15, relatively strong initially
'diffuse'
Set the dof to be 2.5, relatively diffuse initially
'manual'
Set the dof with the parameter `proposal_dof'</dd>
<dt><strong><code>target_acceptance_rate</code></strong> :&ensp;<code>float, str</code></dt>
<dd>This is the target_acceptance rate. Options:
'auto', 'optimal'
Set to 0.44
float
This is the value you want</dd>
<dt><strong><code>tune</code></strong> :&ensp;<code>str, int</code></dt>
<dd>This is how often you want to update the proposal dof
int
'auto'
Set to every 50 iterations</dd>
<dt><strong><code>end_tune</code></strong> :&ensp;<code>str, int</code></dt>
<dd>This is when to stop the tuning
'half-burnin', 'auto'
Half of burnin, rounded down
int</dd>
<dt><strong><code>delay</code></strong> :&ensp;<code>int</code></dt>
<dd>How many iterations to delay updating the value of the variance</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def initialize(self, value_option: str, dof_option: str, scale_option: str, 
    proposal_option: str, target_acceptance_rate: Union[float,str], tune: Union[float, str], 
    end_tune: Union[str, int], value: float=None, dof: float=None, scale: float=None, 
    proposal_dof: float=None, delay: int=0):
    &#39;&#39;&#39;Initialize the parameters of the distribution and the 
    proposal distribution

    Parameters
    ----------
    value_option : str
        Different ways to initialize the values
        Options
            &#39;manual&#39;
                Set the value manually, `value` must also be specified
            &#39;unregularized&#39;
                Do unregularized regression and the value is set to the
                variance of the growth values
            &#39;prior-mean&#39;, &#39;auto&#39;
                Set the value to the prior of the mean
    scale_option : str
        Different ways to initialize the scale of the prior
        Options
            &#39;manual&#39;
                Set the value manually, `scale` must also be specified
            &#39;auto&#39;, &#39;inflated-median&#39;
                We set the scale such that the mean of the prior is
                equal to the median growth values calculated
                with linear regression squared and inflated by 100.
    dof_option : str
        How informative the prior should be (setting the dof)
            &#39;diffuse&#39;: set to the mimumum value (2)
            &#39;weak&#39;: set so that 10% of the posterior comes from the prior
            &#39;strong&#39;: set so that 50% of the posterior comes from the prior
            &#39;manual&#39;: set to the value provided in the parameter `shape`
            &#39;auto&#39;: Set to &#39;weak&#39;
    proposal_option : str
        How to set the initial dof of the proposal - this will get adjusted with
        tuning
            &#39;tight&#39;, &#39;auto&#39;
                Set the dof to be 15, relatively strong initially
            &#39;diffuse&#39;
                Set the dof to be 2.5, relatively diffuse initially
            &#39;manual&#39;
                Set the dof with the parameter `proposal_dof&#39;
    target_acceptance_rate : float, str
        This is the target_acceptance rate. Options:
            &#39;auto&#39;, &#39;optimal&#39;
                Set to 0.44
            float
                This is the value you want
    tune : str, int
        This is how often you want to update the proposal dof
            int
            &#39;auto&#39;
                Set to every 50 iterations
    end_tune : str, int
        This is when to stop the tuning
            &#39;half-burnin&#39;, &#39;auto&#39;
                Half of burnin, rounded down
            int
    delay : int
        How many iterations to delay updating the value of the variance
    &#39;&#39;&#39;
    if not pl.isint(delay):
        raise TypeError(&#39;`delay` ({}) must be an int&#39;.format(type(delay)))
    if delay &lt; 0:
        raise ValueError(&#39;`delay` ({}) must be &gt;= 0&#39;.format(delay))
    self.delay = delay

    # Set the proposal dof
    if not pl.isstr(proposal_option):
        raise TypeError(&#39;`proposal_option` ({}) must be a str&#39;.format(
            type(proposal_option)))
    elif proposal_option == &#39;manual&#39;:
        if not pl.isnumeric(proposal_dof):
            raise TypeError(&#39;`proposal_dof` ({}) must be a numeric&#39;.format(
                type(proposal_dof)))
        if proposal_dof &lt; 2:
            raise ValueError(&#39;`proposal_dof` ({}) not proper&#39;.format(proposal_dof))
    elif proposal_option in [&#39;tight&#39;, &#39;auto&#39;]:
        proposal_dof = 15
    elif proposal_option == &#39;diffuse&#39;:
        proposal_dof = 2.5
    else:
        raise ValueError(&#39;`proposal_option` ({}) not recognized&#39;.format(
            proposal_option))
    self.proposal.dof.value = proposal_dof

    # Set the propsal parameters
    if pl.isstr(target_acceptance_rate):
        if target_acceptance_rate in [&#39;optimal&#39;, &#39;auto&#39;]:
            target_acceptance_rate = 0.44
        else:
            raise ValueError(&#39;`target_acceptance_rate` ({}) not recognized&#39;.format(
                target_acceptance_rate))
    elif pl.isfloat(target_acceptance_rate):
        if target_acceptance_rate &lt; 0 or target_acceptance_rate &gt; 1:
            raise ValueError(&#39;`target_acceptance_rate` ({}) out of range&#39;.format(
                target_acceptance_rate))
    else:
        raise TypeError(&#39;`target_acceptance_rate` ({}) type not recognized&#39;.format(
            type(target_acceptance_rate)))
    self.target_acceptance_rate = target_acceptance_rate

    if pl.isstr(tune):
        if tune in [&#39;auto&#39;]:
            tune = 50
        else:
            raise ValueError(&#39;`tune` ({}) not recognized&#39;.format(tune))
    elif pl.isint(tune):
        if tune &lt; 0:
            raise ValueError(&#39;`tune` ({}) must be &gt; 0&#39;.format(
                tune))
    else:
        raise TypeError(&#39;`tune` ({}) type not recognized&#39;.format(type(tune)))
    self.tune = tune

    if pl.isstr(end_tune):
        if end_tune in [&#39;auto&#39;, &#39;half-burnin&#39;]:
            end_tune = int(self.G.inference.burnin/2)
        else:
            raise ValueError(&#39;`tune` ({}) not recognized&#39;.format(end_tune))
    elif pl.isint(end_tune):
        if end_tune &lt; 0 or end_tune &gt; self.G.inference.burnin:
            raise ValueError(&#39;`end_tune` ({}) out of range (0, {})&#39;.format(
                end_tune, self.G.inference.burnin))
    else:
        raise TypeError(&#39;`end_tune` ({}) type not recognized&#39;.format(type(end_tune)))
    self.end_tune = end_tune

    # Set the prior dof
    if not pl.isstr(dof_option):
        raise TypeError(&#39;`dof_option` ({}) must be a str&#39;.format(type(dof_option)))
    if dof_option == &#39;manual&#39;:
        if not pl.isnumeric(dof):
            raise TypeError(&#39;`dof` ({}) must be a numeric&#39;.format(type(dof)))
        if dof &lt; 2:
            raise ValueError(&#39;`dof` ({}) must be &gt;= 2&#39;.format(dof))
    elif dof_option == &#39;diffuse&#39;:
        dof = 2.5
    elif dof_option in [&#39;weak&#39;, &#39;auto&#39;]:
        dof = len(self.G.data.taxa)/9
    elif dof_option == &#39;strong&#39;:
        dof = len(self.G.data.taxa)/2
    else:
        raise ValueError(&#39;`dof_option` ({}) not recognized&#39;.format(dof_option))
    if dof &lt; 2:
        raise ValueError(&#39;`dof` ({}) must be strictly larger than 2 to be a proper&#39; \
            &#39; prior&#39;.format(dof))
    self.prior.dof.override_value(dof)

    # Set the prior scale
    if not pl.isstr(scale_option):
        raise TypeError(&#39;`scale_option` ({}) must be a str&#39;.format(type(scale_option)))
    if scale_option == &#39;manual&#39;:
        if not pl.isnumeric(scale):
            raise TypeError(&#39;`scale` ({}) must be a numeric&#39;.format(type(scale)))
        if scale &lt;= 0:
            raise ValueError(&#39;`scale` ({}) must be positive&#39;.format(scale))
    elif scale_option in [&#39;auto&#39;, &#39;inflated-median&#39;]:
        # Perform linear regression
        rhs = [STRNAMES.GROWTH_VALUE, STRNAMES.SELF_INTERACTION_VALUE]
        X = self.G.data.construct_rhs(keys=rhs,
            kwargs_dict={STRNAMES.GROWTH_VALUE:{&#39;with_perturbations&#39;:False}},
            index_out_perturbations=True)
        y = self.G.data.construct_lhs(index_out_perturbations=True)

        prec = X.T @ X
        cov = pinv(prec, self)
        mean = cov @ X.T @ y
        if self.child_name == STRNAMES.GROWTH_VALUE:
            mean = 1e4*(np.median(mean[:self.G.data.n_taxa]) ** 2)
        else:
            mean = 1e4*(np.median(mean[self.G.data.n_taxa:]) ** 2)

        # Calculate the scale
        scale = mean * (self.prior.dof.value - 2) / self.prior.dof.value
    else:
        raise ValueError(&#39;`scale_option` ({}) not recognized&#39;.format(scale_option))
    self.prior.scale.override_value(scale)

    # Set the initial value of the prior
    if not pl.isstr(value_option):
        raise TypeError(&#39;`value_option` ({}) must be a str&#39;.format(type(value_option)))
    if value_option == &#39;manual&#39;:
        if not pl.isnumeric(value):
            raise ValueError(&#39;If `value_option` == &#34;manual&#34;, value ({}) &#39; \
                &#39;must be a numeric (float, int)&#39;.format(value.__class__))
    elif value_option in [&#39;inflated-median&#39;]:
        # No interactions
        rhs = [
            STRNAMES.GROWTH_VALUE,
            STRNAMES.SELF_INTERACTION_VALUE]
        X = self.G.data.construct_rhs(keys=rhs,
            kwargs_dict={STRNAMES.GROWTH_VALUE:{&#39;with_perturbations&#39;:False}},
            index_out_perturbations=True)
        y = self.G.data.construct_lhs(index_out_perturbations=True)

        prec = X.T @ X
        cov = pinv(prec, self)
        mean = cov @ X.T @ y
        if self.child_name == STRNAMES.GROWTH_VALUE:
            value = 1e4*(np.median(mean[:self.G.data.n_taxa]) ** 2)
        else:
            value = 1e4*(np.median(mean[self.G.data.n_taxa:]) ** 2)
    elif value_option in [&#39;prior-mean&#39;, &#39;auto&#39;]:
        value = self.prior.mean()
    else:
        raise ValueError(&#39;`value_option` &#34;{}&#34; not recognized&#39;.format(value_option))
    self.value = value</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.PriorVarMH.update"><code class="name flex">
<span>def <span class="ident">update</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>First we check if we need to tune the dof, which we do during
the first half of burnin. We calculate the likelihoods in logspace</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update(self):
    &#39;&#39;&#39;First we check if we need to tune the dof, which we do during
    the first half of burnin. We calculate the likelihoods in logspace
    &#39;&#39;&#39;
    if self.sample_iter &lt; self.delay:
        return
    self.update_dof()

    # Get necessary data of the respective parameter
    var = self.G[self.child_name]
    x = var.value.ravel()
    mu = var.prior.loc.value
    low = var.low
    high = var.high

    # propose a new value
    prev_value = self.value
    prev_value_std = math.sqrt(prev_value)
    self.proposal.scale.value = self.value
    new_value = self.proposal.sample() # Sample a new value
    new_value_std = math.sqrt(new_value)

    # Calculate the target distribution ll
    prev_target_ll = 0
    for i in range(len(x)):
        prev_target_ll += pl.random.truncnormal.logpdf(
            value=x[i], loc=mu, scale=prev_value_std,
            low=low, high=high)
    new_target_ll = 0
    for i in range(len(x)):
        new_target_ll += pl.random.truncnormal.logpdf(
            value=x[i], loc=mu, scale=new_value_std,
            low=low, high=high)

    # Normalize by the ll of the proposal
    prev_prop_ll = self.proposal.logpdf(value=prev_value)
    new_prop_ll = self.proposal.logpdf(value=new_value)

    # Accept or reject
    r = (new_target_ll - prev_prop_ll) - \
        (prev_target_ll - new_prop_ll)
    u = np.log(pl.random.misc.fast_sample_standard_uniform())

    if r &gt;= u:
        self.acceptances[self.sample_iter] = True
        self.value = new_value
        self.temp_acceptances += 1
    else:
        self.value = prev_value</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.PriorVarMH.update_dof"><code class="name flex">
<span>def <span class="ident">update_dof</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Updat the <code>dof</code> parameter so that we adjust the acceptance
rate to <code>target_acceptance_rate</code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_dof(self):
    &#39;&#39;&#39;Updat the `dof` parameter so that we adjust the acceptance
    rate to `target_acceptance_rate`
    &#39;&#39;&#39;
    if self.sample_iter == 0:
        self.temp_acceptances = 0
        self.acceptances = np.zeros(self.G.inference.n_samples, dtype=bool)
    
    elif self.sample_iter &gt; self.end_tune:
        # Don&#39;t do any more updates
        return
    
    elif self.sample_iter % self.tune == 0:
        # Update dof
        acceptance_rate = self.temp_acceptances / self.tune
        if acceptance_rate &gt; self.target_acceptance_rate:
            self.proposal.dof.value = self.proposal.dof.value * 1.5
        else:
            self.proposal.dof.value = self.proposal.dof.value / 1.5
        self.temp_acceptances = 0</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.PriorVarMH.visualize"><code class="name flex">
<span>def <span class="ident">visualize</span></span>(<span>self, path: str, section: str = 'posterior') ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Render the traces in the folder <code>basepath</code> and returns a
<code>pandas.DataFrame</code> with summary statistics</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>path</code></strong> :&ensp;<code>str</code></dt>
<dd>This is the path to write the files to</dd>
<dt><strong><code>f</code></strong> :&ensp;<code>_io.TextIOWrapper</code></dt>
<dd>File that we are writing the values to</dd>
<dt><strong><code>section</code></strong> :&ensp;<code>str</code></dt>
<dd>Section of the trace to compute on. Options:
'posterior' : posterior samples
'burnin' : burn-in samples
'entire' : both burn-in and posterior samples</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pandas.DataFrame</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def visualize(self, path: str, section: str=&#39;posterior&#39;) -&gt; pd.DataFrame:
    &#39;&#39;&#39;Render the traces in the folder `basepath` and returns a
    `pandas.DataFrame` with summary statistics

    Parameters
    ----------
    path : str
        This is the path to write the files to
    f : _io.TextIOWrapper
        File that we are writing the values to
    section : str
        Section of the trace to compute on. Options:
            &#39;posterior&#39; : posterior samples
            &#39;burnin&#39; : burn-in samples
            &#39;entire&#39; : both burn-in and posterior samples

    Returns
    -------
    pandas.DataFrame
    &#39;&#39;&#39;
    if not self.G.inference.tracer.is_being_traced(self):
        logging.info(&#39;`{}` not learned\n\tValue: {}\n&#39;.format(self.name, self.value))
        return pd.DataFrame()
    summ = pl.summary(self, section=section)
    data = [[self.name] + [v for _,v in summ.items()]]
    columns = [&#39;name&#39;] + [k for k in summ]
    index = [&#39;variance&#39;]
    df = pd.DataFrame(data, columns=columns, index=index)

    # Plot the traces
    ax1, ax2 = visualization.render_trace(var=self, plt_type=&#39;both&#39;, section=section,
        include_burnin=True, log_scale=True, rasterized=True)

    # Plot the prior over the posterior
    l,h = ax1.get_xlim()
    xs = np.arange(l,h,step=(h-l)/100) 
    ys = []
    for x in xs:
        ys.append(pl.random.sics.pdf(value=x, 
            dof=self.prior.dof.value,
            scale=self.prior.scale.value))
    ax1.plot(xs, ys, label=&#39;prior&#39;, alpha=0.5, color=&#39;red&#39;, rasterized=True)
    ax1.legend()

    # Plot the acceptance rate over the trace
    ax3 = ax2.twinx()
    ax3 = visualization.render_acceptance_rate_trace(var=self, ax=ax3, 
        label=&#39;Acceptance Rate&#39;, color=&#39;red&#39;, scatter=False, rasterized=True)
    ax3.legend()
    fig = plt.gcf()
    fig.tight_layout()
    fig.suptitle(self.name)
    plt.savefig(path)
    plt.close()

    return df</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="mdsine2.pylab.variables.SICS" href="pylab/variables.html#mdsine2.pylab.variables.SICS">SICS</a></b></code>:
<ul class="hlist">
<li><code><a title="mdsine2.pylab.variables.SICS.T" href="pylab/variables.html#mdsine2.pylab.variables.Variable.T">T</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.add_child" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_child">add_child</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.add_parent" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_parent">add_parent</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.add_prior" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_prior">add_prior</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.add_trace" href="pylab/base.html#mdsine2.pylab.base.Traceable.add_trace">add_trace</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.add_undirected" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_undirected">add_undirected</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.degree" href="pylab/graph.html#mdsine2.pylab.graph.Node.degree">degree</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.delete" href="pylab/graph.html#mdsine2.pylab.graph.BaseNode.delete">delete</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.get_adjacent_keys" href="pylab/graph.html#mdsine2.pylab.graph.Node.get_adjacent_keys">get_adjacent_keys</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.get_iter" href="pylab/base.html#mdsine2.pylab.base.Traceable.get_iter">get_iter</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.get_trace_from_disk" href="pylab/base.html#mdsine2.pylab.base.Traceable.get_trace_from_disk">get_trace_from_disk</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.load" href="pylab/base.html#mdsine2.pylab.base.Saveable.load">load</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.logpdf" href="pylab/variables.html#mdsine2.pylab.variables.SICS.logpdf">logpdf</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.metropolis" href="pylab/graph.html#mdsine2.pylab.graph.Node.metropolis">metropolis</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.overwrite_entire_trace_on_disk" href="pylab/base.html#mdsine2.pylab.base.Traceable.overwrite_entire_trace_on_disk">overwrite_entire_trace_on_disk</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.pdf" href="pylab/variables.html#mdsine2.pylab.variables.SICS.pdf">pdf</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.remove_local_trace" href="pylab/variables.html#mdsine2.pylab.variables.Variable.remove_local_trace">remove_local_trace</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.sample" href="pylab/variables.html#mdsine2.pylab.variables.SICS.sample">sample</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.save" href="pylab/base.html#mdsine2.pylab.base.Saveable.save">save</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.set_save_location" href="pylab/base.html#mdsine2.pylab.base.Saveable.set_save_location">set_save_location</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.set_trace" href="pylab/base.html#mdsine2.pylab.base.Traceable.set_trace">set_trace</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.set_value_shape" href="pylab/variables.html#mdsine2.pylab.variables.Variable.set_value_shape">set_value_shape</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="mdsine2.posterior.PriorVarPerturbationSingle"><code class="flex name class">
<span>class <span class="ident">PriorVarPerturbationSingle</span></span>
<span>(</span><span>prior: <a title="mdsine2.pylab.variables.SICS" href="pylab/variables.html#mdsine2.pylab.variables.SICS">SICS</a>, perturbation: <a title="mdsine2.pylab.contrib.ClusterPerturbationEffect" href="pylab/contrib.html#mdsine2.pylab.contrib.ClusterPerturbationEffect">ClusterPerturbationEffect</a>, value: Union[float, int] = None, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>This is the posterior of the prior variance of regression coefficients
for the interaction (off diagonal) variables</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PriorVarPerturbationSingle(pl.variables.SICS):
    &#39;&#39;&#39;This is the posterior of the prior variance of regression coefficients
    for the interaction (off diagonal) variables
    &#39;&#39;&#39;
    def __init__(self, prior: variables.SICS, perturbation: pl.ClusterPerturbationEffect, 
        value: Union[float, int]=None, **kwargs):

        kwargs[&#39;name&#39;] = STRNAMES.PRIOR_VAR_PERT + &#39;_&#39; + perturbation.name
        pl.variables.SICS.__init__(self, value=value, dtype=float, **kwargs)
        self.add_prior(prior)
        self.perturbation = perturbation

    def initialize(self, value_option: str, dof_option: str, scale_option: str, 
        value: float=None, dof: float=None, scale: float=None, delay: int=0):
        &#39;&#39;&#39;Initialize the hyperparameters of the perturbation prior variance based on the
        passed in option

        Parameters
        ----------
        value_option : str
            - Initialize the value based on the specified option
            - Options
                &#39;manual&#39;
                    Set the value manually, `value` must also be specified
                &#39;auto&#39;, &#39;prior-mean&#39;
                    Set the value to the mean of the prior
                &#39;tight&#39;
                    value = 10^2
                &#39;diffuse&#39;
                    value = 10^4
        scale_option : str
            Initialize the scale of the prior
            Options
                &#39;manual&#39;
                    Set the value manually, `scale` must also be specified
                &#39;auto&#39;, &#39;diffuse&#39;
                    Set so that the mean of the distribution is 10^4
                &#39;tight&#39;
                    Set so that the mean of the distribution is 10^2
        dof_option : str
            Initialize the dof of the parameter
            Options:
                &#39;manual&#39;: Set the value with the parameter `dof`
                &#39;diffuse&#39;: Set the value to 2.5
                &#39;strong&#39;: Set the value to the expected number of interactions
                &#39;auto&#39;: Set to diffuse
        dof, scale : int, float
            User specified values
            Only necessary if  any of the options are &#39;manual&#39;
        &#39;&#39;&#39;
        if not pl.isint(delay):
            raise TypeError(&#39;`delay` ({}) must be an int&#39;.format(type(delay)))
        if delay &lt; 0:
            raise ValueError(&#39;`delay` ({}) must be &gt;= 0&#39;.format(delay))
        self.delay = delay

        if not pl.isstr(dof_option):
            raise TypeError(&#39;`dof_option` ({}) must be a str&#39;.format(type(dof_option)))
        if dof_option == &#39;manual&#39;:
            if not pl.isnumeric(dof):
                raise TypeError(&#39;`dof` ({}) must be a numeric&#39;.format(type(dof)))
            if dof &lt; 0:
                raise ValueError(&#39;`dof` ({}) must be &gt; 0 for it to be a valid prior&#39;.format(shape))
        elif dof_option in [&#39;diffuse&#39;, &#39;auto&#39;]:
            dof = 2.5
        elif dof_option == &#39;strong&#39;:
            dof = expected_n_clusters(G=self.G)
        else:
            raise ValueError(&#39;`dof_option` ({}) not recognized&#39;.format(dof_option))
        self.prior.dof.override_value(dof)

        if not pl.isstr(scale_option):
            raise TypeError(&#39;`scale_option` ({}) must be a str&#39;.format(type(scale_option)))
        if scale_option == &#39;manual&#39;:
            if not pl.isnumeric(scale):
                raise TypeError(&#39;`scale` ({}) must be a numeric&#39;.format(type(scale)))
            if scale &lt; 0:
                raise ValueError(&#39;`scale` ({}) must be &gt; 0 for it to be a valid prior&#39;.format(scale))
        elif scale_option in [&#39;auto&#39;, &#39;diffuse&#39;]:
            # Calculate the mean to be 10
            scale = 1e4 * (self.prior.dof.value - 2) / self.prior.dof.value
        elif scale_option == &#39;tight&#39;:
            scale = 100 * (self.prior.dof.value - 2) / self.prior.dof.value
        else:
            raise ValueError(&#39;`scale_option` ({}) not recognized&#39;.format(scale_option))
        self.prior.scale.override_value(scale)

        if not pl.isstr(value_option):
            raise TypeError(&#39;`value_option` ({}) must be a str&#39;.format(type(value_option)))
        if value_option == &#39;manual&#39;:
            if not pl.isnumeric(value):
                raise ValueError(&#39;`value` ({}) must be numeric (float,int)&#39;.format(value.__class__))
            self.value = value
        elif value_option in [&#39;auto&#39;, &#39;prior-mean&#39;]:
            self.value = self.prior.mean()
        elif value_option == &#39;diffuse&#39;:
            self.value = 1e4
        elif value_option == &#39;tight&#39;:
            self.value = 1e2
        else:
            raise ValueError(&#39;`value_option` ({}) not recognized&#39;.format(value_option))

        logging.info(&#39;Prior Variance Interactions initialization results:\n&#39; \
            &#39;\tprior dof: {}\n&#39; \
            &#39;\tprior scale: {}\n&#39; \
            &#39;\tvalue: {}&#39;.format(
                self.prior.dof.value, self.prior.scale.value, self.value))

    # @profile
    def update(self):
        &#39;&#39;&#39;Calculate the posterior of the prior variance
        &#39;&#39;&#39;
        if self.sample_iter &lt; self.delay:
            return

        x = self.perturbation.cluster_array(only_pos_ind=True)
        mu = self.perturbation.magnitude.prior.loc.value

        se = np.sum(np.square(x - mu))
        n = len(x)

        self.dof.value = self.prior.dof.value + n
        self.scale.value = ((self.prior.scale.value * self.prior.dof.value) + \
           se)/self.dof.value
        self.sample()</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="mdsine2.pylab.variables.SICS" href="pylab/variables.html#mdsine2.pylab.variables.SICS">SICS</a></li>
<li><a title="mdsine2.pylab.variables.Variable" href="pylab/variables.html#mdsine2.pylab.variables.Variable">Variable</a></li>
<li><a title="mdsine2.pylab.graph.Node" href="pylab/graph.html#mdsine2.pylab.graph.Node">Node</a></li>
<li><a title="mdsine2.pylab.graph.BaseNode" href="pylab/graph.html#mdsine2.pylab.graph.BaseNode">BaseNode</a></li>
<li><a title="mdsine2.pylab.base.Saveable" href="pylab/base.html#mdsine2.pylab.base.Saveable">Saveable</a></li>
<li>mdsine2.pylab.variables._BaseArithmeticClass</li>
<li><a title="mdsine2.pylab.base.Traceable" href="pylab/base.html#mdsine2.pylab.base.Traceable">Traceable</a></li>
<li>mdsine2.pylab.variables._RandomBase</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="mdsine2.posterior.PriorVarPerturbationSingle.initialize"><code class="name flex">
<span>def <span class="ident">initialize</span></span>(<span>self, value_option: str, dof_option: str, scale_option: str, value: float = None, dof: float = None, scale: float = None, delay: int = 0)</span>
</code></dt>
<dd>
<div class="desc"><p>Initialize the hyperparameters of the perturbation prior variance based on the
passed in option</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>value_option</code></strong> :&ensp;<code>str</code></dt>
<dd>
<ul>
<li>Initialize the value based on the specified option</li>
<li>Options
'manual'
Set the value manually, <code>value</code> must also be specified
'auto', 'prior-mean'
Set the value to the mean of the prior
'tight'
value = 10^2
'diffuse'
value = 10^4</li>
</ul>
</dd>
<dt><strong><code>scale_option</code></strong> :&ensp;<code>str</code></dt>
<dd>Initialize the scale of the prior
Options
'manual'
Set the value manually, <code>scale</code> must also be specified
'auto', 'diffuse'
Set so that the mean of the distribution is 10^4
'tight'
Set so that the mean of the distribution is 10^2</dd>
<dt><strong><code>dof_option</code></strong> :&ensp;<code>str</code></dt>
<dd>Initialize the dof of the parameter
Options:
'manual': Set the value with the parameter <code>dof</code>
'diffuse': Set the value to 2.5
'strong': Set the value to the expected number of interactions
'auto': Set to diffuse</dd>
<dt><strong><code>dof</code></strong>, <strong><code>scale</code></strong> :&ensp;<code>int, float</code></dt>
<dd>User specified values
Only necessary if
any of the options are 'manual'</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def initialize(self, value_option: str, dof_option: str, scale_option: str, 
    value: float=None, dof: float=None, scale: float=None, delay: int=0):
    &#39;&#39;&#39;Initialize the hyperparameters of the perturbation prior variance based on the
    passed in option

    Parameters
    ----------
    value_option : str
        - Initialize the value based on the specified option
        - Options
            &#39;manual&#39;
                Set the value manually, `value` must also be specified
            &#39;auto&#39;, &#39;prior-mean&#39;
                Set the value to the mean of the prior
            &#39;tight&#39;
                value = 10^2
            &#39;diffuse&#39;
                value = 10^4
    scale_option : str
        Initialize the scale of the prior
        Options
            &#39;manual&#39;
                Set the value manually, `scale` must also be specified
            &#39;auto&#39;, &#39;diffuse&#39;
                Set so that the mean of the distribution is 10^4
            &#39;tight&#39;
                Set so that the mean of the distribution is 10^2
    dof_option : str
        Initialize the dof of the parameter
        Options:
            &#39;manual&#39;: Set the value with the parameter `dof`
            &#39;diffuse&#39;: Set the value to 2.5
            &#39;strong&#39;: Set the value to the expected number of interactions
            &#39;auto&#39;: Set to diffuse
    dof, scale : int, float
        User specified values
        Only necessary if  any of the options are &#39;manual&#39;
    &#39;&#39;&#39;
    if not pl.isint(delay):
        raise TypeError(&#39;`delay` ({}) must be an int&#39;.format(type(delay)))
    if delay &lt; 0:
        raise ValueError(&#39;`delay` ({}) must be &gt;= 0&#39;.format(delay))
    self.delay = delay

    if not pl.isstr(dof_option):
        raise TypeError(&#39;`dof_option` ({}) must be a str&#39;.format(type(dof_option)))
    if dof_option == &#39;manual&#39;:
        if not pl.isnumeric(dof):
            raise TypeError(&#39;`dof` ({}) must be a numeric&#39;.format(type(dof)))
        if dof &lt; 0:
            raise ValueError(&#39;`dof` ({}) must be &gt; 0 for it to be a valid prior&#39;.format(shape))
    elif dof_option in [&#39;diffuse&#39;, &#39;auto&#39;]:
        dof = 2.5
    elif dof_option == &#39;strong&#39;:
        dof = expected_n_clusters(G=self.G)
    else:
        raise ValueError(&#39;`dof_option` ({}) not recognized&#39;.format(dof_option))
    self.prior.dof.override_value(dof)

    if not pl.isstr(scale_option):
        raise TypeError(&#39;`scale_option` ({}) must be a str&#39;.format(type(scale_option)))
    if scale_option == &#39;manual&#39;:
        if not pl.isnumeric(scale):
            raise TypeError(&#39;`scale` ({}) must be a numeric&#39;.format(type(scale)))
        if scale &lt; 0:
            raise ValueError(&#39;`scale` ({}) must be &gt; 0 for it to be a valid prior&#39;.format(scale))
    elif scale_option in [&#39;auto&#39;, &#39;diffuse&#39;]:
        # Calculate the mean to be 10
        scale = 1e4 * (self.prior.dof.value - 2) / self.prior.dof.value
    elif scale_option == &#39;tight&#39;:
        scale = 100 * (self.prior.dof.value - 2) / self.prior.dof.value
    else:
        raise ValueError(&#39;`scale_option` ({}) not recognized&#39;.format(scale_option))
    self.prior.scale.override_value(scale)

    if not pl.isstr(value_option):
        raise TypeError(&#39;`value_option` ({}) must be a str&#39;.format(type(value_option)))
    if value_option == &#39;manual&#39;:
        if not pl.isnumeric(value):
            raise ValueError(&#39;`value` ({}) must be numeric (float,int)&#39;.format(value.__class__))
        self.value = value
    elif value_option in [&#39;auto&#39;, &#39;prior-mean&#39;]:
        self.value = self.prior.mean()
    elif value_option == &#39;diffuse&#39;:
        self.value = 1e4
    elif value_option == &#39;tight&#39;:
        self.value = 1e2
    else:
        raise ValueError(&#39;`value_option` ({}) not recognized&#39;.format(value_option))

    logging.info(&#39;Prior Variance Interactions initialization results:\n&#39; \
        &#39;\tprior dof: {}\n&#39; \
        &#39;\tprior scale: {}\n&#39; \
        &#39;\tvalue: {}&#39;.format(
            self.prior.dof.value, self.prior.scale.value, self.value))</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.PriorVarPerturbationSingle.update"><code class="name flex">
<span>def <span class="ident">update</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate the posterior of the prior variance</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update(self):
    &#39;&#39;&#39;Calculate the posterior of the prior variance
    &#39;&#39;&#39;
    if self.sample_iter &lt; self.delay:
        return

    x = self.perturbation.cluster_array(only_pos_ind=True)
    mu = self.perturbation.magnitude.prior.loc.value

    se = np.sum(np.square(x - mu))
    n = len(x)

    self.dof.value = self.prior.dof.value + n
    self.scale.value = ((self.prior.scale.value * self.prior.dof.value) + \
       se)/self.dof.value
    self.sample()</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="mdsine2.pylab.variables.SICS" href="pylab/variables.html#mdsine2.pylab.variables.SICS">SICS</a></b></code>:
<ul class="hlist">
<li><code><a title="mdsine2.pylab.variables.SICS.T" href="pylab/variables.html#mdsine2.pylab.variables.Variable.T">T</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.add_child" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_child">add_child</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.add_parent" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_parent">add_parent</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.add_prior" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_prior">add_prior</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.add_trace" href="pylab/base.html#mdsine2.pylab.base.Traceable.add_trace">add_trace</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.add_undirected" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_undirected">add_undirected</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.degree" href="pylab/graph.html#mdsine2.pylab.graph.Node.degree">degree</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.delete" href="pylab/graph.html#mdsine2.pylab.graph.BaseNode.delete">delete</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.get_adjacent_keys" href="pylab/graph.html#mdsine2.pylab.graph.Node.get_adjacent_keys">get_adjacent_keys</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.get_iter" href="pylab/base.html#mdsine2.pylab.base.Traceable.get_iter">get_iter</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.get_trace_from_disk" href="pylab/base.html#mdsine2.pylab.base.Traceable.get_trace_from_disk">get_trace_from_disk</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.load" href="pylab/base.html#mdsine2.pylab.base.Saveable.load">load</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.logpdf" href="pylab/variables.html#mdsine2.pylab.variables.SICS.logpdf">logpdf</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.metropolis" href="pylab/graph.html#mdsine2.pylab.graph.Node.metropolis">metropolis</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.overwrite_entire_trace_on_disk" href="pylab/base.html#mdsine2.pylab.base.Traceable.overwrite_entire_trace_on_disk">overwrite_entire_trace_on_disk</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.pdf" href="pylab/variables.html#mdsine2.pylab.variables.SICS.pdf">pdf</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.remove_local_trace" href="pylab/variables.html#mdsine2.pylab.variables.Variable.remove_local_trace">remove_local_trace</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.sample" href="pylab/variables.html#mdsine2.pylab.variables.SICS.sample">sample</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.save" href="pylab/base.html#mdsine2.pylab.base.Saveable.save">save</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.set_save_location" href="pylab/base.html#mdsine2.pylab.base.Saveable.set_save_location">set_save_location</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.set_trace" href="pylab/base.html#mdsine2.pylab.base.Traceable.set_trace">set_trace</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.set_value_shape" href="pylab/variables.html#mdsine2.pylab.variables.Variable.set_value_shape">set_value_shape</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="mdsine2.posterior.PriorVarPerturbations"><code class="flex name class">
<span>class <span class="ident">PriorVarPerturbations</span></span>
<span>(</span><span>**kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Class iterates over the perturbations for updating the variance of the prior
for each perturbation</p>
<p>All perturbations get the same hyperparameters</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PriorVarPerturbations(pl.Variable):
    &#39;&#39;&#39;Class iterates over the perturbations for updating the variance of the prior
    for each perturbation

    All perturbations get the same hyperparameters
    &#39;&#39;&#39;
    def __init__(self, **kwargs):
        kwargs[&#39;name&#39;] = STRNAMES.PRIOR_VAR_PERT
        pl.Variable.__init__(self, **kwargs)
        self.perturbations = self.G.perturbations # mdsine2.pylab.base.Perturbations

        if self.perturbations is None:
            raise TypeError(&#39;Only instantiate this object if there are perturbations&#39;)

    def __str__(self) -&gt; str:
        s = &#39;Perturbation Magnitude Prior Variances&#39;
        for perturbation in self.perturbations:
            s += &#39;\n\tperturbation {}: {}&#39;.format(
                perturbation.name,
                perturbation.magnitude.prior.scale2.value)
        return s

    @property
    def sample_iter(self) -&gt; int:
        return self.perturbations[0].magnitude.prior.scale2.sample_iter

    def initialize(self, **kwargs):
        &#39;&#39;&#39;Every prior variance on the perturbations gets the same hyperparameters
        &#39;&#39;&#39;
        for perturbation in self.perturbations:
            perturbation.magnitude.prior.scale2.initialize(**kwargs)

    def update(self):
        for perturbation in self.perturbations:
            perturbation.magnitude.prior.scale2.update()

    def set_trace(self, *args, **kwargs):
        for perturbation in self.perturbations:
            perturbation.magnitude.prior.scale2.set_trace(*args, **kwargs)

    def add_trace(self, *args, **kwargs):
        for perturbation in self.perturbations:
            perturbation.magnitude.prior.scale2.add_trace(*args, **kwargs)

    def get_single_value_of_perts(self) -&gt; np.ndarray:
        &#39;&#39;&#39;Get the variance for each perturbation
        &#39;&#39;&#39;
        return np.asarray([p.magnitude.prior.scale2.value for p in self.perturbations])

    def diag(self, only_pos_ind: bool=True) -&gt; np.ndarray:
        &#39;&#39;&#39;Return the diagonal of the prior variances stacked up in order

        Parameters
        ----------
        only_pos_ind : bool
            If True, only put in the values for the positively indicated clusters
            for each perturbation

        Returns
        -------
        np.ndarray
        &#39;&#39;&#39;
        ret = []
        for perturbation in self.perturbations:
            if only_pos_ind:
                n = perturbation.indicator.num_on_clusters()
            else:
                n = len(perturbation.clustering)
            ret = np.append(
                ret,
                np.ones(n, dtype=float)*perturbation.magnitude.prior.scale2.value)
        return ret

    def visualize(self, path: str, f: IO, pidx: int, section: str=&#39;posterior&#39;) -&gt; IO:
        &#39;&#39;&#39;Visualize the `pidx`th perturbation prior magnitude

        Parameters
        ----------
        obj : mdsine2.Variable
        path : str
            This is the path to write the files to
        f : _io.TextIOWrapper
            File that we are writing the values to
        section : str
            Section of the trace to compute on. Options:
                &#39;posterior&#39; : posterior samples
                &#39;burnin&#39; : burn-in samples
                &#39;entire&#39; : both burn-in and posterior samples

        Returns
        -------
        _io.TextIOWrapper
        &#39;&#39;&#39;
        if not self.G.inference.is_in_inference_order(self.name):
            return f
        perturbation = self.perturbations[pidx]
        return _scalar_visualize(path=path, f=f, section=section,
            obj=perturbation.magnitude.prior.scale2, log_scale=True)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="mdsine2.pylab.variables.Variable" href="pylab/variables.html#mdsine2.pylab.variables.Variable">Variable</a></li>
<li><a title="mdsine2.pylab.graph.Node" href="pylab/graph.html#mdsine2.pylab.graph.Node">Node</a></li>
<li><a title="mdsine2.pylab.graph.BaseNode" href="pylab/graph.html#mdsine2.pylab.graph.BaseNode">BaseNode</a></li>
<li><a title="mdsine2.pylab.base.Saveable" href="pylab/base.html#mdsine2.pylab.base.Saveable">Saveable</a></li>
<li>mdsine2.pylab.variables._BaseArithmeticClass</li>
<li><a title="mdsine2.pylab.base.Traceable" href="pylab/base.html#mdsine2.pylab.base.Traceable">Traceable</a></li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="mdsine2.posterior.PriorVarPerturbations.sample_iter"><code class="name">var <span class="ident">sample_iter</span> : int</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def sample_iter(self) -&gt; int:
    return self.perturbations[0].magnitude.prior.scale2.sample_iter</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="mdsine2.posterior.PriorVarPerturbations.diag"><code class="name flex">
<span>def <span class="ident">diag</span></span>(<span>self, only_pos_ind: bool = True) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Return the diagonal of the prior variances stacked up in order</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>only_pos_ind</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, only put in the values for the positively indicated clusters
for each perturbation</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.ndarray</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def diag(self, only_pos_ind: bool=True) -&gt; np.ndarray:
    &#39;&#39;&#39;Return the diagonal of the prior variances stacked up in order

    Parameters
    ----------
    only_pos_ind : bool
        If True, only put in the values for the positively indicated clusters
        for each perturbation

    Returns
    -------
    np.ndarray
    &#39;&#39;&#39;
    ret = []
    for perturbation in self.perturbations:
        if only_pos_ind:
            n = perturbation.indicator.num_on_clusters()
        else:
            n = len(perturbation.clustering)
        ret = np.append(
            ret,
            np.ones(n, dtype=float)*perturbation.magnitude.prior.scale2.value)
    return ret</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.PriorVarPerturbations.get_single_value_of_perts"><code class="name flex">
<span>def <span class="ident">get_single_value_of_perts</span></span>(<span>self) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Get the variance for each perturbation</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_single_value_of_perts(self) -&gt; np.ndarray:
    &#39;&#39;&#39;Get the variance for each perturbation
    &#39;&#39;&#39;
    return np.asarray([p.magnitude.prior.scale2.value for p in self.perturbations])</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.PriorVarPerturbations.initialize"><code class="name flex">
<span>def <span class="ident">initialize</span></span>(<span>self, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Every prior variance on the perturbations gets the same hyperparameters</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def initialize(self, **kwargs):
    &#39;&#39;&#39;Every prior variance on the perturbations gets the same hyperparameters
    &#39;&#39;&#39;
    for perturbation in self.perturbations:
        perturbation.magnitude.prior.scale2.initialize(**kwargs)</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.PriorVarPerturbations.update"><code class="name flex">
<span>def <span class="ident">update</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update(self):
    for perturbation in self.perturbations:
        perturbation.magnitude.prior.scale2.update()</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.PriorVarPerturbations.visualize"><code class="name flex">
<span>def <span class="ident">visualize</span></span>(<span>self, path: str, f: <class 'IO'>, pidx: int, section: str = 'posterior') ‑> <class 'IO'></span>
</code></dt>
<dd>
<div class="desc"><p>Visualize the <code>pidx</code>th perturbation prior magnitude</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>obj</code></strong> :&ensp;<code>mdsine2.Variable</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>path</code></strong> :&ensp;<code>str</code></dt>
<dd>This is the path to write the files to</dd>
<dt><strong><code>f</code></strong> :&ensp;<code>_io.TextIOWrapper</code></dt>
<dd>File that we are writing the values to</dd>
<dt><strong><code>section</code></strong> :&ensp;<code>str</code></dt>
<dd>Section of the trace to compute on. Options:
'posterior' : posterior samples
'burnin' : burn-in samples
'entire' : both burn-in and posterior samples</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>_io.TextIOWrapper</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def visualize(self, path: str, f: IO, pidx: int, section: str=&#39;posterior&#39;) -&gt; IO:
    &#39;&#39;&#39;Visualize the `pidx`th perturbation prior magnitude

    Parameters
    ----------
    obj : mdsine2.Variable
    path : str
        This is the path to write the files to
    f : _io.TextIOWrapper
        File that we are writing the values to
    section : str
        Section of the trace to compute on. Options:
            &#39;posterior&#39; : posterior samples
            &#39;burnin&#39; : burn-in samples
            &#39;entire&#39; : both burn-in and posterior samples

    Returns
    -------
    _io.TextIOWrapper
    &#39;&#39;&#39;
    if not self.G.inference.is_in_inference_order(self.name):
        return f
    perturbation = self.perturbations[pidx]
    return _scalar_visualize(path=path, f=f, section=section,
        obj=perturbation.magnitude.prior.scale2, log_scale=True)</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="mdsine2.pylab.variables.Variable" href="pylab/variables.html#mdsine2.pylab.variables.Variable">Variable</a></b></code>:
<ul class="hlist">
<li><code><a title="mdsine2.pylab.variables.Variable.T" href="pylab/variables.html#mdsine2.pylab.variables.Variable.T">T</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.add_child" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_child">add_child</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.add_parent" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_parent">add_parent</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.add_prior" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_prior">add_prior</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.add_trace" href="pylab/base.html#mdsine2.pylab.base.Traceable.add_trace">add_trace</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.add_undirected" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_undirected">add_undirected</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.degree" href="pylab/graph.html#mdsine2.pylab.graph.Node.degree">degree</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.delete" href="pylab/graph.html#mdsine2.pylab.graph.BaseNode.delete">delete</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.get_adjacent_keys" href="pylab/graph.html#mdsine2.pylab.graph.Node.get_adjacent_keys">get_adjacent_keys</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.get_iter" href="pylab/base.html#mdsine2.pylab.base.Traceable.get_iter">get_iter</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.get_trace_from_disk" href="pylab/base.html#mdsine2.pylab.base.Traceable.get_trace_from_disk">get_trace_from_disk</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.load" href="pylab/base.html#mdsine2.pylab.base.Saveable.load">load</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.metropolis" href="pylab/graph.html#mdsine2.pylab.graph.Node.metropolis">metropolis</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.overwrite_entire_trace_on_disk" href="pylab/base.html#mdsine2.pylab.base.Traceable.overwrite_entire_trace_on_disk">overwrite_entire_trace_on_disk</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.remove_local_trace" href="pylab/variables.html#mdsine2.pylab.variables.Variable.remove_local_trace">remove_local_trace</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.save" href="pylab/base.html#mdsine2.pylab.base.Saveable.save">save</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.set_save_location" href="pylab/base.html#mdsine2.pylab.base.Saveable.set_save_location">set_save_location</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.set_trace" href="pylab/base.html#mdsine2.pylab.base.Traceable.set_trace">set_trace</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.set_value_shape" href="pylab/variables.html#mdsine2.pylab.variables.Variable.set_value_shape">set_value_shape</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="mdsine2.posterior.ProcessVarGlobal"><code class="flex name class">
<span>class <span class="ident">ProcessVarGlobal</span></span>
<span>(</span><span>prior: <a title="mdsine2.pylab.variables.SICS" href="pylab/variables.html#mdsine2.pylab.variables.SICS">SICS</a>, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Learn a Process variance where we learn th same process variance
for each Taxa. This assumes that the model we're using uses the logscale
of the data.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>prior</code></strong> :&ensp;<code>pl.variables.SICS</code></dt>
<dd>This is the prior of the distribution</dd>
<dt><strong><code>kwargs</code></strong> :&ensp;<code>dict</code></dt>
<dd>These are the extra parameters for the Variable class</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ProcessVarGlobal(pl.variables.SICS):
    &#39;&#39;&#39;Learn a Process variance where we learn th same process variance
    for each Taxa. This assumes that the model we&#39;re using uses the logscale
    of the data.
    &#39;&#39;&#39;
    def __init__(self, prior: variables.SICS, **kwargs):
        &#39;&#39;&#39;
        Parameters
        ----------
        prior : pl.variables.SICS
            This is the prior of the distribution
        kwargs : dict
            These are the extra parameters for the Variable class
        &#39;&#39;&#39;
        kwargs[&#39;name&#39;] = STRNAMES.PROCESSVAR
        pl.variables.SICS.__init__(self, dtype=float, **kwargs)
        self.add_prior(prior)
        self.global_variance = True
        self._strr = &#39;NA&#39;

    def __str__(self) -&gt; str:
        return self._strr

    def initialize(self, dof_option: str, scale_option: str, value_option: str, 
        dof: Union[float, int]=None, scale: Union[float, int]=None, 
        value: Union[float, int]=None, variance_scaling: Union[float, int]=1,
        delay: int=0):
        &#39;&#39;&#39;Initialize the value and hyperparameter.

        Parameters
        ----------
        dof_option : str
            How to initialize the `dof` parameter. Options:
                &#39;manual&#39;
                    Manually specify the dof with the parameter `dof`
                &#39;half&#39;
                    Set the degrees of freedom to the number of data points
                &#39;auto&#39;, &#39;diffuse&#39;
                    Set the degrees of freedom to a sparse number (2.5)
        scale_option : str
            How to initialize the scale of the parameter. Options:
                &#39;manual&#39;
                    Need to also specify `scale` parameter
                &#39;med&#39;, &#39;auto&#39;
                    Set the scale such that mean of the distribution
                    has medium noise (20%)
                &#39;low&#39;
                    Set the scale such that mean of the distribution
                    has low noise (10%)
                &#39;high&#39;
                    Set the scale such that mean of the distribution
                    has high noise (30%)
        value_option : str
            How to initialize the value
                &#39;manual&#39;
                    Set the value with the `value` parameter
                &#39;prior-mean&#39;, &#39;auto&#39;
                    Set the value to the mean of the prior
        variance_scaling : float, None
            How much to inflate the variance
        &#39;&#39;&#39;
        if not pl.isint(delay):
            raise TypeError(&#39;`delay` ({}) must be an int&#39;.format(type(delay)))
        if delay &lt; 0:
            raise ValueError(&#39;`delay` ({}) must be &gt;= 0&#39;.format(delay))
        self.delay = delay

        # Set the dof
        if not pl.isstr(dof_option):
            raise TypeError(&#39;`dof_option` ({}) must be a str&#39;.format(type(dof_option)))
        if dof_option == &#39;manual&#39;:
            if not pl.isnumeric(dof):
                raise TypeError(&#39;`dof` ({}) must be a numeric&#39;.format(type(dof)))
            if dof &lt;= 0:
                raise ValueError(&#39;`dof` ({}) must be &gt; 0&#39;.format(dof))
            if dof &lt;= 2:
                logging.critical(&#39;Process Variance dof ({}) is set unproper&#39;.format(dof))
        elif dof_option == &#39;half&#39;:
            dof = len(self.G.data.lhs)
        elif dof_option in [&#39;auto&#39;, &#39;diffuse&#39;]:
            dof = 2.5
        else:
            raise ValueError(&#39;`dof_option` ({}) not recognized&#39;.format(dof_option))
        self.prior.dof.override_value(dof)

        # Set the scale
        if not pl.isstr(scale_option):
            raise TypeError(&#39;`scale_option` ({}) must be a str&#39;.format(type(scale_option)))
        if scale_option == &#39;manual&#39;:
            if not pl.isnumeric(scale):
                raise TypeError(&#39;`scale` ({}) must be a numeric&#39;.format(type(scale)))
            if scale &lt;= 0:
                raise ValueError(&#39;`scale` ({}) must be &gt; 0&#39;.format(scale))
        elif scale_option in [&#39;auto&#39;, &#39;med&#39;]:
            scale = (0.2 ** 2) * (self.prior.dof.value - 2) / self.prior.dof.value
        elif scale_option ==  &#39;low&#39;:
            scale = (0.1 ** 2) * (self.prior.dof.value - 2) / self.prior.dof.value
        elif scale_option ==  &#39;high&#39;:
            scale = (0.3 ** 2) * (self.prior.dof.value - 2) / self.prior.dof.value
        else:
            raise ValueError(&#39;`scale_option` ({}) not recognized&#39;.format(scale_option))
        self.prior.scale.override_value(scale)

        # Set the value
        if not pl.isstr(value_option):
            raise TypeError(&#39;`value_option` ({}) must be a str&#39;.format(type(value_option)))
        if value_option == &#39;manual&#39;:
            if not pl.isnumeric(dof):
                raise TypeError(&#39;`value` ({}) must be a numeric&#39;.format(type(value)))
            if value &lt;= 0:
                raise ValueError(&#39;`value` ({}) must be &gt; 0&#39;.format(value))
        elif value_option in [&#39;prior-mean&#39;, &#39;auto&#39;]:
            value = self.prior.mean()
        else:
            raise ValueError(&#39;`value_option` ({}) not recognized&#39;.format(value_option))
        self.value = value
        
        self.rebuild_diag()
        self._there_are_perturbations = self.G.perturbations is not None

    def build_matrix(self, cov: bool, sparse: bool=True) -&gt; Union[scipy.sparse.spmatrix, np.ndarray]:
        &#39;&#39;&#39;Builds the process variance as a covariance or precision
        matrix.

        Parameters
        ----------
        cov : bool
            If True, make the covariance matrix
        sparse : bool
            If True, return the matrix as a sparse matrix

        Returns
        -------
        np.ndarray or scipy.sparse
            Either sparse or dense covariance/precision matrix
        &#39;&#39;&#39;
        a = self.diag
        if not cov:
            a = 1/a
        if sparse:
            return scipy.sparse.dia_matrix((a,[0]), shape=(len(a),len(a))).tocsc()
        else:
            return np.diag(a)

    def rebuild_diag(self):
        &#39;&#39;&#39;Builds up the process variance diagonal that we use to make the matrix
        &#39;&#39;&#39;
        a = self.value / self.G.data.dt_vec
        if self.G.data.zero_inflation_transition_policy is not None:
            a = a[self.G.data.rows_to_include_zero_inflation]
        
        self.diag = a
        self.prec = 1/a

    def update(self):
        &#39;&#39;&#39;Update the process variance

        ..math:: y = (log(x_{k+1}) - log(x_k))/dt
        ..math:: Xb = a_1 (1 + \\gamma) + A x

        % These are our dynamics with the process variance
        ..math:: y ~ Normal(Xb , \\sigma^2_w / dt)

        % Subtract the mean
        ..math:: y - Xb ~ Normal( 0, \sigma^2_w / dt)
        
        % Substitute
        ..math:: z = y - Xb 

        ..math:: z ~ Normal (0, \\sigma^2_w / dt)
        ..math:: z * \\sqrt{dt} ~ Normal (0, \\sigma^2_w)

        This is now in a form we can use to calculate the posterior
        &#39;&#39;&#39;
        if self._there_are_perturbations:
            lhs = [
                STRNAMES.GROWTH_VALUE, 
                STRNAMES.SELF_INTERACTION_VALUE,
                STRNAMES.CLUSTER_INTERACTION_VALUE]
        else:
            lhs = [
                STRNAMES.GROWTH_VALUE, 
                STRNAMES.SELF_INTERACTION_VALUE,
                STRNAMES.CLUSTER_INTERACTION_VALUE]
        
        # This is the residual z = y - Xb
        z = self.G.data.construct_lhs(lhs, 
            kwargs_dict={STRNAMES.GROWTH_VALUE:{
                &#39;with_perturbations&#39;: self._there_are_perturbations}})
        z = np.asarray(z).ravel()
        if self.G.data.zero_inflation_transition_policy is not None:
            z = z * self.G.data.sqrt_dt_vec[self.G.data.rows_to_include_zero_inflation]
        else:
            z = z * self.G.data.sqrt_dt_vec
        residual = np.sum(np.square(z))

        self.dof.value = self.prior.dof.value + len(z)
        self.scale.value = ((self.prior.scale.value * self.prior.dof.value) + \
           residual)/self.dof.value
        
        # shape = 2 + (len(residual)/2)
        # scale = 0.00001 + np.sum(np.square(residual))/2
        # self.value = pl.random.invgamma.sample(shape=shape, scale=scale)
        self.sample()
        self.rebuild_diag()

        self._strr = &#39;{}, empirical_variance: {:.5f}&#39;.format(self.value, 
            residual/len(z))

    def visualize(self, path: str, section: str=&#39;posterior&#39;) -&gt; Any:
        return _scalar_visualize(self, path=path, f=None, section=section)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="mdsine2.pylab.variables.SICS" href="pylab/variables.html#mdsine2.pylab.variables.SICS">SICS</a></li>
<li><a title="mdsine2.pylab.variables.Variable" href="pylab/variables.html#mdsine2.pylab.variables.Variable">Variable</a></li>
<li><a title="mdsine2.pylab.graph.Node" href="pylab/graph.html#mdsine2.pylab.graph.Node">Node</a></li>
<li><a title="mdsine2.pylab.graph.BaseNode" href="pylab/graph.html#mdsine2.pylab.graph.BaseNode">BaseNode</a></li>
<li><a title="mdsine2.pylab.base.Saveable" href="pylab/base.html#mdsine2.pylab.base.Saveable">Saveable</a></li>
<li>mdsine2.pylab.variables._BaseArithmeticClass</li>
<li><a title="mdsine2.pylab.base.Traceable" href="pylab/base.html#mdsine2.pylab.base.Traceable">Traceable</a></li>
<li>mdsine2.pylab.variables._RandomBase</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="mdsine2.posterior.ProcessVarGlobal.build_matrix"><code class="name flex">
<span>def <span class="ident">build_matrix</span></span>(<span>self, cov: bool, sparse: bool = True) ‑> Union[scipy.sparse.base.spmatrix, numpy.ndarray]</span>
</code></dt>
<dd>
<div class="desc"><p>Builds the process variance as a covariance or precision
matrix.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>cov</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, make the covariance matrix</dd>
<dt><strong><code>sparse</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, return the matrix as a sparse matrix</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.ndarray</code> or <code>scipy.sparse</code></dt>
<dd>Either sparse or dense covariance/precision matrix</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def build_matrix(self, cov: bool, sparse: bool=True) -&gt; Union[scipy.sparse.spmatrix, np.ndarray]:
    &#39;&#39;&#39;Builds the process variance as a covariance or precision
    matrix.

    Parameters
    ----------
    cov : bool
        If True, make the covariance matrix
    sparse : bool
        If True, return the matrix as a sparse matrix

    Returns
    -------
    np.ndarray or scipy.sparse
        Either sparse or dense covariance/precision matrix
    &#39;&#39;&#39;
    a = self.diag
    if not cov:
        a = 1/a
    if sparse:
        return scipy.sparse.dia_matrix((a,[0]), shape=(len(a),len(a))).tocsc()
    else:
        return np.diag(a)</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.ProcessVarGlobal.initialize"><code class="name flex">
<span>def <span class="ident">initialize</span></span>(<span>self, dof_option: str, scale_option: str, value_option: str, dof: Union[float, int] = None, scale: Union[float, int] = None, value: Union[float, int] = None, variance_scaling: Union[float, int] = 1, delay: int = 0)</span>
</code></dt>
<dd>
<div class="desc"><p>Initialize the value and hyperparameter.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>dof_option</code></strong> :&ensp;<code>str</code></dt>
<dd>How to initialize the <code>dof</code> parameter. Options:
'manual'
Manually specify the dof with the parameter <code>dof</code>
'half'
Set the degrees of freedom to the number of data points
'auto', 'diffuse'
Set the degrees of freedom to a sparse number (2.5)</dd>
<dt><strong><code>scale_option</code></strong> :&ensp;<code>str</code></dt>
<dd>How to initialize the scale of the parameter. Options:
'manual'
Need to also specify <code>scale</code> parameter
'med', 'auto'
Set the scale such that mean of the distribution
has medium noise (20%)
'low'
Set the scale such that mean of the distribution
has low noise (10%)
'high'
Set the scale such that mean of the distribution
has high noise (30%)</dd>
<dt><strong><code>value_option</code></strong> :&ensp;<code>str</code></dt>
<dd>How to initialize the value
'manual'
Set the value with the <code>value</code> parameter
'prior-mean', 'auto'
Set the value to the mean of the prior</dd>
<dt><strong><code>variance_scaling</code></strong> :&ensp;<code>float, None</code></dt>
<dd>How much to inflate the variance</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def initialize(self, dof_option: str, scale_option: str, value_option: str, 
    dof: Union[float, int]=None, scale: Union[float, int]=None, 
    value: Union[float, int]=None, variance_scaling: Union[float, int]=1,
    delay: int=0):
    &#39;&#39;&#39;Initialize the value and hyperparameter.

    Parameters
    ----------
    dof_option : str
        How to initialize the `dof` parameter. Options:
            &#39;manual&#39;
                Manually specify the dof with the parameter `dof`
            &#39;half&#39;
                Set the degrees of freedom to the number of data points
            &#39;auto&#39;, &#39;diffuse&#39;
                Set the degrees of freedom to a sparse number (2.5)
    scale_option : str
        How to initialize the scale of the parameter. Options:
            &#39;manual&#39;
                Need to also specify `scale` parameter
            &#39;med&#39;, &#39;auto&#39;
                Set the scale such that mean of the distribution
                has medium noise (20%)
            &#39;low&#39;
                Set the scale such that mean of the distribution
                has low noise (10%)
            &#39;high&#39;
                Set the scale such that mean of the distribution
                has high noise (30%)
    value_option : str
        How to initialize the value
            &#39;manual&#39;
                Set the value with the `value` parameter
            &#39;prior-mean&#39;, &#39;auto&#39;
                Set the value to the mean of the prior
    variance_scaling : float, None
        How much to inflate the variance
    &#39;&#39;&#39;
    if not pl.isint(delay):
        raise TypeError(&#39;`delay` ({}) must be an int&#39;.format(type(delay)))
    if delay &lt; 0:
        raise ValueError(&#39;`delay` ({}) must be &gt;= 0&#39;.format(delay))
    self.delay = delay

    # Set the dof
    if not pl.isstr(dof_option):
        raise TypeError(&#39;`dof_option` ({}) must be a str&#39;.format(type(dof_option)))
    if dof_option == &#39;manual&#39;:
        if not pl.isnumeric(dof):
            raise TypeError(&#39;`dof` ({}) must be a numeric&#39;.format(type(dof)))
        if dof &lt;= 0:
            raise ValueError(&#39;`dof` ({}) must be &gt; 0&#39;.format(dof))
        if dof &lt;= 2:
            logging.critical(&#39;Process Variance dof ({}) is set unproper&#39;.format(dof))
    elif dof_option == &#39;half&#39;:
        dof = len(self.G.data.lhs)
    elif dof_option in [&#39;auto&#39;, &#39;diffuse&#39;]:
        dof = 2.5
    else:
        raise ValueError(&#39;`dof_option` ({}) not recognized&#39;.format(dof_option))
    self.prior.dof.override_value(dof)

    # Set the scale
    if not pl.isstr(scale_option):
        raise TypeError(&#39;`scale_option` ({}) must be a str&#39;.format(type(scale_option)))
    if scale_option == &#39;manual&#39;:
        if not pl.isnumeric(scale):
            raise TypeError(&#39;`scale` ({}) must be a numeric&#39;.format(type(scale)))
        if scale &lt;= 0:
            raise ValueError(&#39;`scale` ({}) must be &gt; 0&#39;.format(scale))
    elif scale_option in [&#39;auto&#39;, &#39;med&#39;]:
        scale = (0.2 ** 2) * (self.prior.dof.value - 2) / self.prior.dof.value
    elif scale_option ==  &#39;low&#39;:
        scale = (0.1 ** 2) * (self.prior.dof.value - 2) / self.prior.dof.value
    elif scale_option ==  &#39;high&#39;:
        scale = (0.3 ** 2) * (self.prior.dof.value - 2) / self.prior.dof.value
    else:
        raise ValueError(&#39;`scale_option` ({}) not recognized&#39;.format(scale_option))
    self.prior.scale.override_value(scale)

    # Set the value
    if not pl.isstr(value_option):
        raise TypeError(&#39;`value_option` ({}) must be a str&#39;.format(type(value_option)))
    if value_option == &#39;manual&#39;:
        if not pl.isnumeric(dof):
            raise TypeError(&#39;`value` ({}) must be a numeric&#39;.format(type(value)))
        if value &lt;= 0:
            raise ValueError(&#39;`value` ({}) must be &gt; 0&#39;.format(value))
    elif value_option in [&#39;prior-mean&#39;, &#39;auto&#39;]:
        value = self.prior.mean()
    else:
        raise ValueError(&#39;`value_option` ({}) not recognized&#39;.format(value_option))
    self.value = value
    
    self.rebuild_diag()
    self._there_are_perturbations = self.G.perturbations is not None</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.ProcessVarGlobal.rebuild_diag"><code class="name flex">
<span>def <span class="ident">rebuild_diag</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Builds up the process variance diagonal that we use to make the matrix</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def rebuild_diag(self):
    &#39;&#39;&#39;Builds up the process variance diagonal that we use to make the matrix
    &#39;&#39;&#39;
    a = self.value / self.G.data.dt_vec
    if self.G.data.zero_inflation_transition_policy is not None:
        a = a[self.G.data.rows_to_include_zero_inflation]
    
    self.diag = a
    self.prec = 1/a</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.ProcessVarGlobal.update"><code class="name flex">
<span>def <span class="ident">update</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Update the process variance</p>
<p>[
]
[
]
% These are our dynamics with the process variance
[
]
% Subtract the mean
[
]
% Substitute
[
]
[
]
[
]
This is now in a form we can use to calculate the posterior</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update(self):
    &#39;&#39;&#39;Update the process variance

    ..math:: y = (log(x_{k+1}) - log(x_k))/dt
    ..math:: Xb = a_1 (1 + \\gamma) + A x

    % These are our dynamics with the process variance
    ..math:: y ~ Normal(Xb , \\sigma^2_w / dt)

    % Subtract the mean
    ..math:: y - Xb ~ Normal( 0, \sigma^2_w / dt)
    
    % Substitute
    ..math:: z = y - Xb 

    ..math:: z ~ Normal (0, \\sigma^2_w / dt)
    ..math:: z * \\sqrt{dt} ~ Normal (0, \\sigma^2_w)

    This is now in a form we can use to calculate the posterior
    &#39;&#39;&#39;
    if self._there_are_perturbations:
        lhs = [
            STRNAMES.GROWTH_VALUE, 
            STRNAMES.SELF_INTERACTION_VALUE,
            STRNAMES.CLUSTER_INTERACTION_VALUE]
    else:
        lhs = [
            STRNAMES.GROWTH_VALUE, 
            STRNAMES.SELF_INTERACTION_VALUE,
            STRNAMES.CLUSTER_INTERACTION_VALUE]
    
    # This is the residual z = y - Xb
    z = self.G.data.construct_lhs(lhs, 
        kwargs_dict={STRNAMES.GROWTH_VALUE:{
            &#39;with_perturbations&#39;: self._there_are_perturbations}})
    z = np.asarray(z).ravel()
    if self.G.data.zero_inflation_transition_policy is not None:
        z = z * self.G.data.sqrt_dt_vec[self.G.data.rows_to_include_zero_inflation]
    else:
        z = z * self.G.data.sqrt_dt_vec
    residual = np.sum(np.square(z))

    self.dof.value = self.prior.dof.value + len(z)
    self.scale.value = ((self.prior.scale.value * self.prior.dof.value) + \
       residual)/self.dof.value
    
    # shape = 2 + (len(residual)/2)
    # scale = 0.00001 + np.sum(np.square(residual))/2
    # self.value = pl.random.invgamma.sample(shape=shape, scale=scale)
    self.sample()
    self.rebuild_diag()

    self._strr = &#39;{}, empirical_variance: {:.5f}&#39;.format(self.value, 
        residual/len(z))</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.ProcessVarGlobal.visualize"><code class="name flex">
<span>def <span class="ident">visualize</span></span>(<span>self, path: str, section: str = 'posterior') ‑> Any</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def visualize(self, path: str, section: str=&#39;posterior&#39;) -&gt; Any:
    return _scalar_visualize(self, path=path, f=None, section=section)</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="mdsine2.pylab.variables.SICS" href="pylab/variables.html#mdsine2.pylab.variables.SICS">SICS</a></b></code>:
<ul class="hlist">
<li><code><a title="mdsine2.pylab.variables.SICS.T" href="pylab/variables.html#mdsine2.pylab.variables.Variable.T">T</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.add_child" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_child">add_child</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.add_parent" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_parent">add_parent</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.add_prior" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_prior">add_prior</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.add_trace" href="pylab/base.html#mdsine2.pylab.base.Traceable.add_trace">add_trace</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.add_undirected" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_undirected">add_undirected</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.degree" href="pylab/graph.html#mdsine2.pylab.graph.Node.degree">degree</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.delete" href="pylab/graph.html#mdsine2.pylab.graph.BaseNode.delete">delete</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.get_adjacent_keys" href="pylab/graph.html#mdsine2.pylab.graph.Node.get_adjacent_keys">get_adjacent_keys</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.get_iter" href="pylab/base.html#mdsine2.pylab.base.Traceable.get_iter">get_iter</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.get_trace_from_disk" href="pylab/base.html#mdsine2.pylab.base.Traceable.get_trace_from_disk">get_trace_from_disk</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.load" href="pylab/base.html#mdsine2.pylab.base.Saveable.load">load</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.logpdf" href="pylab/variables.html#mdsine2.pylab.variables.SICS.logpdf">logpdf</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.metropolis" href="pylab/graph.html#mdsine2.pylab.graph.Node.metropolis">metropolis</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.overwrite_entire_trace_on_disk" href="pylab/base.html#mdsine2.pylab.base.Traceable.overwrite_entire_trace_on_disk">overwrite_entire_trace_on_disk</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.pdf" href="pylab/variables.html#mdsine2.pylab.variables.SICS.pdf">pdf</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.remove_local_trace" href="pylab/variables.html#mdsine2.pylab.variables.Variable.remove_local_trace">remove_local_trace</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.sample" href="pylab/variables.html#mdsine2.pylab.variables.SICS.sample">sample</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.save" href="pylab/base.html#mdsine2.pylab.base.Saveable.save">save</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.set_save_location" href="pylab/base.html#mdsine2.pylab.base.Saveable.set_save_location">set_save_location</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.set_trace" href="pylab/base.html#mdsine2.pylab.base.Traceable.set_trace">set_trace</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.set_value_shape" href="pylab/variables.html#mdsine2.pylab.variables.Variable.set_value_shape">set_value_shape</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="mdsine2.posterior.SelfInteractions"><code class="flex name class">
<span>class <span class="ident">SelfInteractions</span></span>
<span>(</span><span>prior: <a title="mdsine2.pylab.variables.TruncatedNormal" href="pylab/variables.html#mdsine2.pylab.variables.TruncatedNormal">TruncatedNormal</a>, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>self-interactions of Lotka-Voltera</p>
<p>Since our dynamics subtract this parameter, this parameter must be positive</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>prior</code></strong> :&ensp;<code>mdsine2.variables.TruncatedNormal</code></dt>
<dd>Prior distribution</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SelfInteractions(pl.variables.TruncatedNormal):
    &#39;&#39;&#39;self-interactions of Lotka-Voltera

    Since our dynamics subtract this parameter, this parameter must be positive

    Parameters
    ----------
    prior : mdsine2.variables.TruncatedNormal
        Prior distribution
    &#39;&#39;&#39;
    def __init__(self, prior: variables.TruncatedNormal, **kwargs):
        kwargs[&#39;name&#39;] = STRNAMES.SELF_INTERACTION_VALUE
        pl.variables.TruncatedNormal.__init__(self, loc=None, scale2=None, low=0.,
            high=float(&#39;inf&#39;), dtype=float, **kwargs)
        self.set_value_shape(shape=(len(self.G.data.taxa),))
        self.add_prior(prior)

    def __str__(self) -&gt; str:
        return str(self.value)

    def update_str(self):
        return

    def initialize(self, value_option: str, truncation_settings: Union[str, Tuple[float, float]],
        value: np.ndarray=None, delay: int=0, q: float=None, rescale_value: float=None):
        &#39;&#39;&#39;Initialize the self-interactions values and hyperparamters

        Parameters
        ----------
        value_option : str
            - How to initialize the values.
            - Options:
               - &#39;manual&#39;
                    - Set the values manually. `value` must also be specified.
                - &#39;fixed-growth&#39;
                    - Fix the growth values and then sample the self-interactions
                - &#39;strict-enforcement-partial&#39;
                    - Do an unregularized regression then take the absolute value of the numbers.
                    - We assume there are no interactions and we index out the time points that have
                      perturbations in them. We assume that we do not know the growths (the growths
                      are being regressed as well).
                - &#39;strict-enforcement-full&#39;
                    - Do an unregularized regression then take the absolute value of the numbers.
                    - We assume there are no interactions and we index out the time points that have
                      perturbations in them. We assume that we know the growths (the growths are on
                      the lhs)
                - &#39;steady-state&#39;, &#39;auto&#39;
                    - Set to the steady state values. Must also provide the quantile with the
                      parameter `q`. In here we assume that the steady state is the `q`th quantile
                      of the off perturbation data
                - &#39;prior-mean&#39;
                    - Set the value to the mean of the prior
        truncation_settings : str, 2-tuple
            - How to set the truncations for the normal distribution
            - (low,high)
                - These are the low and high values
            - &#39;negative&#39;
                - Truncated (-inf, 0)
            - &#39;positive&#39;, &#39;auto&#39;
                - Truncated (0, inf)
            - &#39;human&#39;
                - This assumes that the range of the steady state abundances of the human gut
                  fluctuate between 1e2 and  1e13. We requie that the growth value be initialized first.
                - We set the vlaues to be (growth.high/1e14, growth.low/1e2)
            - &#39;mouse&#39;
                - This assumes that the range of the steady state abundances of the mouse gut
                  fluctuate between 1e2 and  1e12. We requie that the growth value be initialized first.
                - We set the vlaues to be (growth.high/1e13, growth.low/1e2)
        value : array
            Only necessary if `value_option` is &#39;manual&#39;
        mean : array
            Only necessary if `mean_option` is &#39;manual&#39;
        delay : int
            How many MCMC iterations to delay starting to update
        rescale_value : None, float
            - This is the rescale value of the qPCR. This will rescale the truncation settings.
            - This is only used for either the &#39;mouse&#39; or &#39;human&#39; settings
        &#39;&#39;&#39;
        self._there_are_perturbations = self.G.perturbations is not None
        if not pl.isint(delay):
            raise TypeError(&#39;`delay` ({}) must be an int&#39;.format(type(delay)))
        if delay &lt; 0:
            raise ValueError(&#39;`delay` ({}) must be &gt;= 0&#39;.format(delay))
        self.delay = delay

        # Set truncation settings
        if pl.isstr(truncation_settings):
            # if truncation_settings == &#39;negative&#39;:
            #     self.low = float(&#39;-inf&#39;)
            #     self.high= 0
            if truncation_settings in [&#39;mouse&#39;, &#39;human&#39;]:
                growth = self.G[STRNAMES.GROWTH_VALUE]
                if not growth._initialized:
                    raise ValueError(&#39;Growth values `{}` must be initialized first&#39;.format(
                        STRNAMES.GROWTH_VALUE))
                if truncation_settings == &#39;mouse&#39;:
                    high = 1e13
                else:
                    high = 1e14
                low = 1e2
                if rescale_value is not None:
                    if not pl.isnumeric(rescale_value):
                        raise TypeError(&#39;`rescale_value` ({}) must be a numeric&#39;.format(
                            type(rescale_value)))
                    if rescale_value &lt;= 0:
                        raise ValueError(&#39;`rescale_value` ({}) must be &gt; 0&#39;.format(rescale_value))
                    high *= rescale_value
                    low *= rescale_value
                self.low = growth.high/low
                self.high = growth.low/high
            elif truncation_settings in [&#39;auto&#39;, &#39;positive&#39;]:
                self.low = 0
                self.high = float(&#39;inf&#39;)
            else:
                raise ValueError(&#39;`truncation_settings) ({}) not recognized&#39;.format(
                    truncation_settings))
        elif pl.istuple(truncation_settings):
            if len(truncation_settings) != 2:
                raise ValueError(&#39;If `truncation_settings` is a tuple, it must have a &#39; \
                    &#39;length of 2 ({})&#39;.format(len(truncation_settings)))
            l,h = truncation_settings

            if (not pl.isnumeric(l)) or (not pl.isnumeric(h)):
                raise TypeError(&#39;`low` ({}) and `high` ({}) must be numerics&#39;.format(
                    type(l), type(h)))
            if l &lt; 0 or h &lt; 0:
                raise ValueError(&#39;`low` ({}) and `high` ({}) must be &gt;= 0&#39;.format(l,h))
            if h &lt;= l:
                raise ValueError(&#39;`low` ({}) must be strictly less than high ({})&#39;.format(l,h))
            self.high = h
            self.low = l
        else:
            raise TypeError(&#39;`truncation_settings` ({}) must be a tuple or str&#39;.format(
                type(truncation_settings)))

        # Set value option
        if not pl.isstr(value_option):
            raise TypeError(&#39;`value_option` ({}) must be a str&#39;.format(type(value_option)))
        if value_option == &#39;manual&#39;:
            if not pl.isarray(value):
                value = np.ones(len(self.G.data.taxa))*value
            if len(value) != self.G.data.n_taxa:
                raise ValueError(&#39;`value` ({}) must be ({}) long&#39;.format(
                    len(value), len(self.G.data.taxa)))
            self.value = value
        elif value_option == &#39;fixed-growth&#39;:
            X = self.G.data.construct_rhs(keys=[STRNAMES.SELF_INTERACTION_VALUE],
                index_out_perturbations=True)
            y = self.G.data.construct_lhs(keys=[STRNAMES.GROWTH_VALUE], kwargs_dict={
                STRNAMES.GROWTH_VALUE:{&#39;with_perturbations&#39;:False}},
                index_out_perturbations=True)
            prec = X.T @ X
            cov = pinv(prec, self)
            self.value = np.absolute((cov @ X.transpose().dot(y)).ravel())
        elif &#39;strict-enforcement&#39; in value_option:
            if &#39;full&#39; in value_option:
                rhs = [STRNAMES.SELF_INTERACTION_VALUE]
                lhs = [STRNAMES.GROWTH_VALUE]
            elif &#39;partial&#39; in value_option:
                lhs = []
                rhs = [STRNAMES.GROWTH_VALUE, STRNAMES.SELF_INTERACTION_VALUE]
            else:
                raise ValueError(&#39;`value_option` ({}) not recognized&#39;.format(value_option))
            X = self.G.data.construct_rhs(
                keys=rhs, kwargs_dict={STRNAMES.GROWTH_VALUE:{&#39;with_perturbations&#39;:False}},
                index_out_perturbations=True)
            y = self.G.data.construct_lhs(keys=lhs, index_out_perturbations=True)

            prec = X.T @ X
            cov = pinv(prec, self)
            mean = (cov @ X.transpose().dot(y)).ravel()
            self.value = np.absolute(mean[len(self.G.data.taxa):])
        elif value_option == &#39;prior-mean&#39;:
            self.value = self.prior.loc.value * np.ones(self.G.data.n_taxa)
        elif value_option in [&#39;steady-state&#39;, &#39;auto&#39;]:
            # check quantile
            if not pl.isnumeric(q):
                raise TypeError(&#39;`q` ({}) must be numeric&#39;.format(type(q)))
            if q &lt; 0 or q &gt; 1:
                raise ValueError(&#39;`q` ({}) must be [0,1]&#39;.format(q))

            # Get the data off perturbation
            datas = None
            for ridx in range(self.G.data.n_replicates):
                if self._there_are_perturbations:
                    # Exclude the data thats in a perturbation
                    base_idx = 0
                    for start,end in self.G.data.tidxs_in_perturbation[ridx]:
                        if datas is None:
                            datas = self.G.data.data[ridx][:,base_idx:start]
                        else:
                            datas = np.hstack((datas, self.G.data.data[ridx][:,base_idx:start]))
                        base_idx = end
                    if end != self.G.data.data[ridx].shape[1]:
                        datas = np.hstack((datas, self.G.data.data[ridx][:,base_idx:]))
                else:
                    if datas is None:
                        datas = self.G.data.data[ridx]
                    else:
                        datas = np.hstack((datas, self.G.data.data[ridx]))

            # Set the steady-state for each Taxa
            ss = np.quantile(datas, q=q, axis=1)

            # Get the self-interactions by using the values of the growth terms
            self.value = 1/ss
        elif value_option == &#39;linear-regression&#39;:
            
            rhs = [STRNAMES.SELF_INTERACTION_VALUE]
            lhs = [STRNAMES.GROWTH_VALUE]
            X = self.G.data.construct_rhs(keys=rhs,
                index_out_perturbations=True)
            y = self.G.data.construct_lhs(keys=lhs, index_out_perturbations=True,
                kwargs_dict={STRNAMES.GROWTH_VALUE:{&#39;with_perturbations&#39;:False}})

            prec = X.T @ X
            cov = pinv(prec, self)
            mean = cov @ X.T @ y
            self.value = np.asarray(mean).ravel()
        else:
            raise ValueError(&#39;`value_option` ({}) not recognized&#39;.format(value_option))

        logging.info(&#39;Self-interactions value initialization: {}&#39;.format(self.value))
        logging.info(&#39;Self-interactions truncation settings: {}&#39;.format((self.low, self.high)))

    def update(self):
        if self.sample_iter &lt; self.delay:
            return

        self.calculate_posterior()
        self.sample()

        if not pl.isarray(self.value):
            # This will happen if there is 1 Taxa
            self.value = np.array([self.value])

        if np.any(np.isnan(self.value)):
            logging.critical(&#39;mean: {}&#39;.format(self.loc.value))
            logging.critical(&#39;var: {}&#39;.format(self.scale2.value))
            logging.critical(&#39;value: {}&#39;.format(self.value))
            raise ValueError(&#39;`Values in {} are nan: {}&#39;.format(self.name, self.value))

    def calculate_posterior(self):

        rhs = [STRNAMES.SELF_INTERACTION_VALUE]
        if self._there_are_perturbations:
            lhs = [
                STRNAMES.GROWTH_VALUE,
                STRNAMES.CLUSTER_INTERACTION_VALUE]
        else:
            lhs = [
                STRNAMES.GROWTH_VALUE,
                STRNAMES.CLUSTER_INTERACTION_VALUE]
        X = self.G.data.construct_rhs(keys=rhs)
        y = self.G.data.construct_lhs(keys=lhs, kwargs_dict={STRNAMES.GROWTH_VALUE:{
                &#39;with_perturbations&#39;:self._there_are_perturbations}})
        process_prec = self.G[STRNAMES.PROCESSVAR].build_matrix(
            cov=False, sparse=True)
        prior_prec = build_prior_covariance(G=self.G, cov=False,
            order=rhs, sparse=True)

        pm = prior_prec @ (self.prior.loc.value * np.ones(self.G.data.n_taxa).reshape(-1,1))

        prec = X.T @ process_prec @ X + prior_prec
        cov = pinv(prec, self)
        self.loc.value = np.asarray(cov @ (X.T @ process_prec.dot(y) + pm)).ravel()
        self.scale2.value = np.diag(cov)

    def visualize(self, basepath: str, section: str=&#39;posterior&#39;, taxa_formatter: str=&#39;%(name)s&#39;, 
        true_value: np.ndarray=None) -&gt; pd.DataFrame:
        &#39;&#39;&#39;Render the traces in the folder `basepath`. Makes a `pandas.DataFrame` table
        where the index is the Taxa name in `taxa_formatter` and the columns are
        `mean`, `median`, `25th percentile`, `75th` percentile`.

        Parameters
        ----------
        basepath : str
            This is the loction to write the files to
        section : str
            Section of the trace to compute on. Options:
                &#39;posterior&#39; : posterior samples
                &#39;burnin&#39; : burn-in samples
                &#39;entire&#39; : both burn-in and posterior samples
        taxa_formatter : str, None
            This is the format of the label to return for each Taxa. If None, it will return
            the taxon&#39;s name
        true_value : np.ndarray
            Ground truth values of the variable

        Returns
        -------
        pandas.DataFrame
        &#39;&#39;&#39;
        if not self.G.inference.tracer.is_being_traced(self):
            logging.info(&#39;`{}` not learned\n\tValue: {}\n&#39;.format(self.name, self.value))
            return pd.DataFrame()

        taxa = self.G.data.subjects.taxa
        summ = pl.summary(self, section=section)
        data = []
        index = []
        columns = [&#39;name&#39;] + [k for k in summ]

        for idx in range(len(taxa)):
            if taxa_formatter is not None:
                prefix = pl.taxaname_formatter(format=taxa_formatter, taxon=taxa[idx], taxa=taxa)
            else:
                prefix = taxa[idx].name
            index.append(taxa[idx].name)
            temp = [prefix]
            for _,v in summ.items():
                temp.append(v[idx])
            data.append(temp)
        
        df = pd.DataFrame(data, columns=columns, index=index)

        if section == &#39;posterior&#39;:
            len_posterior = self.G.inference.sample_iter + 1 - self.G.inference.burnin
        elif section == &#39;burnin&#39;:
            len_posterior = self.G.inference.burnin
        else:
            len_posterior = self.G.inference.sample_iter + 1

        # Plot the prior on top of the posterior
        if self.G.tracer.is_being_traced(STRNAMES.PRIOR_MEAN_GROWTH):
            prior_mean_trace = self.G[STRNAMES.PRIOR_MEAN_GROWTH].get_trace_from_disk(
                    section=section)
        else:
            prior_mean_trace = self.prior.loc.value * np.ones(len_posterior, dtype=float)
        if self.G.tracer.is_being_traced(STRNAMES.PRIOR_VAR_GROWTH):
            prior_std_trace = np.sqrt(
                self.G[STRNAMES.PRIOR_VAR_GROWTH].get_trace_from_disk(section=section))
        else:
            prior_std_trace = np.sqrt(self.prior.scale2.value) * np.ones(len_posterior, dtype=float)

        for idx in range(len(taxa)):
            fig = plt.figure()
            ax_posterior = fig.add_subplot(1,2,1)
            visualization.render_trace(var=self, idx=idx, plt_type=&#39;hist&#39;,
                label=section, color=&#39;blue&#39;, ax=ax_posterior, section=section,
                include_burnin=True, rasterized=True)

            # Get the limits and only look at the posterior within 20% range +- of
            # this number
            low_x, high_x = ax_posterior.get_xlim()

            arr = np.zeros(len(prior_std_trace), dtype=float)
            for i in range(len(prior_std_trace)):
                arr[i] = pl.random.truncnormal.sample(loc=prior_mean_trace[i], scale=prior_std_trace[i], 
                    low=self.low, high=self.high)
            visualization.render_trace(var=arr, plt_type=&#39;hist&#39;, 
                label=&#39;prior&#39;, color=&#39;red&#39;, ax=ax_posterior, rasterized=True)

            if true_value is not None:
                ax_posterior.axvline(x=true_value[idx], color=&#39;red&#39;, alpha=0.65, 
                    label=&#39;True Value&#39;)

            ax_posterior.legend()
            ax_posterior.set_xlim(left=low_x*.8, right=high_x*1.2)

            # plot the trace
            ax_trace = fig.add_subplot(1,2,2)
            visualization.render_trace(var=self, idx=idx, plt_type=&#39;trace&#39;, 
                ax=ax_trace, section=section, include_burnin=True, rasterized=True)

            if true_value is not None:
                ax_trace.axhline(y=true_value[idx], color=&#39;red&#39;, alpha=0.65, 
                    label=&#39;True Value&#39;)
                ax_trace.legend()

            fig.suptitle(&#39;{}&#39;.format(index[idx]))
            fig.tight_layout()
            fig.subplots_adjust(top=0.85)
            plt.savefig(os.path.join(basepath, &#39;{}.pdf&#39;.format(taxa[idx].name)))
            plt.close()

        return df</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="mdsine2.pylab.variables.TruncatedNormal" href="pylab/variables.html#mdsine2.pylab.variables.TruncatedNormal">TruncatedNormal</a></li>
<li><a title="mdsine2.pylab.variables.Variable" href="pylab/variables.html#mdsine2.pylab.variables.Variable">Variable</a></li>
<li><a title="mdsine2.pylab.graph.Node" href="pylab/graph.html#mdsine2.pylab.graph.Node">Node</a></li>
<li><a title="mdsine2.pylab.graph.BaseNode" href="pylab/graph.html#mdsine2.pylab.graph.BaseNode">BaseNode</a></li>
<li><a title="mdsine2.pylab.base.Saveable" href="pylab/base.html#mdsine2.pylab.base.Saveable">Saveable</a></li>
<li>mdsine2.pylab.variables._BaseArithmeticClass</li>
<li><a title="mdsine2.pylab.base.Traceable" href="pylab/base.html#mdsine2.pylab.base.Traceable">Traceable</a></li>
<li>mdsine2.pylab.variables._RandomBase</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="mdsine2.posterior.SelfInteractions.calculate_posterior"><code class="name flex">
<span>def <span class="ident">calculate_posterior</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def calculate_posterior(self):

    rhs = [STRNAMES.SELF_INTERACTION_VALUE]
    if self._there_are_perturbations:
        lhs = [
            STRNAMES.GROWTH_VALUE,
            STRNAMES.CLUSTER_INTERACTION_VALUE]
    else:
        lhs = [
            STRNAMES.GROWTH_VALUE,
            STRNAMES.CLUSTER_INTERACTION_VALUE]
    X = self.G.data.construct_rhs(keys=rhs)
    y = self.G.data.construct_lhs(keys=lhs, kwargs_dict={STRNAMES.GROWTH_VALUE:{
            &#39;with_perturbations&#39;:self._there_are_perturbations}})
    process_prec = self.G[STRNAMES.PROCESSVAR].build_matrix(
        cov=False, sparse=True)
    prior_prec = build_prior_covariance(G=self.G, cov=False,
        order=rhs, sparse=True)

    pm = prior_prec @ (self.prior.loc.value * np.ones(self.G.data.n_taxa).reshape(-1,1))

    prec = X.T @ process_prec @ X + prior_prec
    cov = pinv(prec, self)
    self.loc.value = np.asarray(cov @ (X.T @ process_prec.dot(y) + pm)).ravel()
    self.scale2.value = np.diag(cov)</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.SelfInteractions.initialize"><code class="name flex">
<span>def <span class="ident">initialize</span></span>(<span>self, value_option: str, truncation_settings: Union[str, Tuple[float, float]], value: numpy.ndarray = None, delay: int = 0, q: float = None, rescale_value: float = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Initialize the self-interactions values and hyperparamters</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>value_option</code></strong> :&ensp;<code>str</code></dt>
<dd>
<ul>
<li>How to initialize the values.</li>
<li>Options:</li>
<li>'manual'
- Set the values manually. <code>value</code> must also be specified.<ul>
<li>'fixed-growth'<ul>
<li>Fix the growth values and then sample the self-interactions</li>
</ul>
</li>
<li>'strict-enforcement-partial'<ul>
<li>Do an unregularized regression then take the absolute value of the numbers.</li>
<li>We assume there are no interactions and we index out the time points that have
perturbations in them. We assume that we do not know the growths (the growths
are being regressed as well).</li>
</ul>
</li>
<li>'strict-enforcement-full'<ul>
<li>Do an unregularized regression then take the absolute value of the numbers.</li>
<li>We assume there are no interactions and we index out the time points that have
perturbations in them. We assume that we know the growths (the growths are on
the lhs)</li>
</ul>
</li>
<li>'steady-state', 'auto'<ul>
<li>Set to the steady state values. Must also provide the quantile with the
parameter <code>q</code>. In here we assume that the steady state is the <code>q</code>th quantile
of the off perturbation data</li>
</ul>
</li>
<li>'prior-mean'<ul>
<li>Set the value to the mean of the prior</li>
</ul>
</li>
</ul>
</li>
</ul>
</dd>
<dt><strong><code>truncation_settings</code></strong> :&ensp;<code>str, 2-tuple</code></dt>
<dd>
<ul>
<li>How to set the truncations for the normal distribution</li>
<li>(low,high)<ul>
<li>These are the low and high values</li>
</ul>
</li>
<li>'negative'<ul>
<li>Truncated (-inf, 0)</li>
</ul>
</li>
<li>'positive', 'auto'<ul>
<li>Truncated (0, inf)</li>
</ul>
</li>
<li>'human'<ul>
<li>This assumes that the range of the steady state abundances of the human gut
fluctuate between 1e2 and
1e13. We requie that the growth value be initialized first.</li>
<li>We set the vlaues to be (growth.high/1e14, growth.low/1e2)</li>
</ul>
</li>
<li>'mouse'<ul>
<li>This assumes that the range of the steady state abundances of the mouse gut
fluctuate between 1e2 and
1e12. We requie that the growth value be initialized first.</li>
<li>We set the vlaues to be (growth.high/1e13, growth.low/1e2)</li>
</ul>
</li>
</ul>
</dd>
<dt><strong><code>value</code></strong> :&ensp;<code>array</code></dt>
<dd>Only necessary if <code>value_option</code> is 'manual'</dd>
<dt><strong><code>mean</code></strong> :&ensp;<code>array</code></dt>
<dd>Only necessary if <code>mean_option</code> is 'manual'</dd>
<dt><strong><code>delay</code></strong> :&ensp;<code>int</code></dt>
<dd>How many MCMC iterations to delay starting to update</dd>
<dt><strong><code>rescale_value</code></strong> :&ensp;<code>None, float</code></dt>
<dd>
<ul>
<li>This is the rescale value of the qPCR. This will rescale the truncation settings.</li>
<li>This is only used for either the 'mouse' or 'human' settings</li>
</ul>
</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def initialize(self, value_option: str, truncation_settings: Union[str, Tuple[float, float]],
    value: np.ndarray=None, delay: int=0, q: float=None, rescale_value: float=None):
    &#39;&#39;&#39;Initialize the self-interactions values and hyperparamters

    Parameters
    ----------
    value_option : str
        - How to initialize the values.
        - Options:
           - &#39;manual&#39;
                - Set the values manually. `value` must also be specified.
            - &#39;fixed-growth&#39;
                - Fix the growth values and then sample the self-interactions
            - &#39;strict-enforcement-partial&#39;
                - Do an unregularized regression then take the absolute value of the numbers.
                - We assume there are no interactions and we index out the time points that have
                  perturbations in them. We assume that we do not know the growths (the growths
                  are being regressed as well).
            - &#39;strict-enforcement-full&#39;
                - Do an unregularized regression then take the absolute value of the numbers.
                - We assume there are no interactions and we index out the time points that have
                  perturbations in them. We assume that we know the growths (the growths are on
                  the lhs)
            - &#39;steady-state&#39;, &#39;auto&#39;
                - Set to the steady state values. Must also provide the quantile with the
                  parameter `q`. In here we assume that the steady state is the `q`th quantile
                  of the off perturbation data
            - &#39;prior-mean&#39;
                - Set the value to the mean of the prior
    truncation_settings : str, 2-tuple
        - How to set the truncations for the normal distribution
        - (low,high)
            - These are the low and high values
        - &#39;negative&#39;
            - Truncated (-inf, 0)
        - &#39;positive&#39;, &#39;auto&#39;
            - Truncated (0, inf)
        - &#39;human&#39;
            - This assumes that the range of the steady state abundances of the human gut
              fluctuate between 1e2 and  1e13. We requie that the growth value be initialized first.
            - We set the vlaues to be (growth.high/1e14, growth.low/1e2)
        - &#39;mouse&#39;
            - This assumes that the range of the steady state abundances of the mouse gut
              fluctuate between 1e2 and  1e12. We requie that the growth value be initialized first.
            - We set the vlaues to be (growth.high/1e13, growth.low/1e2)
    value : array
        Only necessary if `value_option` is &#39;manual&#39;
    mean : array
        Only necessary if `mean_option` is &#39;manual&#39;
    delay : int
        How many MCMC iterations to delay starting to update
    rescale_value : None, float
        - This is the rescale value of the qPCR. This will rescale the truncation settings.
        - This is only used for either the &#39;mouse&#39; or &#39;human&#39; settings
    &#39;&#39;&#39;
    self._there_are_perturbations = self.G.perturbations is not None
    if not pl.isint(delay):
        raise TypeError(&#39;`delay` ({}) must be an int&#39;.format(type(delay)))
    if delay &lt; 0:
        raise ValueError(&#39;`delay` ({}) must be &gt;= 0&#39;.format(delay))
    self.delay = delay

    # Set truncation settings
    if pl.isstr(truncation_settings):
        # if truncation_settings == &#39;negative&#39;:
        #     self.low = float(&#39;-inf&#39;)
        #     self.high= 0
        if truncation_settings in [&#39;mouse&#39;, &#39;human&#39;]:
            growth = self.G[STRNAMES.GROWTH_VALUE]
            if not growth._initialized:
                raise ValueError(&#39;Growth values `{}` must be initialized first&#39;.format(
                    STRNAMES.GROWTH_VALUE))
            if truncation_settings == &#39;mouse&#39;:
                high = 1e13
            else:
                high = 1e14
            low = 1e2
            if rescale_value is not None:
                if not pl.isnumeric(rescale_value):
                    raise TypeError(&#39;`rescale_value` ({}) must be a numeric&#39;.format(
                        type(rescale_value)))
                if rescale_value &lt;= 0:
                    raise ValueError(&#39;`rescale_value` ({}) must be &gt; 0&#39;.format(rescale_value))
                high *= rescale_value
                low *= rescale_value
            self.low = growth.high/low
            self.high = growth.low/high
        elif truncation_settings in [&#39;auto&#39;, &#39;positive&#39;]:
            self.low = 0
            self.high = float(&#39;inf&#39;)
        else:
            raise ValueError(&#39;`truncation_settings) ({}) not recognized&#39;.format(
                truncation_settings))
    elif pl.istuple(truncation_settings):
        if len(truncation_settings) != 2:
            raise ValueError(&#39;If `truncation_settings` is a tuple, it must have a &#39; \
                &#39;length of 2 ({})&#39;.format(len(truncation_settings)))
        l,h = truncation_settings

        if (not pl.isnumeric(l)) or (not pl.isnumeric(h)):
            raise TypeError(&#39;`low` ({}) and `high` ({}) must be numerics&#39;.format(
                type(l), type(h)))
        if l &lt; 0 or h &lt; 0:
            raise ValueError(&#39;`low` ({}) and `high` ({}) must be &gt;= 0&#39;.format(l,h))
        if h &lt;= l:
            raise ValueError(&#39;`low` ({}) must be strictly less than high ({})&#39;.format(l,h))
        self.high = h
        self.low = l
    else:
        raise TypeError(&#39;`truncation_settings` ({}) must be a tuple or str&#39;.format(
            type(truncation_settings)))

    # Set value option
    if not pl.isstr(value_option):
        raise TypeError(&#39;`value_option` ({}) must be a str&#39;.format(type(value_option)))
    if value_option == &#39;manual&#39;:
        if not pl.isarray(value):
            value = np.ones(len(self.G.data.taxa))*value
        if len(value) != self.G.data.n_taxa:
            raise ValueError(&#39;`value` ({}) must be ({}) long&#39;.format(
                len(value), len(self.G.data.taxa)))
        self.value = value
    elif value_option == &#39;fixed-growth&#39;:
        X = self.G.data.construct_rhs(keys=[STRNAMES.SELF_INTERACTION_VALUE],
            index_out_perturbations=True)
        y = self.G.data.construct_lhs(keys=[STRNAMES.GROWTH_VALUE], kwargs_dict={
            STRNAMES.GROWTH_VALUE:{&#39;with_perturbations&#39;:False}},
            index_out_perturbations=True)
        prec = X.T @ X
        cov = pinv(prec, self)
        self.value = np.absolute((cov @ X.transpose().dot(y)).ravel())
    elif &#39;strict-enforcement&#39; in value_option:
        if &#39;full&#39; in value_option:
            rhs = [STRNAMES.SELF_INTERACTION_VALUE]
            lhs = [STRNAMES.GROWTH_VALUE]
        elif &#39;partial&#39; in value_option:
            lhs = []
            rhs = [STRNAMES.GROWTH_VALUE, STRNAMES.SELF_INTERACTION_VALUE]
        else:
            raise ValueError(&#39;`value_option` ({}) not recognized&#39;.format(value_option))
        X = self.G.data.construct_rhs(
            keys=rhs, kwargs_dict={STRNAMES.GROWTH_VALUE:{&#39;with_perturbations&#39;:False}},
            index_out_perturbations=True)
        y = self.G.data.construct_lhs(keys=lhs, index_out_perturbations=True)

        prec = X.T @ X
        cov = pinv(prec, self)
        mean = (cov @ X.transpose().dot(y)).ravel()
        self.value = np.absolute(mean[len(self.G.data.taxa):])
    elif value_option == &#39;prior-mean&#39;:
        self.value = self.prior.loc.value * np.ones(self.G.data.n_taxa)
    elif value_option in [&#39;steady-state&#39;, &#39;auto&#39;]:
        # check quantile
        if not pl.isnumeric(q):
            raise TypeError(&#39;`q` ({}) must be numeric&#39;.format(type(q)))
        if q &lt; 0 or q &gt; 1:
            raise ValueError(&#39;`q` ({}) must be [0,1]&#39;.format(q))

        # Get the data off perturbation
        datas = None
        for ridx in range(self.G.data.n_replicates):
            if self._there_are_perturbations:
                # Exclude the data thats in a perturbation
                base_idx = 0
                for start,end in self.G.data.tidxs_in_perturbation[ridx]:
                    if datas is None:
                        datas = self.G.data.data[ridx][:,base_idx:start]
                    else:
                        datas = np.hstack((datas, self.G.data.data[ridx][:,base_idx:start]))
                    base_idx = end
                if end != self.G.data.data[ridx].shape[1]:
                    datas = np.hstack((datas, self.G.data.data[ridx][:,base_idx:]))
            else:
                if datas is None:
                    datas = self.G.data.data[ridx]
                else:
                    datas = np.hstack((datas, self.G.data.data[ridx]))

        # Set the steady-state for each Taxa
        ss = np.quantile(datas, q=q, axis=1)

        # Get the self-interactions by using the values of the growth terms
        self.value = 1/ss
    elif value_option == &#39;linear-regression&#39;:
        
        rhs = [STRNAMES.SELF_INTERACTION_VALUE]
        lhs = [STRNAMES.GROWTH_VALUE]
        X = self.G.data.construct_rhs(keys=rhs,
            index_out_perturbations=True)
        y = self.G.data.construct_lhs(keys=lhs, index_out_perturbations=True,
            kwargs_dict={STRNAMES.GROWTH_VALUE:{&#39;with_perturbations&#39;:False}})

        prec = X.T @ X
        cov = pinv(prec, self)
        mean = cov @ X.T @ y
        self.value = np.asarray(mean).ravel()
    else:
        raise ValueError(&#39;`value_option` ({}) not recognized&#39;.format(value_option))

    logging.info(&#39;Self-interactions value initialization: {}&#39;.format(self.value))
    logging.info(&#39;Self-interactions truncation settings: {}&#39;.format((self.low, self.high)))</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.SelfInteractions.update"><code class="name flex">
<span>def <span class="ident">update</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update(self):
    if self.sample_iter &lt; self.delay:
        return

    self.calculate_posterior()
    self.sample()

    if not pl.isarray(self.value):
        # This will happen if there is 1 Taxa
        self.value = np.array([self.value])

    if np.any(np.isnan(self.value)):
        logging.critical(&#39;mean: {}&#39;.format(self.loc.value))
        logging.critical(&#39;var: {}&#39;.format(self.scale2.value))
        logging.critical(&#39;value: {}&#39;.format(self.value))
        raise ValueError(&#39;`Values in {} are nan: {}&#39;.format(self.name, self.value))</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.SelfInteractions.update_str"><code class="name flex">
<span>def <span class="ident">update_str</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_str(self):
    return</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.SelfInteractions.visualize"><code class="name flex">
<span>def <span class="ident">visualize</span></span>(<span>self, basepath: str, section: str = 'posterior', taxa_formatter: str = '%(name)s', true_value: numpy.ndarray = None) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Render the traces in the folder <code>basepath</code>. Makes a <code>pandas.DataFrame</code> table
where the index is the Taxa name in <code>taxa_formatter</code> and the columns are
<code>mean</code>, <code>median</code>, <code>25th percentile</code>, <code>75th</code> percentile`.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>basepath</code></strong> :&ensp;<code>str</code></dt>
<dd>This is the loction to write the files to</dd>
<dt><strong><code>section</code></strong> :&ensp;<code>str</code></dt>
<dd>Section of the trace to compute on. Options:
'posterior' : posterior samples
'burnin' : burn-in samples
'entire' : both burn-in and posterior samples</dd>
<dt><strong><code>taxa_formatter</code></strong> :&ensp;<code>str, None</code></dt>
<dd>This is the format of the label to return for each Taxa. If None, it will return
the taxon's name</dd>
<dt><strong><code>true_value</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>Ground truth values of the variable</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pandas.DataFrame</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def visualize(self, basepath: str, section: str=&#39;posterior&#39;, taxa_formatter: str=&#39;%(name)s&#39;, 
    true_value: np.ndarray=None) -&gt; pd.DataFrame:
    &#39;&#39;&#39;Render the traces in the folder `basepath`. Makes a `pandas.DataFrame` table
    where the index is the Taxa name in `taxa_formatter` and the columns are
    `mean`, `median`, `25th percentile`, `75th` percentile`.

    Parameters
    ----------
    basepath : str
        This is the loction to write the files to
    section : str
        Section of the trace to compute on. Options:
            &#39;posterior&#39; : posterior samples
            &#39;burnin&#39; : burn-in samples
            &#39;entire&#39; : both burn-in and posterior samples
    taxa_formatter : str, None
        This is the format of the label to return for each Taxa. If None, it will return
        the taxon&#39;s name
    true_value : np.ndarray
        Ground truth values of the variable

    Returns
    -------
    pandas.DataFrame
    &#39;&#39;&#39;
    if not self.G.inference.tracer.is_being_traced(self):
        logging.info(&#39;`{}` not learned\n\tValue: {}\n&#39;.format(self.name, self.value))
        return pd.DataFrame()

    taxa = self.G.data.subjects.taxa
    summ = pl.summary(self, section=section)
    data = []
    index = []
    columns = [&#39;name&#39;] + [k for k in summ]

    for idx in range(len(taxa)):
        if taxa_formatter is not None:
            prefix = pl.taxaname_formatter(format=taxa_formatter, taxon=taxa[idx], taxa=taxa)
        else:
            prefix = taxa[idx].name
        index.append(taxa[idx].name)
        temp = [prefix]
        for _,v in summ.items():
            temp.append(v[idx])
        data.append(temp)
    
    df = pd.DataFrame(data, columns=columns, index=index)

    if section == &#39;posterior&#39;:
        len_posterior = self.G.inference.sample_iter + 1 - self.G.inference.burnin
    elif section == &#39;burnin&#39;:
        len_posterior = self.G.inference.burnin
    else:
        len_posterior = self.G.inference.sample_iter + 1

    # Plot the prior on top of the posterior
    if self.G.tracer.is_being_traced(STRNAMES.PRIOR_MEAN_GROWTH):
        prior_mean_trace = self.G[STRNAMES.PRIOR_MEAN_GROWTH].get_trace_from_disk(
                section=section)
    else:
        prior_mean_trace = self.prior.loc.value * np.ones(len_posterior, dtype=float)
    if self.G.tracer.is_being_traced(STRNAMES.PRIOR_VAR_GROWTH):
        prior_std_trace = np.sqrt(
            self.G[STRNAMES.PRIOR_VAR_GROWTH].get_trace_from_disk(section=section))
    else:
        prior_std_trace = np.sqrt(self.prior.scale2.value) * np.ones(len_posterior, dtype=float)

    for idx in range(len(taxa)):
        fig = plt.figure()
        ax_posterior = fig.add_subplot(1,2,1)
        visualization.render_trace(var=self, idx=idx, plt_type=&#39;hist&#39;,
            label=section, color=&#39;blue&#39;, ax=ax_posterior, section=section,
            include_burnin=True, rasterized=True)

        # Get the limits and only look at the posterior within 20% range +- of
        # this number
        low_x, high_x = ax_posterior.get_xlim()

        arr = np.zeros(len(prior_std_trace), dtype=float)
        for i in range(len(prior_std_trace)):
            arr[i] = pl.random.truncnormal.sample(loc=prior_mean_trace[i], scale=prior_std_trace[i], 
                low=self.low, high=self.high)
        visualization.render_trace(var=arr, plt_type=&#39;hist&#39;, 
            label=&#39;prior&#39;, color=&#39;red&#39;, ax=ax_posterior, rasterized=True)

        if true_value is not None:
            ax_posterior.axvline(x=true_value[idx], color=&#39;red&#39;, alpha=0.65, 
                label=&#39;True Value&#39;)

        ax_posterior.legend()
        ax_posterior.set_xlim(left=low_x*.8, right=high_x*1.2)

        # plot the trace
        ax_trace = fig.add_subplot(1,2,2)
        visualization.render_trace(var=self, idx=idx, plt_type=&#39;trace&#39;, 
            ax=ax_trace, section=section, include_burnin=True, rasterized=True)

        if true_value is not None:
            ax_trace.axhline(y=true_value[idx], color=&#39;red&#39;, alpha=0.65, 
                label=&#39;True Value&#39;)
            ax_trace.legend()

        fig.suptitle(&#39;{}&#39;.format(index[idx]))
        fig.tight_layout()
        fig.subplots_adjust(top=0.85)
        plt.savefig(os.path.join(basepath, &#39;{}.pdf&#39;.format(taxa[idx].name)))
        plt.close()

    return df</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="mdsine2.pylab.variables.TruncatedNormal" href="pylab/variables.html#mdsine2.pylab.variables.TruncatedNormal">TruncatedNormal</a></b></code>:
<ul class="hlist">
<li><code><a title="mdsine2.pylab.variables.TruncatedNormal.T" href="pylab/variables.html#mdsine2.pylab.variables.Variable.T">T</a></code></li>
<li><code><a title="mdsine2.pylab.variables.TruncatedNormal.add_child" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_child">add_child</a></code></li>
<li><code><a title="mdsine2.pylab.variables.TruncatedNormal.add_parent" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_parent">add_parent</a></code></li>
<li><code><a title="mdsine2.pylab.variables.TruncatedNormal.add_prior" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_prior">add_prior</a></code></li>
<li><code><a title="mdsine2.pylab.variables.TruncatedNormal.add_trace" href="pylab/base.html#mdsine2.pylab.base.Traceable.add_trace">add_trace</a></code></li>
<li><code><a title="mdsine2.pylab.variables.TruncatedNormal.add_undirected" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_undirected">add_undirected</a></code></li>
<li><code><a title="mdsine2.pylab.variables.TruncatedNormal.cdf" href="pylab/variables.html#mdsine2.pylab.variables.TruncatedNormal.cdf">cdf</a></code></li>
<li><code><a title="mdsine2.pylab.variables.TruncatedNormal.degree" href="pylab/graph.html#mdsine2.pylab.graph.Node.degree">degree</a></code></li>
<li><code><a title="mdsine2.pylab.variables.TruncatedNormal.delete" href="pylab/graph.html#mdsine2.pylab.graph.BaseNode.delete">delete</a></code></li>
<li><code><a title="mdsine2.pylab.variables.TruncatedNormal.get_adjacent_keys" href="pylab/graph.html#mdsine2.pylab.graph.Node.get_adjacent_keys">get_adjacent_keys</a></code></li>
<li><code><a title="mdsine2.pylab.variables.TruncatedNormal.get_iter" href="pylab/base.html#mdsine2.pylab.base.Traceable.get_iter">get_iter</a></code></li>
<li><code><a title="mdsine2.pylab.variables.TruncatedNormal.get_trace_from_disk" href="pylab/base.html#mdsine2.pylab.base.Traceable.get_trace_from_disk">get_trace_from_disk</a></code></li>
<li><code><a title="mdsine2.pylab.variables.TruncatedNormal.load" href="pylab/base.html#mdsine2.pylab.base.Saveable.load">load</a></code></li>
<li><code><a title="mdsine2.pylab.variables.TruncatedNormal.logcdf" href="pylab/variables.html#mdsine2.pylab.variables.TruncatedNormal.logcdf">logcdf</a></code></li>
<li><code><a title="mdsine2.pylab.variables.TruncatedNormal.logpdf" href="pylab/variables.html#mdsine2.pylab.variables.TruncatedNormal.logpdf">logpdf</a></code></li>
<li><code><a title="mdsine2.pylab.variables.TruncatedNormal.metropolis" href="pylab/graph.html#mdsine2.pylab.graph.Node.metropolis">metropolis</a></code></li>
<li><code><a title="mdsine2.pylab.variables.TruncatedNormal.overwrite_entire_trace_on_disk" href="pylab/base.html#mdsine2.pylab.base.Traceable.overwrite_entire_trace_on_disk">overwrite_entire_trace_on_disk</a></code></li>
<li><code><a title="mdsine2.pylab.variables.TruncatedNormal.pdf" href="pylab/variables.html#mdsine2.pylab.variables.TruncatedNormal.pdf">pdf</a></code></li>
<li><code><a title="mdsine2.pylab.variables.TruncatedNormal.remove_local_trace" href="pylab/variables.html#mdsine2.pylab.variables.Variable.remove_local_trace">remove_local_trace</a></code></li>
<li><code><a title="mdsine2.pylab.variables.TruncatedNormal.sample" href="pylab/variables.html#mdsine2.pylab.variables.TruncatedNormal.sample">sample</a></code></li>
<li><code><a title="mdsine2.pylab.variables.TruncatedNormal.save" href="pylab/base.html#mdsine2.pylab.base.Saveable.save">save</a></code></li>
<li><code><a title="mdsine2.pylab.variables.TruncatedNormal.set_save_location" href="pylab/base.html#mdsine2.pylab.base.Saveable.set_save_location">set_save_location</a></code></li>
<li><code><a title="mdsine2.pylab.variables.TruncatedNormal.set_trace" href="pylab/base.html#mdsine2.pylab.base.Traceable.set_trace">set_trace</a></code></li>
<li><code><a title="mdsine2.pylab.variables.TruncatedNormal.set_value_shape" href="pylab/variables.html#mdsine2.pylab.variables.Variable.set_value_shape">set_value_shape</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="mdsine2.posterior.SingleClusterFullParallelization"><code class="flex name class">
<span>class <span class="ident">SingleClusterFullParallelization</span></span>
<span>(</span><span>n_taxa: int, total_n_dts_per_taxon: int, n_replicates: int, n_dts_for_replicate: Union[numpy.ndarray, List[int]], there_are_perturbations: bool, keypair2col_interactions: numpy.ndarray, keypair2col_perturbations: numpy.ndarray, n_perturbations: int, base_Xrows: numpy.ndarray, base_Xcols: numpy.ndarray, base_Xshape: Tuple, base_Xpertrows: numpy.ndarray, base_Xpertcols: numpy.ndarray, base_Xpertshape: Tuple, n_rowsM: int, n_rowsMpert: int)</span>
</code></dt>
<dd>
<div class="desc"><p>Make the full parallelization
- Mixture matricies for interactions and perturbations
- calculating the marginalization for the sent in cluster</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>n_taxa</code></strong> :&ensp;<code>int</code></dt>
<dd>Total number of OTUs</dd>
<dt><strong><code>total_n_dts_per_taxon</code></strong> :&ensp;<code>int</code></dt>
<dd>Total number of time changes for each OTU</dd>
<dt><strong><code>n_replicates</code></strong> :&ensp;<code>int</code></dt>
<dd>Total number of replicates</dd>
<dt><strong><code>n_dts_for_replicate</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>Total number of time changes for each replicate</dd>
<dt><strong><code>there_are_perturbations</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, there are perturbations</dd>
<dt><strong><code>keypair2col_interactions</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>These map the OTU indices of the pairs of OTUs to their column index
in <code>big_X</code></dd>
<dt><strong><code>keypair2col_perturbations</code></strong> :&ensp;<code>np.ndarray, None</code></dt>
<dd>These map the OTU indices nad perturbation index to the column in
<code>big_Xpert</code>. If there are no perturbations then this is None</dd>
<dt><strong><code>n_perturbations</code></strong> :&ensp;<code>int, None</code></dt>
<dd>Number of perturbations. None if there are no perturbations</dd>
<dt><strong><code>base_Xrows</code></strong>, <strong><code>base_Xcols</code></strong>, <strong><code>base_Xpertrows</code></strong>, <strong><code>base_Xpertcols</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>These are the rows and columns necessary to build the interaction and perturbation
matrices, respectively. Whats passed in is the data vector and then we build
it using sparse matrices</dd>
<dt><strong><code>n_rowsM</code></strong>, <strong><code>n_rowsMpert</code></strong> :&ensp;<code>int</code></dt>
<dd>These are the number of rows for the mixing matrix for the interactions and
perturbations respectively.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SingleClusterFullParallelization(pl.multiprocessing.PersistentWorker):
    &#39;&#39;&#39;Make the full parallelization
        - Mixture matricies for interactions and perturbations
        - calculating the marginalization for the sent in cluster

    Parameters
    ----------
    n_taxa : int
        Total number of OTUs
    total_n_dts_per_taxon : int
        Total number of time changes for each OTU
    n_replicates : int
        Total number of replicates
    n_dts_for_replicate : np.ndarray
        Total number of time changes for each replicate
    there_are_perturbations : bool
        If True, there are perturbations
    keypair2col_interactions : np.ndarray
        These map the OTU indices of the pairs of OTUs to their column index
        in `big_X`
    keypair2col_perturbations : np.ndarray, None
        These map the OTU indices nad perturbation index to the column in
        `big_Xpert`. If there are no perturbations then this is None
    n_perturbations : int, None
        Number of perturbations. None if there are no perturbations
    base_Xrows, base_Xcols, base_Xpertrows, base_Xpertcols : np.ndarray
        These are the rows and columns necessary to build the interaction and perturbation 
        matrices, respectively. Whats passed in is the data vector and then we build
        it using sparse matrices
    n_rowsM, n_rowsMpert : int
        These are the number of rows for the mixing matrix for the interactions and
        perturbations respectively.
    &#39;&#39;&#39;
    def __init__(self, n_taxa: int, total_n_dts_per_taxon: int, n_replicates: int, 
        n_dts_for_replicate: Union[np.ndarray, List[int]], there_are_perturbations: bool, 
        keypair2col_interactions: np.ndarray, keypair2col_perturbations: np.ndarray,
        n_perturbations: int, base_Xrows: np.ndarray, base_Xcols: np.ndarray, base_Xshape: Tuple, 
        base_Xpertrows: np.ndarray, base_Xpertcols: np.ndarray,
        base_Xpertshape: Tuple, n_rowsM: int, n_rowsMpert: int):
        self.n_taxa = n_taxa
        self.total_n_dts_per_taxon = total_n_dts_per_taxon
        self.n_replicates = n_replicates
        self.n_dts_for_replicate = n_dts_for_replicate
        self.there_are_perturbations = there_are_perturbations
        self.keypair2col_interactions = keypair2col_interactions
        if self.there_are_perturbations:
            self.keypair2col_perturbations = keypair2col_perturbations
            self.n_perturbations = n_perturbations

        self.base_Xrows = base_Xrows
        self.base_Xcols = base_Xcols
        self.base_Xshape = base_Xshape
        self.base_Xpertrows = base_Xpertrows
        self.base_Xpertcols = base_Xpertcols
        self.base_Xpertshape = base_Xpertshape

        self.n_rowsM = n_rowsM
        self.n_rowsMpert = n_rowsMpert

    def initialize_gibbs(self, base_Xdata: scipy.sparse.spmatrix, base_Xpertdata: scipy.sparse.spmatrix, 
        concentration: float, m: int, y: np.ndarray, process_prec_diag: np.ndarray, 
        prior_var_interactions: float, prior_var_pert: np.ndarray, 
        prior_mean_interactions: float, prior_mean_pert: np.ndarray):
        &#39;&#39;&#39;Pass in the information that changes every Gibbs step

        Parameters
        ----------
        base_X : scipy.sparse.csc_matrix
            Sparse matrix for the interaction terms
        base_Xpert : scipy.sparse.csc_matrix, None
            Sparse matrix for the perturbation terms
            If None, there are no perturbations
        concentration : float
            This is the concentration of the system
        m : int
            This is the auxiliary variable for the marginalization
        y : np.ndarray
            This is the observation array
        process_prec_diag : np.ndarray
            This is the process precision diagonal
        &#39;&#39;&#39;
        self.base_X = scipy.sparse.coo_matrix(
            (base_Xdata,(self.base_Xrows,self.base_Xcols)),
            shape=self.base_Xshape).tocsc()
        
        self.concentration = concentration
        self.m = m
        self.y = y.reshape(-1,1)
        self.n_rows = len(y)
        self.n_cols_X = self.base_X.shape[1]
        self.prior_var_interactions = prior_var_interactions
        self.prior_prec_interactions = 1/prior_var_interactions
        self.prior_mean_interactions = prior_mean_interactions

        if self.there_are_perturbations:
            self.base_Xpert = scipy.sparse.coo_matrix(
                (base_Xpertdata,(self.base_Xpertrows,self.base_Xpertcols)),
                shape=self.base_Xpertshape).tocsc()
            self.prior_var_pert = prior_var_pert
            self.prior_prec_pert = 1/prior_var_pert
            self.prior_mean_pert = prior_mean_pert

        self.process_prec_matrix = scipy.sparse.dia_matrix(
            (process_prec_diag,[0]), shape=(len(process_prec_diag),len(process_prec_diag))).tocsc()

    def initialize_oidx(self, interaction_on_idxs: np.ndarray, perturbation_on_idxs: np.ndarray):
        &#39;&#39;&#39;Pass in the parameters that change for every OTU - potentially

        Parameters
        ----------
        
        &#39;&#39;&#39;
        self.saved_interaction_on_idxs = interaction_on_idxs
        self.saved_perturbation_on_idxs = perturbation_on_idxs

    # @profile
    def run(self, interaction_on_idxs: np.ndarray, perturbation_on_idxs: List[np.ndarray], 
        cluster_config: np.ndarray, log_mult_factor: float, cid: int, 
        use_saved_params: bool):
        &#39;&#39;&#39;Pass in the parameters for the specific cluster assignment for
        the OTU and run the marginalization


        Parameters
        ----------
        interaction_on_idxs : np.array(int)
            An array of indices for the interactions that are on. Assumes that the
            clustering is in the order specified in `cluster_config`
        perturbation_on_idxs : list(np.ndarray(int)), None
            If there are perturbations, then we set the perturbation idxs on
            Each element in the list are the indices of that perturbation that are on
        cluster_config : list(list(int))
            This is the cluster configuration and in cluster order.
        log_mult_factor : float
            This is the log multiplication factor that we add onto the marginalization
        use_saved_params : bool
            If True, passed in `interaction_on_idxs` and `perturbation_on_idxs` are None
            and we can use `saved_interaction_on_idxs` and `saved_perturbation_on_idxs`
        &#39;&#39;&#39;
        if use_saved_params:
            interaction_on_idxs = self.saved_interaction_on_idxs
            perturbation_on_idxs = self.saved_perturbation_on_idxs

        # We need to make the arrays for interactions and perturbations
        self.set_clustering(cluster_config=cluster_config)
        Xinteractions = self.build_interactions_matrix(on_columns=interaction_on_idxs)

        if self.there_are_perturbations:
            Xperturbations = self.build_perturbations_matrix(on_columns=perturbation_on_idxs)
            X = scipy.sparse.hstack([Xperturbations, Xinteractions])
        else:
            X = Xinteractions
        self.X = X
        self.prior_mean = self.build_prior_mean(on_interactions=interaction_on_idxs,
            on_perturbations=perturbation_on_idxs)
        self.prior_cov, self.prior_prec, self.prior_prec_diag = self.build_prior_cov_and_prec_and_diag(
            on_interactions=interaction_on_idxs, on_perturbations=perturbation_on_idxs)
        
        return cid, self.calculate_marginal_loglikelihood_slow_fast_sparse() + log_mult_factor

    def set_clustering(self, cluster_config: np.ndarray):
        self.clustering = CondensedClustering(oidx2cidx=cluster_config)
        self.iidx2cidxpair = np.zeros(shape=(len(self.clustering)*(len(self.clustering)-1), 2), 
            dtype=int)
        self.iidx2cidxpair = SingleClusterFullParallelization.make_iidx2cidxpair(
            ret=self.iidx2cidxpair,
            n_clusters=len(self.clustering))

    def build_interactions_matrix(self, on_columns: np.ndarray):
        &#39;&#39;&#39;Build the interaction matrix

        First we make the rows and columns for the mixing matrix,
        then we multiple the base matrix and the mixing matrix.
        &#39;&#39;&#39;
        rows = []
        cols = []

        # c2ciidx = Cluster-to-Cluster Interaction InDeX
        c2ciidx = 0
        for ccc in on_columns:
            tcidx = self.iidx2cidxpair[ccc, 0]
            scidx = self.iidx2cidxpair[ccc, 1]
            
            smems = self.clustering.clusters[scidx]
            tmems = self.clustering.clusters[tcidx]
            
            a = np.zeros(len(smems)*len(tmems), dtype=int)
            rows.append(SingleClusterFullParallelization.get_indices(a,
                self.keypair2col_interactions, tmems, smems))
            cols.append(np.full(len(tmems)*len(smems), fill_value=c2ciidx))
            c2ciidx += 1

        rows = np.asarray(list(itertools.chain.from_iterable(rows)))
        cols = np.asarray(list(itertools.chain.from_iterable(cols)))
        data = np.ones(len(rows), dtype=int)

        M = scipy.sparse.coo_matrix((data,(rows,cols)),
            shape=(self.n_rowsM, c2ciidx)).tocsc()
        ret = self.base_X @ M
        return ret

    def build_perturbations_matrix(self, on_columns: np.ndarray):
        if not self.there_are_perturbations:
            raise ValueError(&#39;You should not be here&#39;)
        
        keypair2col = self.keypair2col_perturbations
        rows = []
        cols = []

        col = 0
        for pidx, pert_ind_idxs in enumerate(on_columns):
            for cidx in pert_ind_idxs:
                for oidx in self.clustering.clusters[cidx]:
                    rows.append(keypair2col[oidx, pidx])
                    cols.append(col)
                col += 1
        
        data = np.ones(len(rows), dtype=np.float64)
        M = scipy.sparse.coo_matrix((data,(rows,cols)),
            shape=(self.n_rowsMpert, col)).tocsc()
        ret = self.base_Xpert @ M
        return ret

    def build_prior_mean(self, on_interactions, on_perturbations):
        &#39;&#39;&#39;Build the prior mean array

        Perturbations go first and then interactions
        &#39;&#39;&#39;
        ret = []
        for pidx, pert in enumerate(on_perturbations):
            ret = np.append(ret, 
                np.full(len(pert), fill_value=self.prior_mean_pert[pidx]))
        ret = np.append(ret, np.full(len(on_interactions), fill_value=self.prior_mean_interactions))
        return ret.reshape(-1,1)

    def build_prior_cov_and_prec_and_diag(self, on_interactions, on_perturbations):
        &#39;&#39;&#39;Build the prior covariance matrices and others
        &#39;&#39;&#39;
        ret = []
        for pidx, pert in enumerate(on_perturbations):
            ret = np.append(ret, 
                np.full(len(pert), fill_value=self.prior_var_pert[pidx]))
        prior_var_diag = np.append(ret, np.full(len(on_interactions), fill_value=self.prior_var_interactions))
        prior_prec_diag = 1/prior_var_diag

        prior_var = scipy.sparse.dia_matrix((prior_var_diag,[0]), 
            shape=(len(prior_var_diag),len(prior_var_diag))).tocsc()
        prior_prec = scipy.sparse.dia_matrix((prior_prec_diag,[0]), 
            shape=(len(prior_prec_diag),len(prior_prec_diag))).tocsc()

        return prior_var, prior_prec, prior_prec_diag

    # @profile
    def calculate_marginal_loglikelihood_slow_fast_sparse(self):
        y = self.y
        X = self.X
        process_prec = self.process_prec_matrix
        prior_mean = self.prior_mean
        prior_cov = self.prior_cov
        prior_prec = self.prior_prec
        prior_prec_diag = self.prior_prec_diag

        a = X.T.dot(process_prec)
        beta_prec = a.dot(X) + prior_prec
        beta_cov = pinv(beta_prec, self)
        beta_mean = beta_cov @ (a.dot(y) + prior_prec.dot(prior_mean))
        beta_mean = np.asarray(beta_mean).reshape(-1,1)

        try:
            beta_logdet = log_det(beta_cov, self)
        except:
            logging.critical(&#39;Crashed in log_det&#39;)
            logging.critical(&#39;beta_cov:\n{}&#39;.format(beta_cov))
            logging.critical(&#39;prior_prec\n{}&#39;.format(prior_prec))
            raise
        
        priorvar_logdet = log_det(prior_cov, self)
        ll2 = 0.5 * (beta_logdet - priorvar_logdet)

        bEbprior = np.asarray(prior_mean.T @ prior_prec.dot(prior_mean))[0,0]
        bEb = np.asarray(beta_mean.T @ beta_prec.dot(beta_mean) )[0,0]
        ll3 = 0.5 * (bEb - bEbprior)

        self.a = a
        self.beta_prec = beta_prec

        return ll2 + ll3

    @staticmethod
    @numba.jit(nopython=True, cache=True)
    def get_indices(a, keypair2col, tmems, smems):
        &#39;&#39;&#39;Use Just in Time compilation to reduce the &#39;getting&#39; time
        by about 95%

        Parameters
        ----------
        keypair2col : np.ndarray
            Maps (target_oidx, source_oidx) pair to the place the interaction
            index would be on a full interactio design matrix on the OTU level
        tmems, smems : np.ndarray
            These are the OTU indices in the target cluster and the source cluster
            respectively
        &#39;&#39;&#39;
        i = 0
        for tidx in tmems:
            for sidx in smems:
                a[i] = keypair2col[tidx, sidx]
                i += 1
        return a

    @staticmethod
    def make_iidx2cidxpair(ret, n_clusters):
        &#39;&#39;&#39;Map the index of a cluster interaction to (dst,src) of clusters

        Parameters
        ----------
        n_clusters : int
            Number of clusters

        Returns
        -------
        np.ndarray(n_interactions,2)
            First column is the destination cluster index, second column is the source
            cluster index
        &#39;&#39;&#39;
        i = 0
        for dst_cidx in range(n_clusters):
            for src_cidx in range(n_clusters):
                if dst_cidx == src_cidx:
                    continue
                ret[i,0] = dst_cidx
                ret[i,1] = src_cidx
                i += 1
        return ret</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="mdsine2.pylab.multiprocessing.PersistentWorker" href="pylab/multiprocessing.html#mdsine2.pylab.multiprocessing.PersistentWorker">PersistentWorker</a></li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="mdsine2.posterior.SingleClusterFullParallelization.get_indices"><code class="name flex">
<span>def <span class="ident">get_indices</span></span>(<span>a, keypair2col, tmems, smems)</span>
</code></dt>
<dd>
<div class="desc"><p>Use Just in Time compilation to reduce the 'getting' time
by about 95%</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>keypair2col</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>Maps (target_oidx, source_oidx) pair to the place the interaction
index would be on a full interactio design matrix on the OTU level</dd>
<dt><strong><code>tmems</code></strong>, <strong><code>smems</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>These are the OTU indices in the target cluster and the source cluster
respectively</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
@numba.jit(nopython=True, cache=True)
def get_indices(a, keypair2col, tmems, smems):
    &#39;&#39;&#39;Use Just in Time compilation to reduce the &#39;getting&#39; time
    by about 95%

    Parameters
    ----------
    keypair2col : np.ndarray
        Maps (target_oidx, source_oidx) pair to the place the interaction
        index would be on a full interactio design matrix on the OTU level
    tmems, smems : np.ndarray
        These are the OTU indices in the target cluster and the source cluster
        respectively
    &#39;&#39;&#39;
    i = 0
    for tidx in tmems:
        for sidx in smems:
            a[i] = keypair2col[tidx, sidx]
            i += 1
    return a</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.SingleClusterFullParallelization.make_iidx2cidxpair"><code class="name flex">
<span>def <span class="ident">make_iidx2cidxpair</span></span>(<span>ret, n_clusters)</span>
</code></dt>
<dd>
<div class="desc"><p>Map the index of a cluster interaction to (dst,src) of clusters</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>n_clusters</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of clusters</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.ndarray(n_interactions,2)</code></dt>
<dd>First column is the destination cluster index, second column is the source
cluster index</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def make_iidx2cidxpair(ret, n_clusters):
    &#39;&#39;&#39;Map the index of a cluster interaction to (dst,src) of clusters

    Parameters
    ----------
    n_clusters : int
        Number of clusters

    Returns
    -------
    np.ndarray(n_interactions,2)
        First column is the destination cluster index, second column is the source
        cluster index
    &#39;&#39;&#39;
    i = 0
    for dst_cidx in range(n_clusters):
        for src_cidx in range(n_clusters):
            if dst_cidx == src_cidx:
                continue
            ret[i,0] = dst_cidx
            ret[i,1] = src_cidx
            i += 1
    return ret</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="mdsine2.posterior.SingleClusterFullParallelization.build_interactions_matrix"><code class="name flex">
<span>def <span class="ident">build_interactions_matrix</span></span>(<span>self, on_columns: numpy.ndarray)</span>
</code></dt>
<dd>
<div class="desc"><p>Build the interaction matrix</p>
<p>First we make the rows and columns for the mixing matrix,
then we multiple the base matrix and the mixing matrix.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def build_interactions_matrix(self, on_columns: np.ndarray):
    &#39;&#39;&#39;Build the interaction matrix

    First we make the rows and columns for the mixing matrix,
    then we multiple the base matrix and the mixing matrix.
    &#39;&#39;&#39;
    rows = []
    cols = []

    # c2ciidx = Cluster-to-Cluster Interaction InDeX
    c2ciidx = 0
    for ccc in on_columns:
        tcidx = self.iidx2cidxpair[ccc, 0]
        scidx = self.iidx2cidxpair[ccc, 1]
        
        smems = self.clustering.clusters[scidx]
        tmems = self.clustering.clusters[tcidx]
        
        a = np.zeros(len(smems)*len(tmems), dtype=int)
        rows.append(SingleClusterFullParallelization.get_indices(a,
            self.keypair2col_interactions, tmems, smems))
        cols.append(np.full(len(tmems)*len(smems), fill_value=c2ciidx))
        c2ciidx += 1

    rows = np.asarray(list(itertools.chain.from_iterable(rows)))
    cols = np.asarray(list(itertools.chain.from_iterable(cols)))
    data = np.ones(len(rows), dtype=int)

    M = scipy.sparse.coo_matrix((data,(rows,cols)),
        shape=(self.n_rowsM, c2ciidx)).tocsc()
    ret = self.base_X @ M
    return ret</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.SingleClusterFullParallelization.build_perturbations_matrix"><code class="name flex">
<span>def <span class="ident">build_perturbations_matrix</span></span>(<span>self, on_columns: numpy.ndarray)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def build_perturbations_matrix(self, on_columns: np.ndarray):
    if not self.there_are_perturbations:
        raise ValueError(&#39;You should not be here&#39;)
    
    keypair2col = self.keypair2col_perturbations
    rows = []
    cols = []

    col = 0
    for pidx, pert_ind_idxs in enumerate(on_columns):
        for cidx in pert_ind_idxs:
            for oidx in self.clustering.clusters[cidx]:
                rows.append(keypair2col[oidx, pidx])
                cols.append(col)
            col += 1
    
    data = np.ones(len(rows), dtype=np.float64)
    M = scipy.sparse.coo_matrix((data,(rows,cols)),
        shape=(self.n_rowsMpert, col)).tocsc()
    ret = self.base_Xpert @ M
    return ret</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.SingleClusterFullParallelization.build_prior_cov_and_prec_and_diag"><code class="name flex">
<span>def <span class="ident">build_prior_cov_and_prec_and_diag</span></span>(<span>self, on_interactions, on_perturbations)</span>
</code></dt>
<dd>
<div class="desc"><p>Build the prior covariance matrices and others</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def build_prior_cov_and_prec_and_diag(self, on_interactions, on_perturbations):
    &#39;&#39;&#39;Build the prior covariance matrices and others
    &#39;&#39;&#39;
    ret = []
    for pidx, pert in enumerate(on_perturbations):
        ret = np.append(ret, 
            np.full(len(pert), fill_value=self.prior_var_pert[pidx]))
    prior_var_diag = np.append(ret, np.full(len(on_interactions), fill_value=self.prior_var_interactions))
    prior_prec_diag = 1/prior_var_diag

    prior_var = scipy.sparse.dia_matrix((prior_var_diag,[0]), 
        shape=(len(prior_var_diag),len(prior_var_diag))).tocsc()
    prior_prec = scipy.sparse.dia_matrix((prior_prec_diag,[0]), 
        shape=(len(prior_prec_diag),len(prior_prec_diag))).tocsc()

    return prior_var, prior_prec, prior_prec_diag</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.SingleClusterFullParallelization.build_prior_mean"><code class="name flex">
<span>def <span class="ident">build_prior_mean</span></span>(<span>self, on_interactions, on_perturbations)</span>
</code></dt>
<dd>
<div class="desc"><p>Build the prior mean array</p>
<p>Perturbations go first and then interactions</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def build_prior_mean(self, on_interactions, on_perturbations):
    &#39;&#39;&#39;Build the prior mean array

    Perturbations go first and then interactions
    &#39;&#39;&#39;
    ret = []
    for pidx, pert in enumerate(on_perturbations):
        ret = np.append(ret, 
            np.full(len(pert), fill_value=self.prior_mean_pert[pidx]))
    ret = np.append(ret, np.full(len(on_interactions), fill_value=self.prior_mean_interactions))
    return ret.reshape(-1,1)</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.SingleClusterFullParallelization.calculate_marginal_loglikelihood_slow_fast_sparse"><code class="name flex">
<span>def <span class="ident">calculate_marginal_loglikelihood_slow_fast_sparse</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def calculate_marginal_loglikelihood_slow_fast_sparse(self):
    y = self.y
    X = self.X
    process_prec = self.process_prec_matrix
    prior_mean = self.prior_mean
    prior_cov = self.prior_cov
    prior_prec = self.prior_prec
    prior_prec_diag = self.prior_prec_diag

    a = X.T.dot(process_prec)
    beta_prec = a.dot(X) + prior_prec
    beta_cov = pinv(beta_prec, self)
    beta_mean = beta_cov @ (a.dot(y) + prior_prec.dot(prior_mean))
    beta_mean = np.asarray(beta_mean).reshape(-1,1)

    try:
        beta_logdet = log_det(beta_cov, self)
    except:
        logging.critical(&#39;Crashed in log_det&#39;)
        logging.critical(&#39;beta_cov:\n{}&#39;.format(beta_cov))
        logging.critical(&#39;prior_prec\n{}&#39;.format(prior_prec))
        raise
    
    priorvar_logdet = log_det(prior_cov, self)
    ll2 = 0.5 * (beta_logdet - priorvar_logdet)

    bEbprior = np.asarray(prior_mean.T @ prior_prec.dot(prior_mean))[0,0]
    bEb = np.asarray(beta_mean.T @ beta_prec.dot(beta_mean) )[0,0]
    ll3 = 0.5 * (bEb - bEbprior)

    self.a = a
    self.beta_prec = beta_prec

    return ll2 + ll3</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.SingleClusterFullParallelization.initialize_gibbs"><code class="name flex">
<span>def <span class="ident">initialize_gibbs</span></span>(<span>self, base_Xdata: scipy.sparse.base.spmatrix, base_Xpertdata: scipy.sparse.base.spmatrix, concentration: float, m: int, y: numpy.ndarray, process_prec_diag: numpy.ndarray, prior_var_interactions: float, prior_var_pert: numpy.ndarray, prior_mean_interactions: float, prior_mean_pert: numpy.ndarray)</span>
</code></dt>
<dd>
<div class="desc"><p>Pass in the information that changes every Gibbs step</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>base_X</code></strong> :&ensp;<code>scipy.sparse.csc_matrix</code></dt>
<dd>Sparse matrix for the interaction terms</dd>
<dt><strong><code>base_Xpert</code></strong> :&ensp;<code>scipy.sparse.csc_matrix, None</code></dt>
<dd>Sparse matrix for the perturbation terms
If None, there are no perturbations</dd>
<dt><strong><code>concentration</code></strong> :&ensp;<code>float</code></dt>
<dd>This is the concentration of the system</dd>
<dt><strong><code>m</code></strong> :&ensp;<code>int</code></dt>
<dd>This is the auxiliary variable for the marginalization</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>This is the observation array</dd>
<dt><strong><code>process_prec_diag</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>This is the process precision diagonal</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def initialize_gibbs(self, base_Xdata: scipy.sparse.spmatrix, base_Xpertdata: scipy.sparse.spmatrix, 
    concentration: float, m: int, y: np.ndarray, process_prec_diag: np.ndarray, 
    prior_var_interactions: float, prior_var_pert: np.ndarray, 
    prior_mean_interactions: float, prior_mean_pert: np.ndarray):
    &#39;&#39;&#39;Pass in the information that changes every Gibbs step

    Parameters
    ----------
    base_X : scipy.sparse.csc_matrix
        Sparse matrix for the interaction terms
    base_Xpert : scipy.sparse.csc_matrix, None
        Sparse matrix for the perturbation terms
        If None, there are no perturbations
    concentration : float
        This is the concentration of the system
    m : int
        This is the auxiliary variable for the marginalization
    y : np.ndarray
        This is the observation array
    process_prec_diag : np.ndarray
        This is the process precision diagonal
    &#39;&#39;&#39;
    self.base_X = scipy.sparse.coo_matrix(
        (base_Xdata,(self.base_Xrows,self.base_Xcols)),
        shape=self.base_Xshape).tocsc()
    
    self.concentration = concentration
    self.m = m
    self.y = y.reshape(-1,1)
    self.n_rows = len(y)
    self.n_cols_X = self.base_X.shape[1]
    self.prior_var_interactions = prior_var_interactions
    self.prior_prec_interactions = 1/prior_var_interactions
    self.prior_mean_interactions = prior_mean_interactions

    if self.there_are_perturbations:
        self.base_Xpert = scipy.sparse.coo_matrix(
            (base_Xpertdata,(self.base_Xpertrows,self.base_Xpertcols)),
            shape=self.base_Xpertshape).tocsc()
        self.prior_var_pert = prior_var_pert
        self.prior_prec_pert = 1/prior_var_pert
        self.prior_mean_pert = prior_mean_pert

    self.process_prec_matrix = scipy.sparse.dia_matrix(
        (process_prec_diag,[0]), shape=(len(process_prec_diag),len(process_prec_diag))).tocsc()</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.SingleClusterFullParallelization.initialize_oidx"><code class="name flex">
<span>def <span class="ident">initialize_oidx</span></span>(<span>self, interaction_on_idxs: numpy.ndarray, perturbation_on_idxs: numpy.ndarray)</span>
</code></dt>
<dd>
<div class="desc"><p>Pass in the parameters that change for every OTU - potentially</p>
<h2 id="parameters">Parameters</h2></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def initialize_oidx(self, interaction_on_idxs: np.ndarray, perturbation_on_idxs: np.ndarray):
    &#39;&#39;&#39;Pass in the parameters that change for every OTU - potentially

    Parameters
    ----------
    
    &#39;&#39;&#39;
    self.saved_interaction_on_idxs = interaction_on_idxs
    self.saved_perturbation_on_idxs = perturbation_on_idxs</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.SingleClusterFullParallelization.run"><code class="name flex">
<span>def <span class="ident">run</span></span>(<span>self, interaction_on_idxs: numpy.ndarray, perturbation_on_idxs: List[numpy.ndarray], cluster_config: numpy.ndarray, log_mult_factor: float, cid: int, use_saved_params: bool)</span>
</code></dt>
<dd>
<div class="desc"><p>Pass in the parameters for the specific cluster assignment for
the OTU and run the marginalization</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>interaction_on_idxs</code></strong> :&ensp;<code>np.array(int)</code></dt>
<dd>An array of indices for the interactions that are on. Assumes that the
clustering is in the order specified in <code>cluster_config</code></dd>
<dt><strong><code>perturbation_on_idxs</code></strong> :&ensp;<code>list(np.ndarray(int)), None</code></dt>
<dd>If there are perturbations, then we set the perturbation idxs on
Each element in the list are the indices of that perturbation that are on</dd>
<dt><strong><code>cluster_config</code></strong> :&ensp;<code>list(list(int))</code></dt>
<dd>This is the cluster configuration and in cluster order.</dd>
<dt><strong><code>log_mult_factor</code></strong> :&ensp;<code>float</code></dt>
<dd>This is the log multiplication factor that we add onto the marginalization</dd>
<dt><strong><code>use_saved_params</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, passed in <code>interaction_on_idxs</code> and <code>perturbation_on_idxs</code> are None
and we can use <code>saved_interaction_on_idxs</code> and <code>saved_perturbation_on_idxs</code></dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run(self, interaction_on_idxs: np.ndarray, perturbation_on_idxs: List[np.ndarray], 
    cluster_config: np.ndarray, log_mult_factor: float, cid: int, 
    use_saved_params: bool):
    &#39;&#39;&#39;Pass in the parameters for the specific cluster assignment for
    the OTU and run the marginalization


    Parameters
    ----------
    interaction_on_idxs : np.array(int)
        An array of indices for the interactions that are on. Assumes that the
        clustering is in the order specified in `cluster_config`
    perturbation_on_idxs : list(np.ndarray(int)), None
        If there are perturbations, then we set the perturbation idxs on
        Each element in the list are the indices of that perturbation that are on
    cluster_config : list(list(int))
        This is the cluster configuration and in cluster order.
    log_mult_factor : float
        This is the log multiplication factor that we add onto the marginalization
    use_saved_params : bool
        If True, passed in `interaction_on_idxs` and `perturbation_on_idxs` are None
        and we can use `saved_interaction_on_idxs` and `saved_perturbation_on_idxs`
    &#39;&#39;&#39;
    if use_saved_params:
        interaction_on_idxs = self.saved_interaction_on_idxs
        perturbation_on_idxs = self.saved_perturbation_on_idxs

    # We need to make the arrays for interactions and perturbations
    self.set_clustering(cluster_config=cluster_config)
    Xinteractions = self.build_interactions_matrix(on_columns=interaction_on_idxs)

    if self.there_are_perturbations:
        Xperturbations = self.build_perturbations_matrix(on_columns=perturbation_on_idxs)
        X = scipy.sparse.hstack([Xperturbations, Xinteractions])
    else:
        X = Xinteractions
    self.X = X
    self.prior_mean = self.build_prior_mean(on_interactions=interaction_on_idxs,
        on_perturbations=perturbation_on_idxs)
    self.prior_cov, self.prior_prec, self.prior_prec_diag = self.build_prior_cov_and_prec_and_diag(
        on_interactions=interaction_on_idxs, on_perturbations=perturbation_on_idxs)
    
    return cid, self.calculate_marginal_loglikelihood_slow_fast_sparse() + log_mult_factor</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.SingleClusterFullParallelization.set_clustering"><code class="name flex">
<span>def <span class="ident">set_clustering</span></span>(<span>self, cluster_config: numpy.ndarray)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_clustering(self, cluster_config: np.ndarray):
    self.clustering = CondensedClustering(oidx2cidx=cluster_config)
    self.iidx2cidxpair = np.zeros(shape=(len(self.clustering)*(len(self.clustering)-1), 2), 
        dtype=int)
    self.iidx2cidxpair = SingleClusterFullParallelization.make_iidx2cidxpair(
        ret=self.iidx2cidxpair,
        n_clusters=len(self.clustering))</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="mdsine2.posterior.SubjectLogTrajectorySetMP"><code class="flex name class">
<span>class <span class="ident">SubjectLogTrajectorySetMP</span></span>
</code></dt>
<dd>
<div class="desc"><p>This performs filtering on a multiprocessing level. We send the
other parameters of the model and return the filtered <code>x</code> and values.
With multiprocessing, this class has ~91% efficiency. Additionally, the code
in this class is optimized to be ~20X faster than the code in <code>Filtering</code>. This
assumes we have the log model.</p>
<p>It might seem unneccessary to have so many local attributes, but it speeds up
the inference considerably if we index a value from an array once and store it
as a float instead of repeatedly indexing the array - the difference in
reality is super small but we do this so often that it adds up to ~40% speedup
as supposed to not doing it - so we do this as often as possible - This speedup
is even greater for indexing keys of dictionaries and getting parameters of objects.</p>
<h2 id="general-efficiency-speedups">General Efficiency Speedups</h2>
<p>All of these are done relative to a non-optimized filtering implementation:
- Specialized sampling and logpdf functions. About 95% faster than
scipy or numpy functions. All of these add up to a ~35% speed up
- Explicit function definitions:
instead of doing <code>self.prior.logpdf(&hellip;)</code>, we do
<code>pl.random.normal.logpdf(&hellip;)</code>, about a ~10% overall speedup
- Precomputation of values so that the least amout of computation
is done on a data level - All of these add up to a ~25% speed up
Benchmarked on a MacPro</p>
<h2 id="nontrivial-efficiency-speedups-wrt-non-multiprocessed-filtering">Non/trivial efficiency speedups w.r.t. non-multiprocessed filtering</h2>
<p>All of the speed ups are done relative to the current implementation
in non-multiprocessed filtering.
- Whenever possible we replace a 2D variable like <code>self.x</code> with a
<code>curr_x</code>, which is 1D, because indexing a 1D array is 10-20% faster
than a 2D array. All of these add up to a ~8% speed up
- We only every compute the forward dynamics (not the reverse), because
we can use the forward of the previous timepoint as the reverse for
the next timepoint. This is about 45% faster and adds up to a ~40%
speedup.
- Whenever possible, we replace a dictionary like <code>read_depths</code>
with a float because indexing a dict is 12-20% slower than a 1D
array. All these add up to a ~7% speed up
- We precompute AS MUCH AS POSSIBLE in <code>update</code> and in <code>initialize</code>,
even simple this as <code>self.curr_tidx_minus_1</code>: all of these add up
to about ~5% speedup
- If an attribute of a class is being referenced more than once in
a subroutine, we "get" it by making it a local variable. Example:
<code>tidx = self.tidx</code>. This has about a 5% speed up PER ADDITIONAL
CALL within the subroutine. All of these add up to ~2.5% speed up.
- If an indexed value gets indexed more than once within a subroutine,
we "get" the value by making it a local variable. All of these
add up to ~4% speed up.
- We "get" all of the means and stds of the qPCR data-structures so we
do not reference an object. This is about 22% faster and adds up
to a ~3% speed up.
Benchmarked on a MacPro</p>
<p>Set all local variables to None</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SubjectLogTrajectorySetMP(pl.multiprocessing.PersistentWorker):
    &#39;&#39;&#39;This performs filtering on a multiprocessing level. We send the
    other parameters of the model and return the filtered `x` and values.
    With multiprocessing, this class has ~91% efficiency. Additionally, the code
    in this class is optimized to be ~20X faster than the code in `Filtering`. This
    assumes we have the log model.

    It might seem unneccessary to have so many local attributes, but it speeds up
    the inference considerably if we index a value from an array once and store it
    as a float instead of repeatedly indexing the array - the difference in
    reality is super small but we do this so often that it adds up to ~40% speedup
    as supposed to not doing it - so we do this as often as possible - This speedup
    is even greater for indexing keys of dictionaries and getting parameters of objects.

    General efficiency speedups
    ---------------------------
    All of these are done relative to a non-optimized filtering implementation:
    - Specialized sampling and logpdf functions. About 95% faster than
      scipy or numpy functions. All of these add up to a ~35% speed up
    - Explicit function definitions:
      instead of doing `self.prior.logpdf(...)`, we do
      `pl.random.normal.logpdf(...)`, about a ~10% overall speedup
    - Precomputation of values so that the least amout of computation
      is done on a data level - All of these add up to a ~25% speed up
    Benchmarked on a MacPro

    Non/trivial efficiency speedups w.r.t. non-multiprocessed filtering
    -------------------------------------------------------------------
    All of the speed ups are done relative to the current implementation
    in non-multiprocessed filtering.
    - Whenever possible we replace a 2D variable like `self.x` with a
      `curr_x`, which is 1D, because indexing a 1D array is 10-20% faster
      than a 2D array. All of these add up to a ~8% speed up
    - We only every compute the forward dynamics (not the reverse), because
      we can use the forward of the previous timepoint as the reverse for
      the next timepoint. This is about 45% faster and adds up to a ~40%
      speedup.
    - Whenever possible, we replace a dictionary like `read_depths`
      with a float because indexing a dict is 12-20% slower than a 1D
      array. All these add up to a ~7% speed up
    - We precompute AS MUCH AS POSSIBLE in `update` and in `initialize`,
      even simple this as `self.curr_tidx_minus_1`: all of these add up
      to about ~5% speedup
    - If an attribute of a class is being referenced more than once in
      a subroutine, we &#34;get&#34; it by making it a local variable. Example:
      `tidx = self.tidx`. This has about a 5% speed up PER ADDITIONAL
      CALL within the subroutine. All of these add up to ~2.5% speed up.
    - If an indexed value gets indexed more than once within a subroutine,
      we &#34;get&#34; the value by making it a local variable. All of these
      add up to ~4% speed up.
    - We &#34;get&#34; all of the means and stds of the qPCR data-structures so we
      do not reference an object. This is about 22% faster and adds up
      to a ~3% speed up.
    Benchmarked on a MacPro
    &#39;&#39;&#39;
    def __init__(self):
        &#39;&#39;&#39;Set all local variables to None
        &#39;&#39;&#39;
        return

    def initialize(self, times: np.ndarray, qpcr_log_measurements: Dict[float, np.ndarray], 
        reads: Dict[float, np.ndarray], there_are_intermediate_timepoints: bool,
        there_are_perturbations: bool, pv_global: bool, x_prior_mean: Union[float, int],
        x_prior_std: Union[float, int], tune: int, delay: int, end_iter: int, proposal_init_scale: float, 
        a0: float, a1: float, x: np.ndarray, calculate_qpcr_loglik: bool,
        pert_starts: np.ndarray, pert_ends: np.ndarray, ridx: int, subjname: str, 
        h5py_xname: str, target_acceptance_rate: float, zero_inflation_transition_policy: Any):
        &#39;&#39;&#39;Initialize the object at the beginning of the inference

        n_o = Number of Taxa
        n_gT = Number of given time points
        n_T = Total number of time points, including intermediate
        n_P = Number of Perturbations

        Parameters
        ----------
        times : np.array((n_T, ))
            Times for each of the time points
        qpcr_log_measurements : dict(t -&gt; np.ndarray(float))
            These are the qPCR observations for every timepoint in log space.
        reads : dict (float -&gt; np.ndarray((n_o, )))
            The counts for each of the given timepoints. Each value is an
            array for the counts for each of the Taxa
        there_are_intermediate_timepoints : bool
            If True, then there are intermediate timepoints, else there are only
            given timepoints
        there_are_perturbations : bool
            If True, that means there are perturbations, else there are no
            perturbations
        pv_global : bool
            If True, it means the process variance is is global for each Taxa. If
            False it means that there is a separate `pv` for each Taxa
        pv : float, np.ndarray
            This is the process variance value. This is a float if `pv_global` is True
            and it is an array if `pv_global` is False.
        x_prior_mean, x_prior_std : numeric
            This is the prior mean and std for `x` used when sampling the reverse for
            the first timepoint
        tune : int
            How often we should update the proposal for each Taxa
        delay : int
            How many MCMC iterations we should delay the start of updating
        end_iter : int
            What iteration we should stop updating the proposal
        proposal_init_scale : float
            Scale to multiply the initial covariance of the poposal
        a0, a1 : floats
            These are the negative binomial dispersion parameters that specify how
            much noise there is in the counts
        x : np.ndarray((n_o, n_T))
            This is the x initialization
        pert_starts, pert_ends : np.ndarray((n_P, ))
            The starts and ends for each one of the perturbations
        ridx : int
            This is the replicate index that this object corresponds to
        subjname : str
            This is the name of the replicate
        h5py_xname : str
            This is the name for the x in the h5py object
        target_acceptance_rate : float
            This is the target acceptance rate for each point
        calculate_qpcr_loglik : bool
            If True, calculate the loglikelihood of the qPCR measurements during the proposal
        zero_inflation_transition_policy : str
            Zero inflation policy
        &#39;&#39;&#39;
        self.subjname = subjname
        self.h5py_xname = h5py_xname
        self.target_acceptance_rate = target_acceptance_rate
        self.zero_inflation_transition_policy = zero_inflation_transition_policy

        self.times = times
        self.qpcr_log_measurements = qpcr_log_measurements
        self.reads = reads
        self.there_are_intermediate_timepoints = there_are_intermediate_timepoints
        self.there_are_perturbations = there_are_perturbations
        self.pv_global = pv_global
        if not pv_global:
            raise TypeError(&#39;Filtering with MP not implemented for non global process variance&#39;)
        self.x_prior_mean = x_prior_mean
        self.x_prior_std = x_prior_std
        self.tune = tune
        self.delay = 0
        self.end_iter = end_iter
        self.proposal_init_scale = proposal_init_scale
        self.a0 = a0
        self.a1 = a1
        self.n_taxa = x.shape[0]
        self.n_timepoints = len(times)
        self.n_timepoints_minus_1 = len(times)-1
        self.logx = np.log(x)
        self.x = x

        # Get the perturbations for this subject
        if self.there_are_perturbations:
            self.pert_starts = []
            self.pert_ends = []
            for pidx in range(len(pert_starts)):
                self.pert_starts.append(pert_starts[pidx][subjname])
                self.pert_ends.append(pert_ends[pidx][subjname])

        self.total_n_points = self.x.shape[0] * self.x.shape[1]
        self.ridx = ridx
        self.calculate_qpcr_loglik = calculate_qpcr_loglik

        self.sample_iter = 0
        self.n_data_points = self.x.shape[0] * self.x.shape[1]

        # latent state
        self.sum_q = np.sum(self.x, axis=0)
        self.trace_iter = 0

        # proposal
        self.proposal_std = np.log(1.5) #np.log(3)
        self.acceptances = 0
        self.n_props_total = 0
        self.n_props_local = 0
        self.total_acceptances = 0
        self.add_trace = True

        # Intermediate timepoints
        if self.there_are_intermediate_timepoints:
            self.is_intermediate_timepoint = {}
            self.data_loglik = self.data_loglik_w_intermediates
            for t in self.times:
                self.is_intermediate_timepoint[t] = t not in self.reads
        else:
            self.data_loglik = self.data_loglik_wo_intermediates

        # Reads
        self.read_depths = {}
        for t in self.reads:
            self.read_depths[t] = float(np.sum(self.reads[t]))

        # t
        self.dts = np.zeros(self.n_timepoints_minus_1)
        self.sqrt_dts = np.zeros(self.n_timepoints_minus_1)
        for k in range(self.n_timepoints_minus_1):
            self.dts[k] = self.times[k+1] - self.times[k]
            self.sqrt_dts[k] = np.sqrt(self.dts[k])
        self.t2tidx = {}
        for tidx, t in enumerate(self.times):
            self.t2tidx[t] = tidx

        self.cnt_accepted_times = np.zeros(len(self.times))

        # Perturbations
        # -------------
        # in_pert_transition : np.ndarray(dtype=bool)
        #   This is a bool array where if it is a true it means that the
        #   forward and reverse growth rates are different
        # fully_in_pert : np.ndarray(dtype=int)
        #   This is an int-array where it tells you which perturbation you are fully in
        #   (the forward and reverse growth rates are the same but not the default).
        #   If there is no perturbation then the value is -1. If it is not -1, then the
        #   number corresponds to what perturbation index you are in.
        #
        # Edge cases
        # ----------
        #   * missing data for start
        #       There could be a situation where there was no sample collection
        #       on the day that they started a perturbation. In this case we
        #       assume that the next time point is the `start` of the perturbation.
        #       i.e. the next time point is the perturbation transition.
        #   * missing data for end
        #       There could be a situation where no sample was collected when the
        #       perturbation ended. In this case we assume that the pervious time
        #       point was the end of the perturbation.
        if self.there_are_perturbations:
            self.in_pert_transition = np.zeros(self.n_timepoints, dtype=bool)
            self.fully_in_pert = np.ones(self.n_timepoints, dtype=int) * -1
            for pidx, t in enumerate(self.pert_starts):
                if t == self.times[-1] or t == self.times[0]:
                    raise ValueError(&#39;The code right now does not support either a perturbation that &#39; \
                        &#39;started on the first day or ended on the last day. The code where this is &#39; \
                        &#39;incompatible is when we checking if we are in a perturbation transition&#39;)
                if t &gt; np.max(self.times):
                    continue
                if t not in self.t2tidx:
                    # Use the next time point
                    tidx = np.searchsorted(self.times, t)
                else:
                    tidx = self.t2tidx[t]
                self.in_pert_transition[tidx] = True
            for pidx, t in enumerate(self.pert_ends):
                if t == self.times[-1] or t == self.times[0]:
                    raise ValueError(&#39;The code right now does not support either a perturbation that &#39; \
                        &#39;started on the first day or ended on the last day. The code where this is &#39; \
                        &#39;incompatible is when we checking if we are in a perturbation transition&#39;)
                if t &lt; np.min(self.times) or t &gt; np.max(self.times):
                    continue
                if t not in self.t2tidx:
                    # Use the previous time point
                    tidx = np.searchsorted(self.times, t) - 1
                else:
                    tidx = self.t2tidx[t]
                self.in_pert_transition[tidx] = True

            # check if anything is weird
            if np.sum(self.in_pert_transition) % 2 != 0:
                raise ValueError(&#39;The number of in_pert_transition periods must be even ({})&#39; \
                    &#39;. There is either something wrong with the data (start and end day are &#39; \
                    &#39;the same) or with the algorithm ({})&#39;.format(
                        np.sum(self.in_pert_transition),
                        self.in_pert_transition))

            # Make the fully in perturbation times
            for pidx in range(len(self.pert_ends)):
                try:
                    start_tidx = self.t2tidx[self.pert_starts[pidx]] + 1
                    end_tidx = self.t2tidx[self.pert_ends[pidx]]
                except:
                    # This means there is a missing datapoint at either the
                    # start or end of the perturbation
                    start_t = self.pert_starts[pidx]
                    end_t = self.pert_ends[pidx]
                    start_tidx = np.searchsorted(self.times, start_t)
                    end_tidx = np.searchsorted(self.times, end_t) - 1

                self.fully_in_pert[start_tidx:end_tidx] = pidx
    
    # @profile
    def persistent_run(self, growth: np.ndarray, self_interactions: np.ndarray, 
        pv: Union[float, int, np.ndarray], interactions: np.ndarray,
        perturbations: np.ndarray, qpcr_variances: np.ndarray, 
        zero_inflation_data: Any) -&gt; Tuple[int, np.ndarray, float]:
        &#39;&#39;&#39;Run an update of the values for a single gibbs step for all of the data points
        in this replicate

        Parameters
        ----------
        growth : np.ndarray((n_taxa, ))
            Growth rates for each Taxa
        self_interactions : np.ndarray((n_taxa, ))
            Self-interactions for each Taxa
        pv : numeric, np.ndarray
            This is the process variance
        interactions : np.ndarray((n_taxa, n_taxa))
            These are the Taxa-Taxa interactions
        perturbations : np.ndarray((n_perturbations, n_taxa))
            Perturbation values in the right perturbation order, per Taxa
        zero_inflation : np.ndarray
            These are the points that are delibertly pushed down to zero
        qpcr_variances : np.ndarray
            These are the sampled qPCR variances as an array - they are in
            time order

        Returns
        -------
        (int, np.ndarray, float)
            1 This is the replicate index
            2 This is the updated latent state for logx
            3 This is the acceptance rate for this past update.
        &#39;&#39;&#39;
        self.master_growth_rate = growth

        if self.sample_iter &lt; self.delay:
            self.sample_iter += 1
            return self.ridx, self.x, np.nan

        self.update_proposals()
        self.n_accepted_iter = 0
        self.pv = pv
        self.pv_std = np.sqrt(pv)
        self.qpcr_stds = np.sqrt(qpcr_variances[self.ridx])
        self.qpcr_stds_d = {}
        # self.zero_inflation_data = zero_inflation_data[self.ridx]
        self.zero_inflation_data = None

        for tidx,t in enumerate(self.qpcr_log_measurements):
            self.qpcr_stds_d[t] = self.qpcr_stds[tidx]

        if self.there_are_perturbations:
            self.growth_rate_non_pert = growth.ravel()
            self.growth_rate_on_pert = growth.reshape(-1,1) * (1 + perturbations)
                
        # Go through each randomly Taxa and go in time order
        oidxs = npr.permutation(self.n_taxa)
        # print(&#39;===============================&#39;)
        # print(&#39;===============================&#39;)
        # print(&#39;ridx&#39;, self.ridx)
        for oidx in oidxs:

            # Set the necessary global parameters
            self.oidx = oidx
            self.curr_x = self.x[oidx, :]
            self.curr_logx = self.logx[oidx, :]
            self.curr_interactions = interactions[oidx, :]
            self.curr_self_interaction = self_interactions[oidx]
            # self.curr_zero_inflation = self.zero_inflation[oidx, :]

            if self.pv_global:
                self.curr_pv_std = self.pv_std
            else:
                self.curr_pv_std = self.pv_std[oidx]

            # Set for first time point
            self.tidx = 0
            self.set_attrs_for_timepoint()
            self.forward_loglik = self.default_forward_loglik
            self.reverse_loglik = self.first_timepoint_reverse
            # Calculate A matrix for forward
            self.forward_interaction_vals = np.nansum(self.x[:, self.tidx] * self.curr_interactions)
            self.update_single()
            self.reverse_loglik = self.default_reverse_loglik
            # Set for middle timepoints
            for tidx in range(1, self.n_timepoints-1):
                # Check if it needs to be zero inflated
                # if not self.curr_zero_inflation[tidx]:
                #     raise NotImplementedError(&#39;Zero inflation not implemented for logmodel&#39;)

                self.tidx = tidx
                self.set_attrs_for_timepoint()

                # Calculate A matrix for forward and reverse
                # Set the reverse of the current time step to the forward of the previous
                self.reverse_interaction_vals = self.forward_interaction_vals #np.sum(self.x[:, self.prev_tidx] * self.curr_interactions)
                self.forward_interaction_vals = np.nansum(self.x[:, self.tidx] * self.curr_interactions)

                # Run single update
                self.update_single()

            # Set for last timepoint
            self.tidx = self.n_timepoints_minus_1
            self.set_attrs_for_timepoint()
            self.forward_loglik = self.last_timepoint_forward
            # Calculate A matrix for reverse
            # Set the reverse of the current time step to the forward of the previous
            self.reverse_interaction_vals = self.forward_interaction_vals # np.sum(self.x[:, self.prev_tidx] * self.curr_interactions)
            self.update_single()

            # if self.sample_iter == 4:
            # sys.exit()

        self.sample_iter += 1
        if self.add_trace:
            self.trace_iter += 1

        # print(self.cnt_accepted_times/self.sample_iter)

        return self.ridx, self.x, self.n_accepted_iter/self.n_data_points

    def set_attrs_for_timepoint(self):
        self.prev_tidx = self.tidx-1
        self.next_tidx = self.tidx+1
        self.forward_growth_rate = self.master_growth_rate[self.oidx]
        self.reverse_growth_rate = self.master_growth_rate[self.oidx]

        if self.there_are_intermediate_timepoints:
            if not self.is_intermediate_timepoint[self.times[self.tidx]]:
                # It is not intermediate timepoints - we need to get the data
                t = self.times[self.tidx]
                self.curr_reads = self.reads[t][self.oidx]
                self.curr_read_depth = self.read_depths[t]
                self.curr_qpcr_log_measurements = self.qpcr_log_measurements[t]
                self.curr_qpcr_std = self.qpcr_stds_d[t]
        else:
            t = self.times[self.tidx]
            self.curr_reads = self.reads[t][self.oidx]
            self.curr_read_depth = self.read_depths[t]
            self.curr_qpcr_log_measurements = self.qpcr_log_measurements[t]
            self.curr_qpcr_std = self.qpcr_stds_d[t]

        # Set perturbation growth rates
        if self.there_are_perturbations:
            if self.in_pert_transition[self.tidx]:
                if self.fully_in_pert[self.tidx-1] != -1:
                    # If the previous time point is in the perturbation, that means
                    # we are going out of the perturbation
                    # self.forward_growth_rate = self.master_growth_rate[self.oidx]
                    pidx = self.fully_in_pert[self.tidx-1]
                    self.reverse_growth_rate = self.growth_rate_on_pert[self.oidx,pidx]
                else:
                    # Else we are going into a perturbation
                    # self.reverse_growth_rate = self.master_growth_rate[self.oidx]
                    pidx = self.fully_in_pert[self.tidx+1]
                    self.forward_growth_rate = self.growth_rate_on_pert[self.oidx,pidx]
            elif self.fully_in_pert[self.tidx] != -1:
                pidx = self.fully_in_pert[self.tidx]
                self.forward_growth_rate = self.growth_rate_on_pert[self.oidx,pidx]
                self.reverse_growth_rate = self.forward_growth_rate

    # @profile
    def update_single(self):
        &#39;&#39;&#39;Update a single oidx, tidx
        &#39;&#39;&#39;
        tidx = self.tidx
        oidx = self.oidx

        # Check if we should update the zero inflation policy
        if self.zero_inflation_transition_policy is not None:
            if self.zero_inflation_transition_policy == &#39;ignore&#39;:
                if not self.zero_inflation_data[oidx,tidx]:
                    self.x[oidx, tidx] = np.nan
                    self.logx[oidx, tidx] = np.nan
                    return
                else:
                    if tidx &lt; self.zero_inflation_data.shape[1]-1:
                        do_forward = self.zero_inflation_data[oidx, tidx+1]
                    else:
                        do_forward = True
                    if tidx &gt; 0:
                        do_reverse = self.zero_inflation_data[oidx, tidx-1]
                    else:
                        do_reverse = True
            else:
                raise NotImplementedError(&#39;Not Implemented&#39;)
        else:
            do_forward = True
            do_reverse = True

        try:
            logx_new = pl.random.misc.fast_sample_normal(
                loc=self.curr_logx[tidx],
                scale=self.proposal_std)
        except:
            print(&#39;mu&#39;, self.curr_logx[tidx])
            print(&#39;std&#39;, self.proposal_std)
            raise
        x_new = np.exp(logx_new)
        prev_logx_value = self.curr_logx[tidx]
        prev_x_value = self.curr_x[tidx]

        if do_forward:
            prev_aaa = self.forward_loglik()
        else:
            prev_aaa = 0
        if do_reverse:
            prev_bbb = self.reverse_loglik()
        else:
            prev_bbb = 0
        prev_ddd = self.data_loglik()

        l_old = prev_aaa + prev_bbb + prev_ddd

        self.curr_x[tidx] = x_new
        self.curr_logx[tidx] = logx_new
        self.sum_q[tidx] = self.sum_q[tidx] - prev_x_value + x_new

        if do_forward:
            new_aaa = self.forward_loglik()
        else:
            new_aaa = 0
        if do_reverse:
            new_bbb = self.reverse_loglik()
        else:
            new_bbb = 0
        new_ddd = self.data_loglik()

        l_new = new_aaa + new_bbb + new_ddd
        r_accept = l_new - l_old

        r = pl.random.misc.fast_sample_standard_uniform()
        if math.log(r) &gt; r_accept:
            self.sum_q[tidx] = self.sum_q[tidx] + prev_x_value - x_new
            self.curr_x[tidx] = prev_x_value
            self.curr_logx[tidx] = prev_logx_value
        else:
            self.x[oidx, tidx] = x_new
            self.logx[oidx, tidx] = logx_new
            self.acceptances += 1
            self.total_acceptances += 1
            self.n_accepted_iter += 1

        self.n_props_local += 1
        self.n_props_total += 1

    def update_proposals(self):
        &#39;&#39;&#39;Update the proposal if necessary
        &#39;&#39;&#39;
        if self.sample_iter &gt; self.end_iter:
            self.add_trace = False
            return
        if self.sample_iter == 0:
            return
        if self.trace_iter - self.delay == self.tune  and self.sample_iter - self.delay &gt; 0:

            # Adjust
            acc_rate = self.acceptances/self.n_props_total
            if acc_rate &lt; 0.1:
                logging.debug(&#39;Very low acceptance rate, scaling down past covariance&#39;)
                self.proposal_std *= 0.01
            elif acc_rate &lt; self.target_acceptance_rate:
                self.proposal_std /= np.sqrt(1.5)
            else:
                self.proposal_std *= np.sqrt(1.5)
            
            self.acceptances = 0
            self.n_props_local = 0

    def last_timepoint_forward(self):
        return 0

    # @profile
    def default_forward_loglik(self) -&gt; float:
        &#39;&#39;&#39;From the current timepoint (tidx) to the next timepoint (tidx+1)
        &#39;&#39;&#39;
        logmu = self.compute_dynamics(
            tidx=self.tidx,
            Axj=self.forward_interaction_vals,
            a1=self.forward_growth_rate)

        try:
            return pl.random.normal.logpdf(
                value=self.curr_logx[self.next_tidx], 
                loc=logmu, scale=self.curr_pv_std*self.sqrt_dts[self.tidx])
        except:
            return _normal_logpdf(
                value=self.curr_logx[self.next_tidx], 
                loc=logmu, scale=self.curr_pv_std*self.sqrt_dts[self.tidx])

    def first_timepoint_reverse(self) -&gt; float:
        # sample from the prior
        try:
            return pl.random.normal.logpdf(value=self.curr_logx[self.tidx],
                loc=self.x_prior_mean, scale=self.x_prior_std)
        except:
            return _normal_logpdf(value=self.curr_logx[self.tidx],
                loc=self.x_prior_mean, scale=self.x_prior_std)

    # @profile
    def default_reverse_loglik(self) -&gt; float:
        &#39;&#39;&#39;From the previous timepoint (tidx-1) to the current time point (tidx)
        &#39;&#39;&#39;
        logmu = self.compute_dynamics(
            tidx=self.prev_tidx,
            Axj=self.reverse_interaction_vals,
            a1=self.reverse_growth_rate)

        try:
            return pl.random.normal.logpdf(value=self.curr_logx[self.tidx], 
                loc=logmu, scale=self.curr_pv_std*self.sqrt_dts[self.prev_tidx])
        except:
            return _normal_logpdf(value=self.curr_logx[self.tidx], 
                loc=logmu, scale=self.curr_pv_std*self.sqrt_dts[self.prev_tidx])

    def data_loglik_w_intermediates(self) -&gt; float:
        &#39;&#39;&#39;data loglikelihood w/ intermediate timepoints
        &#39;&#39;&#39;
        if self.is_intermediate_timepoint[self.times[self.tidx]]:
            return 0
        else:
            return self.data_loglik_wo_intermediates()

    # @profile
    def data_loglik_wo_intermediates(self) -&gt; float:
        &#39;&#39;&#39;data loglikelihood with intermediate timepoints
        &#39;&#39;&#39;
        sum_q = self.sum_q[self.tidx]
        log_sum_q = math.log(sum_q)
        rel = self.curr_x[self.tidx] / sum_q

        try:
            negbin = negbin_loglikelihood_MH_condensed(
                k=self.curr_reads,
                m=self.curr_read_depth * rel,
                dispersion=self.a0/rel + self.a1)
        except:
            negbin = negbin_loglikelihood_MH_condensed_not_fast(
                k=self.curr_reads,
                m=self.curr_read_depth * rel,
                dispersion=self.a0/rel + self.a1)

        qpcr = 0
        if self.calculate_qpcr_loglik:
            for qpcr_val in self.curr_qpcr_log_measurements:
                a = pl.random.normal.logpdf(value=qpcr_val, loc=log_sum_q, scale=self.curr_qpcr_std)
                qpcr += a
        return negbin + qpcr

    def compute_dynamics(self, tidx: int, Axj: np.ndarray, a1: np.ndarray) -&gt; np.ndarray:
        &#39;&#39;&#39;Compute dynamics going into tidx+1

        a1 : growth rates and perturbations (if necessary)
        Axj : cluster interactions with the other abundances already multiplied
        tidx : time index

        Zero-inflation
        --------------
        When we get here, the current taxon at `tidx` is not a structural zero, but 
        there might be other bugs in the system that do have a structural zero there.
        Thus we do nan adds
        &#39;&#39;&#39;
        logxi = self.curr_logx[tidx]
        xi = self.curr_x[tidx]
        return logxi + (a1 - xi*self.curr_self_interaction + Axj) * self.dts[tidx]</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="mdsine2.pylab.multiprocessing.PersistentWorker" href="pylab/multiprocessing.html#mdsine2.pylab.multiprocessing.PersistentWorker">PersistentWorker</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="mdsine2.posterior.SubjectLogTrajectorySetMP.compute_dynamics"><code class="name flex">
<span>def <span class="ident">compute_dynamics</span></span>(<span>self, tidx: int, Axj: numpy.ndarray, a1: numpy.ndarray) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Compute dynamics going into tidx+1</p>
<p>a1 : growth rates and perturbations (if necessary)
Axj : cluster interactions with the other abundances already multiplied
tidx : time index</p>
<h2 id="zero-inflation">Zero-inflation</h2>
<p>When we get here, the current taxon at <code>tidx</code> is not a structural zero, but
there might be other bugs in the system that do have a structural zero there.
Thus we do nan adds</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_dynamics(self, tidx: int, Axj: np.ndarray, a1: np.ndarray) -&gt; np.ndarray:
    &#39;&#39;&#39;Compute dynamics going into tidx+1

    a1 : growth rates and perturbations (if necessary)
    Axj : cluster interactions with the other abundances already multiplied
    tidx : time index

    Zero-inflation
    --------------
    When we get here, the current taxon at `tidx` is not a structural zero, but 
    there might be other bugs in the system that do have a structural zero there.
    Thus we do nan adds
    &#39;&#39;&#39;
    logxi = self.curr_logx[tidx]
    xi = self.curr_x[tidx]
    return logxi + (a1 - xi*self.curr_self_interaction + Axj) * self.dts[tidx]</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.SubjectLogTrajectorySetMP.data_loglik_w_intermediates"><code class="name flex">
<span>def <span class="ident">data_loglik_w_intermediates</span></span>(<span>self) ‑> float</span>
</code></dt>
<dd>
<div class="desc"><p>data loglikelihood w/ intermediate timepoints</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def data_loglik_w_intermediates(self) -&gt; float:
    &#39;&#39;&#39;data loglikelihood w/ intermediate timepoints
    &#39;&#39;&#39;
    if self.is_intermediate_timepoint[self.times[self.tidx]]:
        return 0
    else:
        return self.data_loglik_wo_intermediates()</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.SubjectLogTrajectorySetMP.data_loglik_wo_intermediates"><code class="name flex">
<span>def <span class="ident">data_loglik_wo_intermediates</span></span>(<span>self) ‑> float</span>
</code></dt>
<dd>
<div class="desc"><p>data loglikelihood with intermediate timepoints</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def data_loglik_wo_intermediates(self) -&gt; float:
    &#39;&#39;&#39;data loglikelihood with intermediate timepoints
    &#39;&#39;&#39;
    sum_q = self.sum_q[self.tidx]
    log_sum_q = math.log(sum_q)
    rel = self.curr_x[self.tidx] / sum_q

    try:
        negbin = negbin_loglikelihood_MH_condensed(
            k=self.curr_reads,
            m=self.curr_read_depth * rel,
            dispersion=self.a0/rel + self.a1)
    except:
        negbin = negbin_loglikelihood_MH_condensed_not_fast(
            k=self.curr_reads,
            m=self.curr_read_depth * rel,
            dispersion=self.a0/rel + self.a1)

    qpcr = 0
    if self.calculate_qpcr_loglik:
        for qpcr_val in self.curr_qpcr_log_measurements:
            a = pl.random.normal.logpdf(value=qpcr_val, loc=log_sum_q, scale=self.curr_qpcr_std)
            qpcr += a
    return negbin + qpcr</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.SubjectLogTrajectorySetMP.default_forward_loglik"><code class="name flex">
<span>def <span class="ident">default_forward_loglik</span></span>(<span>self) ‑> float</span>
</code></dt>
<dd>
<div class="desc"><p>From the current timepoint (tidx) to the next timepoint (tidx+1)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def default_forward_loglik(self) -&gt; float:
    &#39;&#39;&#39;From the current timepoint (tidx) to the next timepoint (tidx+1)
    &#39;&#39;&#39;
    logmu = self.compute_dynamics(
        tidx=self.tidx,
        Axj=self.forward_interaction_vals,
        a1=self.forward_growth_rate)

    try:
        return pl.random.normal.logpdf(
            value=self.curr_logx[self.next_tidx], 
            loc=logmu, scale=self.curr_pv_std*self.sqrt_dts[self.tidx])
    except:
        return _normal_logpdf(
            value=self.curr_logx[self.next_tidx], 
            loc=logmu, scale=self.curr_pv_std*self.sqrt_dts[self.tidx])</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.SubjectLogTrajectorySetMP.default_reverse_loglik"><code class="name flex">
<span>def <span class="ident">default_reverse_loglik</span></span>(<span>self) ‑> float</span>
</code></dt>
<dd>
<div class="desc"><p>From the previous timepoint (tidx-1) to the current time point (tidx)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def default_reverse_loglik(self) -&gt; float:
    &#39;&#39;&#39;From the previous timepoint (tidx-1) to the current time point (tidx)
    &#39;&#39;&#39;
    logmu = self.compute_dynamics(
        tidx=self.prev_tidx,
        Axj=self.reverse_interaction_vals,
        a1=self.reverse_growth_rate)

    try:
        return pl.random.normal.logpdf(value=self.curr_logx[self.tidx], 
            loc=logmu, scale=self.curr_pv_std*self.sqrt_dts[self.prev_tidx])
    except:
        return _normal_logpdf(value=self.curr_logx[self.tidx], 
            loc=logmu, scale=self.curr_pv_std*self.sqrt_dts[self.prev_tidx])</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.SubjectLogTrajectorySetMP.first_timepoint_reverse"><code class="name flex">
<span>def <span class="ident">first_timepoint_reverse</span></span>(<span>self) ‑> float</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def first_timepoint_reverse(self) -&gt; float:
    # sample from the prior
    try:
        return pl.random.normal.logpdf(value=self.curr_logx[self.tidx],
            loc=self.x_prior_mean, scale=self.x_prior_std)
    except:
        return _normal_logpdf(value=self.curr_logx[self.tidx],
            loc=self.x_prior_mean, scale=self.x_prior_std)</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.SubjectLogTrajectorySetMP.initialize"><code class="name flex">
<span>def <span class="ident">initialize</span></span>(<span>self, times: numpy.ndarray, qpcr_log_measurements: Dict[float, numpy.ndarray], reads: Dict[float, numpy.ndarray], there_are_intermediate_timepoints: bool, there_are_perturbations: bool, pv_global: bool, x_prior_mean: Union[float, int], x_prior_std: Union[float, int], tune: int, delay: int, end_iter: int, proposal_init_scale: float, a0: float, a1: float, x: numpy.ndarray, calculate_qpcr_loglik: bool, pert_starts: numpy.ndarray, pert_ends: numpy.ndarray, ridx: int, subjname: str, h5py_xname: str, target_acceptance_rate: float, zero_inflation_transition_policy: Any)</span>
</code></dt>
<dd>
<div class="desc"><p>Initialize the object at the beginning of the inference</p>
<p>n_o = Number of Taxa
n_gT = Number of given time points
n_T = Total number of time points, including intermediate
n_P = Number of Perturbations</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>times</code></strong> :&ensp;<code>np.array((n_T, ))</code></dt>
<dd>Times for each of the time points</dd>
<dt><strong><code>qpcr_log_measurements</code></strong> :&ensp;<code>dict(t -&gt; np.ndarray(float))</code></dt>
<dd>These are the qPCR observations for every timepoint in log space.</dd>
<dt><strong><code>reads</code></strong> :&ensp;<code>dict (float -&gt; np.ndarray((n_o, )))</code></dt>
<dd>The counts for each of the given timepoints. Each value is an
array for the counts for each of the Taxa</dd>
<dt><strong><code>there_are_intermediate_timepoints</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, then there are intermediate timepoints, else there are only
given timepoints</dd>
<dt><strong><code>there_are_perturbations</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, that means there are perturbations, else there are no
perturbations</dd>
<dt><strong><code>pv_global</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, it means the process variance is is global for each Taxa. If
False it means that there is a separate <code>pv</code> for each Taxa</dd>
<dt><strong><code>pv</code></strong> :&ensp;<code>float, np.ndarray</code></dt>
<dd>This is the process variance value. This is a float if <code>pv_global</code> is True
and it is an array if <code>pv_global</code> is False.</dd>
<dt><strong><code>x_prior_mean</code></strong>, <strong><code>x_prior_std</code></strong> :&ensp;<code>numeric</code></dt>
<dd>This is the prior mean and std for <code>x</code> used when sampling the reverse for
the first timepoint</dd>
<dt><strong><code>tune</code></strong> :&ensp;<code>int</code></dt>
<dd>How often we should update the proposal for each Taxa</dd>
<dt><strong><code>delay</code></strong> :&ensp;<code>int</code></dt>
<dd>How many MCMC iterations we should delay the start of updating</dd>
<dt><strong><code>end_iter</code></strong> :&ensp;<code>int</code></dt>
<dd>What iteration we should stop updating the proposal</dd>
<dt><strong><code>proposal_init_scale</code></strong> :&ensp;<code>float</code></dt>
<dd>Scale to multiply the initial covariance of the poposal</dd>
<dt><strong><code>a0</code></strong>, <strong><code>a1</code></strong> :&ensp;<code>floats</code></dt>
<dd>These are the negative binomial dispersion parameters that specify how
much noise there is in the counts</dd>
<dt><strong><code>x</code></strong> :&ensp;<code>np.ndarray((n_o, n_T))</code></dt>
<dd>This is the x initialization</dd>
<dt><strong><code>pert_starts</code></strong>, <strong><code>pert_ends</code></strong> :&ensp;<code>np.ndarray((n_P, ))</code></dt>
<dd>The starts and ends for each one of the perturbations</dd>
<dt><strong><code>ridx</code></strong> :&ensp;<code>int</code></dt>
<dd>This is the replicate index that this object corresponds to</dd>
<dt><strong><code>subjname</code></strong> :&ensp;<code>str</code></dt>
<dd>This is the name of the replicate</dd>
<dt><strong><code>h5py_xname</code></strong> :&ensp;<code>str</code></dt>
<dd>This is the name for the x in the h5py object</dd>
<dt><strong><code>target_acceptance_rate</code></strong> :&ensp;<code>float</code></dt>
<dd>This is the target acceptance rate for each point</dd>
<dt><strong><code>calculate_qpcr_loglik</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, calculate the loglikelihood of the qPCR measurements during the proposal</dd>
<dt><strong><code>zero_inflation_transition_policy</code></strong> :&ensp;<code>str</code></dt>
<dd>Zero inflation policy</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def initialize(self, times: np.ndarray, qpcr_log_measurements: Dict[float, np.ndarray], 
    reads: Dict[float, np.ndarray], there_are_intermediate_timepoints: bool,
    there_are_perturbations: bool, pv_global: bool, x_prior_mean: Union[float, int],
    x_prior_std: Union[float, int], tune: int, delay: int, end_iter: int, proposal_init_scale: float, 
    a0: float, a1: float, x: np.ndarray, calculate_qpcr_loglik: bool,
    pert_starts: np.ndarray, pert_ends: np.ndarray, ridx: int, subjname: str, 
    h5py_xname: str, target_acceptance_rate: float, zero_inflation_transition_policy: Any):
    &#39;&#39;&#39;Initialize the object at the beginning of the inference

    n_o = Number of Taxa
    n_gT = Number of given time points
    n_T = Total number of time points, including intermediate
    n_P = Number of Perturbations

    Parameters
    ----------
    times : np.array((n_T, ))
        Times for each of the time points
    qpcr_log_measurements : dict(t -&gt; np.ndarray(float))
        These are the qPCR observations for every timepoint in log space.
    reads : dict (float -&gt; np.ndarray((n_o, )))
        The counts for each of the given timepoints. Each value is an
        array for the counts for each of the Taxa
    there_are_intermediate_timepoints : bool
        If True, then there are intermediate timepoints, else there are only
        given timepoints
    there_are_perturbations : bool
        If True, that means there are perturbations, else there are no
        perturbations
    pv_global : bool
        If True, it means the process variance is is global for each Taxa. If
        False it means that there is a separate `pv` for each Taxa
    pv : float, np.ndarray
        This is the process variance value. This is a float if `pv_global` is True
        and it is an array if `pv_global` is False.
    x_prior_mean, x_prior_std : numeric
        This is the prior mean and std for `x` used when sampling the reverse for
        the first timepoint
    tune : int
        How often we should update the proposal for each Taxa
    delay : int
        How many MCMC iterations we should delay the start of updating
    end_iter : int
        What iteration we should stop updating the proposal
    proposal_init_scale : float
        Scale to multiply the initial covariance of the poposal
    a0, a1 : floats
        These are the negative binomial dispersion parameters that specify how
        much noise there is in the counts
    x : np.ndarray((n_o, n_T))
        This is the x initialization
    pert_starts, pert_ends : np.ndarray((n_P, ))
        The starts and ends for each one of the perturbations
    ridx : int
        This is the replicate index that this object corresponds to
    subjname : str
        This is the name of the replicate
    h5py_xname : str
        This is the name for the x in the h5py object
    target_acceptance_rate : float
        This is the target acceptance rate for each point
    calculate_qpcr_loglik : bool
        If True, calculate the loglikelihood of the qPCR measurements during the proposal
    zero_inflation_transition_policy : str
        Zero inflation policy
    &#39;&#39;&#39;
    self.subjname = subjname
    self.h5py_xname = h5py_xname
    self.target_acceptance_rate = target_acceptance_rate
    self.zero_inflation_transition_policy = zero_inflation_transition_policy

    self.times = times
    self.qpcr_log_measurements = qpcr_log_measurements
    self.reads = reads
    self.there_are_intermediate_timepoints = there_are_intermediate_timepoints
    self.there_are_perturbations = there_are_perturbations
    self.pv_global = pv_global
    if not pv_global:
        raise TypeError(&#39;Filtering with MP not implemented for non global process variance&#39;)
    self.x_prior_mean = x_prior_mean
    self.x_prior_std = x_prior_std
    self.tune = tune
    self.delay = 0
    self.end_iter = end_iter
    self.proposal_init_scale = proposal_init_scale
    self.a0 = a0
    self.a1 = a1
    self.n_taxa = x.shape[0]
    self.n_timepoints = len(times)
    self.n_timepoints_minus_1 = len(times)-1
    self.logx = np.log(x)
    self.x = x

    # Get the perturbations for this subject
    if self.there_are_perturbations:
        self.pert_starts = []
        self.pert_ends = []
        for pidx in range(len(pert_starts)):
            self.pert_starts.append(pert_starts[pidx][subjname])
            self.pert_ends.append(pert_ends[pidx][subjname])

    self.total_n_points = self.x.shape[0] * self.x.shape[1]
    self.ridx = ridx
    self.calculate_qpcr_loglik = calculate_qpcr_loglik

    self.sample_iter = 0
    self.n_data_points = self.x.shape[0] * self.x.shape[1]

    # latent state
    self.sum_q = np.sum(self.x, axis=0)
    self.trace_iter = 0

    # proposal
    self.proposal_std = np.log(1.5) #np.log(3)
    self.acceptances = 0
    self.n_props_total = 0
    self.n_props_local = 0
    self.total_acceptances = 0
    self.add_trace = True

    # Intermediate timepoints
    if self.there_are_intermediate_timepoints:
        self.is_intermediate_timepoint = {}
        self.data_loglik = self.data_loglik_w_intermediates
        for t in self.times:
            self.is_intermediate_timepoint[t] = t not in self.reads
    else:
        self.data_loglik = self.data_loglik_wo_intermediates

    # Reads
    self.read_depths = {}
    for t in self.reads:
        self.read_depths[t] = float(np.sum(self.reads[t]))

    # t
    self.dts = np.zeros(self.n_timepoints_minus_1)
    self.sqrt_dts = np.zeros(self.n_timepoints_minus_1)
    for k in range(self.n_timepoints_minus_1):
        self.dts[k] = self.times[k+1] - self.times[k]
        self.sqrt_dts[k] = np.sqrt(self.dts[k])
    self.t2tidx = {}
    for tidx, t in enumerate(self.times):
        self.t2tidx[t] = tidx

    self.cnt_accepted_times = np.zeros(len(self.times))

    # Perturbations
    # -------------
    # in_pert_transition : np.ndarray(dtype=bool)
    #   This is a bool array where if it is a true it means that the
    #   forward and reverse growth rates are different
    # fully_in_pert : np.ndarray(dtype=int)
    #   This is an int-array where it tells you which perturbation you are fully in
    #   (the forward and reverse growth rates are the same but not the default).
    #   If there is no perturbation then the value is -1. If it is not -1, then the
    #   number corresponds to what perturbation index you are in.
    #
    # Edge cases
    # ----------
    #   * missing data for start
    #       There could be a situation where there was no sample collection
    #       on the day that they started a perturbation. In this case we
    #       assume that the next time point is the `start` of the perturbation.
    #       i.e. the next time point is the perturbation transition.
    #   * missing data for end
    #       There could be a situation where no sample was collected when the
    #       perturbation ended. In this case we assume that the pervious time
    #       point was the end of the perturbation.
    if self.there_are_perturbations:
        self.in_pert_transition = np.zeros(self.n_timepoints, dtype=bool)
        self.fully_in_pert = np.ones(self.n_timepoints, dtype=int) * -1
        for pidx, t in enumerate(self.pert_starts):
            if t == self.times[-1] or t == self.times[0]:
                raise ValueError(&#39;The code right now does not support either a perturbation that &#39; \
                    &#39;started on the first day or ended on the last day. The code where this is &#39; \
                    &#39;incompatible is when we checking if we are in a perturbation transition&#39;)
            if t &gt; np.max(self.times):
                continue
            if t not in self.t2tidx:
                # Use the next time point
                tidx = np.searchsorted(self.times, t)
            else:
                tidx = self.t2tidx[t]
            self.in_pert_transition[tidx] = True
        for pidx, t in enumerate(self.pert_ends):
            if t == self.times[-1] or t == self.times[0]:
                raise ValueError(&#39;The code right now does not support either a perturbation that &#39; \
                    &#39;started on the first day or ended on the last day. The code where this is &#39; \
                    &#39;incompatible is when we checking if we are in a perturbation transition&#39;)
            if t &lt; np.min(self.times) or t &gt; np.max(self.times):
                continue
            if t not in self.t2tidx:
                # Use the previous time point
                tidx = np.searchsorted(self.times, t) - 1
            else:
                tidx = self.t2tidx[t]
            self.in_pert_transition[tidx] = True

        # check if anything is weird
        if np.sum(self.in_pert_transition) % 2 != 0:
            raise ValueError(&#39;The number of in_pert_transition periods must be even ({})&#39; \
                &#39;. There is either something wrong with the data (start and end day are &#39; \
                &#39;the same) or with the algorithm ({})&#39;.format(
                    np.sum(self.in_pert_transition),
                    self.in_pert_transition))

        # Make the fully in perturbation times
        for pidx in range(len(self.pert_ends)):
            try:
                start_tidx = self.t2tidx[self.pert_starts[pidx]] + 1
                end_tidx = self.t2tidx[self.pert_ends[pidx]]
            except:
                # This means there is a missing datapoint at either the
                # start or end of the perturbation
                start_t = self.pert_starts[pidx]
                end_t = self.pert_ends[pidx]
                start_tidx = np.searchsorted(self.times, start_t)
                end_tidx = np.searchsorted(self.times, end_t) - 1

            self.fully_in_pert[start_tidx:end_tidx] = pidx</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.SubjectLogTrajectorySetMP.last_timepoint_forward"><code class="name flex">
<span>def <span class="ident">last_timepoint_forward</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def last_timepoint_forward(self):
    return 0</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.SubjectLogTrajectorySetMP.persistent_run"><code class="name flex">
<span>def <span class="ident">persistent_run</span></span>(<span>self, growth: numpy.ndarray, self_interactions: numpy.ndarray, pv: Union[float, int, numpy.ndarray], interactions: numpy.ndarray, perturbations: numpy.ndarray, qpcr_variances: numpy.ndarray, zero_inflation_data: Any) ‑> Tuple[int, numpy.ndarray, float]</span>
</code></dt>
<dd>
<div class="desc"><p>Run an update of the values for a single gibbs step for all of the data points
in this replicate</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>growth</code></strong> :&ensp;<code>np.ndarray((n_taxa, ))</code></dt>
<dd>Growth rates for each Taxa</dd>
<dt><strong><code>self_interactions</code></strong> :&ensp;<code>np.ndarray((n_taxa, ))</code></dt>
<dd>Self-interactions for each Taxa</dd>
<dt><strong><code>pv</code></strong> :&ensp;<code>numeric, np.ndarray</code></dt>
<dd>This is the process variance</dd>
<dt><strong><code>interactions</code></strong> :&ensp;<code>np.ndarray((n_taxa, n_taxa))</code></dt>
<dd>These are the Taxa-Taxa interactions</dd>
<dt><strong><code>perturbations</code></strong> :&ensp;<code>np.ndarray((n_perturbations, n_taxa))</code></dt>
<dd>Perturbation values in the right perturbation order, per Taxa</dd>
<dt><strong><code>zero_inflation</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>These are the points that are delibertly pushed down to zero</dd>
<dt><strong><code>qpcr_variances</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>These are the sampled qPCR variances as an array - they are in
time order</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>(int, np.ndarray, float)
1 This is the replicate index
2 This is the updated latent state for logx
3 This is the acceptance rate for this past update.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def persistent_run(self, growth: np.ndarray, self_interactions: np.ndarray, 
    pv: Union[float, int, np.ndarray], interactions: np.ndarray,
    perturbations: np.ndarray, qpcr_variances: np.ndarray, 
    zero_inflation_data: Any) -&gt; Tuple[int, np.ndarray, float]:
    &#39;&#39;&#39;Run an update of the values for a single gibbs step for all of the data points
    in this replicate

    Parameters
    ----------
    growth : np.ndarray((n_taxa, ))
        Growth rates for each Taxa
    self_interactions : np.ndarray((n_taxa, ))
        Self-interactions for each Taxa
    pv : numeric, np.ndarray
        This is the process variance
    interactions : np.ndarray((n_taxa, n_taxa))
        These are the Taxa-Taxa interactions
    perturbations : np.ndarray((n_perturbations, n_taxa))
        Perturbation values in the right perturbation order, per Taxa
    zero_inflation : np.ndarray
        These are the points that are delibertly pushed down to zero
    qpcr_variances : np.ndarray
        These are the sampled qPCR variances as an array - they are in
        time order

    Returns
    -------
    (int, np.ndarray, float)
        1 This is the replicate index
        2 This is the updated latent state for logx
        3 This is the acceptance rate for this past update.
    &#39;&#39;&#39;
    self.master_growth_rate = growth

    if self.sample_iter &lt; self.delay:
        self.sample_iter += 1
        return self.ridx, self.x, np.nan

    self.update_proposals()
    self.n_accepted_iter = 0
    self.pv = pv
    self.pv_std = np.sqrt(pv)
    self.qpcr_stds = np.sqrt(qpcr_variances[self.ridx])
    self.qpcr_stds_d = {}
    # self.zero_inflation_data = zero_inflation_data[self.ridx]
    self.zero_inflation_data = None

    for tidx,t in enumerate(self.qpcr_log_measurements):
        self.qpcr_stds_d[t] = self.qpcr_stds[tidx]

    if self.there_are_perturbations:
        self.growth_rate_non_pert = growth.ravel()
        self.growth_rate_on_pert = growth.reshape(-1,1) * (1 + perturbations)
            
    # Go through each randomly Taxa and go in time order
    oidxs = npr.permutation(self.n_taxa)
    # print(&#39;===============================&#39;)
    # print(&#39;===============================&#39;)
    # print(&#39;ridx&#39;, self.ridx)
    for oidx in oidxs:

        # Set the necessary global parameters
        self.oidx = oidx
        self.curr_x = self.x[oidx, :]
        self.curr_logx = self.logx[oidx, :]
        self.curr_interactions = interactions[oidx, :]
        self.curr_self_interaction = self_interactions[oidx]
        # self.curr_zero_inflation = self.zero_inflation[oidx, :]

        if self.pv_global:
            self.curr_pv_std = self.pv_std
        else:
            self.curr_pv_std = self.pv_std[oidx]

        # Set for first time point
        self.tidx = 0
        self.set_attrs_for_timepoint()
        self.forward_loglik = self.default_forward_loglik
        self.reverse_loglik = self.first_timepoint_reverse
        # Calculate A matrix for forward
        self.forward_interaction_vals = np.nansum(self.x[:, self.tidx] * self.curr_interactions)
        self.update_single()
        self.reverse_loglik = self.default_reverse_loglik
        # Set for middle timepoints
        for tidx in range(1, self.n_timepoints-1):
            # Check if it needs to be zero inflated
            # if not self.curr_zero_inflation[tidx]:
            #     raise NotImplementedError(&#39;Zero inflation not implemented for logmodel&#39;)

            self.tidx = tidx
            self.set_attrs_for_timepoint()

            # Calculate A matrix for forward and reverse
            # Set the reverse of the current time step to the forward of the previous
            self.reverse_interaction_vals = self.forward_interaction_vals #np.sum(self.x[:, self.prev_tidx] * self.curr_interactions)
            self.forward_interaction_vals = np.nansum(self.x[:, self.tidx] * self.curr_interactions)

            # Run single update
            self.update_single()

        # Set for last timepoint
        self.tidx = self.n_timepoints_minus_1
        self.set_attrs_for_timepoint()
        self.forward_loglik = self.last_timepoint_forward
        # Calculate A matrix for reverse
        # Set the reverse of the current time step to the forward of the previous
        self.reverse_interaction_vals = self.forward_interaction_vals # np.sum(self.x[:, self.prev_tidx] * self.curr_interactions)
        self.update_single()

        # if self.sample_iter == 4:
        # sys.exit()

    self.sample_iter += 1
    if self.add_trace:
        self.trace_iter += 1

    # print(self.cnt_accepted_times/self.sample_iter)

    return self.ridx, self.x, self.n_accepted_iter/self.n_data_points</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.SubjectLogTrajectorySetMP.set_attrs_for_timepoint"><code class="name flex">
<span>def <span class="ident">set_attrs_for_timepoint</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_attrs_for_timepoint(self):
    self.prev_tidx = self.tidx-1
    self.next_tidx = self.tidx+1
    self.forward_growth_rate = self.master_growth_rate[self.oidx]
    self.reverse_growth_rate = self.master_growth_rate[self.oidx]

    if self.there_are_intermediate_timepoints:
        if not self.is_intermediate_timepoint[self.times[self.tidx]]:
            # It is not intermediate timepoints - we need to get the data
            t = self.times[self.tidx]
            self.curr_reads = self.reads[t][self.oidx]
            self.curr_read_depth = self.read_depths[t]
            self.curr_qpcr_log_measurements = self.qpcr_log_measurements[t]
            self.curr_qpcr_std = self.qpcr_stds_d[t]
    else:
        t = self.times[self.tidx]
        self.curr_reads = self.reads[t][self.oidx]
        self.curr_read_depth = self.read_depths[t]
        self.curr_qpcr_log_measurements = self.qpcr_log_measurements[t]
        self.curr_qpcr_std = self.qpcr_stds_d[t]

    # Set perturbation growth rates
    if self.there_are_perturbations:
        if self.in_pert_transition[self.tidx]:
            if self.fully_in_pert[self.tidx-1] != -1:
                # If the previous time point is in the perturbation, that means
                # we are going out of the perturbation
                # self.forward_growth_rate = self.master_growth_rate[self.oidx]
                pidx = self.fully_in_pert[self.tidx-1]
                self.reverse_growth_rate = self.growth_rate_on_pert[self.oidx,pidx]
            else:
                # Else we are going into a perturbation
                # self.reverse_growth_rate = self.master_growth_rate[self.oidx]
                pidx = self.fully_in_pert[self.tidx+1]
                self.forward_growth_rate = self.growth_rate_on_pert[self.oidx,pidx]
        elif self.fully_in_pert[self.tidx] != -1:
            pidx = self.fully_in_pert[self.tidx]
            self.forward_growth_rate = self.growth_rate_on_pert[self.oidx,pidx]
            self.reverse_growth_rate = self.forward_growth_rate</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.SubjectLogTrajectorySetMP.update_proposals"><code class="name flex">
<span>def <span class="ident">update_proposals</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Update the proposal if necessary</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_proposals(self):
    &#39;&#39;&#39;Update the proposal if necessary
    &#39;&#39;&#39;
    if self.sample_iter &gt; self.end_iter:
        self.add_trace = False
        return
    if self.sample_iter == 0:
        return
    if self.trace_iter - self.delay == self.tune  and self.sample_iter - self.delay &gt; 0:

        # Adjust
        acc_rate = self.acceptances/self.n_props_total
        if acc_rate &lt; 0.1:
            logging.debug(&#39;Very low acceptance rate, scaling down past covariance&#39;)
            self.proposal_std *= 0.01
        elif acc_rate &lt; self.target_acceptance_rate:
            self.proposal_std /= np.sqrt(1.5)
        else:
            self.proposal_std *= np.sqrt(1.5)
        
        self.acceptances = 0
        self.n_props_local = 0</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.SubjectLogTrajectorySetMP.update_single"><code class="name flex">
<span>def <span class="ident">update_single</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Update a single oidx, tidx</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_single(self):
    &#39;&#39;&#39;Update a single oidx, tidx
    &#39;&#39;&#39;
    tidx = self.tidx
    oidx = self.oidx

    # Check if we should update the zero inflation policy
    if self.zero_inflation_transition_policy is not None:
        if self.zero_inflation_transition_policy == &#39;ignore&#39;:
            if not self.zero_inflation_data[oidx,tidx]:
                self.x[oidx, tidx] = np.nan
                self.logx[oidx, tidx] = np.nan
                return
            else:
                if tidx &lt; self.zero_inflation_data.shape[1]-1:
                    do_forward = self.zero_inflation_data[oidx, tidx+1]
                else:
                    do_forward = True
                if tidx &gt; 0:
                    do_reverse = self.zero_inflation_data[oidx, tidx-1]
                else:
                    do_reverse = True
        else:
            raise NotImplementedError(&#39;Not Implemented&#39;)
    else:
        do_forward = True
        do_reverse = True

    try:
        logx_new = pl.random.misc.fast_sample_normal(
            loc=self.curr_logx[tidx],
            scale=self.proposal_std)
    except:
        print(&#39;mu&#39;, self.curr_logx[tidx])
        print(&#39;std&#39;, self.proposal_std)
        raise
    x_new = np.exp(logx_new)
    prev_logx_value = self.curr_logx[tidx]
    prev_x_value = self.curr_x[tidx]

    if do_forward:
        prev_aaa = self.forward_loglik()
    else:
        prev_aaa = 0
    if do_reverse:
        prev_bbb = self.reverse_loglik()
    else:
        prev_bbb = 0
    prev_ddd = self.data_loglik()

    l_old = prev_aaa + prev_bbb + prev_ddd

    self.curr_x[tidx] = x_new
    self.curr_logx[tidx] = logx_new
    self.sum_q[tidx] = self.sum_q[tidx] - prev_x_value + x_new

    if do_forward:
        new_aaa = self.forward_loglik()
    else:
        new_aaa = 0
    if do_reverse:
        new_bbb = self.reverse_loglik()
    else:
        new_bbb = 0
    new_ddd = self.data_loglik()

    l_new = new_aaa + new_bbb + new_ddd
    r_accept = l_new - l_old

    r = pl.random.misc.fast_sample_standard_uniform()
    if math.log(r) &gt; r_accept:
        self.sum_q[tidx] = self.sum_q[tidx] + prev_x_value - x_new
        self.curr_x[tidx] = prev_x_value
        self.curr_logx[tidx] = prev_logx_value
    else:
        self.x[oidx, tidx] = x_new
        self.logx[oidx, tidx] = logx_new
        self.acceptances += 1
        self.total_acceptances += 1
        self.n_accepted_iter += 1

    self.n_props_local += 1
    self.n_props_total += 1</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="mdsine2.posterior.TrajectorySet"><code class="flex name class">
<span>class <span class="ident">TrajectorySet</span></span>
<span>(</span><span>G: <a title="mdsine2.pylab.graph.Graph" href="pylab/graph.html#mdsine2.pylab.graph.Graph">Graph</a>, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>This aggregates a set of trajectories from each set</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>G</code></strong> :&ensp;<code>pylab.graph.Graph</code></dt>
<dd>Graph object to attach it to</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TrajectorySet(pl.graph.Node):
    &#39;&#39;&#39;This aggregates a set of trajectories from each set

    Parameters
    ----------
    G : pylab.graph.Graph
        Graph object to attach it to
    &#39;&#39;&#39;
    def __init__(self, G: Graph, **kwargs):
        name = STRNAMES.LATENT_TRAJECTORY
        pl.graph.Node.__init__(self, name=name, G=G)
        self.value = []
        n_taxa = self.G.data.n_taxa

        for ridx, subj in enumerate(self.G.data.subjects):
            n_timepoints = self.G.data.n_timepoints_for_replicate[ridx]

            # initialize values to zeros for initialization
            self.value.append(pl.variables.Variable(
                name=name+&#39;_{}&#39;.format(subj.name), G=G, shape=(n_taxa, n_timepoints),
                value=np.zeros((n_taxa, n_timepoints), dtype=float), **kwargs))
        prior = pl.variables.Normal(
            loc=pl.variables.Constant(name=self.name+&#39;_prior_loc&#39;, value=0, G=self.G),
            scale2=pl.variables.Constant(name=self.name+&#39;_prior_scale2&#39;, value=1, G=self.G),
            name=self.name+&#39;_prior&#39;, G=self.G)
        self.add_prior(prior)

    def __getitem__(self, ridx: int) -&gt; variables.Variable:
        return self.value[ridx]

    @property
    def sample_iter(self) -&gt; int:
        return self.value[0].sample_iter

    def reset_value_size(self):
        &#39;&#39;&#39;Change the size of the trajectory when we set the intermediate timepoints
        &#39;&#39;&#39;
        n_taxa = self.G.data.n_taxa
        for ridx in range(len(self.value)):
            n_timepoints = self.G.data.n_timepoints_for_replicate[ridx]
            self.value[ridx].value = np.zeros((n_taxa, n_timepoints),dtype=float)
            self.value[ridx].set_value_shape(self.value[ridx].value.shape)

    def _vectorize(self) -&gt; np.ndarray:
        &#39;&#39;&#39;Get all the data in vector form
        &#39;&#39;&#39;
        vals = np.array([])
        for data in self.value:
            vals = np.append(vals, data.value)
        return vals

    def set_trace(self, *args, **kwargs):
        for ridx in range(len(self.value)):
            self.value[ridx].set_trace(*args, **kwargs)

    def add_trace(self):
        for ridx in range(len(self.value)):
            # Set the zero inflation values to nans
            self.value[ridx].value[~self.G[STRNAMES.ZERO_INFLATION].value[ridx]] = np.nan
            self.value[ridx].add_trace()

    def visualize(self, ridx: int, section: str, basepath: str, taxa_formatter: str, 
        vmin: Union[float, int]=None, vmax: Union[float, int]=None):
        &#39;&#39;&#39;Visualize the replicate at index `ridx`
        &#39;&#39;&#39;
        subjset = self.G.data.subjects
        taxa = subjset.taxa
        subj = subjset.iloc(ridx)
        obj = self.value[ridx]
        logging.info(&#39;Retrieving trace for subject {}&#39;.format(subj.name))

        given_data = subj.matrix()[&#39;abs&#39;]
        given_times = subj.times
        
        trace = obj.get_trace_from_disk(section=section)
        summ = pl.summary(trace)
        data_times = self.G.data.times[ridx]
        index = [taxon.name for taxon in taxa]

        # Make the tables
        for k,arr in summ.items():
            df = pd.DataFrame(arr, index=index, columns=data_times)
            df.to_csv(os.path.join(basepath, &#39;{}.tsv&#39;.format(k)), 
                sep=&#39;\t&#39;, index=True, header=True)

        percentile2_5 = np.nanpercentile(trace, q=2.5, axis=0)
        percentile97_5 = np.nanpercentile(trace, q=97.5, axis=0)
        
        acceptance_rates = []
        for oidx in range(len(subjset.taxa)):
            fig = plt.figure()
            title = pl.taxaname_formatter(format=taxa_formatter, taxon=oidx, taxa=taxa)
            title += &#39;\nSubject {}, {}&#39;.format(subj.name, taxa[oidx].name)
            fig.suptitle(title)
            ax = fig.add_subplot(111)

            med_traj = summ[&#39;median&#39;][oidx, :]
            low_traj = percentile2_5[oidx, :]
            high_traj = percentile97_5[oidx, :]

            ax.plot(data_times, med_traj, color=&#39;blue&#39;, marker=&#39;.&#39;, label=&#39;median&#39;)
            ax.fill_between(data_times, y1=low_traj, y2=high_traj, color=&#39;blue&#39;, 
                alpha=0.15, label=&#39;95th percentile&#39;)
            ax.plot(given_times, given_data[oidx, :], marker=&#39;x&#39;, color=&#39;black&#39;, 
                linestyle=&#39;:&#39;, label=&#39;data&#39;)

            ax.set_yscale(&#39;log&#39;)
            ax.set_xlabel(&#39;Time (days)&#39;)
            ax.set_ylabel(&#39;CFU/g&#39;)

            ax = visualization.shade_in_perturbations(ax, perturbations=subjset.perturbations, subj=subj)
            ax.legend(bbox_to_anchor=(1.05, 1))
            fig.tight_layout()
            plt.savefig(os.path.join(basepath, &#39;{}.pdf&#39;.format(taxa[oidx].name)))
            plt.close()

            # Calculate the acceptance rate at every taxon and timepoint
            temp = []
            for tidx in range(trace.shape[-1]):
                temp.append(pl.metropolis.acceptance_rate(x=trace[:, oidx, tidx], 
                    start=0, end=trace.shape[0]))
            acceptance_rates.append(temp)

        df = pd.DataFrame(acceptance_rates, index=index, columns=data_times)
        df.to_csv(os.path.join(basepath, &#39;acceptance_rates.tsv&#39;), sep=&#39;\t&#39;, index=True, header=True)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="mdsine2.pylab.graph.Node" href="pylab/graph.html#mdsine2.pylab.graph.Node">Node</a></li>
<li><a title="mdsine2.pylab.graph.BaseNode" href="pylab/graph.html#mdsine2.pylab.graph.BaseNode">BaseNode</a></li>
<li><a title="mdsine2.pylab.base.Saveable" href="pylab/base.html#mdsine2.pylab.base.Saveable">Saveable</a></li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="mdsine2.posterior.TrajectorySet.sample_iter"><code class="name">var <span class="ident">sample_iter</span> : int</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def sample_iter(self) -&gt; int:
    return self.value[0].sample_iter</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="mdsine2.posterior.TrajectorySet.add_trace"><code class="name flex">
<span>def <span class="ident">add_trace</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add_trace(self):
    for ridx in range(len(self.value)):
        # Set the zero inflation values to nans
        self.value[ridx].value[~self.G[STRNAMES.ZERO_INFLATION].value[ridx]] = np.nan
        self.value[ridx].add_trace()</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.TrajectorySet.reset_value_size"><code class="name flex">
<span>def <span class="ident">reset_value_size</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Change the size of the trajectory when we set the intermediate timepoints</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reset_value_size(self):
    &#39;&#39;&#39;Change the size of the trajectory when we set the intermediate timepoints
    &#39;&#39;&#39;
    n_taxa = self.G.data.n_taxa
    for ridx in range(len(self.value)):
        n_timepoints = self.G.data.n_timepoints_for_replicate[ridx]
        self.value[ridx].value = np.zeros((n_taxa, n_timepoints),dtype=float)
        self.value[ridx].set_value_shape(self.value[ridx].value.shape)</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.TrajectorySet.set_trace"><code class="name flex">
<span>def <span class="ident">set_trace</span></span>(<span>self, *args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_trace(self, *args, **kwargs):
    for ridx in range(len(self.value)):
        self.value[ridx].set_trace(*args, **kwargs)</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.TrajectorySet.visualize"><code class="name flex">
<span>def <span class="ident">visualize</span></span>(<span>self, ridx: int, section: str, basepath: str, taxa_formatter: str, vmin: Union[float, int] = None, vmax: Union[float, int] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Visualize the replicate at index <code>ridx</code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def visualize(self, ridx: int, section: str, basepath: str, taxa_formatter: str, 
    vmin: Union[float, int]=None, vmax: Union[float, int]=None):
    &#39;&#39;&#39;Visualize the replicate at index `ridx`
    &#39;&#39;&#39;
    subjset = self.G.data.subjects
    taxa = subjset.taxa
    subj = subjset.iloc(ridx)
    obj = self.value[ridx]
    logging.info(&#39;Retrieving trace for subject {}&#39;.format(subj.name))

    given_data = subj.matrix()[&#39;abs&#39;]
    given_times = subj.times
    
    trace = obj.get_trace_from_disk(section=section)
    summ = pl.summary(trace)
    data_times = self.G.data.times[ridx]
    index = [taxon.name for taxon in taxa]

    # Make the tables
    for k,arr in summ.items():
        df = pd.DataFrame(arr, index=index, columns=data_times)
        df.to_csv(os.path.join(basepath, &#39;{}.tsv&#39;.format(k)), 
            sep=&#39;\t&#39;, index=True, header=True)

    percentile2_5 = np.nanpercentile(trace, q=2.5, axis=0)
    percentile97_5 = np.nanpercentile(trace, q=97.5, axis=0)
    
    acceptance_rates = []
    for oidx in range(len(subjset.taxa)):
        fig = plt.figure()
        title = pl.taxaname_formatter(format=taxa_formatter, taxon=oidx, taxa=taxa)
        title += &#39;\nSubject {}, {}&#39;.format(subj.name, taxa[oidx].name)
        fig.suptitle(title)
        ax = fig.add_subplot(111)

        med_traj = summ[&#39;median&#39;][oidx, :]
        low_traj = percentile2_5[oidx, :]
        high_traj = percentile97_5[oidx, :]

        ax.plot(data_times, med_traj, color=&#39;blue&#39;, marker=&#39;.&#39;, label=&#39;median&#39;)
        ax.fill_between(data_times, y1=low_traj, y2=high_traj, color=&#39;blue&#39;, 
            alpha=0.15, label=&#39;95th percentile&#39;)
        ax.plot(given_times, given_data[oidx, :], marker=&#39;x&#39;, color=&#39;black&#39;, 
            linestyle=&#39;:&#39;, label=&#39;data&#39;)

        ax.set_yscale(&#39;log&#39;)
        ax.set_xlabel(&#39;Time (days)&#39;)
        ax.set_ylabel(&#39;CFU/g&#39;)

        ax = visualization.shade_in_perturbations(ax, perturbations=subjset.perturbations, subj=subj)
        ax.legend(bbox_to_anchor=(1.05, 1))
        fig.tight_layout()
        plt.savefig(os.path.join(basepath, &#39;{}.pdf&#39;.format(taxa[oidx].name)))
        plt.close()

        # Calculate the acceptance rate at every taxon and timepoint
        temp = []
        for tidx in range(trace.shape[-1]):
            temp.append(pl.metropolis.acceptance_rate(x=trace[:, oidx, tidx], 
                start=0, end=trace.shape[0]))
        acceptance_rates.append(temp)

    df = pd.DataFrame(acceptance_rates, index=index, columns=data_times)
    df.to_csv(os.path.join(basepath, &#39;acceptance_rates.tsv&#39;), sep=&#39;\t&#39;, index=True, header=True)</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="mdsine2.pylab.graph.Node" href="pylab/graph.html#mdsine2.pylab.graph.Node">Node</a></b></code>:
<ul class="hlist">
<li><code><a title="mdsine2.pylab.graph.Node.add_child" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_child">add_child</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.add_parent" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_parent">add_parent</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.add_prior" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_prior">add_prior</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.add_undirected" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_undirected">add_undirected</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.degree" href="pylab/graph.html#mdsine2.pylab.graph.Node.degree">degree</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.delete" href="pylab/graph.html#mdsine2.pylab.graph.BaseNode.delete">delete</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.get_adjacent_keys" href="pylab/graph.html#mdsine2.pylab.graph.Node.get_adjacent_keys">get_adjacent_keys</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.load" href="pylab/base.html#mdsine2.pylab.base.Saveable.load">load</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.metropolis" href="pylab/graph.html#mdsine2.pylab.graph.Node.metropolis">metropolis</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.save" href="pylab/base.html#mdsine2.pylab.base.Saveable.save">save</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.set_save_location" href="pylab/base.html#mdsine2.pylab.base.Saveable.set_save_location">set_save_location</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="mdsine2.posterior.ZeroInflation"><code class="flex name class">
<span>class <span class="ident">ZeroInflation</span></span>
<span>(</span><span>**kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>This is the posterior distribution for the zero inflation model. These are used
to learn when the model should use use the data and when it should not. We do not need
to trace this object because we set the structural zeros to nans in the trace for
filtering.</p>
<p>TODO: Parallel version of the class</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>mp</code></strong> :&ensp;<code>str</code></dt>
<dd>This is the type of parallelization to use. This is not implemented yet.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ZeroInflation(pl.graph.Node):
    &#39;&#39;&#39;This is the posterior distribution for the zero inflation model. These are used
    to learn when the model should use use the data and when it should not. We do not need
    to trace this object because we set the structural zeros to nans in the trace for 
    filtering.

    TODO: Parallel version of the class
    &#39;&#39;&#39;

    def __init__(self, **kwargs):
        &#39;&#39;&#39;
        Parameters
        ----------
        mp : str
            This is the type of parallelization to use. This is not implemented yet.
        &#39;&#39;&#39;
        kwargs[&#39;name&#39;] = STRNAMES.ZERO_INFLATION
        pl.graph.Node.__init__(self, **kwargs)
        self.value = []
        self._strr = &#39;NA&#39;

        for ridx in range(self.G.data.n_replicates):
            n_timepoints = self.G.data.n_timepoints_for_replicate[ridx]
            self.value.append(np.ones(shape=(len(self.G.data.taxa), n_timepoints), dtype=bool))

    def reset_value_size(self):
        &#39;&#39;&#39;Change the size of the trajectory when we set the intermediate timepoints
        &#39;&#39;&#39;
        n_taxa = self.G.data.n_taxa
        for ridx in range(len(self.value)):
            n_timepoints = self.G.data.n_timepoints_for_replicate[ridx]
            self.value[ridx] = np.ones((n_taxa, n_timepoints), dtype=bool)

    def __str__(self) -&gt; str:
        return self._strr

    def initialize(self, value_option: str, delay: int=0):
        &#39;&#39;&#39;Initialize the values. Right now this is static and we are not learning this so
        do not do anything fancy

        Parameters
        ----------

        delay : None, int
            How much to delay starting the sampling
        &#39;&#39;&#39;
        if delay is None:
            delay = 0
        if not pl.isint(delay):
            raise TypeError(&#39;`delay` ({}) must be an int&#39;.format(type(delay)))
        self.delay = delay

        if value_option in [None, &#39;auto&#39;]:
            # Set everything to on
            self.value = []
            for ridx in range(self.G.data.n_replicates):
                n_timepoints = self.G.data.n_timepoints_for_replicate[ridx]
                self.value.append(np.ones(
                    shape=(len(self.G.data.taxa), n_timepoints), dtype=bool))
            turn_on = None
            turn_off = None

        elif value_option == &#39;mdsine-cdiff&#39;:
            # Set everything to on except for cdiff before day 28 for every subject
            self.value = []
            for ridx in range(self.G.data.n_replicates):
                n_timepoints = self.G.data.n_timepoints_for_replicate[ridx]
                self.value.append(np.ones(
                    shape=(len(self.G.data.taxa), n_timepoints), dtype=bool))

            # Get cdiff
            cdiff_idx = self.G.data.taxa[&#39;Clostridium-difficile&#39;].idx
            turn_off = []
            turn_on = []
            for ridx in range(self.G.data.n_replicates):
                for tidx, t in enumerate(self.G.data.times[ridx]):
                    for oidx in range(len(self.G.data.taxa)):
                        if t &lt; 28 and oidx == cdiff_idx:
                            self.value[ridx][cdiff_idx, tidx] = False
                            turn_off.append((ridx, tidx, cdiff_idx))
                        else:
                            turn_on.append((ridx, tidx, oidx))

        else:
            raise ValueError(&#39;`value_option` ({}) not recognized&#39;.format(value_option))

        self.G.data.set_zero_inflation(turn_on=turn_on, turn_off=turn_off)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="mdsine2.pylab.graph.Node" href="pylab/graph.html#mdsine2.pylab.graph.Node">Node</a></li>
<li><a title="mdsine2.pylab.graph.BaseNode" href="pylab/graph.html#mdsine2.pylab.graph.BaseNode">BaseNode</a></li>
<li><a title="mdsine2.pylab.base.Saveable" href="pylab/base.html#mdsine2.pylab.base.Saveable">Saveable</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="mdsine2.posterior.ZeroInflation.initialize"><code class="name flex">
<span>def <span class="ident">initialize</span></span>(<span>self, value_option: str, delay: int = 0)</span>
</code></dt>
<dd>
<div class="desc"><p>Initialize the values. Right now this is static and we are not learning this so
do not do anything fancy</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>delay</code></strong> :&ensp;<code>None, int</code></dt>
<dd>How much to delay starting the sampling</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def initialize(self, value_option: str, delay: int=0):
    &#39;&#39;&#39;Initialize the values. Right now this is static and we are not learning this so
    do not do anything fancy

    Parameters
    ----------

    delay : None, int
        How much to delay starting the sampling
    &#39;&#39;&#39;
    if delay is None:
        delay = 0
    if not pl.isint(delay):
        raise TypeError(&#39;`delay` ({}) must be an int&#39;.format(type(delay)))
    self.delay = delay

    if value_option in [None, &#39;auto&#39;]:
        # Set everything to on
        self.value = []
        for ridx in range(self.G.data.n_replicates):
            n_timepoints = self.G.data.n_timepoints_for_replicate[ridx]
            self.value.append(np.ones(
                shape=(len(self.G.data.taxa), n_timepoints), dtype=bool))
        turn_on = None
        turn_off = None

    elif value_option == &#39;mdsine-cdiff&#39;:
        # Set everything to on except for cdiff before day 28 for every subject
        self.value = []
        for ridx in range(self.G.data.n_replicates):
            n_timepoints = self.G.data.n_timepoints_for_replicate[ridx]
            self.value.append(np.ones(
                shape=(len(self.G.data.taxa), n_timepoints), dtype=bool))

        # Get cdiff
        cdiff_idx = self.G.data.taxa[&#39;Clostridium-difficile&#39;].idx
        turn_off = []
        turn_on = []
        for ridx in range(self.G.data.n_replicates):
            for tidx, t in enumerate(self.G.data.times[ridx]):
                for oidx in range(len(self.G.data.taxa)):
                    if t &lt; 28 and oidx == cdiff_idx:
                        self.value[ridx][cdiff_idx, tidx] = False
                        turn_off.append((ridx, tidx, cdiff_idx))
                    else:
                        turn_on.append((ridx, tidx, oidx))

    else:
        raise ValueError(&#39;`value_option` ({}) not recognized&#39;.format(value_option))

    self.G.data.set_zero_inflation(turn_on=turn_on, turn_off=turn_off)</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.ZeroInflation.reset_value_size"><code class="name flex">
<span>def <span class="ident">reset_value_size</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Change the size of the trajectory when we set the intermediate timepoints</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reset_value_size(self):
    &#39;&#39;&#39;Change the size of the trajectory when we set the intermediate timepoints
    &#39;&#39;&#39;
    n_taxa = self.G.data.n_taxa
    for ridx in range(len(self.value)):
        n_timepoints = self.G.data.n_timepoints_for_replicate[ridx]
        self.value[ridx] = np.ones((n_taxa, n_timepoints), dtype=bool)</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="mdsine2.pylab.graph.Node" href="pylab/graph.html#mdsine2.pylab.graph.Node">Node</a></b></code>:
<ul class="hlist">
<li><code><a title="mdsine2.pylab.graph.Node.add_child" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_child">add_child</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.add_parent" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_parent">add_parent</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.add_prior" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_prior">add_prior</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.add_undirected" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_undirected">add_undirected</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.degree" href="pylab/graph.html#mdsine2.pylab.graph.Node.degree">degree</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.delete" href="pylab/graph.html#mdsine2.pylab.graph.BaseNode.delete">delete</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.get_adjacent_keys" href="pylab/graph.html#mdsine2.pylab.graph.Node.get_adjacent_keys">get_adjacent_keys</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.load" href="pylab/base.html#mdsine2.pylab.base.Saveable.load">load</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.metropolis" href="pylab/graph.html#mdsine2.pylab.graph.Node.metropolis">metropolis</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.save" href="pylab/base.html#mdsine2.pylab.base.Saveable.save">save</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.set_save_location" href="pylab/base.html#mdsine2.pylab.base.Saveable.set_save_location">set_save_location</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="mdsine2.posterior.qPCRDegsOfFreedomL"><code class="flex name class">
<span>class <span class="ident">qPCRDegsOfFreedomL</span></span>
<span>(</span><span>L, l, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Posterior for a single qPCR degrees of freedom parameter for a SICS set</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>L</code></strong> :&ensp;<code>int</code></dt>
<dd>How many qPCR variance groupings there are</dd>
<dt><strong><code>l</code></strong> :&ensp;<code>int</code></dt>
<dd>Which specific grouping this hyperprior is</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class qPCRDegsOfFreedomL(pl.variables.Uniform):
    &#39;&#39;&#39;Posterior for a single qPCR degrees of freedom parameter for a SICS set
    
    Parameters
    ----------
    L : int
        How many qPCR variance groupings there are
    l : int
        Which specific grouping this hyperprior is
    &#39;&#39;&#39;
    def __init__(self, L, l, **kwargs):

        self.L = L
        self.l = l
        kwargs[&#39;name&#39;] = STRNAMES.QPCR_DOFS + &#39;_{}&#39;.format(l)
        pl.variables.Uniform.__init__(self, **kwargs)

        self.data_locs = []
        self.proposal = pl.variables.TruncatedNormal(loc=None, scale2=None, value=None)

    def __str__(self):
        # If this fails, it is because we are dividing by 0 sampler_iter
        # If which case we just return the value 
        try:
            s = &#39;Value: {}, Acceptance rate: {}&#39;.format(
                self.value, np.mean(self.acceptances[
                    np.max([self.sample_iter-50, 0]):self.sample_iter]))
        except:
            s = str(self.value)
        return s

    def set_shape(self):
        &#39;&#39;&#39;Set the shape of the array (how many qPCR variances this is a prior for)
        &#39;&#39;&#39;
        self.set_value_shape(shape=(len(self.data_locs), ))

    def add_qpcr_measurement(self, ridx, tidx):
        &#39;&#39;&#39;Add the qPCR measurement for subject index `ridx` and time index
        `tidx` to 
        &#39;&#39;&#39;
        self.data_locs.append((ridx, tidx))

    def initialize(self, value_option, low_option, high_option, proposal_option, 
        target_acceptance_rate, tune, end_tune, value=None, low=None, high=None, 
        proposal_var=None, delay=0):
        &#39;&#39;&#39;Initialize the values and hyperparameters. The proposal truncation is 
        always set to the same as the parameterization of the prior.

        Parameters
        ----------
        value_option : str
            How to initialize the value. Options:
                &#39;auto&#39;, &#39;diffuse&#39;
                    Set the value to 2.5
                &#39;strong&#39;
                    Set to be 50% of the data
                &#39;manual&#39;
                    `value` must also be specified
        low_option : str
            How to set the low parameter of the prior
            &#39;auto&#39;, &#39;valid&#39;
                Set to 2 so that the prior stays proper during inference
            &#39;zero&#39;
                Set to 0
            &#39;manual&#39;
                Specify the value with the parameter `low`
        high_option : str
            How to set the high parameter of the prior
            &#39;auto&#39;, &#39;med&#39;
                Set to 10 X the maximum in the set
            &#39;high&#39;
                Set to 100 X the maximum in the set
            &#39;low&#39;
                Set to 1 X the maximum in the set
            &#39;manual&#39;
                Set the value with the parameter `high`
        proposal_option : str
            How to initialize the proposal variance:
                &#39;auto&#39;
                    mean**2 / 100
                &#39;manual&#39;
                    `proposal_var` must also be supplied
        &#39;&#39;&#39;
        if not pl.isint(delay):
            raise TypeError(&#39;`delay` ({}) must be an int&#39;.format(type(delay)))
        if delay &lt; 0:
            raise ValueError(&#39;`delay` ({}) must be &gt;= 0&#39;.format(delay))
        self.delay = delay

        self.qpcr_data = []
        for ridx, tidx in self.data_locs:
            t = self.G.data.given_timepoints[ridx][tidx]
            self.qpcr_data = np.append(self.qpcr_data, 
                self.G.data.qpcr[ridx][t].log_data)
        
        # Set the prior low
        if not pl.isstr(low_option):
            raise TypeError(&#39;`low_option` ({}) must be a str&#39;.format(type(low_option)))
        if low_option == &#39;manual&#39;:
            if not pl.isnumeric(low):
                raise TypeError(&#39;`low` ({}) must be a numeric&#39;.format(type(low)))
            if low &lt; 2:
                raise ValueError(&#39;`low` ({}) must be &gt;= 2&#39;.format(low))
        elif low_option in [&#39;valid&#39;, &#39;auto&#39;]:
            low = 2
        elif low_option == &#39;zero&#39;:
            low = 0
        else:
            raise ValueError(&#39;`low_option` ({}) not recognized&#39;.format(low_option))
        self.prior.low.override_value(low)  

        # Set the prior high
        if not pl.isstr(high_option):
            raise TypeError(&#39;`high_option` ({}) must be a str&#39;.format(type(high_option)))
        if high_option == &#39;manual&#39;:
            if not pl.isnumeric(high):
                raise TypeError(&#39;`high` ({}) must be a numeric&#39;.format(type(high)))
        elif high_option in [&#39;med&#39;, &#39;auto&#39;]:
            high = 10 * len(self.data_locs)
        elif high_option == &#39;low&#39;:
            high = len(self.data_locs)
        elif high_option == &#39;high&#39;:
            high = 100 * len(self.data_locs)
        else:
            raise ValueError(&#39;`high_option` ({}) not recognized&#39;.format(high_option))
        if high &lt; self.prior.low.value:
            raise ValueError(&#39;`high` ({}) must be &gt;= low ({})&#39;.format(high, 
                self.prior.low.value))
        self.prior.high.override_value(high)

        # Set the value
        if not pl.isstr(value_option):
            raise TypeError(&#39;`value_option` ({}) must be a str&#39;.format(type(value_option)))
        if value_option == &#39;manual&#39;:
            if not pl.isnumeric(value):
                raise TypeError(&#39;`value` ({}) must be a numeric&#39;.format(type(value)))
        elif value_option in [&#39;auto&#39;, &#39;diffuse&#39;]:
            value = 2.5
        elif value_option == &#39;strong&#39;:
            value = len(self.data_locs)
        else:
            raise ValueError(&#39;`value_option` ({}) not recognized&#39;.format(value_option))
        self.value = value
        if self.value &lt;= self.prior.low.value or self.value &gt;= self.prior.high.value:
            raise ValueError(&#39;`value` ({}) out of range ({})&#39;.format(self.value))

        # Set the propsal parameters
        if pl.isstr(target_acceptance_rate):
            if target_acceptance_rate in [&#39;optimal&#39;, &#39;auto&#39;]:
                target_acceptance_rate = 0.44
            else:
                raise ValueError(&#39;`target_acceptance_rate` ({}) not recognized&#39;.format(
                    target_acceptance_rate))
        elif pl.isfloat(target_acceptance_rate):
            if target_acceptance_rate &lt; 0 or target_acceptance_rate &gt; 1:
                raise ValueError(&#39;`target_acceptance_rate` ({}) out of range&#39;.format(
                    target_acceptance_rate))
        else:
            raise TypeError(&#39;`target_acceptance_rate` ({}) type not recognized&#39;.format(
                type(target_acceptance_rate)))
        self.target_acceptance_rate = target_acceptance_rate
        
        if pl.isstr(tune):
            if tune in [&#39;auto&#39;]:
                tune = 50
            else:
                raise ValueError(&#39;`tune` ({}) not recognized&#39;.format(tune))
        elif pl.isint(tune):
            if tune &lt; 0:
                raise ValueError(&#39;`tune` ({}) must be &gt; 0&#39;.format(
                    tune))
        else:
            raise TypeError(&#39;`tune` ({}) type not recognized&#39;.format(type(tune)))
        self.tune = tune

        if pl.isstr(end_tune):
            if end_tune in [&#39;auto&#39;, &#39;half-burnin&#39;]:
                end_tune = int(self.G.inference.burnin/2)
            else:
                raise ValueError(&#39;`tune` ({}) not recognized&#39;.format(end_tune))
        elif pl.isint(end_tune):
            if end_tune &lt; 0 or end_tune &gt; self.G.inference.burnin:
                raise ValueError(&#39;`end_tune` ({}) out of range (0, {})&#39;.format(
                    end_tune, self.G.inference.burnin))
        else:
            raise TypeError(&#39;`end_tune` ({}) type not recognized&#39;.format(type(end_tune)))
        self.end_tune = end_tune

        # Set the proposal variance
        if not pl.isstr(proposal_option):
            raise TypeError(&#39;`proposal_option` ({}) must be a str&#39;.format(
                type(proposal_option)))
        elif proposal_option == &#39;manual&#39;:
            if not pl.isnumeric(proposal_var):
                raise TypeError(&#39;`proposal_var` ({}) must be a numeric&#39;.format(
                    type(proposal_var)))
            if proposal_var &lt;= 0:
                raise ValueError(&#39;`proposal_var` ({}) not proper&#39;.format(proposal_var))
        elif proposal_option in [&#39;auto&#39;]:
            proposal_var = (self.value ** 2)/10
        else:
            raise ValueError(&#39;`proposal_option` ({}) not recognized&#39;.format(
                proposal_option))
        self.proposal.scale2.value = proposal_var
        self.proposal.low = self.prior.low.value
        self.proposal.high = self.prior.high.value

    def update_var(self):
        &#39;&#39;&#39;Update the variance of the proposal
        &#39;&#39;&#39;
        if self.sample_iter == 0:
            self.temp_acceptances = 0
            self.acceptances = np.zeros(self.G.inference.n_samples, dtype=bool)
        
        elif self.sample_iter &gt; self.end_tune:
            # Don&#39;t do any more updates
            return
        
        elif self.sample_iter % self.tune == 0:
            # Update var
            acceptance_rate = self.temp_acceptances / self.tune
            if acceptance_rate &gt; self.target_acceptance_rate:
                self.proposal.scale2.value *= 1.5
            else:
                self.proposal.scale2.value /= 1.5
            self.temp_acceptances = 0

    def update(self):
        &#39;&#39;&#39;First we update the proposal (if necessary) and then we do a MH step
        &#39;&#39;&#39;
        self.update_var()
        proposal_std = np.sqrt(self.proposal.scale2.value)

        # Get the data
        xs = []
        for ridx, tidx in self.data_locs:
            xs.append(self.G[STRNAMES.QPCR_VARIANCES].value[ridx].value[tidx])

        # Get the scale
        scale = self.G[STRNAMES.QPCR_SCALES].value[self.l].value

        # Propose a new value for the dof
        prev_dof = self.value
        self.proposal.loc.value = self.value
        new_dof = self.proposal.sample()

        if new_dof &lt; self.prior.low.value or new_dof &gt; self.prior.high.value:
            # Automatic reject
            self.value = prev_dof
            return

        # Calculate the target distribution log likelihood
        prev_target_ll = 0
        for x in xs:
            prev_target_ll += pl.random.sics.logpdf(value=x,
                scale=scale, dof=prev_dof)
        new_target_ll = 0
        for x in xs:
            new_target_ll += pl.random.sics.logpdf(value=x,
                scale=scale, dof=new_dof)

        # Normalize by the loglikelihood of the proposal
        prev_prop_ll = pl.random.truncnormal.logpdf(
            value=prev_dof, loc=new_dof, scale=proposal_std,
            low=self.proposal.low, high=self.proposal.high)
        new_prop_ll = pl.random.truncnormal.logpdf(
            value=new_dof, loc=prev_dof, scale=proposal_std,
            low=self.proposal.low, high=self.proposal.high)

        # Accept or reject
        r = (new_target_ll - prev_prop_ll) - \
            (prev_target_ll - new_prop_ll)
        u = np.log(pl.random.misc.fast_sample_standard_uniform())
        if r &gt;= u:
            self.acceptances[self.sample_iter] = True
            self.value = new_dof
            self.temp_acceptances += 1
        else:
            self.value = prev_dof</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="mdsine2.pylab.variables.Uniform" href="pylab/variables.html#mdsine2.pylab.variables.Uniform">Uniform</a></li>
<li><a title="mdsine2.pylab.variables.Variable" href="pylab/variables.html#mdsine2.pylab.variables.Variable">Variable</a></li>
<li><a title="mdsine2.pylab.graph.Node" href="pylab/graph.html#mdsine2.pylab.graph.Node">Node</a></li>
<li><a title="mdsine2.pylab.graph.BaseNode" href="pylab/graph.html#mdsine2.pylab.graph.BaseNode">BaseNode</a></li>
<li><a title="mdsine2.pylab.base.Saveable" href="pylab/base.html#mdsine2.pylab.base.Saveable">Saveable</a></li>
<li>mdsine2.pylab.variables._BaseArithmeticClass</li>
<li><a title="mdsine2.pylab.base.Traceable" href="pylab/base.html#mdsine2.pylab.base.Traceable">Traceable</a></li>
<li>mdsine2.pylab.variables._RandomBase</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="mdsine2.posterior.qPCRDegsOfFreedomL.add_qpcr_measurement"><code class="name flex">
<span>def <span class="ident">add_qpcr_measurement</span></span>(<span>self, ridx, tidx)</span>
</code></dt>
<dd>
<div class="desc"><p>Add the qPCR measurement for subject index <code>ridx</code> and time index
<code>tidx</code> to</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add_qpcr_measurement(self, ridx, tidx):
    &#39;&#39;&#39;Add the qPCR measurement for subject index `ridx` and time index
    `tidx` to 
    &#39;&#39;&#39;
    self.data_locs.append((ridx, tidx))</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.qPCRDegsOfFreedomL.initialize"><code class="name flex">
<span>def <span class="ident">initialize</span></span>(<span>self, value_option, low_option, high_option, proposal_option, target_acceptance_rate, tune, end_tune, value=None, low=None, high=None, proposal_var=None, delay=0)</span>
</code></dt>
<dd>
<div class="desc"><p>Initialize the values and hyperparameters. The proposal truncation is
always set to the same as the parameterization of the prior.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>value_option</code></strong> :&ensp;<code>str</code></dt>
<dd>How to initialize the value. Options:
'auto', 'diffuse'
Set the value to 2.5
'strong'
Set to be 50% of the data
'manual'
<code>value</code> must also be specified</dd>
<dt><strong><code>low_option</code></strong> :&ensp;<code>str</code></dt>
<dd>How to set the low parameter of the prior
'auto', 'valid'
Set to 2 so that the prior stays proper during inference
'zero'
Set to 0
'manual'
Specify the value with the parameter <code>low</code></dd>
<dt><strong><code>high_option</code></strong> :&ensp;<code>str</code></dt>
<dd>How to set the high parameter of the prior
'auto', 'med'
Set to 10 X the maximum in the set
'high'
Set to 100 X the maximum in the set
'low'
Set to 1 X the maximum in the set
'manual'
Set the value with the parameter <code>high</code></dd>
<dt><strong><code>proposal_option</code></strong> :&ensp;<code>str</code></dt>
<dd>How to initialize the proposal variance:
'auto'
mean**2 / 100
'manual'
<code>proposal_var</code> must also be supplied</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def initialize(self, value_option, low_option, high_option, proposal_option, 
    target_acceptance_rate, tune, end_tune, value=None, low=None, high=None, 
    proposal_var=None, delay=0):
    &#39;&#39;&#39;Initialize the values and hyperparameters. The proposal truncation is 
    always set to the same as the parameterization of the prior.

    Parameters
    ----------
    value_option : str
        How to initialize the value. Options:
            &#39;auto&#39;, &#39;diffuse&#39;
                Set the value to 2.5
            &#39;strong&#39;
                Set to be 50% of the data
            &#39;manual&#39;
                `value` must also be specified
    low_option : str
        How to set the low parameter of the prior
        &#39;auto&#39;, &#39;valid&#39;
            Set to 2 so that the prior stays proper during inference
        &#39;zero&#39;
            Set to 0
        &#39;manual&#39;
            Specify the value with the parameter `low`
    high_option : str
        How to set the high parameter of the prior
        &#39;auto&#39;, &#39;med&#39;
            Set to 10 X the maximum in the set
        &#39;high&#39;
            Set to 100 X the maximum in the set
        &#39;low&#39;
            Set to 1 X the maximum in the set
        &#39;manual&#39;
            Set the value with the parameter `high`
    proposal_option : str
        How to initialize the proposal variance:
            &#39;auto&#39;
                mean**2 / 100
            &#39;manual&#39;
                `proposal_var` must also be supplied
    &#39;&#39;&#39;
    if not pl.isint(delay):
        raise TypeError(&#39;`delay` ({}) must be an int&#39;.format(type(delay)))
    if delay &lt; 0:
        raise ValueError(&#39;`delay` ({}) must be &gt;= 0&#39;.format(delay))
    self.delay = delay

    self.qpcr_data = []
    for ridx, tidx in self.data_locs:
        t = self.G.data.given_timepoints[ridx][tidx]
        self.qpcr_data = np.append(self.qpcr_data, 
            self.G.data.qpcr[ridx][t].log_data)
    
    # Set the prior low
    if not pl.isstr(low_option):
        raise TypeError(&#39;`low_option` ({}) must be a str&#39;.format(type(low_option)))
    if low_option == &#39;manual&#39;:
        if not pl.isnumeric(low):
            raise TypeError(&#39;`low` ({}) must be a numeric&#39;.format(type(low)))
        if low &lt; 2:
            raise ValueError(&#39;`low` ({}) must be &gt;= 2&#39;.format(low))
    elif low_option in [&#39;valid&#39;, &#39;auto&#39;]:
        low = 2
    elif low_option == &#39;zero&#39;:
        low = 0
    else:
        raise ValueError(&#39;`low_option` ({}) not recognized&#39;.format(low_option))
    self.prior.low.override_value(low)  

    # Set the prior high
    if not pl.isstr(high_option):
        raise TypeError(&#39;`high_option` ({}) must be a str&#39;.format(type(high_option)))
    if high_option == &#39;manual&#39;:
        if not pl.isnumeric(high):
            raise TypeError(&#39;`high` ({}) must be a numeric&#39;.format(type(high)))
    elif high_option in [&#39;med&#39;, &#39;auto&#39;]:
        high = 10 * len(self.data_locs)
    elif high_option == &#39;low&#39;:
        high = len(self.data_locs)
    elif high_option == &#39;high&#39;:
        high = 100 * len(self.data_locs)
    else:
        raise ValueError(&#39;`high_option` ({}) not recognized&#39;.format(high_option))
    if high &lt; self.prior.low.value:
        raise ValueError(&#39;`high` ({}) must be &gt;= low ({})&#39;.format(high, 
            self.prior.low.value))
    self.prior.high.override_value(high)

    # Set the value
    if not pl.isstr(value_option):
        raise TypeError(&#39;`value_option` ({}) must be a str&#39;.format(type(value_option)))
    if value_option == &#39;manual&#39;:
        if not pl.isnumeric(value):
            raise TypeError(&#39;`value` ({}) must be a numeric&#39;.format(type(value)))
    elif value_option in [&#39;auto&#39;, &#39;diffuse&#39;]:
        value = 2.5
    elif value_option == &#39;strong&#39;:
        value = len(self.data_locs)
    else:
        raise ValueError(&#39;`value_option` ({}) not recognized&#39;.format(value_option))
    self.value = value
    if self.value &lt;= self.prior.low.value or self.value &gt;= self.prior.high.value:
        raise ValueError(&#39;`value` ({}) out of range ({})&#39;.format(self.value))

    # Set the propsal parameters
    if pl.isstr(target_acceptance_rate):
        if target_acceptance_rate in [&#39;optimal&#39;, &#39;auto&#39;]:
            target_acceptance_rate = 0.44
        else:
            raise ValueError(&#39;`target_acceptance_rate` ({}) not recognized&#39;.format(
                target_acceptance_rate))
    elif pl.isfloat(target_acceptance_rate):
        if target_acceptance_rate &lt; 0 or target_acceptance_rate &gt; 1:
            raise ValueError(&#39;`target_acceptance_rate` ({}) out of range&#39;.format(
                target_acceptance_rate))
    else:
        raise TypeError(&#39;`target_acceptance_rate` ({}) type not recognized&#39;.format(
            type(target_acceptance_rate)))
    self.target_acceptance_rate = target_acceptance_rate
    
    if pl.isstr(tune):
        if tune in [&#39;auto&#39;]:
            tune = 50
        else:
            raise ValueError(&#39;`tune` ({}) not recognized&#39;.format(tune))
    elif pl.isint(tune):
        if tune &lt; 0:
            raise ValueError(&#39;`tune` ({}) must be &gt; 0&#39;.format(
                tune))
    else:
        raise TypeError(&#39;`tune` ({}) type not recognized&#39;.format(type(tune)))
    self.tune = tune

    if pl.isstr(end_tune):
        if end_tune in [&#39;auto&#39;, &#39;half-burnin&#39;]:
            end_tune = int(self.G.inference.burnin/2)
        else:
            raise ValueError(&#39;`tune` ({}) not recognized&#39;.format(end_tune))
    elif pl.isint(end_tune):
        if end_tune &lt; 0 or end_tune &gt; self.G.inference.burnin:
            raise ValueError(&#39;`end_tune` ({}) out of range (0, {})&#39;.format(
                end_tune, self.G.inference.burnin))
    else:
        raise TypeError(&#39;`end_tune` ({}) type not recognized&#39;.format(type(end_tune)))
    self.end_tune = end_tune

    # Set the proposal variance
    if not pl.isstr(proposal_option):
        raise TypeError(&#39;`proposal_option` ({}) must be a str&#39;.format(
            type(proposal_option)))
    elif proposal_option == &#39;manual&#39;:
        if not pl.isnumeric(proposal_var):
            raise TypeError(&#39;`proposal_var` ({}) must be a numeric&#39;.format(
                type(proposal_var)))
        if proposal_var &lt;= 0:
            raise ValueError(&#39;`proposal_var` ({}) not proper&#39;.format(proposal_var))
    elif proposal_option in [&#39;auto&#39;]:
        proposal_var = (self.value ** 2)/10
    else:
        raise ValueError(&#39;`proposal_option` ({}) not recognized&#39;.format(
            proposal_option))
    self.proposal.scale2.value = proposal_var
    self.proposal.low = self.prior.low.value
    self.proposal.high = self.prior.high.value</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.qPCRDegsOfFreedomL.set_shape"><code class="name flex">
<span>def <span class="ident">set_shape</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Set the shape of the array (how many qPCR variances this is a prior for)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_shape(self):
    &#39;&#39;&#39;Set the shape of the array (how many qPCR variances this is a prior for)
    &#39;&#39;&#39;
    self.set_value_shape(shape=(len(self.data_locs), ))</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.qPCRDegsOfFreedomL.update"><code class="name flex">
<span>def <span class="ident">update</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>First we update the proposal (if necessary) and then we do a MH step</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update(self):
    &#39;&#39;&#39;First we update the proposal (if necessary) and then we do a MH step
    &#39;&#39;&#39;
    self.update_var()
    proposal_std = np.sqrt(self.proposal.scale2.value)

    # Get the data
    xs = []
    for ridx, tidx in self.data_locs:
        xs.append(self.G[STRNAMES.QPCR_VARIANCES].value[ridx].value[tidx])

    # Get the scale
    scale = self.G[STRNAMES.QPCR_SCALES].value[self.l].value

    # Propose a new value for the dof
    prev_dof = self.value
    self.proposal.loc.value = self.value
    new_dof = self.proposal.sample()

    if new_dof &lt; self.prior.low.value or new_dof &gt; self.prior.high.value:
        # Automatic reject
        self.value = prev_dof
        return

    # Calculate the target distribution log likelihood
    prev_target_ll = 0
    for x in xs:
        prev_target_ll += pl.random.sics.logpdf(value=x,
            scale=scale, dof=prev_dof)
    new_target_ll = 0
    for x in xs:
        new_target_ll += pl.random.sics.logpdf(value=x,
            scale=scale, dof=new_dof)

    # Normalize by the loglikelihood of the proposal
    prev_prop_ll = pl.random.truncnormal.logpdf(
        value=prev_dof, loc=new_dof, scale=proposal_std,
        low=self.proposal.low, high=self.proposal.high)
    new_prop_ll = pl.random.truncnormal.logpdf(
        value=new_dof, loc=prev_dof, scale=proposal_std,
        low=self.proposal.low, high=self.proposal.high)

    # Accept or reject
    r = (new_target_ll - prev_prop_ll) - \
        (prev_target_ll - new_prop_ll)
    u = np.log(pl.random.misc.fast_sample_standard_uniform())
    if r &gt;= u:
        self.acceptances[self.sample_iter] = True
        self.value = new_dof
        self.temp_acceptances += 1
    else:
        self.value = prev_dof</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.qPCRDegsOfFreedomL.update_var"><code class="name flex">
<span>def <span class="ident">update_var</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Update the variance of the proposal</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_var(self):
    &#39;&#39;&#39;Update the variance of the proposal
    &#39;&#39;&#39;
    if self.sample_iter == 0:
        self.temp_acceptances = 0
        self.acceptances = np.zeros(self.G.inference.n_samples, dtype=bool)
    
    elif self.sample_iter &gt; self.end_tune:
        # Don&#39;t do any more updates
        return
    
    elif self.sample_iter % self.tune == 0:
        # Update var
        acceptance_rate = self.temp_acceptances / self.tune
        if acceptance_rate &gt; self.target_acceptance_rate:
            self.proposal.scale2.value *= 1.5
        else:
            self.proposal.scale2.value /= 1.5
        self.temp_acceptances = 0</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="mdsine2.pylab.variables.Uniform" href="pylab/variables.html#mdsine2.pylab.variables.Uniform">Uniform</a></b></code>:
<ul class="hlist">
<li><code><a title="mdsine2.pylab.variables.Uniform.T" href="pylab/variables.html#mdsine2.pylab.variables.Variable.T">T</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Uniform.add_child" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_child">add_child</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Uniform.add_parent" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_parent">add_parent</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Uniform.add_prior" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_prior">add_prior</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Uniform.add_trace" href="pylab/base.html#mdsine2.pylab.base.Traceable.add_trace">add_trace</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Uniform.add_undirected" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_undirected">add_undirected</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Uniform.cdf" href="pylab/variables.html#mdsine2.pylab.variables.Uniform.cdf">cdf</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Uniform.degree" href="pylab/graph.html#mdsine2.pylab.graph.Node.degree">degree</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Uniform.delete" href="pylab/graph.html#mdsine2.pylab.graph.BaseNode.delete">delete</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Uniform.get_adjacent_keys" href="pylab/graph.html#mdsine2.pylab.graph.Node.get_adjacent_keys">get_adjacent_keys</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Uniform.get_iter" href="pylab/base.html#mdsine2.pylab.base.Traceable.get_iter">get_iter</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Uniform.get_trace_from_disk" href="pylab/base.html#mdsine2.pylab.base.Traceable.get_trace_from_disk">get_trace_from_disk</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Uniform.load" href="pylab/base.html#mdsine2.pylab.base.Saveable.load">load</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Uniform.logcdf" href="pylab/variables.html#mdsine2.pylab.variables.Uniform.logcdf">logcdf</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Uniform.logpdf" href="pylab/variables.html#mdsine2.pylab.variables.Uniform.logpdf">logpdf</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Uniform.metropolis" href="pylab/graph.html#mdsine2.pylab.graph.Node.metropolis">metropolis</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Uniform.overwrite_entire_trace_on_disk" href="pylab/base.html#mdsine2.pylab.base.Traceable.overwrite_entire_trace_on_disk">overwrite_entire_trace_on_disk</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Uniform.pdf" href="pylab/variables.html#mdsine2.pylab.variables.Uniform.pdf">pdf</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Uniform.remove_local_trace" href="pylab/variables.html#mdsine2.pylab.variables.Variable.remove_local_trace">remove_local_trace</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Uniform.sample" href="pylab/variables.html#mdsine2.pylab.variables.Uniform.sample">sample</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Uniform.save" href="pylab/base.html#mdsine2.pylab.base.Saveable.save">save</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Uniform.set_save_location" href="pylab/base.html#mdsine2.pylab.base.Saveable.set_save_location">set_save_location</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Uniform.set_trace" href="pylab/base.html#mdsine2.pylab.base.Traceable.set_trace">set_trace</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Uniform.set_value_shape" href="pylab/variables.html#mdsine2.pylab.variables.Variable.set_value_shape">set_value_shape</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="mdsine2.posterior.qPCRDegsOfFreedoms"><code class="flex name class">
<span>class <span class="ident">qPCRDegsOfFreedoms</span></span>
<span>(</span><span>L, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Aggregation class for a degree of freedom parameter of qPCR variance</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>L</code></strong> :&ensp;<code>int</code></dt>
<dd>How many qPCR variance groupings there are</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class qPCRDegsOfFreedoms(_qPCRPriorAggVar):
    &#39;&#39;&#39;Aggregation class for a degree of freedom parameter of qPCR variance

    Parameters
    ----------
    L : int
        How many qPCR variance groupings there are
    &#39;&#39;&#39;
    def __init__(self, L, **kwargs):
        kwargs[&#39;name&#39;] = STRNAMES.QPCR_DOFS
        _qPCRPriorAggVar.__init__(self, L=L, child=qPCRDegsOfFreedomL, **kwargs)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>mdsine2.posterior._qPCRPriorAggVar</li>
<li>mdsine2.posterior._qPCRBase</li>
<li><a title="mdsine2.pylab.variables.Variable" href="pylab/variables.html#mdsine2.pylab.variables.Variable">Variable</a></li>
<li><a title="mdsine2.pylab.graph.Node" href="pylab/graph.html#mdsine2.pylab.graph.Node">Node</a></li>
<li><a title="mdsine2.pylab.graph.BaseNode" href="pylab/graph.html#mdsine2.pylab.graph.BaseNode">BaseNode</a></li>
<li><a title="mdsine2.pylab.base.Saveable" href="pylab/base.html#mdsine2.pylab.base.Saveable">Saveable</a></li>
<li>mdsine2.pylab.variables._BaseArithmeticClass</li>
<li><a title="mdsine2.pylab.base.Traceable" href="pylab/base.html#mdsine2.pylab.base.Traceable">Traceable</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="mdsine2.pylab.variables.Variable" href="pylab/variables.html#mdsine2.pylab.variables.Variable">Variable</a></b></code>:
<ul class="hlist">
<li><code><a title="mdsine2.pylab.variables.Variable.T" href="pylab/variables.html#mdsine2.pylab.variables.Variable.T">T</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.add_child" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_child">add_child</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.add_parent" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_parent">add_parent</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.add_prior" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_prior">add_prior</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.add_trace" href="pylab/base.html#mdsine2.pylab.base.Traceable.add_trace">add_trace</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.add_undirected" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_undirected">add_undirected</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.degree" href="pylab/graph.html#mdsine2.pylab.graph.Node.degree">degree</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.delete" href="pylab/graph.html#mdsine2.pylab.graph.BaseNode.delete">delete</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.get_adjacent_keys" href="pylab/graph.html#mdsine2.pylab.graph.Node.get_adjacent_keys">get_adjacent_keys</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.get_iter" href="pylab/base.html#mdsine2.pylab.base.Traceable.get_iter">get_iter</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.get_trace_from_disk" href="pylab/base.html#mdsine2.pylab.base.Traceable.get_trace_from_disk">get_trace_from_disk</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.load" href="pylab/base.html#mdsine2.pylab.base.Saveable.load">load</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.metropolis" href="pylab/graph.html#mdsine2.pylab.graph.Node.metropolis">metropolis</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.overwrite_entire_trace_on_disk" href="pylab/base.html#mdsine2.pylab.base.Traceable.overwrite_entire_trace_on_disk">overwrite_entire_trace_on_disk</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.remove_local_trace" href="pylab/variables.html#mdsine2.pylab.variables.Variable.remove_local_trace">remove_local_trace</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.save" href="pylab/base.html#mdsine2.pylab.base.Saveable.save">save</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.set_save_location" href="pylab/base.html#mdsine2.pylab.base.Saveable.set_save_location">set_save_location</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.set_trace" href="pylab/base.html#mdsine2.pylab.base.Traceable.set_trace">set_trace</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.set_value_shape" href="pylab/variables.html#mdsine2.pylab.variables.Variable.set_value_shape">set_value_shape</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="mdsine2.posterior.qPCRScaleL"><code class="flex name class">
<span>class <span class="ident">qPCRScaleL</span></span>
<span>(</span><span>L, l, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Posterior for a single qPCR scale set</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class qPCRScaleL(pl.variables.SICS):
    &#39;&#39;&#39;Posterior for a single qPCR scale set
    &#39;&#39;&#39;
    def __init__(self, L, l, **kwargs):

        self.L = L
        self.l = l
        kwargs[&#39;name&#39;] = STRNAMES.QPCR_SCALES + &#39;_{}&#39;.format(l)
        pl.variables.Uniform.__init__(self, **kwargs)

        self.data_locs = []
        self.proposal = pl.variables.TruncatedNormal(loc=None, scale2=None, value=None)

    def __str__(self):
        # If this fails, it is because we are dividing by 0 sampler_iter
        # If which case we just return the value 
        try:
            s = &#39;Value: {}, Acceptance rate: {}&#39;.format(
                self.value, np.mean(self.acceptances[
                    np.max([self.sample_iter-50, 0]):self.sample_iter]))
        except:
            s = str(self.value)
        return s

    def set_shape(self):
        &#39;&#39;&#39;Set the shape of the array (how many qPCR variances this is a prior for)
        &#39;&#39;&#39;
        self.set_value_shape(shape=(len(self.data_locs), ))

    def add_qpcr_measurement(self, ridx, tidx):
        &#39;&#39;&#39;Add the qPCR measurement for subject index `ridx` and time index
        `tidx` to 
        &#39;&#39;&#39;
        self.data_locs.append((ridx, tidx))

    def initialize(self, value_option, scale_option, dof_option, proposal_option, 
        target_acceptance_rate, tune, end_tune, value=None, dof=None, scale=None,
        proposal_var=None, delay=0):
        &#39;&#39;&#39;Initialize the values and hyperparameters

        Parameters
        ----------
        value_option : str
            How to initialize the value. Options:
                &#39;auto&#39;, &#39;prior-mean&#39;
                    Set to the prior mean
                &#39;manual&#39;
                    `value` must also be specified
        dof_option : str
            How to set the prior dof
                &#39;auto&#39;, &#39;diffuse&#39;
                    2.5
                &#39;manual&#39;:
                    set with `dof`
        scale_option : str
            How to set the prior scale
                &#39;empirical&#39;, &#39;auto&#39;
                    Set to the variance of the data assigned to the set
                &#39;manual&#39;
                    Set with the parameter `scale`
        proposal_option : str
            How to initialize the proposal variance:
                &#39;auto&#39;
                    mean**2 / 100
                &#39;manual&#39;
                    `proposal_var` must also be supplied     
        &#39;&#39;&#39;
        if not pl.isint(delay):
            raise TypeError(&#39;`delay` ({}) must be an int&#39;.format(type(delay)))
        if delay &lt; 0:
            raise ValueError(&#39;`delay` ({}) must be &gt;= 0&#39;.format(delay))
        self.delay = delay

        self.qpcr_data = []
        for ridx, tidx in self.data_locs:
            t = self.G.data.given_timepoints[ridx][tidx]
            self.qpcr_data = np.append(self.qpcr_data, 
                self.G.data.qpcr[ridx][t].log_data)

        # Set the prior dof
        if not pl.isstr(dof_option):
            raise TypeError(&#39;`dof_option` ({}) must be a str&#39;.format(type(dof_option)))
        if dof_option == &#39;manual&#39;:
            if not pl.isnumeric(dof):
                raise TypeError(&#39;`dof` ({}) must be a numeric&#39;.format(type(dof)))
            if dof &lt; 2:
                raise ValueError(&#39;`dof` ({}) must be &gt;= 2&#39;.format(dof))
        elif dof_option in [&#39;diffuse&#39;, &#39;auto&#39;]:
            dof = 2.5
        else:
            raise ValueError(&#39;`dof_option` ({}) not recognized&#39;.format(dof_option))
        if dof &lt; 2:
            raise ValueError(&#39;`dof` ({}) must be strictly larger than 2 to be a proper&#39; \
                &#39; prior&#39;.format(dof))
        self.prior.dof.override_value(dof)

        # Set the prior scale
        if not pl.isstr(scale_option):
            raise TypeError(&#39;`scale_option` ({}) must be a str&#39;.format(type(scale_option)))
        if scale_option == &#39;manual&#39;:
            if not pl.isnumeric(scale):
                raise TypeError(&#39;`scale` ({}) must be a numeric&#39;.format(type(scale)))
            if scale &lt;= 0:
                raise ValueError(&#39;`scale` ({}) must be positive&#39;.format(scale))
        elif scale_option in [&#39;auto&#39;, &#39;empirical&#39;]:
            v = np.var(self.qpcr_data)
            scale = v * (self.prior.dof.value - 2) / self.prior.dof.value
        else:
            raise ValueError(&#39;`scale_option` ({}) not recognized&#39;.format(scale_option))
        self.prior.scale.override_value(scale)

        # Set the value
        if not pl.isstr(value_option):
            raise TypeError(&#39;`value_option` ({}) must be a str&#39;.format(type(value_option)))
        if value_option == &#39;manual&#39;:
            if not pl.isnumeric(value):
                raise TypeError(&#39;`value` ({}) must be a numeric&#39;.format(type(value)))
        elif value_option in [&#39;auto&#39;, &#39;prior-mean&#39;]:
            value = self.prior.mean()
        else:
            raise ValueError(&#39;`value_option` ({}) not recognized&#39;.format(value_option))
        self.value = value

        # Set the propsal parameters
        if pl.isstr(target_acceptance_rate):
            if target_acceptance_rate in [&#39;optimal&#39;, &#39;auto&#39;]:
                target_acceptance_rate = 0.44
            else:
                raise ValueError(&#39;`target_acceptance_rate` ({}) not recognized&#39;.format(
                    target_acceptance_rate))
        elif pl.isfloat(target_acceptance_rate):
            if target_acceptance_rate &lt; 0 or target_acceptance_rate &gt; 1:
                raise ValueError(&#39;`target_acceptance_rate` ({}) out of range&#39;.format(
                    target_acceptance_rate))
        else:
            raise TypeError(&#39;`target_acceptance_rate` ({}) type not recognized&#39;.format(
                type(target_acceptance_rate)))
        self.target_acceptance_rate = target_acceptance_rate

        if pl.isstr(tune):
            if tune in [&#39;auto&#39;]:
                tune = 50
            else:
                raise ValueError(&#39;`tune` ({}) not recognized&#39;.format(tune))
        elif pl.isint(tune):
            if tune &lt; 0:
                raise ValueError(&#39;`tune` ({}) must be &gt; 0&#39;.format(
                    tune))
        else:
            raise TypeError(&#39;`tune` ({}) type not recognized&#39;.format(type(tune)))
        self.tune = tune

        if pl.isstr(end_tune):
            if end_tune in [&#39;auto&#39;, &#39;half-burnin&#39;]:
                end_tune = int(self.G.inference.burnin/2)
            else:
                raise ValueError(&#39;`tune` ({}) not recognized&#39;.format(end_tune))
        elif pl.isint(end_tune):
            if end_tune &lt; 0 or end_tune &gt; self.G.inference.burnin:
                raise ValueError(&#39;`end_tune` ({}) out of range (0, {})&#39;.format(
                    end_tune, self.G.inference.burnin))
        else:
            raise TypeError(&#39;`end_tune` ({}) type not recognized&#39;.format(type(end_tune)))
        self.end_tune = end_tune

        # Set the proposal variance
        if not pl.isstr(proposal_option):
            raise TypeError(&#39;`proposal_option` ({}) must be a str&#39;.format(
                type(proposal_option)))
        elif proposal_option == &#39;manual&#39;:
            if not pl.isnumeric(proposal_var):
                raise TypeError(&#39;`proposal_var` ({}) must be a numeric&#39;.format(
                    type(proposal_var)))
            if proposal_var &lt;= 0:
                raise ValueError(&#39;`proposal_var` ({}) not proper&#39;.format(proposal_var))
        elif proposal_option in [&#39;auto&#39;]:
            proposal_var = (self.value ** 2)/10
        else:
            raise ValueError(&#39;`proposal_option` ({}) not recognized&#39;.format(
                proposal_option))
        self.proposal.scale2.value = proposal_var
        self.proposal.low = 0
        self.proposal.high = float(&#39;inf&#39;)

    def update_var(self):
        &#39;&#39;&#39;Update the variance of the proposal
        &#39;&#39;&#39;
        if self.sample_iter == 0:
            self.temp_acceptances = 0
            self.acceptances = np.zeros(self.G.inference.n_samples, dtype=bool)
        
        elif self.sample_iter &gt; self.end_tune:
            # Don&#39;t do any more updates
            return
        
        elif self.sample_iter % self.tune == 0:
            # Update var
            acceptance_rate = self.temp_acceptances / self.tune
            if acceptance_rate &gt; self.target_acceptance_rate:
                self.proposal.scale2.value *= 1.5
            else:
                self.proposal.scale2.value /= 1.5
            self.temp_acceptances = 0

    def update(self):
        &#39;&#39;&#39;First we update the proposal (if necessary) and then we do a MH step
        &#39;&#39;&#39;
        self.update_var()
        proposal_std = np.sqrt(self.proposal.scale2.value)

        # Get the data
        xs = []
        for ridx, tidx in self.data_locs:
            xs.append(self.G[STRNAMES.QPCR_VARIANCES].value[ridx].value[tidx])

        # Get the dof
        dof = self.G[STRNAMES.QPCR_DOFS].value[self.l].value

        # Propose a new value for the scale
        prev_scale = self.value
        self.proposal.loc.value = self.value
        new_scale = self.proposal.sample()

        # Calculate the target distribution log likelihood
        prev_target_ll = pl.random.sics.logpdf(value=prev_scale, 
            dof=self.prior.dof.value, scale=self.prior.scale.value)
        for x in xs:
            prev_target_ll += pl.random.sics.logpdf(value=x,
                scale=prev_scale, dof=dof)
        new_target_ll = pl.random.sics.logpdf(value=new_scale, 
            dof=self.prior.dof.value, scale=self.prior.scale.value)
        for x in xs:
            new_target_ll += pl.random.sics.logpdf(value=x,
                scale=new_scale, dof=dof)

        # Normalize by the loglikelihood of the proposal
        prev_prop_ll = pl.random.truncnormal.logpdf(
            value=prev_scale, loc=new_scale, scale=proposal_std,
            low=self.proposal.low, high=self.proposal.high)
        new_prop_ll = pl.random.truncnormal.logpdf(
            value=new_scale, loc=prev_scale, scale=proposal_std,
            low=self.proposal.low, high=self.proposal.high)

        # Accept or reject
        r = (new_target_ll - prev_prop_ll) - \
            (prev_target_ll - new_prop_ll)
        u = np.log(pl.random.misc.fast_sample_standard_uniform())

        if r &gt;= u:
            self.acceptances[self.sample_iter] = True
            self.value = new_scale
            self.temp_acceptances += 1
        else:
            self.value = prev_scale</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="mdsine2.pylab.variables.SICS" href="pylab/variables.html#mdsine2.pylab.variables.SICS">SICS</a></li>
<li><a title="mdsine2.pylab.variables.Variable" href="pylab/variables.html#mdsine2.pylab.variables.Variable">Variable</a></li>
<li><a title="mdsine2.pylab.graph.Node" href="pylab/graph.html#mdsine2.pylab.graph.Node">Node</a></li>
<li><a title="mdsine2.pylab.graph.BaseNode" href="pylab/graph.html#mdsine2.pylab.graph.BaseNode">BaseNode</a></li>
<li><a title="mdsine2.pylab.base.Saveable" href="pylab/base.html#mdsine2.pylab.base.Saveable">Saveable</a></li>
<li>mdsine2.pylab.variables._BaseArithmeticClass</li>
<li><a title="mdsine2.pylab.base.Traceable" href="pylab/base.html#mdsine2.pylab.base.Traceable">Traceable</a></li>
<li>mdsine2.pylab.variables._RandomBase</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="mdsine2.posterior.qPCRScaleL.add_qpcr_measurement"><code class="name flex">
<span>def <span class="ident">add_qpcr_measurement</span></span>(<span>self, ridx, tidx)</span>
</code></dt>
<dd>
<div class="desc"><p>Add the qPCR measurement for subject index <code>ridx</code> and time index
<code>tidx</code> to</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add_qpcr_measurement(self, ridx, tidx):
    &#39;&#39;&#39;Add the qPCR measurement for subject index `ridx` and time index
    `tidx` to 
    &#39;&#39;&#39;
    self.data_locs.append((ridx, tidx))</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.qPCRScaleL.initialize"><code class="name flex">
<span>def <span class="ident">initialize</span></span>(<span>self, value_option, scale_option, dof_option, proposal_option, target_acceptance_rate, tune, end_tune, value=None, dof=None, scale=None, proposal_var=None, delay=0)</span>
</code></dt>
<dd>
<div class="desc"><p>Initialize the values and hyperparameters</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>value_option</code></strong> :&ensp;<code>str</code></dt>
<dd>How to initialize the value. Options:
'auto', 'prior-mean'
Set to the prior mean
'manual'
<code>value</code> must also be specified</dd>
<dt><strong><code>dof_option</code></strong> :&ensp;<code>str</code></dt>
<dd>How to set the prior dof
'auto', 'diffuse'
2.5
'manual':
set with <code>dof</code></dd>
<dt><strong><code>scale_option</code></strong> :&ensp;<code>str</code></dt>
<dd>How to set the prior scale
'empirical', 'auto'
Set to the variance of the data assigned to the set
'manual'
Set with the parameter <code>scale</code></dd>
<dt><strong><code>proposal_option</code></strong> :&ensp;<code>str</code></dt>
<dd>How to initialize the proposal variance:
'auto'
mean**2 / 100
'manual'
<code>proposal_var</code> must also be supplied</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def initialize(self, value_option, scale_option, dof_option, proposal_option, 
    target_acceptance_rate, tune, end_tune, value=None, dof=None, scale=None,
    proposal_var=None, delay=0):
    &#39;&#39;&#39;Initialize the values and hyperparameters

    Parameters
    ----------
    value_option : str
        How to initialize the value. Options:
            &#39;auto&#39;, &#39;prior-mean&#39;
                Set to the prior mean
            &#39;manual&#39;
                `value` must also be specified
    dof_option : str
        How to set the prior dof
            &#39;auto&#39;, &#39;diffuse&#39;
                2.5
            &#39;manual&#39;:
                set with `dof`
    scale_option : str
        How to set the prior scale
            &#39;empirical&#39;, &#39;auto&#39;
                Set to the variance of the data assigned to the set
            &#39;manual&#39;
                Set with the parameter `scale`
    proposal_option : str
        How to initialize the proposal variance:
            &#39;auto&#39;
                mean**2 / 100
            &#39;manual&#39;
                `proposal_var` must also be supplied     
    &#39;&#39;&#39;
    if not pl.isint(delay):
        raise TypeError(&#39;`delay` ({}) must be an int&#39;.format(type(delay)))
    if delay &lt; 0:
        raise ValueError(&#39;`delay` ({}) must be &gt;= 0&#39;.format(delay))
    self.delay = delay

    self.qpcr_data = []
    for ridx, tidx in self.data_locs:
        t = self.G.data.given_timepoints[ridx][tidx]
        self.qpcr_data = np.append(self.qpcr_data, 
            self.G.data.qpcr[ridx][t].log_data)

    # Set the prior dof
    if not pl.isstr(dof_option):
        raise TypeError(&#39;`dof_option` ({}) must be a str&#39;.format(type(dof_option)))
    if dof_option == &#39;manual&#39;:
        if not pl.isnumeric(dof):
            raise TypeError(&#39;`dof` ({}) must be a numeric&#39;.format(type(dof)))
        if dof &lt; 2:
            raise ValueError(&#39;`dof` ({}) must be &gt;= 2&#39;.format(dof))
    elif dof_option in [&#39;diffuse&#39;, &#39;auto&#39;]:
        dof = 2.5
    else:
        raise ValueError(&#39;`dof_option` ({}) not recognized&#39;.format(dof_option))
    if dof &lt; 2:
        raise ValueError(&#39;`dof` ({}) must be strictly larger than 2 to be a proper&#39; \
            &#39; prior&#39;.format(dof))
    self.prior.dof.override_value(dof)

    # Set the prior scale
    if not pl.isstr(scale_option):
        raise TypeError(&#39;`scale_option` ({}) must be a str&#39;.format(type(scale_option)))
    if scale_option == &#39;manual&#39;:
        if not pl.isnumeric(scale):
            raise TypeError(&#39;`scale` ({}) must be a numeric&#39;.format(type(scale)))
        if scale &lt;= 0:
            raise ValueError(&#39;`scale` ({}) must be positive&#39;.format(scale))
    elif scale_option in [&#39;auto&#39;, &#39;empirical&#39;]:
        v = np.var(self.qpcr_data)
        scale = v * (self.prior.dof.value - 2) / self.prior.dof.value
    else:
        raise ValueError(&#39;`scale_option` ({}) not recognized&#39;.format(scale_option))
    self.prior.scale.override_value(scale)

    # Set the value
    if not pl.isstr(value_option):
        raise TypeError(&#39;`value_option` ({}) must be a str&#39;.format(type(value_option)))
    if value_option == &#39;manual&#39;:
        if not pl.isnumeric(value):
            raise TypeError(&#39;`value` ({}) must be a numeric&#39;.format(type(value)))
    elif value_option in [&#39;auto&#39;, &#39;prior-mean&#39;]:
        value = self.prior.mean()
    else:
        raise ValueError(&#39;`value_option` ({}) not recognized&#39;.format(value_option))
    self.value = value

    # Set the propsal parameters
    if pl.isstr(target_acceptance_rate):
        if target_acceptance_rate in [&#39;optimal&#39;, &#39;auto&#39;]:
            target_acceptance_rate = 0.44
        else:
            raise ValueError(&#39;`target_acceptance_rate` ({}) not recognized&#39;.format(
                target_acceptance_rate))
    elif pl.isfloat(target_acceptance_rate):
        if target_acceptance_rate &lt; 0 or target_acceptance_rate &gt; 1:
            raise ValueError(&#39;`target_acceptance_rate` ({}) out of range&#39;.format(
                target_acceptance_rate))
    else:
        raise TypeError(&#39;`target_acceptance_rate` ({}) type not recognized&#39;.format(
            type(target_acceptance_rate)))
    self.target_acceptance_rate = target_acceptance_rate

    if pl.isstr(tune):
        if tune in [&#39;auto&#39;]:
            tune = 50
        else:
            raise ValueError(&#39;`tune` ({}) not recognized&#39;.format(tune))
    elif pl.isint(tune):
        if tune &lt; 0:
            raise ValueError(&#39;`tune` ({}) must be &gt; 0&#39;.format(
                tune))
    else:
        raise TypeError(&#39;`tune` ({}) type not recognized&#39;.format(type(tune)))
    self.tune = tune

    if pl.isstr(end_tune):
        if end_tune in [&#39;auto&#39;, &#39;half-burnin&#39;]:
            end_tune = int(self.G.inference.burnin/2)
        else:
            raise ValueError(&#39;`tune` ({}) not recognized&#39;.format(end_tune))
    elif pl.isint(end_tune):
        if end_tune &lt; 0 or end_tune &gt; self.G.inference.burnin:
            raise ValueError(&#39;`end_tune` ({}) out of range (0, {})&#39;.format(
                end_tune, self.G.inference.burnin))
    else:
        raise TypeError(&#39;`end_tune` ({}) type not recognized&#39;.format(type(end_tune)))
    self.end_tune = end_tune

    # Set the proposal variance
    if not pl.isstr(proposal_option):
        raise TypeError(&#39;`proposal_option` ({}) must be a str&#39;.format(
            type(proposal_option)))
    elif proposal_option == &#39;manual&#39;:
        if not pl.isnumeric(proposal_var):
            raise TypeError(&#39;`proposal_var` ({}) must be a numeric&#39;.format(
                type(proposal_var)))
        if proposal_var &lt;= 0:
            raise ValueError(&#39;`proposal_var` ({}) not proper&#39;.format(proposal_var))
    elif proposal_option in [&#39;auto&#39;]:
        proposal_var = (self.value ** 2)/10
    else:
        raise ValueError(&#39;`proposal_option` ({}) not recognized&#39;.format(
            proposal_option))
    self.proposal.scale2.value = proposal_var
    self.proposal.low = 0
    self.proposal.high = float(&#39;inf&#39;)</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.qPCRScaleL.set_shape"><code class="name flex">
<span>def <span class="ident">set_shape</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Set the shape of the array (how many qPCR variances this is a prior for)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_shape(self):
    &#39;&#39;&#39;Set the shape of the array (how many qPCR variances this is a prior for)
    &#39;&#39;&#39;
    self.set_value_shape(shape=(len(self.data_locs), ))</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.qPCRScaleL.update"><code class="name flex">
<span>def <span class="ident">update</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>First we update the proposal (if necessary) and then we do a MH step</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update(self):
    &#39;&#39;&#39;First we update the proposal (if necessary) and then we do a MH step
    &#39;&#39;&#39;
    self.update_var()
    proposal_std = np.sqrt(self.proposal.scale2.value)

    # Get the data
    xs = []
    for ridx, tidx in self.data_locs:
        xs.append(self.G[STRNAMES.QPCR_VARIANCES].value[ridx].value[tidx])

    # Get the dof
    dof = self.G[STRNAMES.QPCR_DOFS].value[self.l].value

    # Propose a new value for the scale
    prev_scale = self.value
    self.proposal.loc.value = self.value
    new_scale = self.proposal.sample()

    # Calculate the target distribution log likelihood
    prev_target_ll = pl.random.sics.logpdf(value=prev_scale, 
        dof=self.prior.dof.value, scale=self.prior.scale.value)
    for x in xs:
        prev_target_ll += pl.random.sics.logpdf(value=x,
            scale=prev_scale, dof=dof)
    new_target_ll = pl.random.sics.logpdf(value=new_scale, 
        dof=self.prior.dof.value, scale=self.prior.scale.value)
    for x in xs:
        new_target_ll += pl.random.sics.logpdf(value=x,
            scale=new_scale, dof=dof)

    # Normalize by the loglikelihood of the proposal
    prev_prop_ll = pl.random.truncnormal.logpdf(
        value=prev_scale, loc=new_scale, scale=proposal_std,
        low=self.proposal.low, high=self.proposal.high)
    new_prop_ll = pl.random.truncnormal.logpdf(
        value=new_scale, loc=prev_scale, scale=proposal_std,
        low=self.proposal.low, high=self.proposal.high)

    # Accept or reject
    r = (new_target_ll - prev_prop_ll) - \
        (prev_target_ll - new_prop_ll)
    u = np.log(pl.random.misc.fast_sample_standard_uniform())

    if r &gt;= u:
        self.acceptances[self.sample_iter] = True
        self.value = new_scale
        self.temp_acceptances += 1
    else:
        self.value = prev_scale</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.qPCRScaleL.update_var"><code class="name flex">
<span>def <span class="ident">update_var</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Update the variance of the proposal</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_var(self):
    &#39;&#39;&#39;Update the variance of the proposal
    &#39;&#39;&#39;
    if self.sample_iter == 0:
        self.temp_acceptances = 0
        self.acceptances = np.zeros(self.G.inference.n_samples, dtype=bool)
    
    elif self.sample_iter &gt; self.end_tune:
        # Don&#39;t do any more updates
        return
    
    elif self.sample_iter % self.tune == 0:
        # Update var
        acceptance_rate = self.temp_acceptances / self.tune
        if acceptance_rate &gt; self.target_acceptance_rate:
            self.proposal.scale2.value *= 1.5
        else:
            self.proposal.scale2.value /= 1.5
        self.temp_acceptances = 0</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="mdsine2.pylab.variables.SICS" href="pylab/variables.html#mdsine2.pylab.variables.SICS">SICS</a></b></code>:
<ul class="hlist">
<li><code><a title="mdsine2.pylab.variables.SICS.T" href="pylab/variables.html#mdsine2.pylab.variables.Variable.T">T</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.add_child" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_child">add_child</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.add_parent" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_parent">add_parent</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.add_prior" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_prior">add_prior</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.add_trace" href="pylab/base.html#mdsine2.pylab.base.Traceable.add_trace">add_trace</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.add_undirected" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_undirected">add_undirected</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.degree" href="pylab/graph.html#mdsine2.pylab.graph.Node.degree">degree</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.delete" href="pylab/graph.html#mdsine2.pylab.graph.BaseNode.delete">delete</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.get_adjacent_keys" href="pylab/graph.html#mdsine2.pylab.graph.Node.get_adjacent_keys">get_adjacent_keys</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.get_iter" href="pylab/base.html#mdsine2.pylab.base.Traceable.get_iter">get_iter</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.get_trace_from_disk" href="pylab/base.html#mdsine2.pylab.base.Traceable.get_trace_from_disk">get_trace_from_disk</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.load" href="pylab/base.html#mdsine2.pylab.base.Saveable.load">load</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.logpdf" href="pylab/variables.html#mdsine2.pylab.variables.SICS.logpdf">logpdf</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.metropolis" href="pylab/graph.html#mdsine2.pylab.graph.Node.metropolis">metropolis</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.overwrite_entire_trace_on_disk" href="pylab/base.html#mdsine2.pylab.base.Traceable.overwrite_entire_trace_on_disk">overwrite_entire_trace_on_disk</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.pdf" href="pylab/variables.html#mdsine2.pylab.variables.SICS.pdf">pdf</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.remove_local_trace" href="pylab/variables.html#mdsine2.pylab.variables.Variable.remove_local_trace">remove_local_trace</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.sample" href="pylab/variables.html#mdsine2.pylab.variables.SICS.sample">sample</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.save" href="pylab/base.html#mdsine2.pylab.base.Saveable.save">save</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.set_save_location" href="pylab/base.html#mdsine2.pylab.base.Saveable.set_save_location">set_save_location</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.set_trace" href="pylab/base.html#mdsine2.pylab.base.Traceable.set_trace">set_trace</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.set_value_shape" href="pylab/variables.html#mdsine2.pylab.variables.Variable.set_value_shape">set_value_shape</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="mdsine2.posterior.qPCRScales"><code class="flex name class">
<span>class <span class="ident">qPCRScales</span></span>
<span>(</span><span>L, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Aggregation class for a scale parameter of qPCR variance</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>L</code></strong> :&ensp;<code>int</code></dt>
<dd>How many qPCR variance groupings there are</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class qPCRScales(_qPCRPriorAggVar):
    &#39;&#39;&#39;Aggregation class for a scale parameter of qPCR variance

    Parameters
    ----------
    L : int
        How many qPCR variance groupings there are
    &#39;&#39;&#39;
    def __init__(self, L, **kwargs):
        kwargs[&#39;name&#39;] = STRNAMES.QPCR_SCALES
        _qPCRPriorAggVar.__init__(self, L=L, child=qPCRScaleL, **kwargs)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>mdsine2.posterior._qPCRPriorAggVar</li>
<li>mdsine2.posterior._qPCRBase</li>
<li><a title="mdsine2.pylab.variables.Variable" href="pylab/variables.html#mdsine2.pylab.variables.Variable">Variable</a></li>
<li><a title="mdsine2.pylab.graph.Node" href="pylab/graph.html#mdsine2.pylab.graph.Node">Node</a></li>
<li><a title="mdsine2.pylab.graph.BaseNode" href="pylab/graph.html#mdsine2.pylab.graph.BaseNode">BaseNode</a></li>
<li><a title="mdsine2.pylab.base.Saveable" href="pylab/base.html#mdsine2.pylab.base.Saveable">Saveable</a></li>
<li>mdsine2.pylab.variables._BaseArithmeticClass</li>
<li><a title="mdsine2.pylab.base.Traceable" href="pylab/base.html#mdsine2.pylab.base.Traceable">Traceable</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="mdsine2.pylab.variables.Variable" href="pylab/variables.html#mdsine2.pylab.variables.Variable">Variable</a></b></code>:
<ul class="hlist">
<li><code><a title="mdsine2.pylab.variables.Variable.T" href="pylab/variables.html#mdsine2.pylab.variables.Variable.T">T</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.add_child" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_child">add_child</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.add_parent" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_parent">add_parent</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.add_prior" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_prior">add_prior</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.add_trace" href="pylab/base.html#mdsine2.pylab.base.Traceable.add_trace">add_trace</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.add_undirected" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_undirected">add_undirected</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.degree" href="pylab/graph.html#mdsine2.pylab.graph.Node.degree">degree</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.delete" href="pylab/graph.html#mdsine2.pylab.graph.BaseNode.delete">delete</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.get_adjacent_keys" href="pylab/graph.html#mdsine2.pylab.graph.Node.get_adjacent_keys">get_adjacent_keys</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.get_iter" href="pylab/base.html#mdsine2.pylab.base.Traceable.get_iter">get_iter</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.get_trace_from_disk" href="pylab/base.html#mdsine2.pylab.base.Traceable.get_trace_from_disk">get_trace_from_disk</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.load" href="pylab/base.html#mdsine2.pylab.base.Saveable.load">load</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.metropolis" href="pylab/graph.html#mdsine2.pylab.graph.Node.metropolis">metropolis</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.overwrite_entire_trace_on_disk" href="pylab/base.html#mdsine2.pylab.base.Traceable.overwrite_entire_trace_on_disk">overwrite_entire_trace_on_disk</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.remove_local_trace" href="pylab/variables.html#mdsine2.pylab.variables.Variable.remove_local_trace">remove_local_trace</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.save" href="pylab/base.html#mdsine2.pylab.base.Saveable.save">save</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.set_save_location" href="pylab/base.html#mdsine2.pylab.base.Saveable.set_save_location">set_save_location</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.set_trace" href="pylab/base.html#mdsine2.pylab.base.Traceable.set_trace">set_trace</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.set_value_shape" href="pylab/variables.html#mdsine2.pylab.variables.Variable.set_value_shape">set_value_shape</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="mdsine2.posterior.qPCRVarianceReplicate"><code class="flex name class">
<span>class <span class="ident">qPCRVarianceReplicate</span></span>
<span>(</span><span>ridx, L, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Posterior for a set of single qPCR variances for replicate <code>ridx</code></p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>ridx</code></strong> :&ensp;<code>int</code></dt>
<dd>Which subject replicate index this set of qPCR variances belongs
to.</dd>
<dt><strong><code>L</code></strong> :&ensp;<code>int</code></dt>
<dd>How many qPCR variance groupings there are</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class qPCRVarianceReplicate(pl.variables.SICS):
    &#39;&#39;&#39;Posterior for a set of single qPCR variances for replicate `ridx`

    Parameters
    ----------
    ridx : int
        Which subject replicate index this set of qPCR variances belongs
        to.
    L : int
        How many qPCR variance groupings there are
    &#39;&#39;&#39;
    def __init__(self, ridx, L, **kwargs):
        self.ridx = ridx
        self.L = L
        kwargs[&#39;name&#39;] = STRNAMES.QPCR_VARIANCES + &#39;_{}&#39;.format(ridx)
        pl.variables.SICS.__init__(self, **kwargs)
        self.priors_idx = np.full(len(self.G.data.given_timepoints[ridx]), -1, dtype=int)
        self.set_value_shape(shape=(len(self.G.data.given_timepoints[ridx]),))

    def initialize(self, value_option, value=None, inflated=None):
        &#39;&#39;&#39;Initialize the values. We do not set any hyperparameters because those are
        set in their own classes.

        Parameters
        ----------
        value_option : str
            How to initialize the variances
                &#39;empirical&#39;, &#39;auto&#39;
                    Set to the empirical variance of the respective measurements
                &#39;inflated&#39;
                    Set to an inflated value of the empirical variance.
                &#39;manual&#39;
                    Set the values manually
        value : float, np.ndarray(float)
            If float, set all the values to the same number. If array then set the 
            values to each of the parameters
        inflated : float, None
            Necessary if `value_option` == &#39;inflated&#39;
        &#39;&#39;&#39;
        if not pl.isstr(value_option):
            raise TypeError(&#39;`value_option` ({}) must be a str&#39;.format(type(value_option)))
        if value_option in [&#39;empirical&#39;, &#39;auto&#39;]:
            self.value = np.zeros(len(self.G.data.qpcr[self.ridx]), dtype=float)
            for idx, t in enumerate(self.G.data.qpcr[self.ridx]):
                self.value[idx] = np.var(self.G.data.qpcr[self.ridx][t].log_data)

        elif value_option == &#39;inflated&#39;:
            if not pl.isnumeric(inflated):
                raise TypeError(&#39;`inflated` ({}) must be a numeric&#39;.format(type(inflated)))
            if inflated &lt; 0:
                raise ValueError(&#39;`inflated` ({}) must be positive&#39;.format(inflated))
            # Set each variance by the empirical variance * inflated
            self.value = np.zeros(len(self.G.data.qpcr[self.ridx]), dtype=float)
            for idx, t in enumerate(self.G.data.qpcr[self.ridx]):
                self.value[idx] = np.var(self.G.data.qpcr[self.ridx][t].log_data) * inflated

        elif value_option == &#39;manual&#39;:
            raise NotImplementedError(&#39;Need to implement&#39;)
        else:
            raise ValueError(&#39;`value_option` ({}) not recognized&#39;.format(value_option))

        # Set the qPCR measurements
        self.qpcr_measurements = []
        for tidx, t in enumerate(self.G.data.given_timepoints[self.ridx]):
            self.qpcr_measurements.append(self.G.data.qpcr[self.ridx][t].log_data)

    def update(self):

        prior_dofs = []
        prior_scales = []
        for l in range(self.L):
            prior_dofs.append(self.G[STRNAMES.QPCR_DOFS].value[l].value)
            prior_scales.append(self.G[STRNAMES.QPCR_SCALES].value[l].value)

        for tidx in range(len(self.priors_idx)):
            t = self.G.data.given_timepoints[self.ridx][tidx]
            l = self.priors_idx[tidx]
            prior_dof = prior_dofs[l]
            prior_scale = prior_scales[l]

            # qPCR measurements (these are already in log space)
            values = self.qpcr_measurements[tidx]

            # Current mean is the log of the sum of latent abundance
            tidx_in_arr = self.G.data.timepoint2index[self.ridx][t]
            mean = np.log(np.sum(self.G.data.data[self.ridx][:, tidx_in_arr]))

            # Calculate the residual sum
            resid_sum = np.sum(np.square(values - mean))

            # posterior
            dof = prior_dof + len(values)
            scale = ((prior_scale * prior_dof) + resid_sum)/dof
            self.value[tidx] = pl.random.sics.sample(dof, scale)

    def add_qpcr_measurement(self, tidx, l):
        &#39;&#39;&#39;Add qPCR measurement for subject index `ridx` and time index `tidx`

        Parameters
        ----------
        ridx : int
            Subject index
        tidx : int
            Time index
        &#39;&#39;&#39;
        if not pl.isint(tidx):
            raise TypeError(&#39;`tidx` ({}) must be an int&#39;.format(type(tidx)))
        if tidx &gt;= len(self.G.data.given_timepoints[self.ridx]):
            raise ValueError(&#39;`tidx` ({}) out of range ({})&#39;.format(tidx, 
                len(self.G.data.given_timepoints[self.ridx])))
        if not pl.isint(l):
            raise TypeError(&#39;`l` ({}) must be an int&#39;.format(type(l)))
        self.priors_idx[tidx] = l</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="mdsine2.pylab.variables.SICS" href="pylab/variables.html#mdsine2.pylab.variables.SICS">SICS</a></li>
<li><a title="mdsine2.pylab.variables.Variable" href="pylab/variables.html#mdsine2.pylab.variables.Variable">Variable</a></li>
<li><a title="mdsine2.pylab.graph.Node" href="pylab/graph.html#mdsine2.pylab.graph.Node">Node</a></li>
<li><a title="mdsine2.pylab.graph.BaseNode" href="pylab/graph.html#mdsine2.pylab.graph.BaseNode">BaseNode</a></li>
<li><a title="mdsine2.pylab.base.Saveable" href="pylab/base.html#mdsine2.pylab.base.Saveable">Saveable</a></li>
<li>mdsine2.pylab.variables._BaseArithmeticClass</li>
<li><a title="mdsine2.pylab.base.Traceable" href="pylab/base.html#mdsine2.pylab.base.Traceable">Traceable</a></li>
<li>mdsine2.pylab.variables._RandomBase</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="mdsine2.posterior.qPCRVarianceReplicate.add_qpcr_measurement"><code class="name flex">
<span>def <span class="ident">add_qpcr_measurement</span></span>(<span>self, tidx, l)</span>
</code></dt>
<dd>
<div class="desc"><p>Add qPCR measurement for subject index <code>ridx</code> and time index <code>tidx</code></p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>ridx</code></strong> :&ensp;<code>int</code></dt>
<dd>Subject index</dd>
<dt><strong><code>tidx</code></strong> :&ensp;<code>int</code></dt>
<dd>Time index</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add_qpcr_measurement(self, tidx, l):
    &#39;&#39;&#39;Add qPCR measurement for subject index `ridx` and time index `tidx`

    Parameters
    ----------
    ridx : int
        Subject index
    tidx : int
        Time index
    &#39;&#39;&#39;
    if not pl.isint(tidx):
        raise TypeError(&#39;`tidx` ({}) must be an int&#39;.format(type(tidx)))
    if tidx &gt;= len(self.G.data.given_timepoints[self.ridx]):
        raise ValueError(&#39;`tidx` ({}) out of range ({})&#39;.format(tidx, 
            len(self.G.data.given_timepoints[self.ridx])))
    if not pl.isint(l):
        raise TypeError(&#39;`l` ({}) must be an int&#39;.format(type(l)))
    self.priors_idx[tidx] = l</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.qPCRVarianceReplicate.initialize"><code class="name flex">
<span>def <span class="ident">initialize</span></span>(<span>self, value_option, value=None, inflated=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Initialize the values. We do not set any hyperparameters because those are
set in their own classes.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>value_option</code></strong> :&ensp;<code>str</code></dt>
<dd>How to initialize the variances
'empirical', 'auto'
Set to the empirical variance of the respective measurements
'inflated'
Set to an inflated value of the empirical variance.
'manual'
Set the values manually</dd>
<dt><strong><code>value</code></strong> :&ensp;<code>float, np.ndarray(float)</code></dt>
<dd>If float, set all the values to the same number. If array then set the
values to each of the parameters</dd>
<dt><strong><code>inflated</code></strong> :&ensp;<code>float, None</code></dt>
<dd>Necessary if <code>value_option</code> == 'inflated'</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def initialize(self, value_option, value=None, inflated=None):
    &#39;&#39;&#39;Initialize the values. We do not set any hyperparameters because those are
    set in their own classes.

    Parameters
    ----------
    value_option : str
        How to initialize the variances
            &#39;empirical&#39;, &#39;auto&#39;
                Set to the empirical variance of the respective measurements
            &#39;inflated&#39;
                Set to an inflated value of the empirical variance.
            &#39;manual&#39;
                Set the values manually
    value : float, np.ndarray(float)
        If float, set all the values to the same number. If array then set the 
        values to each of the parameters
    inflated : float, None
        Necessary if `value_option` == &#39;inflated&#39;
    &#39;&#39;&#39;
    if not pl.isstr(value_option):
        raise TypeError(&#39;`value_option` ({}) must be a str&#39;.format(type(value_option)))
    if value_option in [&#39;empirical&#39;, &#39;auto&#39;]:
        self.value = np.zeros(len(self.G.data.qpcr[self.ridx]), dtype=float)
        for idx, t in enumerate(self.G.data.qpcr[self.ridx]):
            self.value[idx] = np.var(self.G.data.qpcr[self.ridx][t].log_data)

    elif value_option == &#39;inflated&#39;:
        if not pl.isnumeric(inflated):
            raise TypeError(&#39;`inflated` ({}) must be a numeric&#39;.format(type(inflated)))
        if inflated &lt; 0:
            raise ValueError(&#39;`inflated` ({}) must be positive&#39;.format(inflated))
        # Set each variance by the empirical variance * inflated
        self.value = np.zeros(len(self.G.data.qpcr[self.ridx]), dtype=float)
        for idx, t in enumerate(self.G.data.qpcr[self.ridx]):
            self.value[idx] = np.var(self.G.data.qpcr[self.ridx][t].log_data) * inflated

    elif value_option == &#39;manual&#39;:
        raise NotImplementedError(&#39;Need to implement&#39;)
    else:
        raise ValueError(&#39;`value_option` ({}) not recognized&#39;.format(value_option))

    # Set the qPCR measurements
    self.qpcr_measurements = []
    for tidx, t in enumerate(self.G.data.given_timepoints[self.ridx]):
        self.qpcr_measurements.append(self.G.data.qpcr[self.ridx][t].log_data)</code></pre>
</details>
</dd>
<dt id="mdsine2.posterior.qPCRVarianceReplicate.update"><code class="name flex">
<span>def <span class="ident">update</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update(self):

    prior_dofs = []
    prior_scales = []
    for l in range(self.L):
        prior_dofs.append(self.G[STRNAMES.QPCR_DOFS].value[l].value)
        prior_scales.append(self.G[STRNAMES.QPCR_SCALES].value[l].value)

    for tidx in range(len(self.priors_idx)):
        t = self.G.data.given_timepoints[self.ridx][tidx]
        l = self.priors_idx[tidx]
        prior_dof = prior_dofs[l]
        prior_scale = prior_scales[l]

        # qPCR measurements (these are already in log space)
        values = self.qpcr_measurements[tidx]

        # Current mean is the log of the sum of latent abundance
        tidx_in_arr = self.G.data.timepoint2index[self.ridx][t]
        mean = np.log(np.sum(self.G.data.data[self.ridx][:, tidx_in_arr]))

        # Calculate the residual sum
        resid_sum = np.sum(np.square(values - mean))

        # posterior
        dof = prior_dof + len(values)
        scale = ((prior_scale * prior_dof) + resid_sum)/dof
        self.value[tidx] = pl.random.sics.sample(dof, scale)</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="mdsine2.pylab.variables.SICS" href="pylab/variables.html#mdsine2.pylab.variables.SICS">SICS</a></b></code>:
<ul class="hlist">
<li><code><a title="mdsine2.pylab.variables.SICS.T" href="pylab/variables.html#mdsine2.pylab.variables.Variable.T">T</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.add_child" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_child">add_child</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.add_parent" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_parent">add_parent</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.add_prior" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_prior">add_prior</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.add_trace" href="pylab/base.html#mdsine2.pylab.base.Traceable.add_trace">add_trace</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.add_undirected" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_undirected">add_undirected</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.degree" href="pylab/graph.html#mdsine2.pylab.graph.Node.degree">degree</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.delete" href="pylab/graph.html#mdsine2.pylab.graph.BaseNode.delete">delete</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.get_adjacent_keys" href="pylab/graph.html#mdsine2.pylab.graph.Node.get_adjacent_keys">get_adjacent_keys</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.get_iter" href="pylab/base.html#mdsine2.pylab.base.Traceable.get_iter">get_iter</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.get_trace_from_disk" href="pylab/base.html#mdsine2.pylab.base.Traceable.get_trace_from_disk">get_trace_from_disk</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.load" href="pylab/base.html#mdsine2.pylab.base.Saveable.load">load</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.logpdf" href="pylab/variables.html#mdsine2.pylab.variables.SICS.logpdf">logpdf</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.metropolis" href="pylab/graph.html#mdsine2.pylab.graph.Node.metropolis">metropolis</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.overwrite_entire_trace_on_disk" href="pylab/base.html#mdsine2.pylab.base.Traceable.overwrite_entire_trace_on_disk">overwrite_entire_trace_on_disk</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.pdf" href="pylab/variables.html#mdsine2.pylab.variables.SICS.pdf">pdf</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.remove_local_trace" href="pylab/variables.html#mdsine2.pylab.variables.Variable.remove_local_trace">remove_local_trace</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.sample" href="pylab/variables.html#mdsine2.pylab.variables.SICS.sample">sample</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.save" href="pylab/base.html#mdsine2.pylab.base.Saveable.save">save</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.set_save_location" href="pylab/base.html#mdsine2.pylab.base.Saveable.set_save_location">set_save_location</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.set_trace" href="pylab/base.html#mdsine2.pylab.base.Traceable.set_trace">set_trace</a></code></li>
<li><code><a title="mdsine2.pylab.variables.SICS.set_value_shape" href="pylab/variables.html#mdsine2.pylab.variables.Variable.set_value_shape">set_value_shape</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="mdsine2.posterior.qPCRVariances"><code class="flex name class">
<span>class <span class="ident">qPCRVariances</span></span>
<span>(</span><span>**kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Aggregation class for qPCR variance for a set of qPCR variances.
The qPCR variances are </p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>L</code></strong> :&ensp;<code>int</code></dt>
<dd>How many qPCR variance groupings there are</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class qPCRVariances(_qPCRBase):
    &#39;&#39;&#39;Aggregation class for qPCR variance for a set of qPCR variances. 
    The qPCR variances are 

    Parameters
    ----------
    L : int
        How many qPCR variance groupings there are
    &#39;&#39;&#39;
    def __init__(self, **kwargs):
        kwargs[&#39;name&#39;] = STRNAMES.QPCR_VARIANCES
        _qPCRBase.__init__(self, **kwargs)
        self.G.data.qpcr_variances = self
        
        for ridx in range(self.n_replicates):
            self.value.append( 
                qPCRVarianceReplicate(ridx=ridx, **kwargs))

    def add_qpcr_measurement(self, ridx, tidx, l):
        &#39;&#39;&#39;Add a qPCR measurement for subject `ridx` at time index
        `tidx` to qPCR set `l`

        Parameters
        ----------
        ridx : int
            Subject index
        tidx : int
            Time index
        l : int
            qPCR set index
        &#39;&#39;&#39;
        if not pl.isint(ridx):
            raise TypeError(&#39;`ridx` ({}) must be an int&#39;.format(type(ridx)))
        if ridx &gt;= self.G.data.n_replicates:
            raise ValueError(&#39;`ridx` ({}) out of range ({})&#39;.format(ridx, 
                self.G.data.n_replicates))
        if not pl.isint(tidx):
            raise TypeError(&#39;`tidx` ({}) must be an int&#39;.format(type(tidx)))
        if tidx &gt;= len(self.G.data.given_timepoints[ridx]):
            raise ValueError(&#39;`tidx` ({}) out of range ({})&#39;.format(tidx, 
                len(self.G.data.given_timepoints[ridx])))
        if not pl.isint(l):
            raise TypeError(&#39;`l` ({}) must be an int&#39;.format(type(l)))
        if l &gt;= self.L:
            raise ValueError(&#39;`l` ({}) out of range ({})&#39;.format(tidx, 
                self.L))
        self.value[ridx].add_qpcr_measurement(tidx=tidx, l=l)        </code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>mdsine2.posterior._qPCRBase</li>
<li><a title="mdsine2.pylab.variables.Variable" href="pylab/variables.html#mdsine2.pylab.variables.Variable">Variable</a></li>
<li><a title="mdsine2.pylab.graph.Node" href="pylab/graph.html#mdsine2.pylab.graph.Node">Node</a></li>
<li><a title="mdsine2.pylab.graph.BaseNode" href="pylab/graph.html#mdsine2.pylab.graph.BaseNode">BaseNode</a></li>
<li><a title="mdsine2.pylab.base.Saveable" href="pylab/base.html#mdsine2.pylab.base.Saveable">Saveable</a></li>
<li>mdsine2.pylab.variables._BaseArithmeticClass</li>
<li><a title="mdsine2.pylab.base.Traceable" href="pylab/base.html#mdsine2.pylab.base.Traceable">Traceable</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="mdsine2.posterior.qPCRVariances.add_qpcr_measurement"><code class="name flex">
<span>def <span class="ident">add_qpcr_measurement</span></span>(<span>self, ridx, tidx, l)</span>
</code></dt>
<dd>
<div class="desc"><p>Add a qPCR measurement for subject <code>ridx</code> at time index
<code>tidx</code> to qPCR set <code>l</code></p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>ridx</code></strong> :&ensp;<code>int</code></dt>
<dd>Subject index</dd>
<dt><strong><code>tidx</code></strong> :&ensp;<code>int</code></dt>
<dd>Time index</dd>
<dt><strong><code>l</code></strong> :&ensp;<code>int</code></dt>
<dd>qPCR set index</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add_qpcr_measurement(self, ridx, tidx, l):
    &#39;&#39;&#39;Add a qPCR measurement for subject `ridx` at time index
    `tidx` to qPCR set `l`

    Parameters
    ----------
    ridx : int
        Subject index
    tidx : int
        Time index
    l : int
        qPCR set index
    &#39;&#39;&#39;
    if not pl.isint(ridx):
        raise TypeError(&#39;`ridx` ({}) must be an int&#39;.format(type(ridx)))
    if ridx &gt;= self.G.data.n_replicates:
        raise ValueError(&#39;`ridx` ({}) out of range ({})&#39;.format(ridx, 
            self.G.data.n_replicates))
    if not pl.isint(tidx):
        raise TypeError(&#39;`tidx` ({}) must be an int&#39;.format(type(tidx)))
    if tidx &gt;= len(self.G.data.given_timepoints[ridx]):
        raise ValueError(&#39;`tidx` ({}) out of range ({})&#39;.format(tidx, 
            len(self.G.data.given_timepoints[ridx])))
    if not pl.isint(l):
        raise TypeError(&#39;`l` ({}) must be an int&#39;.format(type(l)))
    if l &gt;= self.L:
        raise ValueError(&#39;`l` ({}) out of range ({})&#39;.format(tidx, 
            self.L))
    self.value[ridx].add_qpcr_measurement(tidx=tidx, l=l)        </code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="mdsine2.pylab.variables.Variable" href="pylab/variables.html#mdsine2.pylab.variables.Variable">Variable</a></b></code>:
<ul class="hlist">
<li><code><a title="mdsine2.pylab.variables.Variable.T" href="pylab/variables.html#mdsine2.pylab.variables.Variable.T">T</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.add_child" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_child">add_child</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.add_parent" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_parent">add_parent</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.add_prior" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_prior">add_prior</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.add_trace" href="pylab/base.html#mdsine2.pylab.base.Traceable.add_trace">add_trace</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.add_undirected" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_undirected">add_undirected</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.degree" href="pylab/graph.html#mdsine2.pylab.graph.Node.degree">degree</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.delete" href="pylab/graph.html#mdsine2.pylab.graph.BaseNode.delete">delete</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.get_adjacent_keys" href="pylab/graph.html#mdsine2.pylab.graph.Node.get_adjacent_keys">get_adjacent_keys</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.get_iter" href="pylab/base.html#mdsine2.pylab.base.Traceable.get_iter">get_iter</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.get_trace_from_disk" href="pylab/base.html#mdsine2.pylab.base.Traceable.get_trace_from_disk">get_trace_from_disk</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.load" href="pylab/base.html#mdsine2.pylab.base.Saveable.load">load</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.metropolis" href="pylab/graph.html#mdsine2.pylab.graph.Node.metropolis">metropolis</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.overwrite_entire_trace_on_disk" href="pylab/base.html#mdsine2.pylab.base.Traceable.overwrite_entire_trace_on_disk">overwrite_entire_trace_on_disk</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.remove_local_trace" href="pylab/variables.html#mdsine2.pylab.variables.Variable.remove_local_trace">remove_local_trace</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.save" href="pylab/base.html#mdsine2.pylab.base.Saveable.save">save</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.set_save_location" href="pylab/base.html#mdsine2.pylab.base.Saveable.set_save_location">set_save_location</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.set_trace" href="pylab/base.html#mdsine2.pylab.base.Traceable.set_trace">set_trace</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Variable.set_value_shape" href="pylab/variables.html#mdsine2.pylab.variables.Variable.set_value_shape">set_value_shape</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="mdsine2" href="index.html">mdsine2</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="mdsine2.posterior.build_prior_covariance" href="#mdsine2.posterior.build_prior_covariance">build_prior_covariance</a></code></li>
<li><code><a title="mdsine2.posterior.build_prior_mean" href="#mdsine2.posterior.build_prior_mean">build_prior_mean</a></code></li>
<li><code><a title="mdsine2.posterior.expected_n_clusters" href="#mdsine2.posterior.expected_n_clusters">expected_n_clusters</a></code></li>
<li><code><a title="mdsine2.posterior.log_det" href="#mdsine2.posterior.log_det">log_det</a></code></li>
<li><code><a title="mdsine2.posterior.negbin_loglikelihood" href="#mdsine2.posterior.negbin_loglikelihood">negbin_loglikelihood</a></code></li>
<li><code><a title="mdsine2.posterior.negbin_loglikelihood_MH_condensed" href="#mdsine2.posterior.negbin_loglikelihood_MH_condensed">negbin_loglikelihood_MH_condensed</a></code></li>
<li><code><a title="mdsine2.posterior.negbin_loglikelihood_MH_condensed_not_fast" href="#mdsine2.posterior.negbin_loglikelihood_MH_condensed_not_fast">negbin_loglikelihood_MH_condensed_not_fast</a></code></li>
<li><code><a title="mdsine2.posterior.pinv" href="#mdsine2.posterior.pinv">pinv</a></code></li>
<li><code><a title="mdsine2.posterior.prod_gaussians" href="#mdsine2.posterior.prod_gaussians">prod_gaussians</a></code></li>
<li><code><a title="mdsine2.posterior.sample_categorical_log" href="#mdsine2.posterior.sample_categorical_log">sample_categorical_log</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="mdsine2.posterior.ClusterAssignments" href="#mdsine2.posterior.ClusterAssignments">ClusterAssignments</a></code></h4>
<ul class="">
<li><code><a title="mdsine2.posterior.ClusterAssignments.add_trace" href="#mdsine2.posterior.ClusterAssignments.add_trace">add_trace</a></code></li>
<li><code><a title="mdsine2.posterior.ClusterAssignments.calculate_marginal_loglikelihood_slow" href="#mdsine2.posterior.ClusterAssignments.calculate_marginal_loglikelihood_slow">calculate_marginal_loglikelihood_slow</a></code></li>
<li><code><a title="mdsine2.posterior.ClusterAssignments.calculate_marginal_loglikelihood_slow_fast" href="#mdsine2.posterior.ClusterAssignments.calculate_marginal_loglikelihood_slow_fast">calculate_marginal_loglikelihood_slow_fast</a></code></li>
<li><code><a title="mdsine2.posterior.ClusterAssignments.calculate_marginal_loglikelihood_slow_fast_sparse" href="#mdsine2.posterior.ClusterAssignments.calculate_marginal_loglikelihood_slow_fast_sparse">calculate_marginal_loglikelihood_slow_fast_sparse</a></code></li>
<li><code><a title="mdsine2.posterior.ClusterAssignments.gibbs_update_single_taxon_parallel" href="#mdsine2.posterior.ClusterAssignments.gibbs_update_single_taxon_parallel">gibbs_update_single_taxon_parallel</a></code></li>
<li><code><a title="mdsine2.posterior.ClusterAssignments.gibbs_update_single_taxon_slow" href="#mdsine2.posterior.ClusterAssignments.gibbs_update_single_taxon_slow">gibbs_update_single_taxon_slow</a></code></li>
<li><code><a title="mdsine2.posterior.ClusterAssignments.gibbs_update_single_taxon_slow_fast" href="#mdsine2.posterior.ClusterAssignments.gibbs_update_single_taxon_slow_fast">gibbs_update_single_taxon_slow_fast</a></code></li>
<li><code><a title="mdsine2.posterior.ClusterAssignments.initialize" href="#mdsine2.posterior.ClusterAssignments.initialize">initialize</a></code></li>
<li><code><a title="mdsine2.posterior.ClusterAssignments.kill" href="#mdsine2.posterior.ClusterAssignments.kill">kill</a></code></li>
<li><code><a title="mdsine2.posterior.ClusterAssignments.remove_local_trace" href="#mdsine2.posterior.ClusterAssignments.remove_local_trace">remove_local_trace</a></code></li>
<li><code><a title="mdsine2.posterior.ClusterAssignments.sample_iter" href="#mdsine2.posterior.ClusterAssignments.sample_iter">sample_iter</a></code></li>
<li><code><a title="mdsine2.posterior.ClusterAssignments.set_trace" href="#mdsine2.posterior.ClusterAssignments.set_trace">set_trace</a></code></li>
<li><code><a title="mdsine2.posterior.ClusterAssignments.update_mp" href="#mdsine2.posterior.ClusterAssignments.update_mp">update_mp</a></code></li>
<li><code><a title="mdsine2.posterior.ClusterAssignments.update_slow" href="#mdsine2.posterior.ClusterAssignments.update_slow">update_slow</a></code></li>
<li><code><a title="mdsine2.posterior.ClusterAssignments.update_slow_fast" href="#mdsine2.posterior.ClusterAssignments.update_slow_fast">update_slow_fast</a></code></li>
<li><code><a title="mdsine2.posterior.ClusterAssignments.value" href="#mdsine2.posterior.ClusterAssignments.value">value</a></code></li>
<li><code><a title="mdsine2.posterior.ClusterAssignments.visualize" href="#mdsine2.posterior.ClusterAssignments.visualize">visualize</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="mdsine2.posterior.ClusterInteractionIndicatorProbability" href="#mdsine2.posterior.ClusterInteractionIndicatorProbability">ClusterInteractionIndicatorProbability</a></code></h4>
<ul class="">
<li><code><a title="mdsine2.posterior.ClusterInteractionIndicatorProbability.initialize" href="#mdsine2.posterior.ClusterInteractionIndicatorProbability.initialize">initialize</a></code></li>
<li><code><a title="mdsine2.posterior.ClusterInteractionIndicatorProbability.update" href="#mdsine2.posterior.ClusterInteractionIndicatorProbability.update">update</a></code></li>
<li><code><a title="mdsine2.posterior.ClusterInteractionIndicatorProbability.visualize" href="#mdsine2.posterior.ClusterInteractionIndicatorProbability.visualize">visualize</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="mdsine2.posterior.ClusterInteractionIndicators" href="#mdsine2.posterior.ClusterInteractionIndicators">ClusterInteractionIndicators</a></code></h4>
<ul class="">
<li><code><a title="mdsine2.posterior.ClusterInteractionIndicators.calculate_marginal_loglikelihood" href="#mdsine2.posterior.ClusterInteractionIndicators.calculate_marginal_loglikelihood">calculate_marginal_loglikelihood</a></code></li>
<li><code><a title="mdsine2.posterior.ClusterInteractionIndicators.calculate_relative_marginal_loglikelihood" href="#mdsine2.posterior.ClusterInteractionIndicators.calculate_relative_marginal_loglikelihood">calculate_relative_marginal_loglikelihood</a></code></li>
<li><code><a title="mdsine2.posterior.ClusterInteractionIndicators.initialize" href="#mdsine2.posterior.ClusterInteractionIndicators.initialize">initialize</a></code></li>
<li><code><a title="mdsine2.posterior.ClusterInteractionIndicators.kill" href="#mdsine2.posterior.ClusterInteractionIndicators.kill">kill</a></code></li>
<li><code><a title="mdsine2.posterior.ClusterInteractionIndicators.make_rel_params" href="#mdsine2.posterior.ClusterInteractionIndicators.make_rel_params">make_rel_params</a></code></li>
<li><code><a title="mdsine2.posterior.ClusterInteractionIndicators.update_cnt_indicators" href="#mdsine2.posterior.ClusterInteractionIndicators.update_cnt_indicators">update_cnt_indicators</a></code></li>
<li><code><a title="mdsine2.posterior.ClusterInteractionIndicators.update_relative" href="#mdsine2.posterior.ClusterInteractionIndicators.update_relative">update_relative</a></code></li>
<li><code><a title="mdsine2.posterior.ClusterInteractionIndicators.update_single_idx_fast" href="#mdsine2.posterior.ClusterInteractionIndicators.update_single_idx_fast">update_single_idx_fast</a></code></li>
<li><code><a title="mdsine2.posterior.ClusterInteractionIndicators.update_single_idx_slow" href="#mdsine2.posterior.ClusterInteractionIndicators.update_single_idx_slow">update_single_idx_slow</a></code></li>
<li><code><a title="mdsine2.posterior.ClusterInteractionIndicators.update_slow" href="#mdsine2.posterior.ClusterInteractionIndicators.update_slow">update_slow</a></code></li>
<li><code><a title="mdsine2.posterior.ClusterInteractionIndicators.visualize" href="#mdsine2.posterior.ClusterInteractionIndicators.visualize">visualize</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="mdsine2.posterior.ClusterInteractionValue" href="#mdsine2.posterior.ClusterInteractionValue">ClusterInteractionValue</a></code></h4>
<ul class="">
<li><code><a title="mdsine2.posterior.ClusterInteractionValue.initialize" href="#mdsine2.posterior.ClusterInteractionValue.initialize">initialize</a></code></li>
<li><code><a title="mdsine2.posterior.ClusterInteractionValue.set_values" href="#mdsine2.posterior.ClusterInteractionValue.set_values">set_values</a></code></li>
<li><code><a title="mdsine2.posterior.ClusterInteractionValue.update" href="#mdsine2.posterior.ClusterInteractionValue.update">update</a></code></li>
<li><code><a title="mdsine2.posterior.ClusterInteractionValue.update_str" href="#mdsine2.posterior.ClusterInteractionValue.update_str">update_str</a></code></li>
<li><code><a title="mdsine2.posterior.ClusterInteractionValue.visualize" href="#mdsine2.posterior.ClusterInteractionValue.visualize">visualize</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="mdsine2.posterior.Concentration" href="#mdsine2.posterior.Concentration">Concentration</a></code></h4>
<ul class="">
<li><code><a title="mdsine2.posterior.Concentration.initialize" href="#mdsine2.posterior.Concentration.initialize">initialize</a></code></li>
<li><code><a title="mdsine2.posterior.Concentration.update" href="#mdsine2.posterior.Concentration.update">update</a></code></li>
<li><code><a title="mdsine2.posterior.Concentration.visualize" href="#mdsine2.posterior.Concentration.visualize">visualize</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="mdsine2.posterior.CondensedClustering" href="#mdsine2.posterior.CondensedClustering">CondensedClustering</a></code></h4>
</li>
<li>
<h4><code><a title="mdsine2.posterior.FilteringLogMP" href="#mdsine2.posterior.FilteringLogMP">FilteringLogMP</a></code></h4>
<ul class="two-column">
<li><code><a title="mdsine2.posterior.FilteringLogMP.add_trace" href="#mdsine2.posterior.FilteringLogMP.add_trace">add_trace</a></code></li>
<li><code><a title="mdsine2.posterior.FilteringLogMP.initialize" href="#mdsine2.posterior.FilteringLogMP.initialize">initialize</a></code></li>
<li><code><a title="mdsine2.posterior.FilteringLogMP.kill" href="#mdsine2.posterior.FilteringLogMP.kill">kill</a></code></li>
<li><code><a title="mdsine2.posterior.FilteringLogMP.sample_iter" href="#mdsine2.posterior.FilteringLogMP.sample_iter">sample_iter</a></code></li>
<li><code><a title="mdsine2.posterior.FilteringLogMP.set_latent_as_data" href="#mdsine2.posterior.FilteringLogMP.set_latent_as_data">set_latent_as_data</a></code></li>
<li><code><a title="mdsine2.posterior.FilteringLogMP.set_trace" href="#mdsine2.posterior.FilteringLogMP.set_trace">set_trace</a></code></li>
<li><code><a title="mdsine2.posterior.FilteringLogMP.update" href="#mdsine2.posterior.FilteringLogMP.update">update</a></code></li>
<li><code><a title="mdsine2.posterior.FilteringLogMP.visualize" href="#mdsine2.posterior.FilteringLogMP.visualize">visualize</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="mdsine2.posterior.GLVParameters" href="#mdsine2.posterior.GLVParameters">GLVParameters</a></code></h4>
<ul class="">
<li><code><a title="mdsine2.posterior.GLVParameters.add_trace" href="#mdsine2.posterior.GLVParameters.add_trace">add_trace</a></code></li>
<li><code><a title="mdsine2.posterior.GLVParameters.asarray" href="#mdsine2.posterior.GLVParameters.asarray">asarray</a></code></li>
<li><code><a title="mdsine2.posterior.GLVParameters.initialize" href="#mdsine2.posterior.GLVParameters.initialize">initialize</a></code></li>
<li><code><a title="mdsine2.posterior.GLVParameters.update" href="#mdsine2.posterior.GLVParameters.update">update</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="mdsine2.posterior.Growth" href="#mdsine2.posterior.Growth">Growth</a></code></h4>
<ul class="">
<li><code><a title="mdsine2.posterior.Growth.calculate_posterior" href="#mdsine2.posterior.Growth.calculate_posterior">calculate_posterior</a></code></li>
<li><code><a title="mdsine2.posterior.Growth.initialize" href="#mdsine2.posterior.Growth.initialize">initialize</a></code></li>
<li><code><a title="mdsine2.posterior.Growth.update" href="#mdsine2.posterior.Growth.update">update</a></code></li>
<li><code><a title="mdsine2.posterior.Growth.update_str" href="#mdsine2.posterior.Growth.update_str">update_str</a></code></li>
<li><code><a title="mdsine2.posterior.Growth.visualize" href="#mdsine2.posterior.Growth.visualize">visualize</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="mdsine2.posterior.PerturbationIndicators" href="#mdsine2.posterior.PerturbationIndicators">PerturbationIndicators</a></code></h4>
<ul class="">
<li><code><a title="mdsine2.posterior.PerturbationIndicators.add_trace" href="#mdsine2.posterior.PerturbationIndicators.add_trace">add_trace</a></code></li>
<li><code><a title="mdsine2.posterior.PerturbationIndicators.calculate_marginal_loglikelihood" href="#mdsine2.posterior.PerturbationIndicators.calculate_marginal_loglikelihood">calculate_marginal_loglikelihood</a></code></li>
<li><code><a title="mdsine2.posterior.PerturbationIndicators.calculate_relative_marginal_loglikelihood" href="#mdsine2.posterior.PerturbationIndicators.calculate_relative_marginal_loglikelihood">calculate_relative_marginal_loglikelihood</a></code></li>
<li><code><a title="mdsine2.posterior.PerturbationIndicators.initialize" href="#mdsine2.posterior.PerturbationIndicators.initialize">initialize</a></code></li>
<li><code><a title="mdsine2.posterior.PerturbationIndicators.make_rel_params" href="#mdsine2.posterior.PerturbationIndicators.make_rel_params">make_rel_params</a></code></li>
<li><code><a title="mdsine2.posterior.PerturbationIndicators.sample_iter" href="#mdsine2.posterior.PerturbationIndicators.sample_iter">sample_iter</a></code></li>
<li><code><a title="mdsine2.posterior.PerturbationIndicators.set_trace" href="#mdsine2.posterior.PerturbationIndicators.set_trace">set_trace</a></code></li>
<li><code><a title="mdsine2.posterior.PerturbationIndicators.total_on" href="#mdsine2.posterior.PerturbationIndicators.total_on">total_on</a></code></li>
<li><code><a title="mdsine2.posterior.PerturbationIndicators.update_relative" href="#mdsine2.posterior.PerturbationIndicators.update_relative">update_relative</a></code></li>
<li><code><a title="mdsine2.posterior.PerturbationIndicators.update_single_idx_fast" href="#mdsine2.posterior.PerturbationIndicators.update_single_idx_fast">update_single_idx_fast</a></code></li>
<li><code><a title="mdsine2.posterior.PerturbationIndicators.update_single_idx_slow" href="#mdsine2.posterior.PerturbationIndicators.update_single_idx_slow">update_single_idx_slow</a></code></li>
<li><code><a title="mdsine2.posterior.PerturbationIndicators.update_slow" href="#mdsine2.posterior.PerturbationIndicators.update_slow">update_slow</a></code></li>
<li><code><a title="mdsine2.posterior.PerturbationIndicators.visualize" href="#mdsine2.posterior.PerturbationIndicators.visualize">visualize</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="mdsine2.posterior.PerturbationMagnitudes" href="#mdsine2.posterior.PerturbationMagnitudes">PerturbationMagnitudes</a></code></h4>
<ul class="two-column">
<li><code><a title="mdsine2.posterior.PerturbationMagnitudes.asarray" href="#mdsine2.posterior.PerturbationMagnitudes.asarray">asarray</a></code></li>
<li><code><a title="mdsine2.posterior.PerturbationMagnitudes.initialize" href="#mdsine2.posterior.PerturbationMagnitudes.initialize">initialize</a></code></li>
<li><code><a title="mdsine2.posterior.PerturbationMagnitudes.sample_iter" href="#mdsine2.posterior.PerturbationMagnitudes.sample_iter">sample_iter</a></code></li>
<li><code><a title="mdsine2.posterior.PerturbationMagnitudes.set_values" href="#mdsine2.posterior.PerturbationMagnitudes.set_values">set_values</a></code></li>
<li><code><a title="mdsine2.posterior.PerturbationMagnitudes.toarray" href="#mdsine2.posterior.PerturbationMagnitudes.toarray">toarray</a></code></li>
<li><code><a title="mdsine2.posterior.PerturbationMagnitudes.update" href="#mdsine2.posterior.PerturbationMagnitudes.update">update</a></code></li>
<li><code><a title="mdsine2.posterior.PerturbationMagnitudes.update_str" href="#mdsine2.posterior.PerturbationMagnitudes.update_str">update_str</a></code></li>
<li><code><a title="mdsine2.posterior.PerturbationMagnitudes.visualize" href="#mdsine2.posterior.PerturbationMagnitudes.visualize">visualize</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="mdsine2.posterior.PerturbationProbabilities" href="#mdsine2.posterior.PerturbationProbabilities">PerturbationProbabilities</a></code></h4>
<ul class="two-column">
<li><code><a title="mdsine2.posterior.PerturbationProbabilities.add_trace" href="#mdsine2.posterior.PerturbationProbabilities.add_trace">add_trace</a></code></li>
<li><code><a title="mdsine2.posterior.PerturbationProbabilities.initialize" href="#mdsine2.posterior.PerturbationProbabilities.initialize">initialize</a></code></li>
<li><code><a title="mdsine2.posterior.PerturbationProbabilities.sample_iter" href="#mdsine2.posterior.PerturbationProbabilities.sample_iter">sample_iter</a></code></li>
<li><code><a title="mdsine2.posterior.PerturbationProbabilities.set_trace" href="#mdsine2.posterior.PerturbationProbabilities.set_trace">set_trace</a></code></li>
<li><code><a title="mdsine2.posterior.PerturbationProbabilities.update" href="#mdsine2.posterior.PerturbationProbabilities.update">update</a></code></li>
<li><code><a title="mdsine2.posterior.PerturbationProbabilities.visualize" href="#mdsine2.posterior.PerturbationProbabilities.visualize">visualize</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="mdsine2.posterior.PriorMeanInteractions" href="#mdsine2.posterior.PriorMeanInteractions">PriorMeanInteractions</a></code></h4>
<ul class="">
<li><code><a title="mdsine2.posterior.PriorMeanInteractions.initialize" href="#mdsine2.posterior.PriorMeanInteractions.initialize">initialize</a></code></li>
<li><code><a title="mdsine2.posterior.PriorMeanInteractions.update" href="#mdsine2.posterior.PriorMeanInteractions.update">update</a></code></li>
<li><code><a title="mdsine2.posterior.PriorMeanInteractions.visualize" href="#mdsine2.posterior.PriorMeanInteractions.visualize">visualize</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="mdsine2.posterior.PriorMeanMH" href="#mdsine2.posterior.PriorMeanMH">PriorMeanMH</a></code></h4>
<ul class="">
<li><code><a title="mdsine2.posterior.PriorMeanMH.initialize" href="#mdsine2.posterior.PriorMeanMH.initialize">initialize</a></code></li>
<li><code><a title="mdsine2.posterior.PriorMeanMH.update" href="#mdsine2.posterior.PriorMeanMH.update">update</a></code></li>
<li><code><a title="mdsine2.posterior.PriorMeanMH.update_var" href="#mdsine2.posterior.PriorMeanMH.update_var">update_var</a></code></li>
<li><code><a title="mdsine2.posterior.PriorMeanMH.visualize" href="#mdsine2.posterior.PriorMeanMH.visualize">visualize</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="mdsine2.posterior.PriorMeanPerturbationSingle" href="#mdsine2.posterior.PriorMeanPerturbationSingle">PriorMeanPerturbationSingle</a></code></h4>
<ul class="">
<li><code><a title="mdsine2.posterior.PriorMeanPerturbationSingle.initialize" href="#mdsine2.posterior.PriorMeanPerturbationSingle.initialize">initialize</a></code></li>
<li><code><a title="mdsine2.posterior.PriorMeanPerturbationSingle.update" href="#mdsine2.posterior.PriorMeanPerturbationSingle.update">update</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="mdsine2.posterior.PriorMeanPerturbations" href="#mdsine2.posterior.PriorMeanPerturbations">PriorMeanPerturbations</a></code></h4>
<ul class="">
<li><code><a title="mdsine2.posterior.PriorMeanPerturbations.get_single_value_of_perts" href="#mdsine2.posterior.PriorMeanPerturbations.get_single_value_of_perts">get_single_value_of_perts</a></code></li>
<li><code><a title="mdsine2.posterior.PriorMeanPerturbations.initialize" href="#mdsine2.posterior.PriorMeanPerturbations.initialize">initialize</a></code></li>
<li><code><a title="mdsine2.posterior.PriorMeanPerturbations.sample_iter" href="#mdsine2.posterior.PriorMeanPerturbations.sample_iter">sample_iter</a></code></li>
<li><code><a title="mdsine2.posterior.PriorMeanPerturbations.toarray" href="#mdsine2.posterior.PriorMeanPerturbations.toarray">toarray</a></code></li>
<li><code><a title="mdsine2.posterior.PriorMeanPerturbations.update" href="#mdsine2.posterior.PriorMeanPerturbations.update">update</a></code></li>
<li><code><a title="mdsine2.posterior.PriorMeanPerturbations.visualize" href="#mdsine2.posterior.PriorMeanPerturbations.visualize">visualize</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="mdsine2.posterior.PriorVarInteractions" href="#mdsine2.posterior.PriorVarInteractions">PriorVarInteractions</a></code></h4>
<ul class="">
<li><code><a title="mdsine2.posterior.PriorVarInteractions.initialize" href="#mdsine2.posterior.PriorVarInteractions.initialize">initialize</a></code></li>
<li><code><a title="mdsine2.posterior.PriorVarInteractions.update" href="#mdsine2.posterior.PriorVarInteractions.update">update</a></code></li>
<li><code><a title="mdsine2.posterior.PriorVarInteractions.visualize" href="#mdsine2.posterior.PriorVarInteractions.visualize">visualize</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="mdsine2.posterior.PriorVarMH" href="#mdsine2.posterior.PriorVarMH">PriorVarMH</a></code></h4>
<ul class="">
<li><code><a title="mdsine2.posterior.PriorVarMH.initialize" href="#mdsine2.posterior.PriorVarMH.initialize">initialize</a></code></li>
<li><code><a title="mdsine2.posterior.PriorVarMH.update" href="#mdsine2.posterior.PriorVarMH.update">update</a></code></li>
<li><code><a title="mdsine2.posterior.PriorVarMH.update_dof" href="#mdsine2.posterior.PriorVarMH.update_dof">update_dof</a></code></li>
<li><code><a title="mdsine2.posterior.PriorVarMH.visualize" href="#mdsine2.posterior.PriorVarMH.visualize">visualize</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="mdsine2.posterior.PriorVarPerturbationSingle" href="#mdsine2.posterior.PriorVarPerturbationSingle">PriorVarPerturbationSingle</a></code></h4>
<ul class="">
<li><code><a title="mdsine2.posterior.PriorVarPerturbationSingle.initialize" href="#mdsine2.posterior.PriorVarPerturbationSingle.initialize">initialize</a></code></li>
<li><code><a title="mdsine2.posterior.PriorVarPerturbationSingle.update" href="#mdsine2.posterior.PriorVarPerturbationSingle.update">update</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="mdsine2.posterior.PriorVarPerturbations" href="#mdsine2.posterior.PriorVarPerturbations">PriorVarPerturbations</a></code></h4>
<ul class="">
<li><code><a title="mdsine2.posterior.PriorVarPerturbations.diag" href="#mdsine2.posterior.PriorVarPerturbations.diag">diag</a></code></li>
<li><code><a title="mdsine2.posterior.PriorVarPerturbations.get_single_value_of_perts" href="#mdsine2.posterior.PriorVarPerturbations.get_single_value_of_perts">get_single_value_of_perts</a></code></li>
<li><code><a title="mdsine2.posterior.PriorVarPerturbations.initialize" href="#mdsine2.posterior.PriorVarPerturbations.initialize">initialize</a></code></li>
<li><code><a title="mdsine2.posterior.PriorVarPerturbations.sample_iter" href="#mdsine2.posterior.PriorVarPerturbations.sample_iter">sample_iter</a></code></li>
<li><code><a title="mdsine2.posterior.PriorVarPerturbations.update" href="#mdsine2.posterior.PriorVarPerturbations.update">update</a></code></li>
<li><code><a title="mdsine2.posterior.PriorVarPerturbations.visualize" href="#mdsine2.posterior.PriorVarPerturbations.visualize">visualize</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="mdsine2.posterior.ProcessVarGlobal" href="#mdsine2.posterior.ProcessVarGlobal">ProcessVarGlobal</a></code></h4>
<ul class="">
<li><code><a title="mdsine2.posterior.ProcessVarGlobal.build_matrix" href="#mdsine2.posterior.ProcessVarGlobal.build_matrix">build_matrix</a></code></li>
<li><code><a title="mdsine2.posterior.ProcessVarGlobal.initialize" href="#mdsine2.posterior.ProcessVarGlobal.initialize">initialize</a></code></li>
<li><code><a title="mdsine2.posterior.ProcessVarGlobal.rebuild_diag" href="#mdsine2.posterior.ProcessVarGlobal.rebuild_diag">rebuild_diag</a></code></li>
<li><code><a title="mdsine2.posterior.ProcessVarGlobal.update" href="#mdsine2.posterior.ProcessVarGlobal.update">update</a></code></li>
<li><code><a title="mdsine2.posterior.ProcessVarGlobal.visualize" href="#mdsine2.posterior.ProcessVarGlobal.visualize">visualize</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="mdsine2.posterior.SelfInteractions" href="#mdsine2.posterior.SelfInteractions">SelfInteractions</a></code></h4>
<ul class="">
<li><code><a title="mdsine2.posterior.SelfInteractions.calculate_posterior" href="#mdsine2.posterior.SelfInteractions.calculate_posterior">calculate_posterior</a></code></li>
<li><code><a title="mdsine2.posterior.SelfInteractions.initialize" href="#mdsine2.posterior.SelfInteractions.initialize">initialize</a></code></li>
<li><code><a title="mdsine2.posterior.SelfInteractions.update" href="#mdsine2.posterior.SelfInteractions.update">update</a></code></li>
<li><code><a title="mdsine2.posterior.SelfInteractions.update_str" href="#mdsine2.posterior.SelfInteractions.update_str">update_str</a></code></li>
<li><code><a title="mdsine2.posterior.SelfInteractions.visualize" href="#mdsine2.posterior.SelfInteractions.visualize">visualize</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="mdsine2.posterior.SingleClusterFullParallelization" href="#mdsine2.posterior.SingleClusterFullParallelization">SingleClusterFullParallelization</a></code></h4>
<ul class="">
<li><code><a title="mdsine2.posterior.SingleClusterFullParallelization.build_interactions_matrix" href="#mdsine2.posterior.SingleClusterFullParallelization.build_interactions_matrix">build_interactions_matrix</a></code></li>
<li><code><a title="mdsine2.posterior.SingleClusterFullParallelization.build_perturbations_matrix" href="#mdsine2.posterior.SingleClusterFullParallelization.build_perturbations_matrix">build_perturbations_matrix</a></code></li>
<li><code><a title="mdsine2.posterior.SingleClusterFullParallelization.build_prior_cov_and_prec_and_diag" href="#mdsine2.posterior.SingleClusterFullParallelization.build_prior_cov_and_prec_and_diag">build_prior_cov_and_prec_and_diag</a></code></li>
<li><code><a title="mdsine2.posterior.SingleClusterFullParallelization.build_prior_mean" href="#mdsine2.posterior.SingleClusterFullParallelization.build_prior_mean">build_prior_mean</a></code></li>
<li><code><a title="mdsine2.posterior.SingleClusterFullParallelization.calculate_marginal_loglikelihood_slow_fast_sparse" href="#mdsine2.posterior.SingleClusterFullParallelization.calculate_marginal_loglikelihood_slow_fast_sparse">calculate_marginal_loglikelihood_slow_fast_sparse</a></code></li>
<li><code><a title="mdsine2.posterior.SingleClusterFullParallelization.get_indices" href="#mdsine2.posterior.SingleClusterFullParallelization.get_indices">get_indices</a></code></li>
<li><code><a title="mdsine2.posterior.SingleClusterFullParallelization.initialize_gibbs" href="#mdsine2.posterior.SingleClusterFullParallelization.initialize_gibbs">initialize_gibbs</a></code></li>
<li><code><a title="mdsine2.posterior.SingleClusterFullParallelization.initialize_oidx" href="#mdsine2.posterior.SingleClusterFullParallelization.initialize_oidx">initialize_oidx</a></code></li>
<li><code><a title="mdsine2.posterior.SingleClusterFullParallelization.make_iidx2cidxpair" href="#mdsine2.posterior.SingleClusterFullParallelization.make_iidx2cidxpair">make_iidx2cidxpair</a></code></li>
<li><code><a title="mdsine2.posterior.SingleClusterFullParallelization.run" href="#mdsine2.posterior.SingleClusterFullParallelization.run">run</a></code></li>
<li><code><a title="mdsine2.posterior.SingleClusterFullParallelization.set_clustering" href="#mdsine2.posterior.SingleClusterFullParallelization.set_clustering">set_clustering</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="mdsine2.posterior.SubjectLogTrajectorySetMP" href="#mdsine2.posterior.SubjectLogTrajectorySetMP">SubjectLogTrajectorySetMP</a></code></h4>
<ul class="">
<li><code><a title="mdsine2.posterior.SubjectLogTrajectorySetMP.compute_dynamics" href="#mdsine2.posterior.SubjectLogTrajectorySetMP.compute_dynamics">compute_dynamics</a></code></li>
<li><code><a title="mdsine2.posterior.SubjectLogTrajectorySetMP.data_loglik_w_intermediates" href="#mdsine2.posterior.SubjectLogTrajectorySetMP.data_loglik_w_intermediates">data_loglik_w_intermediates</a></code></li>
<li><code><a title="mdsine2.posterior.SubjectLogTrajectorySetMP.data_loglik_wo_intermediates" href="#mdsine2.posterior.SubjectLogTrajectorySetMP.data_loglik_wo_intermediates">data_loglik_wo_intermediates</a></code></li>
<li><code><a title="mdsine2.posterior.SubjectLogTrajectorySetMP.default_forward_loglik" href="#mdsine2.posterior.SubjectLogTrajectorySetMP.default_forward_loglik">default_forward_loglik</a></code></li>
<li><code><a title="mdsine2.posterior.SubjectLogTrajectorySetMP.default_reverse_loglik" href="#mdsine2.posterior.SubjectLogTrajectorySetMP.default_reverse_loglik">default_reverse_loglik</a></code></li>
<li><code><a title="mdsine2.posterior.SubjectLogTrajectorySetMP.first_timepoint_reverse" href="#mdsine2.posterior.SubjectLogTrajectorySetMP.first_timepoint_reverse">first_timepoint_reverse</a></code></li>
<li><code><a title="mdsine2.posterior.SubjectLogTrajectorySetMP.initialize" href="#mdsine2.posterior.SubjectLogTrajectorySetMP.initialize">initialize</a></code></li>
<li><code><a title="mdsine2.posterior.SubjectLogTrajectorySetMP.last_timepoint_forward" href="#mdsine2.posterior.SubjectLogTrajectorySetMP.last_timepoint_forward">last_timepoint_forward</a></code></li>
<li><code><a title="mdsine2.posterior.SubjectLogTrajectorySetMP.persistent_run" href="#mdsine2.posterior.SubjectLogTrajectorySetMP.persistent_run">persistent_run</a></code></li>
<li><code><a title="mdsine2.posterior.SubjectLogTrajectorySetMP.set_attrs_for_timepoint" href="#mdsine2.posterior.SubjectLogTrajectorySetMP.set_attrs_for_timepoint">set_attrs_for_timepoint</a></code></li>
<li><code><a title="mdsine2.posterior.SubjectLogTrajectorySetMP.update_proposals" href="#mdsine2.posterior.SubjectLogTrajectorySetMP.update_proposals">update_proposals</a></code></li>
<li><code><a title="mdsine2.posterior.SubjectLogTrajectorySetMP.update_single" href="#mdsine2.posterior.SubjectLogTrajectorySetMP.update_single">update_single</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="mdsine2.posterior.TrajectorySet" href="#mdsine2.posterior.TrajectorySet">TrajectorySet</a></code></h4>
<ul class="">
<li><code><a title="mdsine2.posterior.TrajectorySet.add_trace" href="#mdsine2.posterior.TrajectorySet.add_trace">add_trace</a></code></li>
<li><code><a title="mdsine2.posterior.TrajectorySet.reset_value_size" href="#mdsine2.posterior.TrajectorySet.reset_value_size">reset_value_size</a></code></li>
<li><code><a title="mdsine2.posterior.TrajectorySet.sample_iter" href="#mdsine2.posterior.TrajectorySet.sample_iter">sample_iter</a></code></li>
<li><code><a title="mdsine2.posterior.TrajectorySet.set_trace" href="#mdsine2.posterior.TrajectorySet.set_trace">set_trace</a></code></li>
<li><code><a title="mdsine2.posterior.TrajectorySet.visualize" href="#mdsine2.posterior.TrajectorySet.visualize">visualize</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="mdsine2.posterior.ZeroInflation" href="#mdsine2.posterior.ZeroInflation">ZeroInflation</a></code></h4>
<ul class="">
<li><code><a title="mdsine2.posterior.ZeroInflation.initialize" href="#mdsine2.posterior.ZeroInflation.initialize">initialize</a></code></li>
<li><code><a title="mdsine2.posterior.ZeroInflation.reset_value_size" href="#mdsine2.posterior.ZeroInflation.reset_value_size">reset_value_size</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="mdsine2.posterior.qPCRDegsOfFreedomL" href="#mdsine2.posterior.qPCRDegsOfFreedomL">qPCRDegsOfFreedomL</a></code></h4>
<ul class="">
<li><code><a title="mdsine2.posterior.qPCRDegsOfFreedomL.add_qpcr_measurement" href="#mdsine2.posterior.qPCRDegsOfFreedomL.add_qpcr_measurement">add_qpcr_measurement</a></code></li>
<li><code><a title="mdsine2.posterior.qPCRDegsOfFreedomL.initialize" href="#mdsine2.posterior.qPCRDegsOfFreedomL.initialize">initialize</a></code></li>
<li><code><a title="mdsine2.posterior.qPCRDegsOfFreedomL.set_shape" href="#mdsine2.posterior.qPCRDegsOfFreedomL.set_shape">set_shape</a></code></li>
<li><code><a title="mdsine2.posterior.qPCRDegsOfFreedomL.update" href="#mdsine2.posterior.qPCRDegsOfFreedomL.update">update</a></code></li>
<li><code><a title="mdsine2.posterior.qPCRDegsOfFreedomL.update_var" href="#mdsine2.posterior.qPCRDegsOfFreedomL.update_var">update_var</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="mdsine2.posterior.qPCRDegsOfFreedoms" href="#mdsine2.posterior.qPCRDegsOfFreedoms">qPCRDegsOfFreedoms</a></code></h4>
</li>
<li>
<h4><code><a title="mdsine2.posterior.qPCRScaleL" href="#mdsine2.posterior.qPCRScaleL">qPCRScaleL</a></code></h4>
<ul class="">
<li><code><a title="mdsine2.posterior.qPCRScaleL.add_qpcr_measurement" href="#mdsine2.posterior.qPCRScaleL.add_qpcr_measurement">add_qpcr_measurement</a></code></li>
<li><code><a title="mdsine2.posterior.qPCRScaleL.initialize" href="#mdsine2.posterior.qPCRScaleL.initialize">initialize</a></code></li>
<li><code><a title="mdsine2.posterior.qPCRScaleL.set_shape" href="#mdsine2.posterior.qPCRScaleL.set_shape">set_shape</a></code></li>
<li><code><a title="mdsine2.posterior.qPCRScaleL.update" href="#mdsine2.posterior.qPCRScaleL.update">update</a></code></li>
<li><code><a title="mdsine2.posterior.qPCRScaleL.update_var" href="#mdsine2.posterior.qPCRScaleL.update_var">update_var</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="mdsine2.posterior.qPCRScales" href="#mdsine2.posterior.qPCRScales">qPCRScales</a></code></h4>
</li>
<li>
<h4><code><a title="mdsine2.posterior.qPCRVarianceReplicate" href="#mdsine2.posterior.qPCRVarianceReplicate">qPCRVarianceReplicate</a></code></h4>
<ul class="">
<li><code><a title="mdsine2.posterior.qPCRVarianceReplicate.add_qpcr_measurement" href="#mdsine2.posterior.qPCRVarianceReplicate.add_qpcr_measurement">add_qpcr_measurement</a></code></li>
<li><code><a title="mdsine2.posterior.qPCRVarianceReplicate.initialize" href="#mdsine2.posterior.qPCRVarianceReplicate.initialize">initialize</a></code></li>
<li><code><a title="mdsine2.posterior.qPCRVarianceReplicate.update" href="#mdsine2.posterior.qPCRVarianceReplicate.update">update</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="mdsine2.posterior.qPCRVariances" href="#mdsine2.posterior.qPCRVariances">qPCRVariances</a></code></h4>
<ul class="">
<li><code><a title="mdsine2.posterior.qPCRVariances.add_qpcr_measurement" href="#mdsine2.posterior.qPCRVariances.add_qpcr_measurement">add_qpcr_measurement</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.1</a>.</p>
</footer>
</body>
</html>