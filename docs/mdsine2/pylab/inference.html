<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.1" />
<title>mdsine2.pylab.inference API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>mdsine2.pylab.inference</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import os
import copy
import h5py
import logging
import time
import shutil
import inspect
import pickle
from orderedset import OrderedSet
import numpy as np

# Typing
from typing import TypeVar, Generic, Any, Union, Dict, Iterator, Tuple, \
    Callable, Type

from .graph import get_default_graph, isgraph, isnode, Graph, Node
from .base import Saveable
from .variables import isVariable, Variable
from .errors import UndefinedError, InheritanceError
from . import util
from .random import seed as set_seed

# Constants
DEFAULT_LOG_EVERY = 5
REQUIRED_ATTRS = [&#39;update&#39;, &#39;initialize&#39;, &#39;set_trace&#39;, &#39;add_trace&#39;]
GLOBAL_PARAMETER = -1

def isMCMC(x: Any) -&gt; bool:
    &#39;&#39;&#39;Checks if the input array is an MCMC inference object

    Parameters
    ----------
    x : any
        Instance we are checking
    
    Returns
    -------
    bool
        True if `x` is a an MCMC inference object
    &#39;&#39;&#39;
    return x is not None and issubclass(x.__class__, BaseMCMC)

def ismodel(x: Any) -&gt; bool:
    &#39;&#39;&#39;Checks if the input array is a model object

    Parameters
    ----------
    x : any
        Instance we are checking
    
    Returns
    -------
    bool
        True if `x` is a model object
    &#39;&#39;&#39;
    return x is not None and issubclass(x.__class__, BaseModel)


class BaseModel(Saveable):
    &#39;&#39;&#39;Base class for a model

    Parameters
    ----------
    graph : pylab.graph.Graph, Optional
        The graph we want to do the inference over
        If nothing is provided, it grabs the default graph
    &#39;&#39;&#39;
    def __init__(self, graph: Graph=None):
        if graph is None:
            graph = get_default_graph()
        if not isgraph(graph):
            raise TypeError(&#39;`graph` ({}) must be None or a pylab.graph.Graph object&#39; \
                &#39;&#39;.format(type(graph)))
        self.graph = graph
        self.ran = False
        self.graph.inference = self


class BaseMCMC(BaseModel):
    &#39;&#39;&#39;Base MCMC over a graph. This only runs 1 chain.

    Typical use
    -----------
    ```
    &gt;&gt;&gt; # You first initialize the object with the graph and parameters you want
    &gt;&gt;&gt; inf = BaseMCMC(burnin=1000, n_samples=2000, graph=G)
    &gt;&gt;&gt; # Then you set the inference order
    &gt;&gt;&gt; inf.set_inference_order([&#39;a&#39;, &#39;b&#39;, &#39;c&#39;])
    &gt;&gt;&gt; # Then set some diagnostic variables (Optional)
    &gt;&gt;&gt; inf.set_diagnostic_variables([&#39;e&#39;])
    &gt;&gt;&gt; # Then we can run the inference
    &gt;&gt;&gt; inf.run(log_every=5, checkpoint=100, tracer_filename=&#39;./output/tracer.hdf5&#39;)
    ```

    Inference order
    ---------------
    This datastructure will perform the inference specified in the inference 
    order, which is set using the `set_inference_order` method. Each element
    in `inf_order` must be an ID in `graph` that implements the functions: `update` 
    (how we sample the variable during inference), `set_trace` (how the 
    tracing gets set up), `add_trace` (how the current value gets added to the 
    trace (this is called immediately after `update`)), and `initialize` (
    how the values get initialized before inference).

    If your posterior class directly inherits a class that is a subclass of 
    `pylab.variables.Variable` then the functions `set_trace` and 
    `add_trace` are already implemented for you.

    Sometimes we want to randomize the order that we update variables. For
    example, if I am updating the growth, self_interactions, and interactions,
    I may want to randomize the order that I update them so that there is no
    unintentional bias during inference. In that case I can make a new object 
    called `gLVParams` where the `update` function randomizes the order that
    growth.update(), self_interactions.update(), and interactions.update() 
    functions are called. 

    Parameters
    ----------
    burnin : int
        Number of initial samples to throw away
    n_samples : int
        Total number of samples of the posterior
        Number of posterior samples = n_samples-burnin
    &#39;&#39;&#39;

    def __init__(self, burnin: int, n_samples: int, * args, **kwargs):
        BaseModel.__init__(self, *args, **kwargs)
        if not util.isint(burnin):
            raise TypeError(&#39;`burnin` ({}) must be an int&#39;.format(type(burnin)))
        if not util.isint(n_samples):
            raise TypeError(&#39;`n_samples` ({}) must be an int&#39;.format(type(n_samples)))
        if n_samples &lt; burnin:
            raise TypeError(&#39;The total number of sample (n_samples) must be&#39;\
                &#39;larger than the burn in (burnin)&#39;)

        self.burnin = burnin
        self.n_samples = n_samples
        self.sample_iter = 0 # This holds the current sample we are at
        self.inf_order = None # This is a list of the inference order
        self.diagnostic_variables = None # DEPRECIATED
        self.tracer = None # This is a pointer to the pylab.inference.Tracer object

        # This is the sample iteration to start on. Unless we are loading from a 
        # saved MCMC object, then this will be 0
        self.start_step = None 

        # Functions to save intermediately, this is used for semisynthetic
        self._intermediate_func = None # callable
        self._intermediate_t = None # float
        self._intermediate_kwargs = None # dict

    @classmethod
    def load(cls, filename: str):
        &#39;&#39;&#39;Override base Saveable to redo the filename of the
        tracer object if it has one
        
        Paramters
        ---------
        filename : str
            This is the location of the file to unpickle
        &#39;&#39;&#39;
        with open(str(filename), &#39;rb&#39;) as handle:
            b = pickle.load(handle)
        
        # redo the filename to the new path if it has a save location
        if hasattr(b, &#39;_save_loc&#39;):
            filename = os.path.abspath(filename)
            b._save_loc = filename

            # Redo the filename of the tracer object if necessart
            if b.tracer is not None:
                if b.tracer.filename is not None:
                    _, tracer_fname = os.path.split(b.tracer.filename)
                    currpath, _ = os.path.split(b._save_loc)

                    new_loc = os.path.join(currpath, tracer_fname)

                    if os.path.isfile(new_loc):
                        b.tracer.filename = new_loc
                    else:
                        raise ValueError(&#39;Looking for tracer hdf5 object in {}, could not find it &#39; \
                            &#39;in the local path even though inference says it contains the object. On &#39; \
                            &#39;file it shows the location {}&#39;.format(
                                new_loc, b.tracer.filename))

        return b

    @property
    def ckpt(self) -&gt; int:
        &#39;&#39;&#39;For backwards compatibility
        &#39;&#39;&#39;
        return self.checkpoint

    @ckpt.setter
    def ckpt(self, a: int):
        &#39;&#39;&#39;Set ckpt
        &#39;&#39;&#39;
        self.ckpt = a

    def names(self) -&gt; Iterator[str]:
        &#39;&#39;&#39;Get the names of the nodes in the inference object

        Returns
        -------
        list(str)
        &#39;&#39;&#39;
        return list(self.graph.name2id.keys())

    def ids(self) -&gt; Iterator[int]:
        &#39;&#39;&#39;Get the IDs of the nodes in the inference object

        Returns
        -------
        list(int)
        &#39;&#39;&#39;
        return list(self.graph.nodes.keys())

    def set_inference_order(self, order: Iterator[Union[str, int]]):
        &#39;&#39;&#39;`order` is an array of nodes we want to sample in the order
        that we want. Check that they are all in the graph. Can put in the 
        ID or the name

        Parameters
        ----------
        order : array_like(int or str)
            Order to do the inference. This must be the IDs of the nodes and can
            be an ID (int) or the name (str)
        &#39;&#39;&#39;
        if not util.isarray(order):
            raise TypeError(&#39;order ({}) must be an array&#39;.format(order))

        ret = []
        for nid in order:
            if nid not in self.graph:
                raise ValueError(&#39;Node ({}) not found in graph ({})&#39;.format(
                    nid, self.names()))
            node = self.graph[nid]

            for attr in REQUIRED_ATTRS:
                if hasattr(node, attr):
                    if not callable(getattr(node, attr)):
                        raise UndefinedError(&#39;node ({}) must have `{}` be callable&#39;.format(
                            node.name, attr))
                else:
                    raise UndefinedError(&#39;node ({}) must have the function `{}`&#39;.format(
                        node.name, attr))
            ret.append(node.id)
        self.inf_order = ret

    def is_in_inference_order(self, var: Union[int, str, Variable]) -&gt; bool:
        &#39;&#39;&#39;Checks if the variable is in the inference order

        Parameters
        ----------
        var : int, str, pylab.variables.Variable
            Identifier for the variable
            If it is an int - it is assumed this is the id of the variable
            If is is a str - it is assumed this is the name of the variable

        Returns
        -------
        bool
        &#39;&#39;&#39;
        if not isnode(var):
            var = self.graph[var].id
        else:
            var = var.id
        return var in self.inf_order

    def set_diagnostic_variables(self, vars):
        &#39;&#39;&#39;DEPRECIATED
        
        A list of variables that you want to track over time. These variables
        do not necessarily have to be variables that we are tracing, rather they
        can be any variable that changes over time in the inference. However,
        the variables that we are tracing need to be a subclass of
        pylab.variables.Variable

        We manually override the tracing for each of the variables.

        Parameters
        ----------
        vars : array_like, set, 1-dim
            - A list of the variables you want to track (the actual object)
            - The elements in the list should be of type pylab.variables.Variable
        &#39;&#39;&#39;
        # wrap vars as an iterable if it is not
        if not hasattr(vars, &#39;__iter__&#39;):
            vars = [vars]

        if len(vars) == 0:
            raise ValueError(&#39;No values in `vars`&#39;)

        # Check parameters passed in
        for var in vars:
            if not isVariable(var):
                raise InheritanceError(&#39;Each variable passed in should be of a subclass of&#39; \
                    &#39; `pylab.variables.Variable`. This is of type `{}`&#39;.format(
                        var.__class__.__name__))

        # Add variables to dictionary
        self.diagnostic_variables = {}
        for var in vars:
            if var.name in self.diagnostic_variables:
                raise ValueError(&#39;Two different diagnostic variables cannot have &#39; \
                    &#39;the same name.&#39;)
            self.diagnostic_variables[var.name] = var

    def set_tracer(self, filename: str, checkpoint: int=100):
        &#39;&#39;&#39;Sets up the tracing object

        Parameters
        ----------
        filename : str, None
            File location to save the hdf5 object
        checkpoint : int, None
            Saves the current progress of the inference chain every `checkpoint` iterations
            If None then there is no intermediate checkpointing
        &#39;&#39;&#39;
        if checkpoint is None:
            checkpoint = self.n_samples
        self.tracer_filename = filename
        self.checkpoint = checkpoint
        self.tracer = Tracer(mcmc=self, filename=filename)

    def set_intermediate_validation(self, t: float, func: Callable, kwargs: Dict[str, Any]=None):
        &#39;&#39;&#39;Every `t`, run the function `func` during validation in a new process during inference.

        Parameters
        ----------
        t : int
            How often to run in number of seconds
        func : callable
            This is the function the intermediate must call. It assumes the
            only parameters are:
                chain : this is the chain object (self)
                burnin : number of burnin
                n_samples : number of total samples
                sample_iter : current iteration number
        kwargs : dict
            Extra arguments
        &#39;&#39;&#39;
        if not util.isint(t):
            raise TypeError(&#39;`t` ({}) must be an int&#39;.format(type(t)))
        if t &lt; 0:
            raise ValueError(&#39;`t` ({}) must be &gt; 0&#39;.format(t))
        if kwargs is None:
            kwargs = {}
        for k,v in kwargs.items():
            if not util.isstr(k):
                raise ValueError(&#39;Keys in kwargs ({}) must be strings&#39;.format(type(k)))
        if not callable(func):
            raise TypeError(&#39;`func` ({}) must be callable&#39;.format(type(str)))

        # Check if the function has the right arguments
        kwarg_keys = list(kwargs.keys())
        valid_args = OrderedSet([&#39;burnin&#39;, &#39;n_samples&#39;, &#39;chain&#39;, &#39;sample_iter&#39;] + kwarg_keys)
        args = inspect.getargspec(func).args
        for arg in args:
            if arg not in valid_args:
                raise ValueError(&#39;Function `{}` does not have the correct arguments. &#39; \
                    &#39;It has the argument `{}` when it should only have the arguments {}.&#39;.format(
                        func.__name__, arg, list(valid_args)))
        for arg in valid_args:
            if arg not in args:
                raise ValueError(&#39;Function `{}` does not have the correct arguments. &#39; \
                    &#39;It is excluding the argument `{}`, which must be included&#39;.format(
                        func.__name__, arg))

        self._intermediate_t = t
        self._intermediate_func = func
        self._intermediate_kwargs = kwargs

    def run(self, log_every: int=1) -&gt; &#34;BaseMCMC&#34;:
        &#39;&#39;&#39;Run the inference.

        Parameters
        ----------
        log_every : int, None
            Logs the values of the variables you are learning every `log_every` iterations.
            If the logger is set to DEBUG then override to log every iteration.
        
        Returns
        -------
        pylab.inference.BaseMCMC
            Output from the inference, self
        &#39;&#39;&#39;
        set_seed(self.graph.seed)
        if self.start_step is None:
            self.start_step = 0
        try:
            if self.inf_order is None:
                raise UndefinedError(&#39;Cannot run mcmc until you have set the inference order.&#39; \
                    &#39; Set with the function `self.set_inference_order`.&#39;)
            if self.tracer is None:
                logging.warning(&#39;No tracer set - assume you do not want to write to disk&#39;)
                self.tracer_filename = None
                self.tracer = Tracer(mcmc=self, filename=None)
                self.checkpoint = None

                logging.info(&#39;Setting the trace of learned parameters&#39;)
                logging.info(&#39;#######################################&#39;)
                for nid in self.inf_order:
                    logging.info(&#39;Setting the trace of {}&#39;.format(self.graph[nid].name))
                    self.graph[nid].set_trace()
                logging.info(&#39;Setting the trace for diagnostic variables&#39;)
                logging.info(&#39;##########################################&#39;)
                if self.diagnostic_variables is not None:
                    for nid in self.diagnostic_variables:
                        logging.info(&#39;Setting the trace of {}&#39;.format(self.graph[nid].name))
                        self.graph[nid].set_trace()

            total_time = time.time()
            if log_every is None:
                log_every = DEFAULT_LOG_EVERY

            start = time.time()
            intermediate_time = time.time()
            for i in range(self.start_step, self.n_samples):
                self.sample_iter = i

                # Check if we need to run the intermediate script
                try:
                    if self._intermediate_t is not None:
                        if time.time() - intermediate_time &gt; self._intermediate_t:
                            logging.info(&#39;Running intermediate script {}&#39;.format(
                                self._intermediate_func.__name__))
                            kwargs = {
                                &#39;chain&#39;: self, &#39;burnin&#39;: self.burnin, 
                                &#39;n_samples&#39;: self.n_samples, &#39;sample_iter&#39;: self.sample_iter}
                            for k,v in self._intermediate_kwargs.items():
                                kwargs[k] = v
                            try:
                                self._intermediate_func(**kwargs)
                            except:
                                raise ValueError(&#39;failed in intermediate function&#39;)
                            intermediate_time = time.time() 
                except AttributeError:
                    logging.info(&#39;Pre Pylab 3.0.0 implementation. Ignore&#39;)
                except:
                    logging.critical(&#39;unknown error&#39;)
                    raise

                # Log where necessary
                if i % log_every == 0:
                    logging.info(&#39;\n\nInference iteration {}/{}, time: {}&#39;.format(
                        i, self.n_samples, time.time() - start))
                    start = time.time()
                    for id in self.inf_order:
                        if type(id) == list:
                            for id_ in id:
                                logging.info(&#39;{}: {}&#39;.format(self.graph.nodes[id_].name,
                                    str(self.graph.nodes[id_])))
                        else:
                            logging.info(&#39;{}: {}&#39;.format(self.graph.nodes[id].name,
                                str(self.graph.nodes[id])))

                # Sample posterior in the order indicated and add the trace
                for _id in self.inf_order:
                    try:
                        self.graph.nodes[_id].update()
                        self.graph.nodes[_id].add_trace()
                    except:
                        logging.critical(&#39;Crashed in `{}`&#39;.format(self.graph[_id].name))
                        # self.graph.tracer.finish_tracing()
                        raise

                # Save diagnostic variables where necessary
                if self.diagnostic_variables is not None:
                    for name in self.diagnostic_variables:
                        if self.diagnostic_variables[name].sample_iter == self.sample_iter:
                            self.diagnostic_variables[name].add_trace()

                # If we just saved the traces to disk, save the MCMC object
                if self.sample_iter % self.checkpoint == 0 and self.sample_iter &gt; 0:
                    try:
                        self.save()
                    except:
                        logging.critical(&#39;If you want to checkpoint, you must set the save location &#39; \
                            &#39;of tracer, graph, and mcmc using the function self.set_save_location()&#39;)
                        print(self._save_loc)
                        raise

            # Finish the tracing
            self.graph.tracer.finish_tracing()
            self.ran = True
            logging.info(&#39;Inference total time: {}/Gibb step&#39;.format(
                (time.time() - total_time)/self.n_samples))
            
            # Remove the local traces
            if self.tracer is not None:
                logging.info(&#39;remove local traces&#39;)
                for node in self.graph:
                    if isVariable(node):
                        node.remove_local_trace()
            self.save()

            return self
        except:
            for a in self.graph._persistent_pntr:
                a.kill()
            raise

    def continue_inference(self, gibb_step_start: int):
        &#39;&#39;&#39;Resume inference at the gibb step number `gibb_step_start`. Note that
        we do not resume the random seed where it was at that point of the inference.

        Parameters
        ----------
        gibb_step_start : int
            Gibb step to start
        &#39;&#39;&#39;
        self.tracer.continue_inference(gibb_step_start=gibb_step_start)
        self.start_step = gibb_step_start


class Tracer(Saveable): 
    &#39;&#39;&#39;This sets up the graph to be traced using h5py.

    Able to checkpoint the values of the graph through inference.
    Write the data to disk. The only variables that are traced are the 
    variables in the set `being_traced`.

    There might be other processes/threads that are reading the current 
    chain to plot it intermittently, so we create the file in SWMR 
    (Single Writer, Multiple Reader) mode which will make the 
    file always readable (never ina  corrupt state from writing).

    Parameters
    ----------
    mcmc : pylab.inference.BaseMCMC
        This is the inference object that we are tracing
    filename : str
        This is where to save it
    &#39;&#39;&#39;
    def __init__(self, mcmc: BaseMCMC, filename: str):
        # Check parameters and set up the attributes
        self.mcmc = mcmc # This is a pointer to the BaseMCMC object
        self.graph = self.mcmc.graph # This is a pointer to the pylab.graph.Graph object
        self.graph.tracer = self # Sets up internal pointers
        self.mcmc.tracer = self # Sets up internal pointers

        self.filename  = filename
        if self.filename is not None:
            if not util.isstr(filename):
                raise TypeError(&#39;filename ({}) must be a str&#39;.format(type(filename)))
            self.filename = os.path.abspath(filename)
            self.f = h5py.File(self.filename, &#39;w&#39;, libver=&#39;latest&#39;) #This is the h5py file
            self.being_traced = OrderedSet()

            self.f.attrs[&#39;burnin&#39;] = self.mcmc.burnin
            self.f.attrs[&#39;n_samples&#39;] = self.mcmc.n_samples
            self.f.attrs[&#39;ckpt&#39;] = self.mcmc.checkpoint

            self.burnin = self.mcmc.burnin
            self.n_samples = self.mcmc.n_samples
            self.checkpoint = self.mcmc.checkpoint

            # Get the inference order and diagnostic variables
            ret = []
            for nid in self.mcmc.inf_order:
                ret.append(self.mcmc.graph[nid].name)
            self.f.attrs[&#39;inf_order&#39;] = ret
            if self.mcmc.diagnostic_variables is not None:
                a = list(self.mcmc.diagnostic_variables.keys())
            else:
                a = []
            self.f.attrs[&#39;diagnostic_variables&#39;] = a
            logging.info(&#39;Setting Single Write, Multiple Read Mode&#39;)
            self.f.swmr_mode = True # single writer, multiple reader mode
            self.close()

            # Add all of the variables being traced to the file
            logging.info(&#39;Setting the trace of learned parameters&#39;)
            logging.info(&#39;#######################################&#39;)
            for nid in self.mcmc.inf_order:
                logging.info(&#39;Setting the trace of {}&#39;.format(self.graph[nid].name))
                self.graph[nid].set_trace()
            logging.info(&#39;Setting the trace for diagnostic variables&#39;)
            logging.info(&#39;##########################################&#39;)
            if self.mcmc.diagnostic_variables is not None:
                for nid in self.mcmc.diagnostic_variables:
                    logging.info(&#39;Setting the trace of {}&#39;.format(self.graph[nid].name))
                    self.graph[nid].set_trace()

    # def __enter__(self):
    #     self.f = h5py.File(self.filename, &#39;r+&#39;)
    #     return self.f

    # def __exit__(self, type, value, traceback):
    #     self.close()

    @property
    def ckpt(self) -&gt; int:
        &#39;&#39;&#39;For backwards compatitbility
        &#39;&#39;&#39;
        return self.ckpt

    @ckpt.setter
    def ckpt(self, a: int):
        &#39;&#39;&#39;Set ckpt
        &#39;&#39;&#39;
        self.ckpt = a

    def close(self):
        &#39;&#39;&#39;Close the file
        &#39;&#39;&#39;
        self.f.close()
        self.f = None

    def open(self):
        &#39;&#39;&#39;Open the file
        &#39;&#39;&#39;
        self.f = h5py.File(self.filename, &#39;r+&#39;, libver=&#39;latest&#39;)

    def copy(self) -&gt; &#34;Tracer&#34;:
        &#39;&#39;&#39;Return a copy of the object but do not copy the underlying hdf5 object
        &#39;&#39;&#39;
        new_obj = type(self)(mcmc=self.mcmc, filename=self.filename)
        new_obj.__dict__.update(self.__dict__)
        return new_obj

    def deepcopy(self, hdf5_dst: str=None) -&gt; &#34;Tracer&#34;:
        &#39;&#39;&#39;Return a deepcopy of the object and copy the underlying hdf5 object as well

        Parameters
        ----------
        hdf5_dst : str, None
            Destination to copy the hdf5 object. If Nothing is provided then we 
            will just append &#39;_copy&#39; to the name of the current filename
        &#39;&#39;&#39;
        if hdf5_dst is None:
            dst = self.filename.replace(&#39;.hdf5&#39;, &#39;_copy.hdf5&#39;)
        else:
            if not util.isstr(hdf5_dst):
                raise TypeError(&#39;`hdf5_dst` ({}) must be a str&#39;)
            dst = hdf5_dst


        new_obj = copy.deepcopy(self)
        new_obj.filename = dst
        shutil.copyfile(src=self.filename, dst=new_obj.filename)
        return new_obj

    def is_being_traced(self, var: Union[int, str, Node]) -&gt; bool:
        &#39;&#39;&#39;Checks if the variable is being traced

        Parameters
        ----------
        var : int, str, pylab.graph.Node
            An ID of a graph object or the object

        Returns
        -------
        bool
        &#39;&#39;&#39;
        if isnode(var):
            var = var.name
        else:
            if var in self.graph:
                var = self.graph[var].name
            else:
                # raise IndexError(&#39;`var` ({}) ({}) not recognized in graph&#39;.format(type(var), var))
                return False
        return var in self.being_traced

    def set_trace(self, name: str, shape: Tuple[int], dtype: Type):
        &#39;&#39;&#39;Set up a dataset for the variable. If a group is specified, it will 
        add it to the group

        Parameters
        ----------
        name : str
            This is the name of the variable.        
        shape : tuple, None
            This is the shape of the variable. This should not include the 
            trace length, that is added in this function. If it is a scalar,
            then the shape is None
        dtype : Type
            This is the type of the trace
        group : str, h5py.Group
            This is the group you want it added to 
        &#39;&#39;&#39;
        if name in self.being_traced:
            logging.info(&#39;Skipping adding the trace of `{}` because it is already being&#39; \
                &#39; traced ({})&#39;.format(name, list(self.being_traced)))
            return
        if not util.isstr(name):
            raise TypeError(&#39;`name` ({}) must be a str&#39;.format(type(name)))
        if not (util.istuple(shape) or shape is None):
            raise TypeError(&#39;`shape` ({}) must be a tuple or None&#39;.format(type(shape)))
        if not util.istype(dtype):
            raise TypeError(&#39;`dtype` ({}) ({}) must be a Type&#39;.format(dtype, type(dtype)))
        self.open()
        
        if shape is not None:
            shape = (self.n_samples, ) + shape
        else:
            shape = (self.n_samples, )
        dset = self.f.create_dataset(name=name, shape=shape, dtype=dtype, chunks=True)
        dset.attrs[&#39;end_iter&#39;] = 0
        self.being_traced.add(name)
        self.close()

    def write_to_disk(self, name: str):
        &#39;&#39;&#39;Append the RAM trace of the variable `name` into disk. Copies the data
        from RAM (self.graph[name]/trace) into disk memory.

        Parameters
        ----------
        name : str
            Name of the variable we are writing 
        &#39;&#39;&#39;
        if self.filename is None:
            raise ValueError(&#39;Tracing to disk not setup&#39;)
        self.open()
        dset = self.f[name]
        i = dset.attrs[&#39;end_iter&#39;]
        node = self.graph[name]
        l = node.ckpt_iter
        
        # print(&#39;\nwriting to disk,&#39;, name)
        # print(&#39;ckpt_iter&#39;, l)
        # print(&#39;trace.shape&#39;, node.trace.shape)
        # print(dset[i:i+l].shape)

        dset[i:i+l] = node.trace
        dset.attrs[&#39;end_iter&#39;] = i + l
        self.close()

    def overwrite_entire_trace_on_disk(self, name: str, data: np.ndarray, dtype: Type=None):
        &#39;&#39;&#39;Overwrite all the data we have on disk for the variable 
        with name `name` and data `data`. Blanks everything out and 
        sets the end variable to the end of data
        
        Parameters
        ----------
        name : str
            Name of the variable we are overwriting
        data : np.ndarray
            Array we are overwriting the data with
        &#39;&#39;&#39;
        if not util.isarray(data):
            raise TypeError(&#39;`data` ({}) must be an array&#39;.format(type(data)))
        data = np.asarray(data)

        self.open()
        dset = self.f[name]
        shape = dset.shape
        if dtype is None:
            dtype = data.dtype

        # Delete the old dataset and make a new one
        del self.f[name]
        self.close()
        self.being_traced.remove(name)

        self.set_trace(name=name, shape=shape, dtype=dtype)
        
        self.open()
        dset = self.f[name]
        dset[:data.shape[0]] = data
        dset.attrs[&#39;end_iter&#39;] = data.shape[0]
        self.close()

    def finish_tracing(self):
        &#39;&#39;&#39;Append the rest of the buffers to the dataset. Delete the local trace
        &#39;&#39;&#39;
        if self.filename is None:
            return
        self.open()
        for name in self.being_traced:
            dset = self.f[name]
            i = dset.attrs[&#39;end_iter&#39;]
            node = self.graph[name]
            l = node.ckpt_iter
            dset[i:i+l] = node.trace
            dset.attrs[&#39;end_iter&#39;] = i+l

            self.graph[name].trace = None
        self.close()

    def get_disk_trace_iteration(self) -&gt; int:
        &#39;&#39;&#39;Returns the last iteration the disk is saved to

        Returns
        -------
        int
        &#39;&#39;&#39;
        self.open()
        name = list(self.f.keys())[0]
        dset = self.f[name]
        n = dset.attrs[&#39;end_iter&#39;]
        self.close()
        return n

    def get_trace(self, name: str, section: str=&#39;posterior&#39;, slices: slice=None) -&gt; np.ndarray:
        &#39;&#39;&#39;Return the trace that corresponds with the name.

        Depdending on the parameter `section`, it will return different parts of
        the trace. Options:
            &#39;posterior&#39;: 
                Returns the trace after the burnin
            &#39;burnin&#39;
                Returns the samples that were in the burnin
            &#39;entire&#39;
                Returns all the samples
            &#39;slice&#39;
                All of the arguments for slicing are in slices
            
        Parameters
        ----------
        name : str
            Name of the variable
        section : str
            Which part of the trace to return - description above
        slices : list(slice), slice
            A list of slicing objects or a slice object.

            slice(start, stop, step)
            Example, single dimension:
                slice(None) == :
                slice(5) == :5
                slice(4, None, None) == 4:
                slice(9, 22,None) == 9:22
            Example, multiple dimensions:
                [slice(None), slice(4, None, None)] == :, 4:
                [slice(None), 4, 5] == :, 4, 5

        Returns
        -------
        np.ndarray
        &#39;&#39;&#39;
        if not util.isstr(section):
            raise TypeError(&#39;`section` ({}) must be a str&#39;.format(type(str)))
        
        self.open()
        dset = self.f[name]
        if section == &#39;posterior&#39;:
            high = dset.attrs[&#39;end_iter&#39;]
            low = self.burnin
            if low &lt; self.burnin:
                self.close()
                raise IndexError(&#39;Last iteration ({}) is less than the burnin ({}). &#39; \
                    &#39;Cannot get the trace&#39;.format(i, self.burnin))
        elif section == &#39;burnin&#39;:
            high = dset.attrs[&#39;end_iter&#39;]
            low = 0
            if high &gt; self.burnin:
                high = self.burnin
        elif section == &#39;entire&#39;:
            low = 0
            high = dset.attrs[&#39;end_iter&#39;]
        elif section != &#39;slice&#39;:
            raise ValueError(&#39;`section` ({}) not recognized&#39;.format(section))
        
        if section != &#39;slice&#39;:
            if slices is not None:
                slices = [slice(low,high,None)] + list(slices)
            else:
                slices = slice(low,high,None)

        ret = dset[slices]
        self.close()
        return ret

    def get_iter(self, name: str) -&gt; int:
        &#39;&#39;&#39;Return the end index that has been saved so far for variable with
        name `name`

        Parameters
        ----------
        name : str
            Name of the variable
        
        Returns
        -------
        int
        &#39;&#39;&#39;
        self.open()
        dset = self.f[name]
        i = dset.attrs[&#39;end_iter&#39;]
        self.close()
        return i

    def continue_inference(self, gibb_step_start: int):
        &#39;&#39;&#39;Restart the inference at the gibbs step provided

        Parameters
        ----------
        gibb_step_start : int
            Gibb step to start at
        &#39;&#39;&#39;
        self.open()
        for name in self.f:
            dset = self.f[name]
            dset.attrs[&#39;end_iter&#39;] = gibb_step_start
        self.close()


def r_hat(chains: Iterator[BaseMCMC], vname: str, start: int, end: int, idx: Union[int, slice]=None, 
    returnBW: bool=False) -&gt; Union[float, Dict[str, float]]:
    &#39;&#39;&#39;Calculate the measure `R^` for the variable called `vname` at the index `idx`.
    If `idx` is None then we assume that the variable is scalar.

    Definition
    ----------
    `R^` is defined in [1] as:
        Let m be the number of distinct chains
        Let n be the length of each chain

        \mu_j = mean for chain j
        \mu = mean over all the chains
        X_{ij} = Value in posterior for sample i in chain j

        % Variance between sequences
        B = [ n/(m-1) ] * \sum^m_{j=1} ( \mu_j - \mu )^2

        % Variance within sequences
        W = [ \sum^m_{j=1} [ \sum^n_{i=1} ( X_{ij} - \mu_j )^2 ] ] / m

        \hat{R} = \sqrt{ [ (n-1)*W/n + B/n ] / W }

    It is assumed that these chains were run with different initial conditions
    and that the data is the same. An error will be thrown if the total number of
    burn-in or total samples are different.

    The values range from [1, inf], where 1 is completely mixed.

    Parameters
    ----------
    chains : list(pylab.inference.BaseMCMC),list(str)
        An iterable object of pl.inference.BaseMCMC objects. If it is a string then
        it is the saved location of the chain object.
    vanme : str, int
        This is the index of the variable we want to calculate `R^` from. This can
        either be the name (str) or the graph ID (int) to identify it.
    idx : int, tuple(int)
        This is the index of the item you&#39;re looking at. This is only necessary if
        this variable is a vector. If it is a scalar then this is ignored.
    returnBW : bool
        If True, returns B (between sequence variance) and W (within sequence variance)
        as well as the :math:`\hat{R}` metric. Returns it as a dictionary

    From the R documentation (https://www.rdocumentation.org/packages/asbio/versions/1.6-5/topics/R.hat):
    Gelman et al. (2003, pg. 296) provides insufficient details to reproduce this function.
    To get the real function see Gelman and Rubin (1992). The authors list one other change
    in their Statlab version of this function at http://lib.stat.cmu.edu/S/itsim. They
    recommend multiplying sqrt(postvar/W) by sqrt((df + 3)/t(df + 1)). The original code
    and this function can produce estimates below 1.

    References
    ----------
    - [1] Gelman, A. and D. B. Rubin (1992) Inference from iterative simulation using multiple
        sequences (with discussion). Statistical Science, 7:457-511.
    - [2] Brooks and Gelman (1998). Journal of Computational and Graphical Statistics, 7(4)434-455.
    - [3] A. Gelman, H. S. Stern, J. B. Carlin, D. B. Dunson, A. Vehtari, and D. B. Rubin, Bayesian Data
        Analysis Third Edition. Chapman and Hall/CRC, 2013.
    - [4] https://stats.stackexchange.com/questions/348984/stan-hatr-versus-gelman-rubin-hatr-definition
    &#39;&#39;&#39;
    # Check that all of the chains are consistent with each other
    if not util.isarray(chains):
        raise TypeError(&#39;`chains` ({}) must be array_like&#39;.format(type(chains)))
    if len(chains) &lt;= 1:
        raise ValueError(&#39;There must be at least 2 items in chains. There are {}&#39;.format(
            len(chains)))
    if not util.isint(start):
        raise TypeError(&#39;`start` ({}) must be an int&#39;.format(start))
    if not util.isint(end):
        raise TypeError(&#39;`end` ({}) must be an int&#39;.format(end))
    for i in range(len(chains)):
        chain = chains[i]
        if util.isstr(chain):
            chains[i] = BaseMCMC.load(chains[i])
            chain = chains[i]
        if not isMCMC(chain):
            raise TypeError(&#39;`chain` ({}) must be a pylab.inference.BaseMCMC object&#39;.format(
                type(chain)))

    # Get the variables from each of the chains and check that they are the proper shapes
    if not (util.isstr(vname) or util.isint(vname)):
        raise TypeError(&#39;`vname` ({}) must be an int or a str&#39;.format(type(vname)))
    traces = [chain.graph[vname].get_trace_from_disk(section=&#39;entire&#39;) for chain in chains]
    traces = [trace[start:end] for trace in traces]
    if idx is not None:
        traces = [trace[:, idx] for trace in traces]

    traces = np.nan_to_num(traces, nan=0.0)

    # Calculate r_hat
    m = traces.shape[0]  # no. of sequences
    n = traces.shape[1]  # length of each chain after burn-in
    try:
        num_var = traces.shape[2]  # Number of indexed variables being sampled
    except:
        # This fails when the variable is a scalar, reshape it so that it 
        # has a final dimension of 1
        traces = traces.reshape(n,m,1)
        num_var = 1
    s_hat_i = np.var(traces, axis=1, ddof=1)
    x_bar_i = np.mean(traces, axis=1)
    x_bar = np.mean(x_bar_i, axis=0)

    # W: Mean of sample variances
    W = np.mean(s_hat_i, axis=0)

    # B: Sample variance of means (times n)
    B = n * np.var(x_bar_i, axis=0, ddof=1)

    # Sigma-hat and V-hat:
    sigma_hat = ((n - 1) / n) * W + (1 / n) * B
    v_hat = sigma_hat + B * (1 / (n * m))

    var_v_hat = (
            (((n - 1) / n) ** 2) * (1 / m) * np.var(s_hat_i, axis=0)
            + (((m + 1) / (m * n)) ** 2) * (2 / (m - 1)) * np.power(B, 2)
            + 2 * (m + 1) * (n - 1) * (1 / m ** 2) * (1 / n) * np.array([
                np.cov(s_hat_i[:, k], np.power(x_bar_i[:, k], 2))[0, 1]
                - 2 * x_bar[k] * np.cov(s_hat_i[:, k], x_bar_i[:, k])[0, 1]
                for k in range(num_var)
            ])
    )

    # degrees of freedom
    dof = 2 * np.power(v_hat, 2) * var_v_hat

    # R-hat (Empirical conditional variance, divided by W)
    rhat = np.sqrt(
        v_hat * (dof+3) * np.reciprocal(W * (dof + 1))
    )

    if returnBW:
        return {&#39;B&#39;: B, &#39;W&#39;: W, &#39;rhat&#39;: rhat}
    else:
        return rhat</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="mdsine2.pylab.inference.isMCMC"><code class="name flex">
<span>def <span class="ident">isMCMC</span></span>(<span>x: Any) ‑> bool</span>
</code></dt>
<dd>
<div class="desc"><p>Checks if the input array is an MCMC inference object</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>any</code></dt>
<dd>Instance we are checking</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>bool</code></dt>
<dd>True if <code>x</code> is a an MCMC inference object</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def isMCMC(x: Any) -&gt; bool:
    &#39;&#39;&#39;Checks if the input array is an MCMC inference object

    Parameters
    ----------
    x : any
        Instance we are checking
    
    Returns
    -------
    bool
        True if `x` is a an MCMC inference object
    &#39;&#39;&#39;
    return x is not None and issubclass(x.__class__, BaseMCMC)</code></pre>
</details>
</dd>
<dt id="mdsine2.pylab.inference.ismodel"><code class="name flex">
<span>def <span class="ident">ismodel</span></span>(<span>x: Any) ‑> bool</span>
</code></dt>
<dd>
<div class="desc"><p>Checks if the input array is a model object</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>any</code></dt>
<dd>Instance we are checking</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>bool</code></dt>
<dd>True if <code>x</code> is a model object</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ismodel(x: Any) -&gt; bool:
    &#39;&#39;&#39;Checks if the input array is a model object

    Parameters
    ----------
    x : any
        Instance we are checking
    
    Returns
    -------
    bool
        True if `x` is a model object
    &#39;&#39;&#39;
    return x is not None and issubclass(x.__class__, BaseModel)</code></pre>
</details>
</dd>
<dt id="mdsine2.pylab.inference.r_hat"><code class="name flex">
<span>def <span class="ident">r_hat</span></span>(<span>chains: Iterator[<a title="mdsine2.pylab.inference.BaseMCMC" href="#mdsine2.pylab.inference.BaseMCMC">BaseMCMC</a>], vname: str, start: int, end: int, idx: Union[int, slice] = None, returnBW: bool = False) ‑> Union[float, Dict[str, float]]</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate the measure <code>R^</code> for the variable called <code>vname</code> at the index <code>idx</code>.
If <code>idx</code> is None then we assume that the variable is scalar.</p>
<h2 id="definition">Definition</h2>
<p><code>R^</code> is defined in [1] as:
Let m be the number of distinct chains
Let n be the length of each chain</p>
<pre><code>\mu_j = mean for chain j
\mu = mean over all the chains
X_{ij} = Value in posterior for sample i in chain j

% Variance between sequences
B = [ n/(m-1) ] * \sum^m_{j=1} ( \mu_j - \mu )^2

% Variance within sequences
W = [ \sum^m_{j=1} [ \sum^n_{i=1} ( X_{ij} - \mu_j )^2 ] ] / m

\hat{R} = \sqrt{ [ (n-1)*W/n + B/n ] / W }
</code></pre>
<p>It is assumed that these chains were run with different initial conditions
and that the data is the same. An error will be thrown if the total number of
burn-in or total samples are different.</p>
<p>The values range from [1, inf], where 1 is completely mixed.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>chains</code></strong> :&ensp;<code>list(pylab.inference.BaseMCMC),list(str)</code></dt>
<dd>An iterable object of pl.inference.BaseMCMC objects. If it is a string then
it is the saved location of the chain object.</dd>
<dt><strong><code>vanme</code></strong> :&ensp;<code>str, int</code></dt>
<dd>This is the index of the variable we want to calculate <code>R^</code> from. This can
either be the name (str) or the graph ID (int) to identify it.</dd>
<dt><strong><code>idx</code></strong> :&ensp;<code>int, tuple(int)</code></dt>
<dd>This is the index of the item you're looking at. This is only necessary if
this variable is a vector. If it is a scalar then this is ignored.</dd>
<dt><strong><code>returnBW</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, returns B (between sequence variance) and W (within sequence variance)
as well as the :math:<code>\hat{R}</code> metric. Returns it as a dictionary</dd>
</dl>
<p>From the R documentation (<a href="https://www.rdocumentation.org/packages/asbio/versions/1.6-5/topics/R.hat">https://www.rdocumentation.org/packages/asbio/versions/1.6-5/topics/R.hat</a>):
Gelman et al. (2003, pg. 296) provides insufficient details to reproduce this function.
To get the real function see Gelman and Rubin (1992). The authors list one other change
in their Statlab version of this function at <a href="http://lib.stat.cmu.edu/S/itsim.">http://lib.stat.cmu.edu/S/itsim.</a> They
recommend multiplying sqrt(postvar/W) by sqrt((df + 3)/t(df + 1)). The original code
and this function can produce estimates below 1.</p>
<h2 id="references">References</h2>
<ul>
<li>[1] Gelman, A. and D. B. Rubin (1992) Inference from iterative simulation using multiple
sequences (with discussion). Statistical Science, 7:457-511.</li>
<li>[2] Brooks and Gelman (1998). Journal of Computational and Graphical Statistics, 7(4)434-455.</li>
<li>[3] A. Gelman, H. S. Stern, J. B. Carlin, D. B. Dunson, A. Vehtari, and D. B. Rubin, Bayesian Data
Analysis Third Edition. Chapman and Hall/CRC, 2013.</li>
<li>[4] <a href="https://stats.stackexchange.com/questions/348984/stan-hatr-versus-gelman-rubin-hatr-definition">https://stats.stackexchange.com/questions/348984/stan-hatr-versus-gelman-rubin-hatr-definition</a></li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def r_hat(chains: Iterator[BaseMCMC], vname: str, start: int, end: int, idx: Union[int, slice]=None, 
    returnBW: bool=False) -&gt; Union[float, Dict[str, float]]:
    &#39;&#39;&#39;Calculate the measure `R^` for the variable called `vname` at the index `idx`.
    If `idx` is None then we assume that the variable is scalar.

    Definition
    ----------
    `R^` is defined in [1] as:
        Let m be the number of distinct chains
        Let n be the length of each chain

        \mu_j = mean for chain j
        \mu = mean over all the chains
        X_{ij} = Value in posterior for sample i in chain j

        % Variance between sequences
        B = [ n/(m-1) ] * \sum^m_{j=1} ( \mu_j - \mu )^2

        % Variance within sequences
        W = [ \sum^m_{j=1} [ \sum^n_{i=1} ( X_{ij} - \mu_j )^2 ] ] / m

        \hat{R} = \sqrt{ [ (n-1)*W/n + B/n ] / W }

    It is assumed that these chains were run with different initial conditions
    and that the data is the same. An error will be thrown if the total number of
    burn-in or total samples are different.

    The values range from [1, inf], where 1 is completely mixed.

    Parameters
    ----------
    chains : list(pylab.inference.BaseMCMC),list(str)
        An iterable object of pl.inference.BaseMCMC objects. If it is a string then
        it is the saved location of the chain object.
    vanme : str, int
        This is the index of the variable we want to calculate `R^` from. This can
        either be the name (str) or the graph ID (int) to identify it.
    idx : int, tuple(int)
        This is the index of the item you&#39;re looking at. This is only necessary if
        this variable is a vector. If it is a scalar then this is ignored.
    returnBW : bool
        If True, returns B (between sequence variance) and W (within sequence variance)
        as well as the :math:`\hat{R}` metric. Returns it as a dictionary

    From the R documentation (https://www.rdocumentation.org/packages/asbio/versions/1.6-5/topics/R.hat):
    Gelman et al. (2003, pg. 296) provides insufficient details to reproduce this function.
    To get the real function see Gelman and Rubin (1992). The authors list one other change
    in their Statlab version of this function at http://lib.stat.cmu.edu/S/itsim. They
    recommend multiplying sqrt(postvar/W) by sqrt((df + 3)/t(df + 1)). The original code
    and this function can produce estimates below 1.

    References
    ----------
    - [1] Gelman, A. and D. B. Rubin (1992) Inference from iterative simulation using multiple
        sequences (with discussion). Statistical Science, 7:457-511.
    - [2] Brooks and Gelman (1998). Journal of Computational and Graphical Statistics, 7(4)434-455.
    - [3] A. Gelman, H. S. Stern, J. B. Carlin, D. B. Dunson, A. Vehtari, and D. B. Rubin, Bayesian Data
        Analysis Third Edition. Chapman and Hall/CRC, 2013.
    - [4] https://stats.stackexchange.com/questions/348984/stan-hatr-versus-gelman-rubin-hatr-definition
    &#39;&#39;&#39;
    # Check that all of the chains are consistent with each other
    if not util.isarray(chains):
        raise TypeError(&#39;`chains` ({}) must be array_like&#39;.format(type(chains)))
    if len(chains) &lt;= 1:
        raise ValueError(&#39;There must be at least 2 items in chains. There are {}&#39;.format(
            len(chains)))
    if not util.isint(start):
        raise TypeError(&#39;`start` ({}) must be an int&#39;.format(start))
    if not util.isint(end):
        raise TypeError(&#39;`end` ({}) must be an int&#39;.format(end))
    for i in range(len(chains)):
        chain = chains[i]
        if util.isstr(chain):
            chains[i] = BaseMCMC.load(chains[i])
            chain = chains[i]
        if not isMCMC(chain):
            raise TypeError(&#39;`chain` ({}) must be a pylab.inference.BaseMCMC object&#39;.format(
                type(chain)))

    # Get the variables from each of the chains and check that they are the proper shapes
    if not (util.isstr(vname) or util.isint(vname)):
        raise TypeError(&#39;`vname` ({}) must be an int or a str&#39;.format(type(vname)))
    traces = [chain.graph[vname].get_trace_from_disk(section=&#39;entire&#39;) for chain in chains]
    traces = [trace[start:end] for trace in traces]
    if idx is not None:
        traces = [trace[:, idx] for trace in traces]

    traces = np.nan_to_num(traces, nan=0.0)

    # Calculate r_hat
    m = traces.shape[0]  # no. of sequences
    n = traces.shape[1]  # length of each chain after burn-in
    try:
        num_var = traces.shape[2]  # Number of indexed variables being sampled
    except:
        # This fails when the variable is a scalar, reshape it so that it 
        # has a final dimension of 1
        traces = traces.reshape(n,m,1)
        num_var = 1
    s_hat_i = np.var(traces, axis=1, ddof=1)
    x_bar_i = np.mean(traces, axis=1)
    x_bar = np.mean(x_bar_i, axis=0)

    # W: Mean of sample variances
    W = np.mean(s_hat_i, axis=0)

    # B: Sample variance of means (times n)
    B = n * np.var(x_bar_i, axis=0, ddof=1)

    # Sigma-hat and V-hat:
    sigma_hat = ((n - 1) / n) * W + (1 / n) * B
    v_hat = sigma_hat + B * (1 / (n * m))

    var_v_hat = (
            (((n - 1) / n) ** 2) * (1 / m) * np.var(s_hat_i, axis=0)
            + (((m + 1) / (m * n)) ** 2) * (2 / (m - 1)) * np.power(B, 2)
            + 2 * (m + 1) * (n - 1) * (1 / m ** 2) * (1 / n) * np.array([
                np.cov(s_hat_i[:, k], np.power(x_bar_i[:, k], 2))[0, 1]
                - 2 * x_bar[k] * np.cov(s_hat_i[:, k], x_bar_i[:, k])[0, 1]
                for k in range(num_var)
            ])
    )

    # degrees of freedom
    dof = 2 * np.power(v_hat, 2) * var_v_hat

    # R-hat (Empirical conditional variance, divided by W)
    rhat = np.sqrt(
        v_hat * (dof+3) * np.reciprocal(W * (dof + 1))
    )

    if returnBW:
        return {&#39;B&#39;: B, &#39;W&#39;: W, &#39;rhat&#39;: rhat}
    else:
        return rhat</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="mdsine2.pylab.inference.BaseMCMC"><code class="flex name class">
<span>class <span class="ident">BaseMCMC</span></span>
<span>(</span><span>burnin: int, n_samples: int, *args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Base MCMC over a graph. This only runs 1 chain.</p>
<h2 id="typical-use">Typical Use</h2>
<pre><code>&gt;&gt;&gt; # You first initialize the object with the graph and parameters you want
&gt;&gt;&gt; inf = BaseMCMC(burnin=1000, n_samples=2000, graph=G)
&gt;&gt;&gt; # Then you set the inference order
&gt;&gt;&gt; inf.set_inference_order(['a', 'b', 'c'])
&gt;&gt;&gt; # Then set some diagnostic variables (Optional)
&gt;&gt;&gt; inf.set_diagnostic_variables(['e'])
&gt;&gt;&gt; # Then we can run the inference
&gt;&gt;&gt; inf.run(log_every=5, checkpoint=100, tracer_filename='./output/tracer.hdf5')
</code></pre>
<h2 id="inference-order">Inference Order</h2>
<p>This datastructure will perform the inference specified in the inference
order, which is set using the <code>set_inference_order</code> method. Each element
in <code>inf_order</code> must be an ID in <code>graph</code> that implements the functions: <code>update</code>
(how we sample the variable during inference), <code>set_trace</code> (how the
tracing gets set up), <code>add_trace</code> (how the current value gets added to the
trace (this is called immediately after <code>update</code>)), and <code>initialize</code> (
how the values get initialized before inference).</p>
<p>If your posterior class directly inherits a class that is a subclass of
<code>pylab.variables.Variable</code> then the functions <code>set_trace</code> and
<code>add_trace</code> are already implemented for you.</p>
<p>Sometimes we want to randomize the order that we update variables. For
example, if I am updating the growth, self_interactions, and interactions,
I may want to randomize the order that I update them so that there is no
unintentional bias during inference. In that case I can make a new object
called <code>gLVParams</code> where the <code>update</code> function randomizes the order that
growth.update(), self_interactions.update(), and interactions.update()
functions are called. </p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>burnin</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of initial samples to throw away</dd>
<dt><strong><code>n_samples</code></strong> :&ensp;<code>int</code></dt>
<dd>Total number of samples of the posterior
Number of posterior samples = n_samples-burnin</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class BaseMCMC(BaseModel):
    &#39;&#39;&#39;Base MCMC over a graph. This only runs 1 chain.

    Typical use
    -----------
    ```
    &gt;&gt;&gt; # You first initialize the object with the graph and parameters you want
    &gt;&gt;&gt; inf = BaseMCMC(burnin=1000, n_samples=2000, graph=G)
    &gt;&gt;&gt; # Then you set the inference order
    &gt;&gt;&gt; inf.set_inference_order([&#39;a&#39;, &#39;b&#39;, &#39;c&#39;])
    &gt;&gt;&gt; # Then set some diagnostic variables (Optional)
    &gt;&gt;&gt; inf.set_diagnostic_variables([&#39;e&#39;])
    &gt;&gt;&gt; # Then we can run the inference
    &gt;&gt;&gt; inf.run(log_every=5, checkpoint=100, tracer_filename=&#39;./output/tracer.hdf5&#39;)
    ```

    Inference order
    ---------------
    This datastructure will perform the inference specified in the inference 
    order, which is set using the `set_inference_order` method. Each element
    in `inf_order` must be an ID in `graph` that implements the functions: `update` 
    (how we sample the variable during inference), `set_trace` (how the 
    tracing gets set up), `add_trace` (how the current value gets added to the 
    trace (this is called immediately after `update`)), and `initialize` (
    how the values get initialized before inference).

    If your posterior class directly inherits a class that is a subclass of 
    `pylab.variables.Variable` then the functions `set_trace` and 
    `add_trace` are already implemented for you.

    Sometimes we want to randomize the order that we update variables. For
    example, if I am updating the growth, self_interactions, and interactions,
    I may want to randomize the order that I update them so that there is no
    unintentional bias during inference. In that case I can make a new object 
    called `gLVParams` where the `update` function randomizes the order that
    growth.update(), self_interactions.update(), and interactions.update() 
    functions are called. 

    Parameters
    ----------
    burnin : int
        Number of initial samples to throw away
    n_samples : int
        Total number of samples of the posterior
        Number of posterior samples = n_samples-burnin
    &#39;&#39;&#39;

    def __init__(self, burnin: int, n_samples: int, * args, **kwargs):
        BaseModel.__init__(self, *args, **kwargs)
        if not util.isint(burnin):
            raise TypeError(&#39;`burnin` ({}) must be an int&#39;.format(type(burnin)))
        if not util.isint(n_samples):
            raise TypeError(&#39;`n_samples` ({}) must be an int&#39;.format(type(n_samples)))
        if n_samples &lt; burnin:
            raise TypeError(&#39;The total number of sample (n_samples) must be&#39;\
                &#39;larger than the burn in (burnin)&#39;)

        self.burnin = burnin
        self.n_samples = n_samples
        self.sample_iter = 0 # This holds the current sample we are at
        self.inf_order = None # This is a list of the inference order
        self.diagnostic_variables = None # DEPRECIATED
        self.tracer = None # This is a pointer to the pylab.inference.Tracer object

        # This is the sample iteration to start on. Unless we are loading from a 
        # saved MCMC object, then this will be 0
        self.start_step = None 

        # Functions to save intermediately, this is used for semisynthetic
        self._intermediate_func = None # callable
        self._intermediate_t = None # float
        self._intermediate_kwargs = None # dict

    @classmethod
    def load(cls, filename: str):
        &#39;&#39;&#39;Override base Saveable to redo the filename of the
        tracer object if it has one
        
        Paramters
        ---------
        filename : str
            This is the location of the file to unpickle
        &#39;&#39;&#39;
        with open(str(filename), &#39;rb&#39;) as handle:
            b = pickle.load(handle)
        
        # redo the filename to the new path if it has a save location
        if hasattr(b, &#39;_save_loc&#39;):
            filename = os.path.abspath(filename)
            b._save_loc = filename

            # Redo the filename of the tracer object if necessart
            if b.tracer is not None:
                if b.tracer.filename is not None:
                    _, tracer_fname = os.path.split(b.tracer.filename)
                    currpath, _ = os.path.split(b._save_loc)

                    new_loc = os.path.join(currpath, tracer_fname)

                    if os.path.isfile(new_loc):
                        b.tracer.filename = new_loc
                    else:
                        raise ValueError(&#39;Looking for tracer hdf5 object in {}, could not find it &#39; \
                            &#39;in the local path even though inference says it contains the object. On &#39; \
                            &#39;file it shows the location {}&#39;.format(
                                new_loc, b.tracer.filename))

        return b

    @property
    def ckpt(self) -&gt; int:
        &#39;&#39;&#39;For backwards compatibility
        &#39;&#39;&#39;
        return self.checkpoint

    @ckpt.setter
    def ckpt(self, a: int):
        &#39;&#39;&#39;Set ckpt
        &#39;&#39;&#39;
        self.ckpt = a

    def names(self) -&gt; Iterator[str]:
        &#39;&#39;&#39;Get the names of the nodes in the inference object

        Returns
        -------
        list(str)
        &#39;&#39;&#39;
        return list(self.graph.name2id.keys())

    def ids(self) -&gt; Iterator[int]:
        &#39;&#39;&#39;Get the IDs of the nodes in the inference object

        Returns
        -------
        list(int)
        &#39;&#39;&#39;
        return list(self.graph.nodes.keys())

    def set_inference_order(self, order: Iterator[Union[str, int]]):
        &#39;&#39;&#39;`order` is an array of nodes we want to sample in the order
        that we want. Check that they are all in the graph. Can put in the 
        ID or the name

        Parameters
        ----------
        order : array_like(int or str)
            Order to do the inference. This must be the IDs of the nodes and can
            be an ID (int) or the name (str)
        &#39;&#39;&#39;
        if not util.isarray(order):
            raise TypeError(&#39;order ({}) must be an array&#39;.format(order))

        ret = []
        for nid in order:
            if nid not in self.graph:
                raise ValueError(&#39;Node ({}) not found in graph ({})&#39;.format(
                    nid, self.names()))
            node = self.graph[nid]

            for attr in REQUIRED_ATTRS:
                if hasattr(node, attr):
                    if not callable(getattr(node, attr)):
                        raise UndefinedError(&#39;node ({}) must have `{}` be callable&#39;.format(
                            node.name, attr))
                else:
                    raise UndefinedError(&#39;node ({}) must have the function `{}`&#39;.format(
                        node.name, attr))
            ret.append(node.id)
        self.inf_order = ret

    def is_in_inference_order(self, var: Union[int, str, Variable]) -&gt; bool:
        &#39;&#39;&#39;Checks if the variable is in the inference order

        Parameters
        ----------
        var : int, str, pylab.variables.Variable
            Identifier for the variable
            If it is an int - it is assumed this is the id of the variable
            If is is a str - it is assumed this is the name of the variable

        Returns
        -------
        bool
        &#39;&#39;&#39;
        if not isnode(var):
            var = self.graph[var].id
        else:
            var = var.id
        return var in self.inf_order

    def set_diagnostic_variables(self, vars):
        &#39;&#39;&#39;DEPRECIATED
        
        A list of variables that you want to track over time. These variables
        do not necessarily have to be variables that we are tracing, rather they
        can be any variable that changes over time in the inference. However,
        the variables that we are tracing need to be a subclass of
        pylab.variables.Variable

        We manually override the tracing for each of the variables.

        Parameters
        ----------
        vars : array_like, set, 1-dim
            - A list of the variables you want to track (the actual object)
            - The elements in the list should be of type pylab.variables.Variable
        &#39;&#39;&#39;
        # wrap vars as an iterable if it is not
        if not hasattr(vars, &#39;__iter__&#39;):
            vars = [vars]

        if len(vars) == 0:
            raise ValueError(&#39;No values in `vars`&#39;)

        # Check parameters passed in
        for var in vars:
            if not isVariable(var):
                raise InheritanceError(&#39;Each variable passed in should be of a subclass of&#39; \
                    &#39; `pylab.variables.Variable`. This is of type `{}`&#39;.format(
                        var.__class__.__name__))

        # Add variables to dictionary
        self.diagnostic_variables = {}
        for var in vars:
            if var.name in self.diagnostic_variables:
                raise ValueError(&#39;Two different diagnostic variables cannot have &#39; \
                    &#39;the same name.&#39;)
            self.diagnostic_variables[var.name] = var

    def set_tracer(self, filename: str, checkpoint: int=100):
        &#39;&#39;&#39;Sets up the tracing object

        Parameters
        ----------
        filename : str, None
            File location to save the hdf5 object
        checkpoint : int, None
            Saves the current progress of the inference chain every `checkpoint` iterations
            If None then there is no intermediate checkpointing
        &#39;&#39;&#39;
        if checkpoint is None:
            checkpoint = self.n_samples
        self.tracer_filename = filename
        self.checkpoint = checkpoint
        self.tracer = Tracer(mcmc=self, filename=filename)

    def set_intermediate_validation(self, t: float, func: Callable, kwargs: Dict[str, Any]=None):
        &#39;&#39;&#39;Every `t`, run the function `func` during validation in a new process during inference.

        Parameters
        ----------
        t : int
            How often to run in number of seconds
        func : callable
            This is the function the intermediate must call. It assumes the
            only parameters are:
                chain : this is the chain object (self)
                burnin : number of burnin
                n_samples : number of total samples
                sample_iter : current iteration number
        kwargs : dict
            Extra arguments
        &#39;&#39;&#39;
        if not util.isint(t):
            raise TypeError(&#39;`t` ({}) must be an int&#39;.format(type(t)))
        if t &lt; 0:
            raise ValueError(&#39;`t` ({}) must be &gt; 0&#39;.format(t))
        if kwargs is None:
            kwargs = {}
        for k,v in kwargs.items():
            if not util.isstr(k):
                raise ValueError(&#39;Keys in kwargs ({}) must be strings&#39;.format(type(k)))
        if not callable(func):
            raise TypeError(&#39;`func` ({}) must be callable&#39;.format(type(str)))

        # Check if the function has the right arguments
        kwarg_keys = list(kwargs.keys())
        valid_args = OrderedSet([&#39;burnin&#39;, &#39;n_samples&#39;, &#39;chain&#39;, &#39;sample_iter&#39;] + kwarg_keys)
        args = inspect.getargspec(func).args
        for arg in args:
            if arg not in valid_args:
                raise ValueError(&#39;Function `{}` does not have the correct arguments. &#39; \
                    &#39;It has the argument `{}` when it should only have the arguments {}.&#39;.format(
                        func.__name__, arg, list(valid_args)))
        for arg in valid_args:
            if arg not in args:
                raise ValueError(&#39;Function `{}` does not have the correct arguments. &#39; \
                    &#39;It is excluding the argument `{}`, which must be included&#39;.format(
                        func.__name__, arg))

        self._intermediate_t = t
        self._intermediate_func = func
        self._intermediate_kwargs = kwargs

    def run(self, log_every: int=1) -&gt; &#34;BaseMCMC&#34;:
        &#39;&#39;&#39;Run the inference.

        Parameters
        ----------
        log_every : int, None
            Logs the values of the variables you are learning every `log_every` iterations.
            If the logger is set to DEBUG then override to log every iteration.
        
        Returns
        -------
        pylab.inference.BaseMCMC
            Output from the inference, self
        &#39;&#39;&#39;
        set_seed(self.graph.seed)
        if self.start_step is None:
            self.start_step = 0
        try:
            if self.inf_order is None:
                raise UndefinedError(&#39;Cannot run mcmc until you have set the inference order.&#39; \
                    &#39; Set with the function `self.set_inference_order`.&#39;)
            if self.tracer is None:
                logging.warning(&#39;No tracer set - assume you do not want to write to disk&#39;)
                self.tracer_filename = None
                self.tracer = Tracer(mcmc=self, filename=None)
                self.checkpoint = None

                logging.info(&#39;Setting the trace of learned parameters&#39;)
                logging.info(&#39;#######################################&#39;)
                for nid in self.inf_order:
                    logging.info(&#39;Setting the trace of {}&#39;.format(self.graph[nid].name))
                    self.graph[nid].set_trace()
                logging.info(&#39;Setting the trace for diagnostic variables&#39;)
                logging.info(&#39;##########################################&#39;)
                if self.diagnostic_variables is not None:
                    for nid in self.diagnostic_variables:
                        logging.info(&#39;Setting the trace of {}&#39;.format(self.graph[nid].name))
                        self.graph[nid].set_trace()

            total_time = time.time()
            if log_every is None:
                log_every = DEFAULT_LOG_EVERY

            start = time.time()
            intermediate_time = time.time()
            for i in range(self.start_step, self.n_samples):
                self.sample_iter = i

                # Check if we need to run the intermediate script
                try:
                    if self._intermediate_t is not None:
                        if time.time() - intermediate_time &gt; self._intermediate_t:
                            logging.info(&#39;Running intermediate script {}&#39;.format(
                                self._intermediate_func.__name__))
                            kwargs = {
                                &#39;chain&#39;: self, &#39;burnin&#39;: self.burnin, 
                                &#39;n_samples&#39;: self.n_samples, &#39;sample_iter&#39;: self.sample_iter}
                            for k,v in self._intermediate_kwargs.items():
                                kwargs[k] = v
                            try:
                                self._intermediate_func(**kwargs)
                            except:
                                raise ValueError(&#39;failed in intermediate function&#39;)
                            intermediate_time = time.time() 
                except AttributeError:
                    logging.info(&#39;Pre Pylab 3.0.0 implementation. Ignore&#39;)
                except:
                    logging.critical(&#39;unknown error&#39;)
                    raise

                # Log where necessary
                if i % log_every == 0:
                    logging.info(&#39;\n\nInference iteration {}/{}, time: {}&#39;.format(
                        i, self.n_samples, time.time() - start))
                    start = time.time()
                    for id in self.inf_order:
                        if type(id) == list:
                            for id_ in id:
                                logging.info(&#39;{}: {}&#39;.format(self.graph.nodes[id_].name,
                                    str(self.graph.nodes[id_])))
                        else:
                            logging.info(&#39;{}: {}&#39;.format(self.graph.nodes[id].name,
                                str(self.graph.nodes[id])))

                # Sample posterior in the order indicated and add the trace
                for _id in self.inf_order:
                    try:
                        self.graph.nodes[_id].update()
                        self.graph.nodes[_id].add_trace()
                    except:
                        logging.critical(&#39;Crashed in `{}`&#39;.format(self.graph[_id].name))
                        # self.graph.tracer.finish_tracing()
                        raise

                # Save diagnostic variables where necessary
                if self.diagnostic_variables is not None:
                    for name in self.diagnostic_variables:
                        if self.diagnostic_variables[name].sample_iter == self.sample_iter:
                            self.diagnostic_variables[name].add_trace()

                # If we just saved the traces to disk, save the MCMC object
                if self.sample_iter % self.checkpoint == 0 and self.sample_iter &gt; 0:
                    try:
                        self.save()
                    except:
                        logging.critical(&#39;If you want to checkpoint, you must set the save location &#39; \
                            &#39;of tracer, graph, and mcmc using the function self.set_save_location()&#39;)
                        print(self._save_loc)
                        raise

            # Finish the tracing
            self.graph.tracer.finish_tracing()
            self.ran = True
            logging.info(&#39;Inference total time: {}/Gibb step&#39;.format(
                (time.time() - total_time)/self.n_samples))
            
            # Remove the local traces
            if self.tracer is not None:
                logging.info(&#39;remove local traces&#39;)
                for node in self.graph:
                    if isVariable(node):
                        node.remove_local_trace()
            self.save()

            return self
        except:
            for a in self.graph._persistent_pntr:
                a.kill()
            raise

    def continue_inference(self, gibb_step_start: int):
        &#39;&#39;&#39;Resume inference at the gibb step number `gibb_step_start`. Note that
        we do not resume the random seed where it was at that point of the inference.

        Parameters
        ----------
        gibb_step_start : int
            Gibb step to start
        &#39;&#39;&#39;
        self.tracer.continue_inference(gibb_step_start=gibb_step_start)
        self.start_step = gibb_step_start</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="mdsine2.pylab.inference.BaseModel" href="#mdsine2.pylab.inference.BaseModel">BaseModel</a></li>
<li><a title="mdsine2.pylab.base.Saveable" href="base.html#mdsine2.pylab.base.Saveable">Saveable</a></li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="mdsine2.pylab.inference.BaseMCMC.load"><code class="name flex">
<span>def <span class="ident">load</span></span>(<span>filename: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Override base Saveable to redo the filename of the
tracer object if it has one</p>
<h2 id="paramters">Paramters</h2>
<p>filename : str
This is the location of the file to unpickle</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def load(cls, filename: str):
    &#39;&#39;&#39;Override base Saveable to redo the filename of the
    tracer object if it has one
    
    Paramters
    ---------
    filename : str
        This is the location of the file to unpickle
    &#39;&#39;&#39;
    with open(str(filename), &#39;rb&#39;) as handle:
        b = pickle.load(handle)
    
    # redo the filename to the new path if it has a save location
    if hasattr(b, &#39;_save_loc&#39;):
        filename = os.path.abspath(filename)
        b._save_loc = filename

        # Redo the filename of the tracer object if necessart
        if b.tracer is not None:
            if b.tracer.filename is not None:
                _, tracer_fname = os.path.split(b.tracer.filename)
                currpath, _ = os.path.split(b._save_loc)

                new_loc = os.path.join(currpath, tracer_fname)

                if os.path.isfile(new_loc):
                    b.tracer.filename = new_loc
                else:
                    raise ValueError(&#39;Looking for tracer hdf5 object in {}, could not find it &#39; \
                        &#39;in the local path even though inference says it contains the object. On &#39; \
                        &#39;file it shows the location {}&#39;.format(
                            new_loc, b.tracer.filename))

    return b</code></pre>
</details>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="mdsine2.pylab.inference.BaseMCMC.ckpt"><code class="name">var <span class="ident">ckpt</span> : int</code></dt>
<dd>
<div class="desc"><p>For backwards compatibility</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def ckpt(self) -&gt; int:
    &#39;&#39;&#39;For backwards compatibility
    &#39;&#39;&#39;
    return self.checkpoint</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="mdsine2.pylab.inference.BaseMCMC.continue_inference"><code class="name flex">
<span>def <span class="ident">continue_inference</span></span>(<span>self, gibb_step_start: int)</span>
</code></dt>
<dd>
<div class="desc"><p>Resume inference at the gibb step number <code>gibb_step_start</code>. Note that
we do not resume the random seed where it was at that point of the inference.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>gibb_step_start</code></strong> :&ensp;<code>int</code></dt>
<dd>Gibb step to start</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def continue_inference(self, gibb_step_start: int):
    &#39;&#39;&#39;Resume inference at the gibb step number `gibb_step_start`. Note that
    we do not resume the random seed where it was at that point of the inference.

    Parameters
    ----------
    gibb_step_start : int
        Gibb step to start
    &#39;&#39;&#39;
    self.tracer.continue_inference(gibb_step_start=gibb_step_start)
    self.start_step = gibb_step_start</code></pre>
</details>
</dd>
<dt id="mdsine2.pylab.inference.BaseMCMC.ids"><code class="name flex">
<span>def <span class="ident">ids</span></span>(<span>self) ‑> Iterator[int]</span>
</code></dt>
<dd>
<div class="desc"><p>Get the IDs of the nodes in the inference object</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>list(int)</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ids(self) -&gt; Iterator[int]:
    &#39;&#39;&#39;Get the IDs of the nodes in the inference object

    Returns
    -------
    list(int)
    &#39;&#39;&#39;
    return list(self.graph.nodes.keys())</code></pre>
</details>
</dd>
<dt id="mdsine2.pylab.inference.BaseMCMC.is_in_inference_order"><code class="name flex">
<span>def <span class="ident">is_in_inference_order</span></span>(<span>self, var: Union[int, str, <a title="mdsine2.pylab.variables.Variable" href="variables.html#mdsine2.pylab.variables.Variable">Variable</a>]) ‑> bool</span>
</code></dt>
<dd>
<div class="desc"><p>Checks if the variable is in the inference order</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>var</code></strong> :&ensp;<code>int, str, pylab.variables.Variable</code></dt>
<dd>Identifier for the variable
If it is an int - it is assumed this is the id of the variable
If is is a str - it is assumed this is the name of the variable</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>bool</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_in_inference_order(self, var: Union[int, str, Variable]) -&gt; bool:
    &#39;&#39;&#39;Checks if the variable is in the inference order

    Parameters
    ----------
    var : int, str, pylab.variables.Variable
        Identifier for the variable
        If it is an int - it is assumed this is the id of the variable
        If is is a str - it is assumed this is the name of the variable

    Returns
    -------
    bool
    &#39;&#39;&#39;
    if not isnode(var):
        var = self.graph[var].id
    else:
        var = var.id
    return var in self.inf_order</code></pre>
</details>
</dd>
<dt id="mdsine2.pylab.inference.BaseMCMC.names"><code class="name flex">
<span>def <span class="ident">names</span></span>(<span>self) ‑> Iterator[str]</span>
</code></dt>
<dd>
<div class="desc"><p>Get the names of the nodes in the inference object</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>list(str)</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def names(self) -&gt; Iterator[str]:
    &#39;&#39;&#39;Get the names of the nodes in the inference object

    Returns
    -------
    list(str)
    &#39;&#39;&#39;
    return list(self.graph.name2id.keys())</code></pre>
</details>
</dd>
<dt id="mdsine2.pylab.inference.BaseMCMC.run"><code class="name flex">
<span>def <span class="ident">run</span></span>(<span>self, log_every: int = 1) ‑> <a title="mdsine2.pylab.inference.BaseMCMC" href="#mdsine2.pylab.inference.BaseMCMC">BaseMCMC</a></span>
</code></dt>
<dd>
<div class="desc"><p>Run the inference.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>log_every</code></strong> :&ensp;<code>int, None</code></dt>
<dd>Logs the values of the variables you are learning every <code>log_every</code> iterations.
If the logger is set to DEBUG then override to log every iteration.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pylab.inference.BaseMCMC</code></dt>
<dd>Output from the inference, self</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run(self, log_every: int=1) -&gt; &#34;BaseMCMC&#34;:
    &#39;&#39;&#39;Run the inference.

    Parameters
    ----------
    log_every : int, None
        Logs the values of the variables you are learning every `log_every` iterations.
        If the logger is set to DEBUG then override to log every iteration.
    
    Returns
    -------
    pylab.inference.BaseMCMC
        Output from the inference, self
    &#39;&#39;&#39;
    set_seed(self.graph.seed)
    if self.start_step is None:
        self.start_step = 0
    try:
        if self.inf_order is None:
            raise UndefinedError(&#39;Cannot run mcmc until you have set the inference order.&#39; \
                &#39; Set with the function `self.set_inference_order`.&#39;)
        if self.tracer is None:
            logging.warning(&#39;No tracer set - assume you do not want to write to disk&#39;)
            self.tracer_filename = None
            self.tracer = Tracer(mcmc=self, filename=None)
            self.checkpoint = None

            logging.info(&#39;Setting the trace of learned parameters&#39;)
            logging.info(&#39;#######################################&#39;)
            for nid in self.inf_order:
                logging.info(&#39;Setting the trace of {}&#39;.format(self.graph[nid].name))
                self.graph[nid].set_trace()
            logging.info(&#39;Setting the trace for diagnostic variables&#39;)
            logging.info(&#39;##########################################&#39;)
            if self.diagnostic_variables is not None:
                for nid in self.diagnostic_variables:
                    logging.info(&#39;Setting the trace of {}&#39;.format(self.graph[nid].name))
                    self.graph[nid].set_trace()

        total_time = time.time()
        if log_every is None:
            log_every = DEFAULT_LOG_EVERY

        start = time.time()
        intermediate_time = time.time()
        for i in range(self.start_step, self.n_samples):
            self.sample_iter = i

            # Check if we need to run the intermediate script
            try:
                if self._intermediate_t is not None:
                    if time.time() - intermediate_time &gt; self._intermediate_t:
                        logging.info(&#39;Running intermediate script {}&#39;.format(
                            self._intermediate_func.__name__))
                        kwargs = {
                            &#39;chain&#39;: self, &#39;burnin&#39;: self.burnin, 
                            &#39;n_samples&#39;: self.n_samples, &#39;sample_iter&#39;: self.sample_iter}
                        for k,v in self._intermediate_kwargs.items():
                            kwargs[k] = v
                        try:
                            self._intermediate_func(**kwargs)
                        except:
                            raise ValueError(&#39;failed in intermediate function&#39;)
                        intermediate_time = time.time() 
            except AttributeError:
                logging.info(&#39;Pre Pylab 3.0.0 implementation. Ignore&#39;)
            except:
                logging.critical(&#39;unknown error&#39;)
                raise

            # Log where necessary
            if i % log_every == 0:
                logging.info(&#39;\n\nInference iteration {}/{}, time: {}&#39;.format(
                    i, self.n_samples, time.time() - start))
                start = time.time()
                for id in self.inf_order:
                    if type(id) == list:
                        for id_ in id:
                            logging.info(&#39;{}: {}&#39;.format(self.graph.nodes[id_].name,
                                str(self.graph.nodes[id_])))
                    else:
                        logging.info(&#39;{}: {}&#39;.format(self.graph.nodes[id].name,
                            str(self.graph.nodes[id])))

            # Sample posterior in the order indicated and add the trace
            for _id in self.inf_order:
                try:
                    self.graph.nodes[_id].update()
                    self.graph.nodes[_id].add_trace()
                except:
                    logging.critical(&#39;Crashed in `{}`&#39;.format(self.graph[_id].name))
                    # self.graph.tracer.finish_tracing()
                    raise

            # Save diagnostic variables where necessary
            if self.diagnostic_variables is not None:
                for name in self.diagnostic_variables:
                    if self.diagnostic_variables[name].sample_iter == self.sample_iter:
                        self.diagnostic_variables[name].add_trace()

            # If we just saved the traces to disk, save the MCMC object
            if self.sample_iter % self.checkpoint == 0 and self.sample_iter &gt; 0:
                try:
                    self.save()
                except:
                    logging.critical(&#39;If you want to checkpoint, you must set the save location &#39; \
                        &#39;of tracer, graph, and mcmc using the function self.set_save_location()&#39;)
                    print(self._save_loc)
                    raise

        # Finish the tracing
        self.graph.tracer.finish_tracing()
        self.ran = True
        logging.info(&#39;Inference total time: {}/Gibb step&#39;.format(
            (time.time() - total_time)/self.n_samples))
        
        # Remove the local traces
        if self.tracer is not None:
            logging.info(&#39;remove local traces&#39;)
            for node in self.graph:
                if isVariable(node):
                    node.remove_local_trace()
        self.save()

        return self
    except:
        for a in self.graph._persistent_pntr:
            a.kill()
        raise</code></pre>
</details>
</dd>
<dt id="mdsine2.pylab.inference.BaseMCMC.set_diagnostic_variables"><code class="name flex">
<span>def <span class="ident">set_diagnostic_variables</span></span>(<span>self, vars)</span>
</code></dt>
<dd>
<div class="desc"><p>DEPRECIATED</p>
<p>A list of variables that you want to track over time. These variables
do not necessarily have to be variables that we are tracing, rather they
can be any variable that changes over time in the inference. However,
the variables that we are tracing need to be a subclass of
pylab.variables.Variable</p>
<p>We manually override the tracing for each of the variables.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>vars</code></strong> :&ensp;<code>array_like, set, 1-dim</code></dt>
<dd>
<ul>
<li>A list of the variables you want to track (the actual object)</li>
<li>The elements in the list should be of type pylab.variables.Variable</li>
</ul>
</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_diagnostic_variables(self, vars):
    &#39;&#39;&#39;DEPRECIATED
    
    A list of variables that you want to track over time. These variables
    do not necessarily have to be variables that we are tracing, rather they
    can be any variable that changes over time in the inference. However,
    the variables that we are tracing need to be a subclass of
    pylab.variables.Variable

    We manually override the tracing for each of the variables.

    Parameters
    ----------
    vars : array_like, set, 1-dim
        - A list of the variables you want to track (the actual object)
        - The elements in the list should be of type pylab.variables.Variable
    &#39;&#39;&#39;
    # wrap vars as an iterable if it is not
    if not hasattr(vars, &#39;__iter__&#39;):
        vars = [vars]

    if len(vars) == 0:
        raise ValueError(&#39;No values in `vars`&#39;)

    # Check parameters passed in
    for var in vars:
        if not isVariable(var):
            raise InheritanceError(&#39;Each variable passed in should be of a subclass of&#39; \
                &#39; `pylab.variables.Variable`. This is of type `{}`&#39;.format(
                    var.__class__.__name__))

    # Add variables to dictionary
    self.diagnostic_variables = {}
    for var in vars:
        if var.name in self.diagnostic_variables:
            raise ValueError(&#39;Two different diagnostic variables cannot have &#39; \
                &#39;the same name.&#39;)
        self.diagnostic_variables[var.name] = var</code></pre>
</details>
</dd>
<dt id="mdsine2.pylab.inference.BaseMCMC.set_inference_order"><code class="name flex">
<span>def <span class="ident">set_inference_order</span></span>(<span>self, order: Iterator[Union[str, int]])</span>
</code></dt>
<dd>
<div class="desc"><p><code>order</code> is an array of nodes we want to sample in the order
that we want. Check that they are all in the graph. Can put in the
ID or the name</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>order</code></strong> :&ensp;<code>array_like(int</code> or <code>str)</code></dt>
<dd>Order to do the inference. This must be the IDs of the nodes and can
be an ID (int) or the name (str)</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_inference_order(self, order: Iterator[Union[str, int]]):
    &#39;&#39;&#39;`order` is an array of nodes we want to sample in the order
    that we want. Check that they are all in the graph. Can put in the 
    ID or the name

    Parameters
    ----------
    order : array_like(int or str)
        Order to do the inference. This must be the IDs of the nodes and can
        be an ID (int) or the name (str)
    &#39;&#39;&#39;
    if not util.isarray(order):
        raise TypeError(&#39;order ({}) must be an array&#39;.format(order))

    ret = []
    for nid in order:
        if nid not in self.graph:
            raise ValueError(&#39;Node ({}) not found in graph ({})&#39;.format(
                nid, self.names()))
        node = self.graph[nid]

        for attr in REQUIRED_ATTRS:
            if hasattr(node, attr):
                if not callable(getattr(node, attr)):
                    raise UndefinedError(&#39;node ({}) must have `{}` be callable&#39;.format(
                        node.name, attr))
            else:
                raise UndefinedError(&#39;node ({}) must have the function `{}`&#39;.format(
                    node.name, attr))
        ret.append(node.id)
    self.inf_order = ret</code></pre>
</details>
</dd>
<dt id="mdsine2.pylab.inference.BaseMCMC.set_intermediate_validation"><code class="name flex">
<span>def <span class="ident">set_intermediate_validation</span></span>(<span>self, t: float, func: Callable, kwargs: Dict[str, Any] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Every <code>t</code>, run the function <code>func</code> during validation in a new process during inference.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>t</code></strong> :&ensp;<code>int</code></dt>
<dd>How often to run in number of seconds</dd>
<dt><strong><code>func</code></strong> :&ensp;<code>callable</code></dt>
<dd>This is the function the intermediate must call. It assumes the
only parameters are:
chain : this is the chain object (self)
burnin : number of burnin
n_samples : number of total samples
sample_iter : current iteration number</dd>
<dt><strong><code>kwargs</code></strong> :&ensp;<code>dict</code></dt>
<dd>Extra arguments</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_intermediate_validation(self, t: float, func: Callable, kwargs: Dict[str, Any]=None):
    &#39;&#39;&#39;Every `t`, run the function `func` during validation in a new process during inference.

    Parameters
    ----------
    t : int
        How often to run in number of seconds
    func : callable
        This is the function the intermediate must call. It assumes the
        only parameters are:
            chain : this is the chain object (self)
            burnin : number of burnin
            n_samples : number of total samples
            sample_iter : current iteration number
    kwargs : dict
        Extra arguments
    &#39;&#39;&#39;
    if not util.isint(t):
        raise TypeError(&#39;`t` ({}) must be an int&#39;.format(type(t)))
    if t &lt; 0:
        raise ValueError(&#39;`t` ({}) must be &gt; 0&#39;.format(t))
    if kwargs is None:
        kwargs = {}
    for k,v in kwargs.items():
        if not util.isstr(k):
            raise ValueError(&#39;Keys in kwargs ({}) must be strings&#39;.format(type(k)))
    if not callable(func):
        raise TypeError(&#39;`func` ({}) must be callable&#39;.format(type(str)))

    # Check if the function has the right arguments
    kwarg_keys = list(kwargs.keys())
    valid_args = OrderedSet([&#39;burnin&#39;, &#39;n_samples&#39;, &#39;chain&#39;, &#39;sample_iter&#39;] + kwarg_keys)
    args = inspect.getargspec(func).args
    for arg in args:
        if arg not in valid_args:
            raise ValueError(&#39;Function `{}` does not have the correct arguments. &#39; \
                &#39;It has the argument `{}` when it should only have the arguments {}.&#39;.format(
                    func.__name__, arg, list(valid_args)))
    for arg in valid_args:
        if arg not in args:
            raise ValueError(&#39;Function `{}` does not have the correct arguments. &#39; \
                &#39;It is excluding the argument `{}`, which must be included&#39;.format(
                    func.__name__, arg))

    self._intermediate_t = t
    self._intermediate_func = func
    self._intermediate_kwargs = kwargs</code></pre>
</details>
</dd>
<dt id="mdsine2.pylab.inference.BaseMCMC.set_tracer"><code class="name flex">
<span>def <span class="ident">set_tracer</span></span>(<span>self, filename: str, checkpoint: int = 100)</span>
</code></dt>
<dd>
<div class="desc"><p>Sets up the tracing object</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>filename</code></strong> :&ensp;<code>str, None</code></dt>
<dd>File location to save the hdf5 object</dd>
<dt><strong><code>checkpoint</code></strong> :&ensp;<code>int, None</code></dt>
<dd>Saves the current progress of the inference chain every <code>checkpoint</code> iterations
If None then there is no intermediate checkpointing</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_tracer(self, filename: str, checkpoint: int=100):
    &#39;&#39;&#39;Sets up the tracing object

    Parameters
    ----------
    filename : str, None
        File location to save the hdf5 object
    checkpoint : int, None
        Saves the current progress of the inference chain every `checkpoint` iterations
        If None then there is no intermediate checkpointing
    &#39;&#39;&#39;
    if checkpoint is None:
        checkpoint = self.n_samples
    self.tracer_filename = filename
    self.checkpoint = checkpoint
    self.tracer = Tracer(mcmc=self, filename=filename)</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="mdsine2.pylab.inference.BaseModel" href="#mdsine2.pylab.inference.BaseModel">BaseModel</a></b></code>:
<ul class="hlist">
<li><code><a title="mdsine2.pylab.inference.BaseModel.save" href="base.html#mdsine2.pylab.base.Saveable.save">save</a></code></li>
<li><code><a title="mdsine2.pylab.inference.BaseModel.set_save_location" href="base.html#mdsine2.pylab.base.Saveable.set_save_location">set_save_location</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="mdsine2.pylab.inference.BaseModel"><code class="flex name class">
<span>class <span class="ident">BaseModel</span></span>
<span>(</span><span>graph: <a title="mdsine2.pylab.graph.Graph" href="graph.html#mdsine2.pylab.graph.Graph">Graph</a> = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for a model</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>graph</code></strong> :&ensp;<code>pylab.graph.Graph, Optional</code></dt>
<dd>The graph we want to do the inference over
If nothing is provided, it grabs the default graph</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class BaseModel(Saveable):
    &#39;&#39;&#39;Base class for a model

    Parameters
    ----------
    graph : pylab.graph.Graph, Optional
        The graph we want to do the inference over
        If nothing is provided, it grabs the default graph
    &#39;&#39;&#39;
    def __init__(self, graph: Graph=None):
        if graph is None:
            graph = get_default_graph()
        if not isgraph(graph):
            raise TypeError(&#39;`graph` ({}) must be None or a pylab.graph.Graph object&#39; \
                &#39;&#39;.format(type(graph)))
        self.graph = graph
        self.ran = False
        self.graph.inference = self</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="mdsine2.pylab.base.Saveable" href="base.html#mdsine2.pylab.base.Saveable">Saveable</a></li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="mdsine2.pylab.inference.BaseMCMC" href="#mdsine2.pylab.inference.BaseMCMC">BaseMCMC</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="mdsine2.pylab.base.Saveable" href="base.html#mdsine2.pylab.base.Saveable">Saveable</a></b></code>:
<ul class="hlist">
<li><code><a title="mdsine2.pylab.base.Saveable.load" href="base.html#mdsine2.pylab.base.Saveable.load">load</a></code></li>
<li><code><a title="mdsine2.pylab.base.Saveable.save" href="base.html#mdsine2.pylab.base.Saveable.save">save</a></code></li>
<li><code><a title="mdsine2.pylab.base.Saveable.set_save_location" href="base.html#mdsine2.pylab.base.Saveable.set_save_location">set_save_location</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="mdsine2.pylab.inference.Tracer"><code class="flex name class">
<span>class <span class="ident">Tracer</span></span>
<span>(</span><span>mcmc: <a title="mdsine2.pylab.inference.BaseMCMC" href="#mdsine2.pylab.inference.BaseMCMC">BaseMCMC</a>, filename: str)</span>
</code></dt>
<dd>
<div class="desc"><p>This sets up the graph to be traced using h5py.</p>
<p>Able to checkpoint the values of the graph through inference.
Write the data to disk. The only variables that are traced are the
variables in the set <code>being_traced</code>.</p>
<p>There might be other processes/threads that are reading the current
chain to plot it intermittently, so we create the file in SWMR
(Single Writer, Multiple Reader) mode which will make the
file always readable (never ina
corrupt state from writing).</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>mcmc</code></strong> :&ensp;<code>pylab.inference.BaseMCMC</code></dt>
<dd>This is the inference object that we are tracing</dd>
<dt><strong><code>filename</code></strong> :&ensp;<code>str</code></dt>
<dd>This is where to save it</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Tracer(Saveable): 
    &#39;&#39;&#39;This sets up the graph to be traced using h5py.

    Able to checkpoint the values of the graph through inference.
    Write the data to disk. The only variables that are traced are the 
    variables in the set `being_traced`.

    There might be other processes/threads that are reading the current 
    chain to plot it intermittently, so we create the file in SWMR 
    (Single Writer, Multiple Reader) mode which will make the 
    file always readable (never ina  corrupt state from writing).

    Parameters
    ----------
    mcmc : pylab.inference.BaseMCMC
        This is the inference object that we are tracing
    filename : str
        This is where to save it
    &#39;&#39;&#39;
    def __init__(self, mcmc: BaseMCMC, filename: str):
        # Check parameters and set up the attributes
        self.mcmc = mcmc # This is a pointer to the BaseMCMC object
        self.graph = self.mcmc.graph # This is a pointer to the pylab.graph.Graph object
        self.graph.tracer = self # Sets up internal pointers
        self.mcmc.tracer = self # Sets up internal pointers

        self.filename  = filename
        if self.filename is not None:
            if not util.isstr(filename):
                raise TypeError(&#39;filename ({}) must be a str&#39;.format(type(filename)))
            self.filename = os.path.abspath(filename)
            self.f = h5py.File(self.filename, &#39;w&#39;, libver=&#39;latest&#39;) #This is the h5py file
            self.being_traced = OrderedSet()

            self.f.attrs[&#39;burnin&#39;] = self.mcmc.burnin
            self.f.attrs[&#39;n_samples&#39;] = self.mcmc.n_samples
            self.f.attrs[&#39;ckpt&#39;] = self.mcmc.checkpoint

            self.burnin = self.mcmc.burnin
            self.n_samples = self.mcmc.n_samples
            self.checkpoint = self.mcmc.checkpoint

            # Get the inference order and diagnostic variables
            ret = []
            for nid in self.mcmc.inf_order:
                ret.append(self.mcmc.graph[nid].name)
            self.f.attrs[&#39;inf_order&#39;] = ret
            if self.mcmc.diagnostic_variables is not None:
                a = list(self.mcmc.diagnostic_variables.keys())
            else:
                a = []
            self.f.attrs[&#39;diagnostic_variables&#39;] = a
            logging.info(&#39;Setting Single Write, Multiple Read Mode&#39;)
            self.f.swmr_mode = True # single writer, multiple reader mode
            self.close()

            # Add all of the variables being traced to the file
            logging.info(&#39;Setting the trace of learned parameters&#39;)
            logging.info(&#39;#######################################&#39;)
            for nid in self.mcmc.inf_order:
                logging.info(&#39;Setting the trace of {}&#39;.format(self.graph[nid].name))
                self.graph[nid].set_trace()
            logging.info(&#39;Setting the trace for diagnostic variables&#39;)
            logging.info(&#39;##########################################&#39;)
            if self.mcmc.diagnostic_variables is not None:
                for nid in self.mcmc.diagnostic_variables:
                    logging.info(&#39;Setting the trace of {}&#39;.format(self.graph[nid].name))
                    self.graph[nid].set_trace()

    # def __enter__(self):
    #     self.f = h5py.File(self.filename, &#39;r+&#39;)
    #     return self.f

    # def __exit__(self, type, value, traceback):
    #     self.close()

    @property
    def ckpt(self) -&gt; int:
        &#39;&#39;&#39;For backwards compatitbility
        &#39;&#39;&#39;
        return self.ckpt

    @ckpt.setter
    def ckpt(self, a: int):
        &#39;&#39;&#39;Set ckpt
        &#39;&#39;&#39;
        self.ckpt = a

    def close(self):
        &#39;&#39;&#39;Close the file
        &#39;&#39;&#39;
        self.f.close()
        self.f = None

    def open(self):
        &#39;&#39;&#39;Open the file
        &#39;&#39;&#39;
        self.f = h5py.File(self.filename, &#39;r+&#39;, libver=&#39;latest&#39;)

    def copy(self) -&gt; &#34;Tracer&#34;:
        &#39;&#39;&#39;Return a copy of the object but do not copy the underlying hdf5 object
        &#39;&#39;&#39;
        new_obj = type(self)(mcmc=self.mcmc, filename=self.filename)
        new_obj.__dict__.update(self.__dict__)
        return new_obj

    def deepcopy(self, hdf5_dst: str=None) -&gt; &#34;Tracer&#34;:
        &#39;&#39;&#39;Return a deepcopy of the object and copy the underlying hdf5 object as well

        Parameters
        ----------
        hdf5_dst : str, None
            Destination to copy the hdf5 object. If Nothing is provided then we 
            will just append &#39;_copy&#39; to the name of the current filename
        &#39;&#39;&#39;
        if hdf5_dst is None:
            dst = self.filename.replace(&#39;.hdf5&#39;, &#39;_copy.hdf5&#39;)
        else:
            if not util.isstr(hdf5_dst):
                raise TypeError(&#39;`hdf5_dst` ({}) must be a str&#39;)
            dst = hdf5_dst


        new_obj = copy.deepcopy(self)
        new_obj.filename = dst
        shutil.copyfile(src=self.filename, dst=new_obj.filename)
        return new_obj

    def is_being_traced(self, var: Union[int, str, Node]) -&gt; bool:
        &#39;&#39;&#39;Checks if the variable is being traced

        Parameters
        ----------
        var : int, str, pylab.graph.Node
            An ID of a graph object or the object

        Returns
        -------
        bool
        &#39;&#39;&#39;
        if isnode(var):
            var = var.name
        else:
            if var in self.graph:
                var = self.graph[var].name
            else:
                # raise IndexError(&#39;`var` ({}) ({}) not recognized in graph&#39;.format(type(var), var))
                return False
        return var in self.being_traced

    def set_trace(self, name: str, shape: Tuple[int], dtype: Type):
        &#39;&#39;&#39;Set up a dataset for the variable. If a group is specified, it will 
        add it to the group

        Parameters
        ----------
        name : str
            This is the name of the variable.        
        shape : tuple, None
            This is the shape of the variable. This should not include the 
            trace length, that is added in this function. If it is a scalar,
            then the shape is None
        dtype : Type
            This is the type of the trace
        group : str, h5py.Group
            This is the group you want it added to 
        &#39;&#39;&#39;
        if name in self.being_traced:
            logging.info(&#39;Skipping adding the trace of `{}` because it is already being&#39; \
                &#39; traced ({})&#39;.format(name, list(self.being_traced)))
            return
        if not util.isstr(name):
            raise TypeError(&#39;`name` ({}) must be a str&#39;.format(type(name)))
        if not (util.istuple(shape) or shape is None):
            raise TypeError(&#39;`shape` ({}) must be a tuple or None&#39;.format(type(shape)))
        if not util.istype(dtype):
            raise TypeError(&#39;`dtype` ({}) ({}) must be a Type&#39;.format(dtype, type(dtype)))
        self.open()
        
        if shape is not None:
            shape = (self.n_samples, ) + shape
        else:
            shape = (self.n_samples, )
        dset = self.f.create_dataset(name=name, shape=shape, dtype=dtype, chunks=True)
        dset.attrs[&#39;end_iter&#39;] = 0
        self.being_traced.add(name)
        self.close()

    def write_to_disk(self, name: str):
        &#39;&#39;&#39;Append the RAM trace of the variable `name` into disk. Copies the data
        from RAM (self.graph[name]/trace) into disk memory.

        Parameters
        ----------
        name : str
            Name of the variable we are writing 
        &#39;&#39;&#39;
        if self.filename is None:
            raise ValueError(&#39;Tracing to disk not setup&#39;)
        self.open()
        dset = self.f[name]
        i = dset.attrs[&#39;end_iter&#39;]
        node = self.graph[name]
        l = node.ckpt_iter
        
        # print(&#39;\nwriting to disk,&#39;, name)
        # print(&#39;ckpt_iter&#39;, l)
        # print(&#39;trace.shape&#39;, node.trace.shape)
        # print(dset[i:i+l].shape)

        dset[i:i+l] = node.trace
        dset.attrs[&#39;end_iter&#39;] = i + l
        self.close()

    def overwrite_entire_trace_on_disk(self, name: str, data: np.ndarray, dtype: Type=None):
        &#39;&#39;&#39;Overwrite all the data we have on disk for the variable 
        with name `name` and data `data`. Blanks everything out and 
        sets the end variable to the end of data
        
        Parameters
        ----------
        name : str
            Name of the variable we are overwriting
        data : np.ndarray
            Array we are overwriting the data with
        &#39;&#39;&#39;
        if not util.isarray(data):
            raise TypeError(&#39;`data` ({}) must be an array&#39;.format(type(data)))
        data = np.asarray(data)

        self.open()
        dset = self.f[name]
        shape = dset.shape
        if dtype is None:
            dtype = data.dtype

        # Delete the old dataset and make a new one
        del self.f[name]
        self.close()
        self.being_traced.remove(name)

        self.set_trace(name=name, shape=shape, dtype=dtype)
        
        self.open()
        dset = self.f[name]
        dset[:data.shape[0]] = data
        dset.attrs[&#39;end_iter&#39;] = data.shape[0]
        self.close()

    def finish_tracing(self):
        &#39;&#39;&#39;Append the rest of the buffers to the dataset. Delete the local trace
        &#39;&#39;&#39;
        if self.filename is None:
            return
        self.open()
        for name in self.being_traced:
            dset = self.f[name]
            i = dset.attrs[&#39;end_iter&#39;]
            node = self.graph[name]
            l = node.ckpt_iter
            dset[i:i+l] = node.trace
            dset.attrs[&#39;end_iter&#39;] = i+l

            self.graph[name].trace = None
        self.close()

    def get_disk_trace_iteration(self) -&gt; int:
        &#39;&#39;&#39;Returns the last iteration the disk is saved to

        Returns
        -------
        int
        &#39;&#39;&#39;
        self.open()
        name = list(self.f.keys())[0]
        dset = self.f[name]
        n = dset.attrs[&#39;end_iter&#39;]
        self.close()
        return n

    def get_trace(self, name: str, section: str=&#39;posterior&#39;, slices: slice=None) -&gt; np.ndarray:
        &#39;&#39;&#39;Return the trace that corresponds with the name.

        Depdending on the parameter `section`, it will return different parts of
        the trace. Options:
            &#39;posterior&#39;: 
                Returns the trace after the burnin
            &#39;burnin&#39;
                Returns the samples that were in the burnin
            &#39;entire&#39;
                Returns all the samples
            &#39;slice&#39;
                All of the arguments for slicing are in slices
            
        Parameters
        ----------
        name : str
            Name of the variable
        section : str
            Which part of the trace to return - description above
        slices : list(slice), slice
            A list of slicing objects or a slice object.

            slice(start, stop, step)
            Example, single dimension:
                slice(None) == :
                slice(5) == :5
                slice(4, None, None) == 4:
                slice(9, 22,None) == 9:22
            Example, multiple dimensions:
                [slice(None), slice(4, None, None)] == :, 4:
                [slice(None), 4, 5] == :, 4, 5

        Returns
        -------
        np.ndarray
        &#39;&#39;&#39;
        if not util.isstr(section):
            raise TypeError(&#39;`section` ({}) must be a str&#39;.format(type(str)))
        
        self.open()
        dset = self.f[name]
        if section == &#39;posterior&#39;:
            high = dset.attrs[&#39;end_iter&#39;]
            low = self.burnin
            if low &lt; self.burnin:
                self.close()
                raise IndexError(&#39;Last iteration ({}) is less than the burnin ({}). &#39; \
                    &#39;Cannot get the trace&#39;.format(i, self.burnin))
        elif section == &#39;burnin&#39;:
            high = dset.attrs[&#39;end_iter&#39;]
            low = 0
            if high &gt; self.burnin:
                high = self.burnin
        elif section == &#39;entire&#39;:
            low = 0
            high = dset.attrs[&#39;end_iter&#39;]
        elif section != &#39;slice&#39;:
            raise ValueError(&#39;`section` ({}) not recognized&#39;.format(section))
        
        if section != &#39;slice&#39;:
            if slices is not None:
                slices = [slice(low,high,None)] + list(slices)
            else:
                slices = slice(low,high,None)

        ret = dset[slices]
        self.close()
        return ret

    def get_iter(self, name: str) -&gt; int:
        &#39;&#39;&#39;Return the end index that has been saved so far for variable with
        name `name`

        Parameters
        ----------
        name : str
            Name of the variable
        
        Returns
        -------
        int
        &#39;&#39;&#39;
        self.open()
        dset = self.f[name]
        i = dset.attrs[&#39;end_iter&#39;]
        self.close()
        return i

    def continue_inference(self, gibb_step_start: int):
        &#39;&#39;&#39;Restart the inference at the gibbs step provided

        Parameters
        ----------
        gibb_step_start : int
            Gibb step to start at
        &#39;&#39;&#39;
        self.open()
        for name in self.f:
            dset = self.f[name]
            dset.attrs[&#39;end_iter&#39;] = gibb_step_start
        self.close()</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="mdsine2.pylab.base.Saveable" href="base.html#mdsine2.pylab.base.Saveable">Saveable</a></li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="mdsine2.pylab.inference.Tracer.ckpt"><code class="name">var <span class="ident">ckpt</span> : int</code></dt>
<dd>
<div class="desc"><p>For backwards compatitbility</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def ckpt(self) -&gt; int:
    &#39;&#39;&#39;For backwards compatitbility
    &#39;&#39;&#39;
    return self.ckpt</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="mdsine2.pylab.inference.Tracer.close"><code class="name flex">
<span>def <span class="ident">close</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Close the file</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def close(self):
    &#39;&#39;&#39;Close the file
    &#39;&#39;&#39;
    self.f.close()
    self.f = None</code></pre>
</details>
</dd>
<dt id="mdsine2.pylab.inference.Tracer.continue_inference"><code class="name flex">
<span>def <span class="ident">continue_inference</span></span>(<span>self, gibb_step_start: int)</span>
</code></dt>
<dd>
<div class="desc"><p>Restart the inference at the gibbs step provided</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>gibb_step_start</code></strong> :&ensp;<code>int</code></dt>
<dd>Gibb step to start at</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def continue_inference(self, gibb_step_start: int):
    &#39;&#39;&#39;Restart the inference at the gibbs step provided

    Parameters
    ----------
    gibb_step_start : int
        Gibb step to start at
    &#39;&#39;&#39;
    self.open()
    for name in self.f:
        dset = self.f[name]
        dset.attrs[&#39;end_iter&#39;] = gibb_step_start
    self.close()</code></pre>
</details>
</dd>
<dt id="mdsine2.pylab.inference.Tracer.copy"><code class="name flex">
<span>def <span class="ident">copy</span></span>(<span>self) ‑> <a title="mdsine2.pylab.inference.Tracer" href="#mdsine2.pylab.inference.Tracer">Tracer</a></span>
</code></dt>
<dd>
<div class="desc"><p>Return a copy of the object but do not copy the underlying hdf5 object</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def copy(self) -&gt; &#34;Tracer&#34;:
    &#39;&#39;&#39;Return a copy of the object but do not copy the underlying hdf5 object
    &#39;&#39;&#39;
    new_obj = type(self)(mcmc=self.mcmc, filename=self.filename)
    new_obj.__dict__.update(self.__dict__)
    return new_obj</code></pre>
</details>
</dd>
<dt id="mdsine2.pylab.inference.Tracer.deepcopy"><code class="name flex">
<span>def <span class="ident">deepcopy</span></span>(<span>self, hdf5_dst: str = None) ‑> <a title="mdsine2.pylab.inference.Tracer" href="#mdsine2.pylab.inference.Tracer">Tracer</a></span>
</code></dt>
<dd>
<div class="desc"><p>Return a deepcopy of the object and copy the underlying hdf5 object as well</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>hdf5_dst</code></strong> :&ensp;<code>str, None</code></dt>
<dd>Destination to copy the hdf5 object. If Nothing is provided then we
will just append '_copy' to the name of the current filename</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def deepcopy(self, hdf5_dst: str=None) -&gt; &#34;Tracer&#34;:
    &#39;&#39;&#39;Return a deepcopy of the object and copy the underlying hdf5 object as well

    Parameters
    ----------
    hdf5_dst : str, None
        Destination to copy the hdf5 object. If Nothing is provided then we 
        will just append &#39;_copy&#39; to the name of the current filename
    &#39;&#39;&#39;
    if hdf5_dst is None:
        dst = self.filename.replace(&#39;.hdf5&#39;, &#39;_copy.hdf5&#39;)
    else:
        if not util.isstr(hdf5_dst):
            raise TypeError(&#39;`hdf5_dst` ({}) must be a str&#39;)
        dst = hdf5_dst


    new_obj = copy.deepcopy(self)
    new_obj.filename = dst
    shutil.copyfile(src=self.filename, dst=new_obj.filename)
    return new_obj</code></pre>
</details>
</dd>
<dt id="mdsine2.pylab.inference.Tracer.finish_tracing"><code class="name flex">
<span>def <span class="ident">finish_tracing</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Append the rest of the buffers to the dataset. Delete the local trace</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def finish_tracing(self):
    &#39;&#39;&#39;Append the rest of the buffers to the dataset. Delete the local trace
    &#39;&#39;&#39;
    if self.filename is None:
        return
    self.open()
    for name in self.being_traced:
        dset = self.f[name]
        i = dset.attrs[&#39;end_iter&#39;]
        node = self.graph[name]
        l = node.ckpt_iter
        dset[i:i+l] = node.trace
        dset.attrs[&#39;end_iter&#39;] = i+l

        self.graph[name].trace = None
    self.close()</code></pre>
</details>
</dd>
<dt id="mdsine2.pylab.inference.Tracer.get_disk_trace_iteration"><code class="name flex">
<span>def <span class="ident">get_disk_trace_iteration</span></span>(<span>self) ‑> int</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the last iteration the disk is saved to</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>int</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_disk_trace_iteration(self) -&gt; int:
    &#39;&#39;&#39;Returns the last iteration the disk is saved to

    Returns
    -------
    int
    &#39;&#39;&#39;
    self.open()
    name = list(self.f.keys())[0]
    dset = self.f[name]
    n = dset.attrs[&#39;end_iter&#39;]
    self.close()
    return n</code></pre>
</details>
</dd>
<dt id="mdsine2.pylab.inference.Tracer.get_iter"><code class="name flex">
<span>def <span class="ident">get_iter</span></span>(<span>self, name: str) ‑> int</span>
</code></dt>
<dd>
<div class="desc"><p>Return the end index that has been saved so far for variable with
name <code>name</code></p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>name</code></strong> :&ensp;<code>str</code></dt>
<dd>Name of the variable</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>int</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_iter(self, name: str) -&gt; int:
    &#39;&#39;&#39;Return the end index that has been saved so far for variable with
    name `name`

    Parameters
    ----------
    name : str
        Name of the variable
    
    Returns
    -------
    int
    &#39;&#39;&#39;
    self.open()
    dset = self.f[name]
    i = dset.attrs[&#39;end_iter&#39;]
    self.close()
    return i</code></pre>
</details>
</dd>
<dt id="mdsine2.pylab.inference.Tracer.get_trace"><code class="name flex">
<span>def <span class="ident">get_trace</span></span>(<span>self, name: str, section: str = 'posterior', slices: slice = None) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Return the trace that corresponds with the name.</p>
<p>Depdending on the parameter <code>section</code>, it will return different parts of
the trace. Options:
'posterior':
Returns the trace after the burnin
'burnin'
Returns the samples that were in the burnin
'entire'
Returns all the samples
'slice'
All of the arguments for slicing are in slices</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>name</code></strong> :&ensp;<code>str</code></dt>
<dd>Name of the variable</dd>
<dt><strong><code>section</code></strong> :&ensp;<code>str</code></dt>
<dd>Which part of the trace to return - description above</dd>
<dt><strong><code>slices</code></strong> :&ensp;<code>list(slice), slice</code></dt>
<dd>
<p>A list of slicing objects or a slice object.</p>
<p>slice(start, stop, step)
Example, single dimension:
slice(None) == :
slice(5) == :5
slice(4, None, None) == 4:
slice(9, 22,None) == 9:22
Example, multiple dimensions:
[slice(None), slice(4, None, None)] == :, 4:
[slice(None), 4, 5] == :, 4, 5</p>
</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.ndarray</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_trace(self, name: str, section: str=&#39;posterior&#39;, slices: slice=None) -&gt; np.ndarray:
    &#39;&#39;&#39;Return the trace that corresponds with the name.

    Depdending on the parameter `section`, it will return different parts of
    the trace. Options:
        &#39;posterior&#39;: 
            Returns the trace after the burnin
        &#39;burnin&#39;
            Returns the samples that were in the burnin
        &#39;entire&#39;
            Returns all the samples
        &#39;slice&#39;
            All of the arguments for slicing are in slices
        
    Parameters
    ----------
    name : str
        Name of the variable
    section : str
        Which part of the trace to return - description above
    slices : list(slice), slice
        A list of slicing objects or a slice object.

        slice(start, stop, step)
        Example, single dimension:
            slice(None) == :
            slice(5) == :5
            slice(4, None, None) == 4:
            slice(9, 22,None) == 9:22
        Example, multiple dimensions:
            [slice(None), slice(4, None, None)] == :, 4:
            [slice(None), 4, 5] == :, 4, 5

    Returns
    -------
    np.ndarray
    &#39;&#39;&#39;
    if not util.isstr(section):
        raise TypeError(&#39;`section` ({}) must be a str&#39;.format(type(str)))
    
    self.open()
    dset = self.f[name]
    if section == &#39;posterior&#39;:
        high = dset.attrs[&#39;end_iter&#39;]
        low = self.burnin
        if low &lt; self.burnin:
            self.close()
            raise IndexError(&#39;Last iteration ({}) is less than the burnin ({}). &#39; \
                &#39;Cannot get the trace&#39;.format(i, self.burnin))
    elif section == &#39;burnin&#39;:
        high = dset.attrs[&#39;end_iter&#39;]
        low = 0
        if high &gt; self.burnin:
            high = self.burnin
    elif section == &#39;entire&#39;:
        low = 0
        high = dset.attrs[&#39;end_iter&#39;]
    elif section != &#39;slice&#39;:
        raise ValueError(&#39;`section` ({}) not recognized&#39;.format(section))
    
    if section != &#39;slice&#39;:
        if slices is not None:
            slices = [slice(low,high,None)] + list(slices)
        else:
            slices = slice(low,high,None)

    ret = dset[slices]
    self.close()
    return ret</code></pre>
</details>
</dd>
<dt id="mdsine2.pylab.inference.Tracer.is_being_traced"><code class="name flex">
<span>def <span class="ident">is_being_traced</span></span>(<span>self, var: Union[int, str, <a title="mdsine2.pylab.graph.Node" href="graph.html#mdsine2.pylab.graph.Node">Node</a>]) ‑> bool</span>
</code></dt>
<dd>
<div class="desc"><p>Checks if the variable is being traced</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>var</code></strong> :&ensp;<code>int, str, pylab.graph.Node</code></dt>
<dd>An ID of a graph object or the object</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>bool</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_being_traced(self, var: Union[int, str, Node]) -&gt; bool:
    &#39;&#39;&#39;Checks if the variable is being traced

    Parameters
    ----------
    var : int, str, pylab.graph.Node
        An ID of a graph object or the object

    Returns
    -------
    bool
    &#39;&#39;&#39;
    if isnode(var):
        var = var.name
    else:
        if var in self.graph:
            var = self.graph[var].name
        else:
            # raise IndexError(&#39;`var` ({}) ({}) not recognized in graph&#39;.format(type(var), var))
            return False
    return var in self.being_traced</code></pre>
</details>
</dd>
<dt id="mdsine2.pylab.inference.Tracer.open"><code class="name flex">
<span>def <span class="ident">open</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Open the file</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def open(self):
    &#39;&#39;&#39;Open the file
    &#39;&#39;&#39;
    self.f = h5py.File(self.filename, &#39;r+&#39;, libver=&#39;latest&#39;)</code></pre>
</details>
</dd>
<dt id="mdsine2.pylab.inference.Tracer.overwrite_entire_trace_on_disk"><code class="name flex">
<span>def <span class="ident">overwrite_entire_trace_on_disk</span></span>(<span>self, name: str, data: numpy.ndarray, dtype: Type = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Overwrite all the data we have on disk for the variable
with name <code>name</code> and data <code>data</code>. Blanks everything out and
sets the end variable to the end of data</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>name</code></strong> :&ensp;<code>str</code></dt>
<dd>Name of the variable we are overwriting</dd>
<dt><strong><code>data</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>Array we are overwriting the data with</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def overwrite_entire_trace_on_disk(self, name: str, data: np.ndarray, dtype: Type=None):
    &#39;&#39;&#39;Overwrite all the data we have on disk for the variable 
    with name `name` and data `data`. Blanks everything out and 
    sets the end variable to the end of data
    
    Parameters
    ----------
    name : str
        Name of the variable we are overwriting
    data : np.ndarray
        Array we are overwriting the data with
    &#39;&#39;&#39;
    if not util.isarray(data):
        raise TypeError(&#39;`data` ({}) must be an array&#39;.format(type(data)))
    data = np.asarray(data)

    self.open()
    dset = self.f[name]
    shape = dset.shape
    if dtype is None:
        dtype = data.dtype

    # Delete the old dataset and make a new one
    del self.f[name]
    self.close()
    self.being_traced.remove(name)

    self.set_trace(name=name, shape=shape, dtype=dtype)
    
    self.open()
    dset = self.f[name]
    dset[:data.shape[0]] = data
    dset.attrs[&#39;end_iter&#39;] = data.shape[0]
    self.close()</code></pre>
</details>
</dd>
<dt id="mdsine2.pylab.inference.Tracer.set_trace"><code class="name flex">
<span>def <span class="ident">set_trace</span></span>(<span>self, name: str, shape: Tuple[int], dtype: Type)</span>
</code></dt>
<dd>
<div class="desc"><p>Set up a dataset for the variable. If a group is specified, it will
add it to the group</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>name</code></strong> :&ensp;<code>str</code></dt>
<dd>This is the name of the variable.</dd>
<dt><strong><code>shape</code></strong> :&ensp;<code>tuple, None</code></dt>
<dd>This is the shape of the variable. This should not include the
trace length, that is added in this function. If it is a scalar,
then the shape is None</dd>
<dt><strong><code>dtype</code></strong> :&ensp;<code>Type</code></dt>
<dd>This is the type of the trace</dd>
<dt><strong><code>group</code></strong> :&ensp;<code>str, h5py.Group</code></dt>
<dd>This is the group you want it added to</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_trace(self, name: str, shape: Tuple[int], dtype: Type):
    &#39;&#39;&#39;Set up a dataset for the variable. If a group is specified, it will 
    add it to the group

    Parameters
    ----------
    name : str
        This is the name of the variable.        
    shape : tuple, None
        This is the shape of the variable. This should not include the 
        trace length, that is added in this function. If it is a scalar,
        then the shape is None
    dtype : Type
        This is the type of the trace
    group : str, h5py.Group
        This is the group you want it added to 
    &#39;&#39;&#39;
    if name in self.being_traced:
        logging.info(&#39;Skipping adding the trace of `{}` because it is already being&#39; \
            &#39; traced ({})&#39;.format(name, list(self.being_traced)))
        return
    if not util.isstr(name):
        raise TypeError(&#39;`name` ({}) must be a str&#39;.format(type(name)))
    if not (util.istuple(shape) or shape is None):
        raise TypeError(&#39;`shape` ({}) must be a tuple or None&#39;.format(type(shape)))
    if not util.istype(dtype):
        raise TypeError(&#39;`dtype` ({}) ({}) must be a Type&#39;.format(dtype, type(dtype)))
    self.open()
    
    if shape is not None:
        shape = (self.n_samples, ) + shape
    else:
        shape = (self.n_samples, )
    dset = self.f.create_dataset(name=name, shape=shape, dtype=dtype, chunks=True)
    dset.attrs[&#39;end_iter&#39;] = 0
    self.being_traced.add(name)
    self.close()</code></pre>
</details>
</dd>
<dt id="mdsine2.pylab.inference.Tracer.write_to_disk"><code class="name flex">
<span>def <span class="ident">write_to_disk</span></span>(<span>self, name: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Append the RAM trace of the variable <code>name</code> into disk. Copies the data
from RAM (self.graph[name]/trace) into disk memory.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>name</code></strong> :&ensp;<code>str</code></dt>
<dd>Name of the variable we are writing</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def write_to_disk(self, name: str):
    &#39;&#39;&#39;Append the RAM trace of the variable `name` into disk. Copies the data
    from RAM (self.graph[name]/trace) into disk memory.

    Parameters
    ----------
    name : str
        Name of the variable we are writing 
    &#39;&#39;&#39;
    if self.filename is None:
        raise ValueError(&#39;Tracing to disk not setup&#39;)
    self.open()
    dset = self.f[name]
    i = dset.attrs[&#39;end_iter&#39;]
    node = self.graph[name]
    l = node.ckpt_iter
    
    # print(&#39;\nwriting to disk,&#39;, name)
    # print(&#39;ckpt_iter&#39;, l)
    # print(&#39;trace.shape&#39;, node.trace.shape)
    # print(dset[i:i+l].shape)

    dset[i:i+l] = node.trace
    dset.attrs[&#39;end_iter&#39;] = i + l
    self.close()</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="mdsine2.pylab.base.Saveable" href="base.html#mdsine2.pylab.base.Saveable">Saveable</a></b></code>:
<ul class="hlist">
<li><code><a title="mdsine2.pylab.base.Saveable.load" href="base.html#mdsine2.pylab.base.Saveable.load">load</a></code></li>
<li><code><a title="mdsine2.pylab.base.Saveable.save" href="base.html#mdsine2.pylab.base.Saveable.save">save</a></code></li>
<li><code><a title="mdsine2.pylab.base.Saveable.set_save_location" href="base.html#mdsine2.pylab.base.Saveable.set_save_location">set_save_location</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="mdsine2.pylab" href="index.html">mdsine2.pylab</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="mdsine2.pylab.inference.isMCMC" href="#mdsine2.pylab.inference.isMCMC">isMCMC</a></code></li>
<li><code><a title="mdsine2.pylab.inference.ismodel" href="#mdsine2.pylab.inference.ismodel">ismodel</a></code></li>
<li><code><a title="mdsine2.pylab.inference.r_hat" href="#mdsine2.pylab.inference.r_hat">r_hat</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="mdsine2.pylab.inference.BaseMCMC" href="#mdsine2.pylab.inference.BaseMCMC">BaseMCMC</a></code></h4>
<ul class="">
<li><code><a title="mdsine2.pylab.inference.BaseMCMC.ckpt" href="#mdsine2.pylab.inference.BaseMCMC.ckpt">ckpt</a></code></li>
<li><code><a title="mdsine2.pylab.inference.BaseMCMC.continue_inference" href="#mdsine2.pylab.inference.BaseMCMC.continue_inference">continue_inference</a></code></li>
<li><code><a title="mdsine2.pylab.inference.BaseMCMC.ids" href="#mdsine2.pylab.inference.BaseMCMC.ids">ids</a></code></li>
<li><code><a title="mdsine2.pylab.inference.BaseMCMC.is_in_inference_order" href="#mdsine2.pylab.inference.BaseMCMC.is_in_inference_order">is_in_inference_order</a></code></li>
<li><code><a title="mdsine2.pylab.inference.BaseMCMC.load" href="#mdsine2.pylab.inference.BaseMCMC.load">load</a></code></li>
<li><code><a title="mdsine2.pylab.inference.BaseMCMC.names" href="#mdsine2.pylab.inference.BaseMCMC.names">names</a></code></li>
<li><code><a title="mdsine2.pylab.inference.BaseMCMC.run" href="#mdsine2.pylab.inference.BaseMCMC.run">run</a></code></li>
<li><code><a title="mdsine2.pylab.inference.BaseMCMC.set_diagnostic_variables" href="#mdsine2.pylab.inference.BaseMCMC.set_diagnostic_variables">set_diagnostic_variables</a></code></li>
<li><code><a title="mdsine2.pylab.inference.BaseMCMC.set_inference_order" href="#mdsine2.pylab.inference.BaseMCMC.set_inference_order">set_inference_order</a></code></li>
<li><code><a title="mdsine2.pylab.inference.BaseMCMC.set_intermediate_validation" href="#mdsine2.pylab.inference.BaseMCMC.set_intermediate_validation">set_intermediate_validation</a></code></li>
<li><code><a title="mdsine2.pylab.inference.BaseMCMC.set_tracer" href="#mdsine2.pylab.inference.BaseMCMC.set_tracer">set_tracer</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="mdsine2.pylab.inference.BaseModel" href="#mdsine2.pylab.inference.BaseModel">BaseModel</a></code></h4>
</li>
<li>
<h4><code><a title="mdsine2.pylab.inference.Tracer" href="#mdsine2.pylab.inference.Tracer">Tracer</a></code></h4>
<ul class="">
<li><code><a title="mdsine2.pylab.inference.Tracer.ckpt" href="#mdsine2.pylab.inference.Tracer.ckpt">ckpt</a></code></li>
<li><code><a title="mdsine2.pylab.inference.Tracer.close" href="#mdsine2.pylab.inference.Tracer.close">close</a></code></li>
<li><code><a title="mdsine2.pylab.inference.Tracer.continue_inference" href="#mdsine2.pylab.inference.Tracer.continue_inference">continue_inference</a></code></li>
<li><code><a title="mdsine2.pylab.inference.Tracer.copy" href="#mdsine2.pylab.inference.Tracer.copy">copy</a></code></li>
<li><code><a title="mdsine2.pylab.inference.Tracer.deepcopy" href="#mdsine2.pylab.inference.Tracer.deepcopy">deepcopy</a></code></li>
<li><code><a title="mdsine2.pylab.inference.Tracer.finish_tracing" href="#mdsine2.pylab.inference.Tracer.finish_tracing">finish_tracing</a></code></li>
<li><code><a title="mdsine2.pylab.inference.Tracer.get_disk_trace_iteration" href="#mdsine2.pylab.inference.Tracer.get_disk_trace_iteration">get_disk_trace_iteration</a></code></li>
<li><code><a title="mdsine2.pylab.inference.Tracer.get_iter" href="#mdsine2.pylab.inference.Tracer.get_iter">get_iter</a></code></li>
<li><code><a title="mdsine2.pylab.inference.Tracer.get_trace" href="#mdsine2.pylab.inference.Tracer.get_trace">get_trace</a></code></li>
<li><code><a title="mdsine2.pylab.inference.Tracer.is_being_traced" href="#mdsine2.pylab.inference.Tracer.is_being_traced">is_being_traced</a></code></li>
<li><code><a title="mdsine2.pylab.inference.Tracer.open" href="#mdsine2.pylab.inference.Tracer.open">open</a></code></li>
<li><code><a title="mdsine2.pylab.inference.Tracer.overwrite_entire_trace_on_disk" href="#mdsine2.pylab.inference.Tracer.overwrite_entire_trace_on_disk">overwrite_entire_trace_on_disk</a></code></li>
<li><code><a title="mdsine2.pylab.inference.Tracer.set_trace" href="#mdsine2.pylab.inference.Tracer.set_trace">set_trace</a></code></li>
<li><code><a title="mdsine2.pylab.inference.Tracer.write_to_disk" href="#mdsine2.pylab.inference.Tracer.write_to_disk">write_to_disk</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.1</a>.</p>
</footer>
</body>
</html>