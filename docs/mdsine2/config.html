<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.1" />
<title>mdsine2.config API documentation</title>
<meta name="description" content="This module holds classes for the logging and model configuration
parameters that are set manually in here. There are also the filtering
functions â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>mdsine2.config</code></h1>
</header>
<section id="section-intro">
<p>This module holds classes for the logging and model configuration
parameters that are set manually in here. There are also the filtering
functions used to preprocess the data</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#39;&#39;&#39;This module holds classes for the logging and model configuration
parameters that are set manually in here. There are also the filtering
functions used to preprocess the data
&#39;&#39;&#39;
import logging
import numpy as np
import pandas as pd
import sys
import os.path

from .names import STRNAMES
from . import pylab as pl

# File locations
GRAPH_NAME = &#39;graph&#39;
MCMC_FILENAME = &#39;mcmc.pkl&#39;
SUBJSET_FILENAME = &#39;subjset.pkl&#39;
VALIDATION_SUBJSET_FILENAME = &#39;validate_subjset.pkl&#39;
SYNDATA_FILENAME = &#39;syndata.pkl&#39;
GRAPH_FILENAME = &#39;graph.pkl&#39;
HDF5_FILENAME = &#39;traces.hdf5&#39;
TRACER_FILENAME = &#39;tracer.pkl&#39;
PARAMS_FILENAME = &#39;params.pkl&#39;
FPARAMS_FILENAME = &#39;filtering_params.pkl&#39;
SYNPARAMS_FILENAME = &#39;synthetic_params.pkl&#39;
MLCRR_RESULTS_FILENAME = &#39;mlcrr_results.pkl&#39;
RESTART_INFERENCE_SEED_RECORD = &#39;restart_seed_record.tsv&#39;
INTERMEDIATE_RESULTS_FILENAME = &#39;intermediate_results.tsv&#39;


PHYLOGENETIC_TREE_FILENAME = &#39;raw_data/phylogenetic_tree_branch_len_preserved.nhx&#39;

def isModelConfig(x):
    &#39;&#39;&#39;Checks if the input array is a model config object

    Parameters
    ----------
    x : any
        Instance we are checking

    Returns
    -------
    bool
        True if `x` is a a model config object
    &#39;&#39;&#39;
    return x is not None and issubclass(x.__class__, _BaseModelConfig)

class _BaseModelConfig(pl.Saveable):

    def __str__(self):
        s = &#39;{}&#39;.format(self.__class__.__name__)
        for k,v in vars(self).items():
            s += &#39;\n\t{}: {}&#39;.format(k,v)
        return s

    def suffix(self):
        raise NotImplementedError(&#39;Need to implement&#39;)


class MDSINE2ModelConfig(_BaseModelConfig):
    &#39;&#39;&#39;Configuration parameters for the model


    System initialization
    ---------------------
    - OUTPUT_BASEPATH, MODEL_PATH : str
        - Path to save the model
    - SEED : int
        - Seed to initialize inference with
    - BURNIN : int
        - Number of initial Gibb steps to throw away
    - N_SAMPLES : int
        - Total number of Gibb steps
    - CHECKPOINT : int
        - How often to write to disk
    - PROCESS_VARIANCE_TYPE : str
        - What type of process variance to do
        - NOTE: There is only one option, do not change
    - DATA_DTYPE : str
        - What kind of data to do inference we
        - NOTE: Model assume you are using absolute abundance, do not change
    - QPCR_NORMALIZATION_MAX_VALUE : float
        - This is value to set the largest qPCR value to. Normalize
        all other qPCR measurements directly to this
    - LEAVE_OUT : str
        - Which subject to leave out, if necessary
    - ZERO_INFLATION_TRANSITION_POLICY : str
        - What type of zero inflation to do. Do not change
    - GROWTH_TRUNCATION_SETTINGS : str
        - How to initialize the truncation settings for the growth parameters
    - SELF_INTERACTIONS_TRUNCATION_SETTINGS : str
        - How to initialize the truncation settings for the self-interaction parameters
    - MP_FILTERING : str
        - How to do multiprocessing for filtering
    - MP_CLUSTERING : str
        - How to do multiprocessing for clustering
    - NEGBINB_A0, NEGBIN_A1 : float
        - Negative binomial dispersion parameters
    - N_QPCR_BUCKETS : int
        - Number of qPCR buckets. This is not learned in the model, do not change.
    - INTERMEDIATE_VALIDATION_T : float
        - How often to do the intermediate validation
    - INTERMEDIATE_VALIDATION_KWARGS : dict
        - Arguemnts for the intermediate valudation
    - LEARN : Dict[str, bool]
        - These are the dictionary of parameters which we are learning in the model.
        - If the name maps to True, then we learn it during inference. If it maps to 
          false, then we do not update its value duting inference.
    - INFERENCE_ORDER : list
        - This is the order to update the parameters during MCMC inference
    - INITIALIZATION_KWARGS : Dict[str, Dict[str, Any]]
        - These are the parameters to send into the `initialize` function for each variable
          that we are learning
    - INITIALIZATION_ORDER : list
        - This is the order to initialize the variables

    Parameters
    ----------
    basepath : str
        This is the base path to save the inference
    seed : int
        This is the seed to start the inference with
    burnin : int
        This is how many gibb steps to throw away originally
    n_samples : int
        This is the total number of Gibb steps to do for inference
    negbin_a0, negbin_a1 : float
        This is the negative binomial dispersion parameters
    leave_out : str
        This is the subject to leave out, if necesssary
    checkpoint : int
        This is how often we should write to disk
    &#39;&#39;&#39;
    def __init__(self, basepath: str, seed: int, burnin: int, n_samples: int,
        negbin_a0: float, negbin_a1: float, leave_out: str=None,
        checkpoint: int=100):
        self.OUTPUT_BASEPATH = os.path.abspath(basepath)
        self.MODEL_PATH = self.OUTPUT_BASEPATH
        self.SEED = seed
        self.BURNIN = burnin
        self.N_SAMPLES = n_samples
        self.CHECKPOINT = checkpoint
        self.PROCESS_VARIANCE_TYPE = &#39;multiplicative-global&#39;
        self.DATA_DTYPE = &#39;abs&#39;

        self.QPCR_NORMALIZATION_MAX_VALUE = 100
        self.LEAVE_OUT = leave_out
        self.ZERO_INFLATION_TRANSITION_POLICY = None #&#39;ignore&#39;

        self.GROWTH_TRUNCATION_SETTINGS = &#39;positive&#39;
        self.SELF_INTERACTIONS_TRUNCATION_SETTINGS = &#39;positive&#39;

        self.MP_FILTERING = &#39;debug&#39;
        self.MP_CLUSTERING = &#39;debug&#39;

        self.NEGBIN_A0 = negbin_a0
        self.NEGBIN_A1 = negbin_a1
        self.N_QPCR_BUCKETS = 3

        self.INTERMEDIATE_VALIDATION_T = 1 * 3600 # Every hour
        self.INTERMEDIATE_VALIDATION_KWARGS = None

        self.LEARN = {
            STRNAMES.GLV_PARAMETERS: True,
            STRNAMES.PROCESSVAR: True,
            STRNAMES.PRIOR_VAR_GROWTH: False,
            STRNAMES.PRIOR_VAR_SELF_INTERACTIONS: False,
            STRNAMES.PRIOR_VAR_INTERACTIONS: True,
            STRNAMES.PRIOR_VAR_PERT: True,
            STRNAMES.PRIOR_MEAN_GROWTH: True,
            STRNAMES.PRIOR_MEAN_SELF_INTERACTIONS: True,
            STRNAMES.PRIOR_MEAN_INTERACTIONS: True,
            STRNAMES.PRIOR_MEAN_PERT: True,
            STRNAMES.FILTERING: True,
            STRNAMES.ZERO_INFLATION: False,
            STRNAMES.CLUSTERING: True,
            STRNAMES.CONCENTRATION: True, 
            STRNAMES.CLUSTER_INTERACTION_INDICATOR: True,
            STRNAMES.CLUSTER_INTERACTION_INDICATOR_PROB: True,
            STRNAMES.PERT_INDICATOR: True,
            STRNAMES.PERT_INDICATOR_PROB: True,
            STRNAMES.QPCR_SCALES: False,
            STRNAMES.QPCR_DOFS: False,
            STRNAMES.QPCR_VARIANCES: False}

        self.INFERENCE_ORDER = [
            STRNAMES.CLUSTER_INTERACTION_INDICATOR,
            STRNAMES.CLUSTER_INTERACTION_INDICATOR_PROB,
            STRNAMES.PERT_INDICATOR,
            STRNAMES.PERT_INDICATOR_PROB,
            STRNAMES.GLV_PARAMETERS,
            STRNAMES.PRIOR_MEAN_INTERACTIONS,
            STRNAMES.PRIOR_MEAN_PERT,
            STRNAMES.PRIOR_MEAN_GROWTH,
            STRNAMES.PRIOR_MEAN_SELF_INTERACTIONS,
            STRNAMES.PRIOR_VAR_GROWTH,
            STRNAMES.PRIOR_VAR_SELF_INTERACTIONS,
            STRNAMES.PRIOR_VAR_INTERACTIONS,
            STRNAMES.PRIOR_VAR_PERT,
            STRNAMES.PROCESSVAR,
            STRNAMES.ZERO_INFLATION,
            STRNAMES.QPCR_SCALES,
            STRNAMES.QPCR_DOFS,
            STRNAMES.QPCR_VARIANCES,
            STRNAMES.FILTERING,
            STRNAMES.CLUSTERING,
            STRNAMES.CONCENTRATION]

        self.INITIALIZATION_KWARGS = {
            STRNAMES.QPCR_VARIANCES: {
                &#39;value_option&#39;: &#39;empirical&#39;},
            STRNAMES.QPCR_SCALES: {
                &#39;value_option&#39;: &#39;prior-mean&#39;,
                &#39;scale_option&#39;: &#39;empirical&#39;,
                &#39;dof_option&#39;: &#39;diffuse&#39;,
                &#39;proposal_option&#39;: &#39;auto&#39;,
                &#39;target_acceptance_rate&#39;: &#39;optimal&#39;,
                &#39;end_tune&#39;: &#39;half-burnin&#39;,
                &#39;tune&#39;: 50,
                &#39;delay&#39;:0},
            STRNAMES.QPCR_DOFS: {
                &#39;value_option&#39;: &#39;diffuse&#39;,
                &#39;low_option&#39;: &#39;valid&#39;,
                &#39;high_option&#39;: &#39;med&#39;,
                &#39;proposal_option&#39;: &#39;auto&#39;,
                &#39;target_acceptance_rate&#39;: &#39;optimal&#39;,
                &#39;end_tune&#39;: &#39;half-burnin&#39;,
                &#39;tune&#39;: 50,
                &#39;delay&#39;: 0},
            STRNAMES.PERT_VALUE: {
                &#39;value_option&#39;: &#39;prior-mean&#39;,
                &#39;delay&#39;:0},
            STRNAMES.PERT_INDICATOR_PROB: {
                &#39;value_option&#39;: &#39;prior-mean&#39;,
                &#39;hyperparam_option&#39;: &#39;strong-sparse&#39;,
                &#39;delay&#39;:0},
            STRNAMES.PERT_INDICATOR: {
                &#39;value_option&#39;: &#39;all-off&#39;,
                &#39;delay&#39;:0},
            STRNAMES.PRIOR_VAR_PERT: {
                &#39;value_option&#39;: &#39;prior-mean&#39;,
                &#39;scale_option&#39;: &#39;diffuse&#39;,
                &#39;dof_option&#39;: &#39;diffuse&#39;,
                &#39;delay&#39;: 0},
            STRNAMES.PRIOR_MEAN_PERT: {
                &#39;value_option&#39;: &#39;prior-mean&#39;,
                &#39;loc_option&#39;: &#39;zero&#39;,
                &#39;scale2_option&#39;: &#39;diffuse&#39;,
                &#39;delay&#39;:0},
            STRNAMES.PRIOR_VAR_GROWTH: {
                &#39;value_option&#39;: &#39;prior-mean&#39;,
                &#39;scale_option&#39;: &#39;inflated-median&#39;,
                &#39;dof_option&#39;: &#39;diffuse&#39;,
                &#39;proposal_option&#39;: &#39;tight&#39;,
                &#39;target_acceptance_rate&#39;: &#39;optimal&#39;,
                &#39;end_tune&#39;: &#39;half-burnin&#39;,
                &#39;tune&#39;: 50,
                &#39;delay&#39;:0},
            STRNAMES.PRIOR_MEAN_GROWTH: {
                &#39;value_option&#39;: &#39;prior-mean&#39;,
                &#39;loc_option&#39;: &#39;manual&#39;,
                &#39;scale2_option&#39;: &#39;diffuse-linear-regression&#39;,
                &#39;proposal_option&#39;: &#39;auto&#39;,
                &#39;target_acceptance_rate&#39;: 0.44,
                &#39;tune&#39;: 50,
                &#39;end_tune&#39;: &#39;half-burnin&#39;,
                &#39;truncation_settings&#39;: self.GROWTH_TRUNCATION_SETTINGS,
                &#39;delay&#39;:0, &#39;loc&#39;: 1},
            STRNAMES.GROWTH_VALUE: {
                &#39;value_option&#39;: &#39;linear-regression&#39;, #&#39;prior-mean&#39;,
                &#39;truncation_settings&#39;: self.GROWTH_TRUNCATION_SETTINGS,
                &#39;delay&#39;: 0},
            STRNAMES.PRIOR_VAR_SELF_INTERACTIONS: {
                &#39;value_option&#39;: &#39;prior-mean&#39;,
                &#39;scale_option&#39;: &#39;inflated-median&#39;,
                &#39;dof_option&#39;: &#39;diffuse&#39;,
                &#39;proposal_option&#39;: &#39;tight&#39;,
                &#39;target_acceptance_rate&#39;: &#39;optimal&#39;,
                &#39;end_tune&#39;: &#39;half-burnin&#39;,
                &#39;tune&#39;: 50,
                &#39;delay&#39;:0},
            STRNAMES.PRIOR_MEAN_SELF_INTERACTIONS: {
                &#39;value_option&#39;: &#39;prior-mean&#39;,
                &#39;loc_option&#39;: &#39;median-linear-regression&#39;,
                &#39;scale2_option&#39;: &#39;diffuse-linear-regression&#39;,
                &#39;proposal_option&#39;: &#39;auto&#39;,
                &#39;target_acceptance_rate&#39;: 0.44,
                &#39;tune&#39;: 50,
                &#39;end_tune&#39;: &#39;half-burnin&#39;,
                &#39;truncation_settings&#39;: self.SELF_INTERACTIONS_TRUNCATION_SETTINGS,
                &#39;delay&#39;:0},
            STRNAMES.SELF_INTERACTION_VALUE: {
                &#39;value_option&#39;: &#39;linear-regression&#39;,
                &#39;truncation_settings&#39;: self.SELF_INTERACTIONS_TRUNCATION_SETTINGS,
                &#39;delay&#39;: 0},
            STRNAMES.PRIOR_VAR_INTERACTIONS: {
                &#39;value_option&#39;: &#39;auto&#39;,
                &#39;dof_option&#39;: &#39;diffuse&#39;,
                &#39;scale_option&#39;: &#39;same-as-aii&#39;,
                &#39;mean_scaling_factor&#39;: 1,
                &#39;delay&#39;: 0},
            STRNAMES.PRIOR_MEAN_INTERACTIONS: {
                &#39;value_option&#39;: &#39;prior-mean&#39;,
                &#39;loc_option&#39;: &#39;zero&#39;,
                &#39;scale2_option&#39;: &#39;same-as-aii&#39;,
                &#39;delay&#39;:0},
            STRNAMES.CLUSTER_INTERACTION_VALUE: {
                &#39;value_option&#39;: &#39;all-off&#39;,
                &#39;delay&#39;: 0},
            STRNAMES.CLUSTER_INTERACTION_INDICATOR: {
                &#39;delay&#39;:0,
                &#39;run_every_n_iterations&#39;: 1},
            STRNAMES.CLUSTER_INTERACTION_INDICATOR_PROB: {
                &#39;value_option&#39;: &#39;auto&#39;,
                &#39;hyperparam_option&#39;: &#39;strong-sparse&#39;,
                &#39;delay&#39;: 0},
            STRNAMES.FILTERING: {
                &#39;x_value_option&#39;:  &#39;loess&#39;,
                &#39;tune&#39;: (int(self.BURNIN/2), 50),
                &#39;a0&#39;: self.NEGBIN_A0,
                &#39;a1&#39;: self.NEGBIN_A1,
                &#39;v1&#39;: 1e-4,
                &#39;v2&#39;: 1e-4,
                &#39;proposal_init_scale&#39;:.001,
                &#39;intermediate_interpolation&#39;: &#39;linear-interpolation&#39;,
                &#39;intermediate_step&#39;: None, #(&#39;step&#39;, (1, None)), 
                &#39;essential_timepoints&#39;: &#39;union&#39;,
                &#39;delay&#39;: 1,
                &#39;window&#39;: 6,
                &#39;target_acceptance_rate&#39;: 0.44},
            STRNAMES.ZERO_INFLATION: {
                &#39;value_option&#39;: None,
                &#39;delay&#39;: 0},
            STRNAMES.CONCENTRATION: {
                &#39;value_option&#39;: &#39;prior-mean&#39;,
                &#39;hyperparam_option&#39;: &#39;diffuse&#39;,
                &#39;delay&#39;: 0, &#39;n_iter&#39;: 20},
            STRNAMES.CLUSTERING: {
                &#39;value_option&#39;: &#39;spearman&#39;, #&#39;fixed-clustering&#39;,
                &#39;delay&#39;: 2,
                &#39;n_clusters&#39;: 30,
                &#39;run_every_n_iterations&#39;: 4},
            STRNAMES.GLV_PARAMETERS: {
                &#39;update_jointly_pert_inter&#39;: True},
            STRNAMES.PROCESSVAR: {
                &#39;dof_option&#39;: &#39;diffuse&#39;, # &#39;half&#39;, 
                &#39;scale_option&#39;: &#39;med&#39;,
                &#39;value_option&#39;: &#39;prior-mean&#39;,
                &#39;delay&#39;: 0}
        }

        self.INITIALIZATION_ORDER = [
            STRNAMES.FILTERING,
            STRNAMES.ZERO_INFLATION,
            STRNAMES.CONCENTRATION,
            STRNAMES.CLUSTERING,
            STRNAMES.PROCESSVAR,
            STRNAMES.PRIOR_MEAN_GROWTH,
            STRNAMES.PRIOR_VAR_GROWTH,
            STRNAMES.GROWTH_VALUE,
            STRNAMES.PRIOR_MEAN_SELF_INTERACTIONS,
            STRNAMES.PRIOR_VAR_SELF_INTERACTIONS,
            STRNAMES.SELF_INTERACTION_VALUE,
            STRNAMES.PRIOR_MEAN_INTERACTIONS,
            STRNAMES.PRIOR_VAR_INTERACTIONS,
            STRNAMES.CLUSTER_INTERACTION_VALUE,
            STRNAMES.CLUSTER_INTERACTION_INDICATOR,
            STRNAMES.CLUSTER_INTERACTION_INDICATOR_PROB,
            STRNAMES.PRIOR_MEAN_PERT,
            STRNAMES.PRIOR_VAR_PERT,
            STRNAMES.PERT_INDICATOR,
                        STRNAMES.PERT_VALUE,
            STRNAMES.PERT_INDICATOR_PROB,
            STRNAMES.GLV_PARAMETERS,
            STRNAMES.QPCR_SCALES,
            STRNAMES.QPCR_DOFS,
            STRNAMES.QPCR_VARIANCES]

    def suffix(self):
        &#39;&#39;&#39;Create a suffix with the parameters
        &#39;&#39;&#39;
        s = &#39;s{}_b{}_ns{}_lo{}_mo{}&#39;.format(
            self.SEED, self.BURNIN, self.N_SAMPLES, self.LEAVE_OUT,
            self.MAX_N_TAXA)
        return s

    def cv_suffix(self):
        &#39;&#39;&#39;Create a master suffix with the parameters
        &#39;&#39;&#39;
        s = &#39;s{}_b{}_ns{}_mo{}&#39;.format(
            self.SEED, self.BURNIN, self.N_SAMPLES,
            self.MAX_N_TAXA)
        return s

    def cv_single_suffix(self):
        &#39;&#39;&#39;Create a suffix for a single cv round
        &#39;&#39;&#39;
        return &#39;leave_out{}&#39;.format(self.LEAVE_OUT)

    def make_metadata_file(self, fname):
        &#39;&#39;&#39;Make a metadata file that does an overview of the parameters in this class
        &#39;&#39;&#39;

        mystr = &#39;Global parameters\n&#39; \
            &#39;-----------------\n&#39; \
            &#39;Random seed: {seed}\n&#39; \
            &#39;Total number of Gibb steps: {n_samples}\n&#39; \
            &#39;Number of Gibb steps for burn-in: {burnin}\n&#39; \
            &#39;Saved location: {model_path}\n\n&#39; \
            &#39;Negative binomial dispersion parameters\n&#39; \
            &#39;---------------------------------------\n&#39; \
            &#39;a0: {a0:.4E}\n&#39; \
            &#39;a1: {a1:.4E}\n\n&#39; \
            &#39;Parameters learned and their order\n&#39; \
            &#39;----------------------------------\n&#39; \
            &#39;{params_learned}\n\n&#39; \
            &#39;Selected Initialization choices\n&#39; \
            &#39;-------------------------------\n&#39; \
            &#39;Cluster interaction probability prior: {clus_ind_prior}\n&#39; \
            &#39;Perturbation probability prior: {pert_ind_prior}\n&#39; \
            &#39;Filtering initialization: {filt}\n&#39; \
            &#39;Cluster initialization: {clus_init}\n&#39;
        # params learned
        # --------------
        i = 0
        params_learned = &#39;&#39;
        for pname in self.INFERENCE_ORDER:
            if self.LEARN[pname]:
                params_learned += &#39;{}: {}\n&#39;.format(i, pname)
                i += 1
        
        # init choices
        # ------------
        if self.INITIALIZATION_KWARGS[STRNAMES.CLUSTERING][&#39;value_option&#39;] == &#39;spearman&#39;:
            clus_init = &#39;Spearman Correlation, {} clusters&#39;.format(
                self.INITIALIZATION_KWARGS[STRNAMES.CLUSTERING][&#39;n_clusters&#39;])
        elif self.INITIALIZATION_KWARGS[STRNAMES.CLUSTERING][&#39;value_option&#39;] == &#39;no-clusters&#39;:
            clus_init = &#39;No clusters, everything in its own cluster&#39;
        elif self.INITIALIZATION_KWARGS[STRNAMES.CLUSTERING][&#39;value_option&#39;] == &#39;fixed-clustering&#39;:
            clus_init = &#39;Same topology as {}&#39;.format(
                self.INITIALIZATION_KWARGS[STRNAMES.CLUSTERING][&#39;value&#39;])
        elif self.INITIALIZATION_KWARGS[STRNAMES.CLUSTERING][&#39;value_option&#39;] == &#39;manual&#39;:
            clus_init = &#39;Manually with assignments {}&#39;.format(
                self.INITIALIZATION_KWARGS[STRNAMES.CLUSTERING][&#39;value&#39;])
        elif self.INITIALIZATION_KWARGS[STRNAMES.CLUSTERING][&#39;value_option&#39;] == &#39;random&#39;:
            clus_init = &#39;Randomly, with {} clusters&#39;.format(
                self.INITIALIZATION_KWARGS[STRNAMES.CLUSTERING][&#39;n_clusters&#39;])
        elif self.INITIALIZATION_KWARGS[STRNAMES.CLUSTERING][&#39;value_option&#39;] == &#39;taxonomy&#39;:
            clus_init = &#39;By taxonomic similarity, with {} clusters&#39;.format(
                self.INITIALIZATION_KWARGS[STRNAMES.CLUSTERING][&#39;n_clusters&#39;])
        else:
            clus_init = &#39;Something slse&#39;


        f = open(fname, &#39;w&#39;)
        f.write(mystr.format(
            seed=self.SEED, n_samples=self.N_SAMPLES,
            burnin=self.BURNIN, model_path=self.MODEL_PATH,
            a0=self.NEGBIN_A0, a1=self.NEGBIN_A1,
            params_learned=params_learned,
            clus_ind_prior=self.INITIALIZATION_KWARGS[STRNAMES.CLUSTER_INTERACTION_INDICATOR_PROB][&#39;hyperparam_option&#39;],
            pert_ind_prior=self.INITIALIZATION_KWARGS[STRNAMES.PERT_INDICATOR_PROB][&#39;hyperparam_option&#39;],
            filt=self.INITIALIZATION_KWARGS[STRNAMES.FILTERING][&#39;x_value_option&#39;],
            clus_init=clus_init))
        f.close()


class FilteringConfig(pl.Saveable):
    &#39;&#39;&#39;These are the parameters for Filtering

    Consistency filtering
    ----------------------------
    Filters the subjects by looking at the consistency of the counts.
    There must be at least `min_num_counts` for at least
    `min_num_consecutive` consecutive timepoints for at least
    `min_num_subjects` subjects for the  to be classified as valid.

    Parameters
    ----------
    min_num_consec: int
        This is the minimum number of consecutive timepoints that there
        must be at least `min_num_counts`
    threshold : float, int
        This is the minimum number of counts/abudnance/relative abundance that 
        there must be at each consecutive timepoint
    min_num_subjects : int, None
        This is how many subjects this must be true for for the Taxa to be
        valid. If it is None then it only requires one subject.
    colonization_time : float
        How many days we consider colonization (ignore during filtering)
    &#39;&#39;&#39;
    def __init__(self, colonization_time: float, threshold: float, 
        min_num_subj: int, min_num_consec: int):

        self.COLONIZATION_TIME = colonization_time
        self.THRESHOLD = threshold
        self.MIN_NUM_SUBJECTS = min_num_subj
        self.MIN_NUM_CONSECUTIVE = min_num_consec

    def __str__(self):
        return &#39;col{}_thresh{}_subj{}_consec{}&#39;.format(
            self.COLONIZATION_TIME, self.THRESHOLD, self.MIN_NUM_SUBJECTS,
            self.MIN_NUM_CONSECUTIVE)

    def suffix(self):
        return str(self)


class LoggingConfig(pl.Saveable):
    &#39;&#39;&#39;These are the parameters for logging

    FORMAT : str
        This is the logging format for stdout
    LEVEL : logging constant, int
        This is the level to log at for stdout
    NUMPY_PRINTOPTIONS : dict
        These are the printing options for numpy.

    Parameters
    ----------
    basepath : str
        If this is specified, then we also want to log to a file. Set up a
        steam and a file
    fmt : str
        This is the format of the logging prefix for the `logging` module
    level : int
        This is the level of logging to log
    &#39;&#39;&#39;
    def __init__(self, basepath: str=None, level: int=logging.INFO, 
        fmt: str=&#39;%(levelname)s:%(module)s.%(lineno)s: %(message)s&#39;):
        self.FORMAT = fmt
        self.LEVEL = level
        self.NUMPY_PRINTOPTIONS = {
            &#39;threshold&#39;: sys.maxsize, &#39;linewidth&#39;: sys.maxsize}

        if basepath is not None:
            path = os.path.join(basepath, &#39;logging.log&#39;)
            self.PATH = path
            handlers = [
                logging.FileHandler(self.PATH, mode=&#39;w&#39;),
                logging.StreamHandler()]
            for handler in logging.root.handlers[:]:
                logging.root.removeHandler(handler)
            logging.basicConfig(level=self.LEVEL, format=self.FORMAT, handlers=handlers)
        else:
            self.PATH = None
            logging.basicConfig(format=self.FORMAT, level=self.LEVEL)
        
        np.set_printoptions(**self.NUMPY_PRINTOPTIONS)
        pd.set_option(&#39;display.max_columns&#39;, None)


class NegBinConfig(_BaseModelConfig):
    &#39;&#39;&#39;Configuration class for learning the negative binomial dispersion
    parameters. Note that these parameters are learned offline.

    System initialization
    ---------------------
    - OUTPUT_BASEPATH : str
        - Path to save the model
    - SEED : int
        - Seed to initialize inference with
    - BURNIN : int
        - Number of initial Gibb steps to throw away
    - N_SAMPLES : int
        - Total number of Gibb steps
    - CHECKPOINT : int
        - How often to write to disk
    - MP_FILTERING : str
        - How to do multiprocessing for filtering
    - LEARN : Dict[str, bool]
        - These are the dictionary of parameters which we are learning in the model.
        - If the name maps to True, then we learn it during inference. If it maps to 
          false, then we do not update its value duting inference.
    - INFERENCE_ORDER : list
        - This is the order to update the parameters during MCMC inference
    - INITIALIZATION_KWARGS : Dict[str, Dict[str, Any]]
        - These are the parameters to send into the `initialize` function for each variable
          that we are learning
    - INITIALIZATION_ORDER : list
        - This is the order to initialize the variables

    Parameters
    ----------
    seed : int
        Seed to start the inderence
    burnin, n_samples : int
        How many iterations for burn-in and total samples, respectively.
    checkpoint : int
        How often to write the trace in RAM to disk. Note that this must be
        a multiple of both `burnin` and `n_samples`
    basepath : str
        This is the basepath to save the graph. A separate folder within
        `basepath` will be created for the specific graph.
    synth : bool
        If True, run with the synthetic data, where the parameters needed
        to learn are `SYNTHETIC_A0` AND `SYNTHETIC_A1`.
    &#39;&#39;&#39;

    def __init__(self, seed: int, burnin: int, n_samples: int, checkpoint: int, 
        basepath: str):
        if basepath[-1] != &#39;/&#39;:
            basepath += &#39;/&#39;

        self.SEED = seed
        self.OUTPUT_BASEPATH = os.path.abspath(basepath)
        self.MODEL_PATH = self.OUTPUT_BASEPATH
        self.BURNIN = burnin
        self.N_SAMPLES = n_samples
        self.CHECKPOINT = checkpoint
        self.MP_FILTERING = &#39;debug&#39;

        self.INFERENCE_ORDER = [
            STRNAMES.NEGBIN_A0,
            STRNAMES.NEGBIN_A1,
            STRNAMES.FILTERING]

        self.LEARN = {
            STRNAMES.NEGBIN_A0: True,
            STRNAMES.NEGBIN_A1: True,
            STRNAMES.FILTERING: True}

        self.INITIALIZATION_ORDER = [
            STRNAMES.FILTERING,
            STRNAMES.NEGBIN_A0,
            STRNAMES.NEGBIN_A1]

        self.INITIALIZATION_KWARGS = {
            STRNAMES.NEGBIN_A0: {
                &#39;value&#39;: 1e-10,
                &#39;truncation_settings&#39;: (0, 1e5),
                &#39;tune&#39;: 50,
                &#39;end_tune&#39;: int(self.BURNIN/2),
                &#39;target_acceptance_rate&#39;: &#39;optimal&#39;, 
                &#39;proposal_option&#39;: &#39;auto&#39;,
                &#39;delay&#39;: 0},
            STRNAMES.NEGBIN_A1: {
                &#39;value&#39;: 0.1,
                &#39;tune&#39;: 50,
                &#39;truncation_settings&#39;: (0, 1e5),
                &#39;end_tune&#39;: int(self.BURNIN/2),
                &#39;target_acceptance_rate&#39;: &#39;optimal&#39;, 
                &#39;proposal_option&#39;: &#39;auto&#39;,
                &#39;delay&#39;: 0},
            STRNAMES.FILTERING: {
                &#39;tune&#39;: 50,
                &#39;end_tune&#39;: int(self.BURNIN/2),
                &#39;target_acceptance_rate&#39;: &#39;optimal&#39;, 
                &#39;qpcr_variance_inflation&#39;: 100,
                &#39;delay&#39;: 100}}

    def suffix(self):
        return &#39;seed{}_nb{}_ns{}&#39;.format(self.SEED, self.BURNIN, self.N_SAMPLES)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="mdsine2.config.isModelConfig"><code class="name flex">
<span>def <span class="ident">isModelConfig</span></span>(<span>x)</span>
</code></dt>
<dd>
<div class="desc"><p>Checks if the input array is a model config object</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>any</code></dt>
<dd>Instance we are checking</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>bool</code></dt>
<dd>True if <code>x</code> is a a model config object</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def isModelConfig(x):
    &#39;&#39;&#39;Checks if the input array is a model config object

    Parameters
    ----------
    x : any
        Instance we are checking

    Returns
    -------
    bool
        True if `x` is a a model config object
    &#39;&#39;&#39;
    return x is not None and issubclass(x.__class__, _BaseModelConfig)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="mdsine2.config.FilteringConfig"><code class="flex name class">
<span>class <span class="ident">FilteringConfig</span></span>
<span>(</span><span>colonization_time:Â float, threshold:Â float, min_num_subj:Â int, min_num_consec:Â int)</span>
</code></dt>
<dd>
<div class="desc"><p>These are the parameters for Filtering</p>
<h2 id="consistency-filtering">Consistency Filtering</h2>
<p>Filters the subjects by looking at the consistency of the counts.
There must be at least <code>min_num_counts</code> for at least
<code>min_num_consecutive</code> consecutive timepoints for at least
<code>min_num_subjects</code> subjects for the
to be classified as valid.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>min_num_consec</code></strong> :&ensp;<code>int</code></dt>
<dd>This is the minimum number of consecutive timepoints that there
must be at least <code>min_num_counts</code></dd>
<dt><strong><code>threshold</code></strong> :&ensp;<code>float, int</code></dt>
<dd>This is the minimum number of counts/abudnance/relative abundance that
there must be at each consecutive timepoint</dd>
<dt><strong><code>min_num_subjects</code></strong> :&ensp;<code>int, None</code></dt>
<dd>This is how many subjects this must be true for for the Taxa to be
valid. If it is None then it only requires one subject.</dd>
<dt><strong><code>colonization_time</code></strong> :&ensp;<code>float</code></dt>
<dd>How many days we consider colonization (ignore during filtering)</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class FilteringConfig(pl.Saveable):
    &#39;&#39;&#39;These are the parameters for Filtering

    Consistency filtering
    ----------------------------
    Filters the subjects by looking at the consistency of the counts.
    There must be at least `min_num_counts` for at least
    `min_num_consecutive` consecutive timepoints for at least
    `min_num_subjects` subjects for the  to be classified as valid.

    Parameters
    ----------
    min_num_consec: int
        This is the minimum number of consecutive timepoints that there
        must be at least `min_num_counts`
    threshold : float, int
        This is the minimum number of counts/abudnance/relative abundance that 
        there must be at each consecutive timepoint
    min_num_subjects : int, None
        This is how many subjects this must be true for for the Taxa to be
        valid. If it is None then it only requires one subject.
    colonization_time : float
        How many days we consider colonization (ignore during filtering)
    &#39;&#39;&#39;
    def __init__(self, colonization_time: float, threshold: float, 
        min_num_subj: int, min_num_consec: int):

        self.COLONIZATION_TIME = colonization_time
        self.THRESHOLD = threshold
        self.MIN_NUM_SUBJECTS = min_num_subj
        self.MIN_NUM_CONSECUTIVE = min_num_consec

    def __str__(self):
        return &#39;col{}_thresh{}_subj{}_consec{}&#39;.format(
            self.COLONIZATION_TIME, self.THRESHOLD, self.MIN_NUM_SUBJECTS,
            self.MIN_NUM_CONSECUTIVE)

    def suffix(self):
        return str(self)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="mdsine2.pylab.base.Saveable" href="pylab/base.html#mdsine2.pylab.base.Saveable">Saveable</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="mdsine2.config.FilteringConfig.suffix"><code class="name flex">
<span>def <span class="ident">suffix</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def suffix(self):
    return str(self)</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="mdsine2.pylab.base.Saveable" href="pylab/base.html#mdsine2.pylab.base.Saveable">Saveable</a></b></code>:
<ul class="hlist">
<li><code><a title="mdsine2.pylab.base.Saveable.load" href="pylab/base.html#mdsine2.pylab.base.Saveable.load">load</a></code></li>
<li><code><a title="mdsine2.pylab.base.Saveable.save" href="pylab/base.html#mdsine2.pylab.base.Saveable.save">save</a></code></li>
<li><code><a title="mdsine2.pylab.base.Saveable.set_save_location" href="pylab/base.html#mdsine2.pylab.base.Saveable.set_save_location">set_save_location</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="mdsine2.config.LoggingConfig"><code class="flex name class">
<span>class <span class="ident">LoggingConfig</span></span>
<span>(</span><span>basepath:Â strÂ =Â None, level:Â intÂ =Â 20, fmt:Â strÂ =Â '%(levelname)s:%(module)s.%(lineno)s: %(message)s')</span>
</code></dt>
<dd>
<div class="desc"><p>These are the parameters for logging</p>
<p>FORMAT : str
This is the logging format for stdout
LEVEL : logging constant, int
This is the level to log at for stdout
NUMPY_PRINTOPTIONS : dict
These are the printing options for numpy.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>basepath</code></strong> :&ensp;<code>str</code></dt>
<dd>If this is specified, then we also want to log to a file. Set up a
steam and a file</dd>
<dt><strong><code>fmt</code></strong> :&ensp;<code>str</code></dt>
<dd>This is the format of the logging prefix for the <code>logging</code> module</dd>
<dt><strong><code>level</code></strong> :&ensp;<code>int</code></dt>
<dd>This is the level of logging to log</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LoggingConfig(pl.Saveable):
    &#39;&#39;&#39;These are the parameters for logging

    FORMAT : str
        This is the logging format for stdout
    LEVEL : logging constant, int
        This is the level to log at for stdout
    NUMPY_PRINTOPTIONS : dict
        These are the printing options for numpy.

    Parameters
    ----------
    basepath : str
        If this is specified, then we also want to log to a file. Set up a
        steam and a file
    fmt : str
        This is the format of the logging prefix for the `logging` module
    level : int
        This is the level of logging to log
    &#39;&#39;&#39;
    def __init__(self, basepath: str=None, level: int=logging.INFO, 
        fmt: str=&#39;%(levelname)s:%(module)s.%(lineno)s: %(message)s&#39;):
        self.FORMAT = fmt
        self.LEVEL = level
        self.NUMPY_PRINTOPTIONS = {
            &#39;threshold&#39;: sys.maxsize, &#39;linewidth&#39;: sys.maxsize}

        if basepath is not None:
            path = os.path.join(basepath, &#39;logging.log&#39;)
            self.PATH = path
            handlers = [
                logging.FileHandler(self.PATH, mode=&#39;w&#39;),
                logging.StreamHandler()]
            for handler in logging.root.handlers[:]:
                logging.root.removeHandler(handler)
            logging.basicConfig(level=self.LEVEL, format=self.FORMAT, handlers=handlers)
        else:
            self.PATH = None
            logging.basicConfig(format=self.FORMAT, level=self.LEVEL)
        
        np.set_printoptions(**self.NUMPY_PRINTOPTIONS)
        pd.set_option(&#39;display.max_columns&#39;, None)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="mdsine2.pylab.base.Saveable" href="pylab/base.html#mdsine2.pylab.base.Saveable">Saveable</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="mdsine2.pylab.base.Saveable" href="pylab/base.html#mdsine2.pylab.base.Saveable">Saveable</a></b></code>:
<ul class="hlist">
<li><code><a title="mdsine2.pylab.base.Saveable.load" href="pylab/base.html#mdsine2.pylab.base.Saveable.load">load</a></code></li>
<li><code><a title="mdsine2.pylab.base.Saveable.save" href="pylab/base.html#mdsine2.pylab.base.Saveable.save">save</a></code></li>
<li><code><a title="mdsine2.pylab.base.Saveable.set_save_location" href="pylab/base.html#mdsine2.pylab.base.Saveable.set_save_location">set_save_location</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="mdsine2.config.MDSINE2ModelConfig"><code class="flex name class">
<span>class <span class="ident">MDSINE2ModelConfig</span></span>
<span>(</span><span>basepath:Â str, seed:Â int, burnin:Â int, n_samples:Â int, negbin_a0:Â float, negbin_a1:Â float, leave_out:Â strÂ =Â None, checkpoint:Â intÂ =Â 100)</span>
</code></dt>
<dd>
<div class="desc"><p>Configuration parameters for the model</p>
<h2 id="system-initialization">System Initialization</h2>
<ul>
<li>OUTPUT_BASEPATH, MODEL_PATH : str<ul>
<li>Path to save the model</li>
</ul>
</li>
<li>SEED : int<ul>
<li>Seed to initialize inference with</li>
</ul>
</li>
<li>BURNIN : int<ul>
<li>Number of initial Gibb steps to throw away</li>
</ul>
</li>
<li>N_SAMPLES : int<ul>
<li>Total number of Gibb steps</li>
</ul>
</li>
<li>CHECKPOINT : int<ul>
<li>How often to write to disk</li>
</ul>
</li>
<li>PROCESS_VARIANCE_TYPE : str<ul>
<li>What type of process variance to do</li>
<li>NOTE: There is only one option, do not change</li>
</ul>
</li>
<li>DATA_DTYPE : str<ul>
<li>What kind of data to do inference we</li>
<li>NOTE: Model assume you are using absolute abundance, do not change</li>
</ul>
</li>
<li>QPCR_NORMALIZATION_MAX_VALUE : float<ul>
<li>This is value to set the largest qPCR value to. Normalize
all other qPCR measurements directly to this</li>
</ul>
</li>
<li>LEAVE_OUT : str<ul>
<li>Which subject to leave out, if necessary</li>
</ul>
</li>
<li>ZERO_INFLATION_TRANSITION_POLICY : str<ul>
<li>What type of zero inflation to do. Do not change</li>
</ul>
</li>
<li>GROWTH_TRUNCATION_SETTINGS : str<ul>
<li>How to initialize the truncation settings for the growth parameters</li>
</ul>
</li>
<li>SELF_INTERACTIONS_TRUNCATION_SETTINGS : str<ul>
<li>How to initialize the truncation settings for the self-interaction parameters</li>
</ul>
</li>
<li>MP_FILTERING : str<ul>
<li>How to do multiprocessing for filtering</li>
</ul>
</li>
<li>MP_CLUSTERING : str<ul>
<li>How to do multiprocessing for clustering</li>
</ul>
</li>
<li>NEGBINB_A0, NEGBIN_A1 : float<ul>
<li>Negative binomial dispersion parameters</li>
</ul>
</li>
<li>N_QPCR_BUCKETS : int<ul>
<li>Number of qPCR buckets. This is not learned in the model, do not change.</li>
</ul>
</li>
<li>INTERMEDIATE_VALIDATION_T : float<ul>
<li>How often to do the intermediate validation</li>
</ul>
</li>
<li>INTERMEDIATE_VALIDATION_KWARGS : dict<ul>
<li>Arguemnts for the intermediate valudation</li>
</ul>
</li>
<li>LEARN : Dict[str, bool]<ul>
<li>These are the dictionary of parameters which we are learning in the model.</li>
<li>If the name maps to True, then we learn it during inference. If it maps to
false, then we do not update its value duting inference.</li>
</ul>
</li>
<li>INFERENCE_ORDER : list<ul>
<li>This is the order to update the parameters during MCMC inference</li>
</ul>
</li>
<li>INITIALIZATION_KWARGS : Dict[str, Dict[str, Any]]<ul>
<li>These are the parameters to send into the <code>initialize</code> function for each variable
that we are learning</li>
</ul>
</li>
<li>INITIALIZATION_ORDER : list<ul>
<li>This is the order to initialize the variables</li>
</ul>
</li>
</ul>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>basepath</code></strong> :&ensp;<code>str</code></dt>
<dd>This is the base path to save the inference</dd>
<dt><strong><code>seed</code></strong> :&ensp;<code>int</code></dt>
<dd>This is the seed to start the inference with</dd>
<dt><strong><code>burnin</code></strong> :&ensp;<code>int</code></dt>
<dd>This is how many gibb steps to throw away originally</dd>
<dt><strong><code>n_samples</code></strong> :&ensp;<code>int</code></dt>
<dd>This is the total number of Gibb steps to do for inference</dd>
<dt><strong><code>negbin_a0</code></strong>, <strong><code>negbin_a1</code></strong> :&ensp;<code>float</code></dt>
<dd>This is the negative binomial dispersion parameters</dd>
<dt><strong><code>leave_out</code></strong> :&ensp;<code>str</code></dt>
<dd>This is the subject to leave out, if necesssary</dd>
<dt><strong><code>checkpoint</code></strong> :&ensp;<code>int</code></dt>
<dd>This is how often we should write to disk</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MDSINE2ModelConfig(_BaseModelConfig):
    &#39;&#39;&#39;Configuration parameters for the model


    System initialization
    ---------------------
    - OUTPUT_BASEPATH, MODEL_PATH : str
        - Path to save the model
    - SEED : int
        - Seed to initialize inference with
    - BURNIN : int
        - Number of initial Gibb steps to throw away
    - N_SAMPLES : int
        - Total number of Gibb steps
    - CHECKPOINT : int
        - How often to write to disk
    - PROCESS_VARIANCE_TYPE : str
        - What type of process variance to do
        - NOTE: There is only one option, do not change
    - DATA_DTYPE : str
        - What kind of data to do inference we
        - NOTE: Model assume you are using absolute abundance, do not change
    - QPCR_NORMALIZATION_MAX_VALUE : float
        - This is value to set the largest qPCR value to. Normalize
        all other qPCR measurements directly to this
    - LEAVE_OUT : str
        - Which subject to leave out, if necessary
    - ZERO_INFLATION_TRANSITION_POLICY : str
        - What type of zero inflation to do. Do not change
    - GROWTH_TRUNCATION_SETTINGS : str
        - How to initialize the truncation settings for the growth parameters
    - SELF_INTERACTIONS_TRUNCATION_SETTINGS : str
        - How to initialize the truncation settings for the self-interaction parameters
    - MP_FILTERING : str
        - How to do multiprocessing for filtering
    - MP_CLUSTERING : str
        - How to do multiprocessing for clustering
    - NEGBINB_A0, NEGBIN_A1 : float
        - Negative binomial dispersion parameters
    - N_QPCR_BUCKETS : int
        - Number of qPCR buckets. This is not learned in the model, do not change.
    - INTERMEDIATE_VALIDATION_T : float
        - How often to do the intermediate validation
    - INTERMEDIATE_VALIDATION_KWARGS : dict
        - Arguemnts for the intermediate valudation
    - LEARN : Dict[str, bool]
        - These are the dictionary of parameters which we are learning in the model.
        - If the name maps to True, then we learn it during inference. If it maps to 
          false, then we do not update its value duting inference.
    - INFERENCE_ORDER : list
        - This is the order to update the parameters during MCMC inference
    - INITIALIZATION_KWARGS : Dict[str, Dict[str, Any]]
        - These are the parameters to send into the `initialize` function for each variable
          that we are learning
    - INITIALIZATION_ORDER : list
        - This is the order to initialize the variables

    Parameters
    ----------
    basepath : str
        This is the base path to save the inference
    seed : int
        This is the seed to start the inference with
    burnin : int
        This is how many gibb steps to throw away originally
    n_samples : int
        This is the total number of Gibb steps to do for inference
    negbin_a0, negbin_a1 : float
        This is the negative binomial dispersion parameters
    leave_out : str
        This is the subject to leave out, if necesssary
    checkpoint : int
        This is how often we should write to disk
    &#39;&#39;&#39;
    def __init__(self, basepath: str, seed: int, burnin: int, n_samples: int,
        negbin_a0: float, negbin_a1: float, leave_out: str=None,
        checkpoint: int=100):
        self.OUTPUT_BASEPATH = os.path.abspath(basepath)
        self.MODEL_PATH = self.OUTPUT_BASEPATH
        self.SEED = seed
        self.BURNIN = burnin
        self.N_SAMPLES = n_samples
        self.CHECKPOINT = checkpoint
        self.PROCESS_VARIANCE_TYPE = &#39;multiplicative-global&#39;
        self.DATA_DTYPE = &#39;abs&#39;

        self.QPCR_NORMALIZATION_MAX_VALUE = 100
        self.LEAVE_OUT = leave_out
        self.ZERO_INFLATION_TRANSITION_POLICY = None #&#39;ignore&#39;

        self.GROWTH_TRUNCATION_SETTINGS = &#39;positive&#39;
        self.SELF_INTERACTIONS_TRUNCATION_SETTINGS = &#39;positive&#39;

        self.MP_FILTERING = &#39;debug&#39;
        self.MP_CLUSTERING = &#39;debug&#39;

        self.NEGBIN_A0 = negbin_a0
        self.NEGBIN_A1 = negbin_a1
        self.N_QPCR_BUCKETS = 3

        self.INTERMEDIATE_VALIDATION_T = 1 * 3600 # Every hour
        self.INTERMEDIATE_VALIDATION_KWARGS = None

        self.LEARN = {
            STRNAMES.GLV_PARAMETERS: True,
            STRNAMES.PROCESSVAR: True,
            STRNAMES.PRIOR_VAR_GROWTH: False,
            STRNAMES.PRIOR_VAR_SELF_INTERACTIONS: False,
            STRNAMES.PRIOR_VAR_INTERACTIONS: True,
            STRNAMES.PRIOR_VAR_PERT: True,
            STRNAMES.PRIOR_MEAN_GROWTH: True,
            STRNAMES.PRIOR_MEAN_SELF_INTERACTIONS: True,
            STRNAMES.PRIOR_MEAN_INTERACTIONS: True,
            STRNAMES.PRIOR_MEAN_PERT: True,
            STRNAMES.FILTERING: True,
            STRNAMES.ZERO_INFLATION: False,
            STRNAMES.CLUSTERING: True,
            STRNAMES.CONCENTRATION: True, 
            STRNAMES.CLUSTER_INTERACTION_INDICATOR: True,
            STRNAMES.CLUSTER_INTERACTION_INDICATOR_PROB: True,
            STRNAMES.PERT_INDICATOR: True,
            STRNAMES.PERT_INDICATOR_PROB: True,
            STRNAMES.QPCR_SCALES: False,
            STRNAMES.QPCR_DOFS: False,
            STRNAMES.QPCR_VARIANCES: False}

        self.INFERENCE_ORDER = [
            STRNAMES.CLUSTER_INTERACTION_INDICATOR,
            STRNAMES.CLUSTER_INTERACTION_INDICATOR_PROB,
            STRNAMES.PERT_INDICATOR,
            STRNAMES.PERT_INDICATOR_PROB,
            STRNAMES.GLV_PARAMETERS,
            STRNAMES.PRIOR_MEAN_INTERACTIONS,
            STRNAMES.PRIOR_MEAN_PERT,
            STRNAMES.PRIOR_MEAN_GROWTH,
            STRNAMES.PRIOR_MEAN_SELF_INTERACTIONS,
            STRNAMES.PRIOR_VAR_GROWTH,
            STRNAMES.PRIOR_VAR_SELF_INTERACTIONS,
            STRNAMES.PRIOR_VAR_INTERACTIONS,
            STRNAMES.PRIOR_VAR_PERT,
            STRNAMES.PROCESSVAR,
            STRNAMES.ZERO_INFLATION,
            STRNAMES.QPCR_SCALES,
            STRNAMES.QPCR_DOFS,
            STRNAMES.QPCR_VARIANCES,
            STRNAMES.FILTERING,
            STRNAMES.CLUSTERING,
            STRNAMES.CONCENTRATION]

        self.INITIALIZATION_KWARGS = {
            STRNAMES.QPCR_VARIANCES: {
                &#39;value_option&#39;: &#39;empirical&#39;},
            STRNAMES.QPCR_SCALES: {
                &#39;value_option&#39;: &#39;prior-mean&#39;,
                &#39;scale_option&#39;: &#39;empirical&#39;,
                &#39;dof_option&#39;: &#39;diffuse&#39;,
                &#39;proposal_option&#39;: &#39;auto&#39;,
                &#39;target_acceptance_rate&#39;: &#39;optimal&#39;,
                &#39;end_tune&#39;: &#39;half-burnin&#39;,
                &#39;tune&#39;: 50,
                &#39;delay&#39;:0},
            STRNAMES.QPCR_DOFS: {
                &#39;value_option&#39;: &#39;diffuse&#39;,
                &#39;low_option&#39;: &#39;valid&#39;,
                &#39;high_option&#39;: &#39;med&#39;,
                &#39;proposal_option&#39;: &#39;auto&#39;,
                &#39;target_acceptance_rate&#39;: &#39;optimal&#39;,
                &#39;end_tune&#39;: &#39;half-burnin&#39;,
                &#39;tune&#39;: 50,
                &#39;delay&#39;: 0},
            STRNAMES.PERT_VALUE: {
                &#39;value_option&#39;: &#39;prior-mean&#39;,
                &#39;delay&#39;:0},
            STRNAMES.PERT_INDICATOR_PROB: {
                &#39;value_option&#39;: &#39;prior-mean&#39;,
                &#39;hyperparam_option&#39;: &#39;strong-sparse&#39;,
                &#39;delay&#39;:0},
            STRNAMES.PERT_INDICATOR: {
                &#39;value_option&#39;: &#39;all-off&#39;,
                &#39;delay&#39;:0},
            STRNAMES.PRIOR_VAR_PERT: {
                &#39;value_option&#39;: &#39;prior-mean&#39;,
                &#39;scale_option&#39;: &#39;diffuse&#39;,
                &#39;dof_option&#39;: &#39;diffuse&#39;,
                &#39;delay&#39;: 0},
            STRNAMES.PRIOR_MEAN_PERT: {
                &#39;value_option&#39;: &#39;prior-mean&#39;,
                &#39;loc_option&#39;: &#39;zero&#39;,
                &#39;scale2_option&#39;: &#39;diffuse&#39;,
                &#39;delay&#39;:0},
            STRNAMES.PRIOR_VAR_GROWTH: {
                &#39;value_option&#39;: &#39;prior-mean&#39;,
                &#39;scale_option&#39;: &#39;inflated-median&#39;,
                &#39;dof_option&#39;: &#39;diffuse&#39;,
                &#39;proposal_option&#39;: &#39;tight&#39;,
                &#39;target_acceptance_rate&#39;: &#39;optimal&#39;,
                &#39;end_tune&#39;: &#39;half-burnin&#39;,
                &#39;tune&#39;: 50,
                &#39;delay&#39;:0},
            STRNAMES.PRIOR_MEAN_GROWTH: {
                &#39;value_option&#39;: &#39;prior-mean&#39;,
                &#39;loc_option&#39;: &#39;manual&#39;,
                &#39;scale2_option&#39;: &#39;diffuse-linear-regression&#39;,
                &#39;proposal_option&#39;: &#39;auto&#39;,
                &#39;target_acceptance_rate&#39;: 0.44,
                &#39;tune&#39;: 50,
                &#39;end_tune&#39;: &#39;half-burnin&#39;,
                &#39;truncation_settings&#39;: self.GROWTH_TRUNCATION_SETTINGS,
                &#39;delay&#39;:0, &#39;loc&#39;: 1},
            STRNAMES.GROWTH_VALUE: {
                &#39;value_option&#39;: &#39;linear-regression&#39;, #&#39;prior-mean&#39;,
                &#39;truncation_settings&#39;: self.GROWTH_TRUNCATION_SETTINGS,
                &#39;delay&#39;: 0},
            STRNAMES.PRIOR_VAR_SELF_INTERACTIONS: {
                &#39;value_option&#39;: &#39;prior-mean&#39;,
                &#39;scale_option&#39;: &#39;inflated-median&#39;,
                &#39;dof_option&#39;: &#39;diffuse&#39;,
                &#39;proposal_option&#39;: &#39;tight&#39;,
                &#39;target_acceptance_rate&#39;: &#39;optimal&#39;,
                &#39;end_tune&#39;: &#39;half-burnin&#39;,
                &#39;tune&#39;: 50,
                &#39;delay&#39;:0},
            STRNAMES.PRIOR_MEAN_SELF_INTERACTIONS: {
                &#39;value_option&#39;: &#39;prior-mean&#39;,
                &#39;loc_option&#39;: &#39;median-linear-regression&#39;,
                &#39;scale2_option&#39;: &#39;diffuse-linear-regression&#39;,
                &#39;proposal_option&#39;: &#39;auto&#39;,
                &#39;target_acceptance_rate&#39;: 0.44,
                &#39;tune&#39;: 50,
                &#39;end_tune&#39;: &#39;half-burnin&#39;,
                &#39;truncation_settings&#39;: self.SELF_INTERACTIONS_TRUNCATION_SETTINGS,
                &#39;delay&#39;:0},
            STRNAMES.SELF_INTERACTION_VALUE: {
                &#39;value_option&#39;: &#39;linear-regression&#39;,
                &#39;truncation_settings&#39;: self.SELF_INTERACTIONS_TRUNCATION_SETTINGS,
                &#39;delay&#39;: 0},
            STRNAMES.PRIOR_VAR_INTERACTIONS: {
                &#39;value_option&#39;: &#39;auto&#39;,
                &#39;dof_option&#39;: &#39;diffuse&#39;,
                &#39;scale_option&#39;: &#39;same-as-aii&#39;,
                &#39;mean_scaling_factor&#39;: 1,
                &#39;delay&#39;: 0},
            STRNAMES.PRIOR_MEAN_INTERACTIONS: {
                &#39;value_option&#39;: &#39;prior-mean&#39;,
                &#39;loc_option&#39;: &#39;zero&#39;,
                &#39;scale2_option&#39;: &#39;same-as-aii&#39;,
                &#39;delay&#39;:0},
            STRNAMES.CLUSTER_INTERACTION_VALUE: {
                &#39;value_option&#39;: &#39;all-off&#39;,
                &#39;delay&#39;: 0},
            STRNAMES.CLUSTER_INTERACTION_INDICATOR: {
                &#39;delay&#39;:0,
                &#39;run_every_n_iterations&#39;: 1},
            STRNAMES.CLUSTER_INTERACTION_INDICATOR_PROB: {
                &#39;value_option&#39;: &#39;auto&#39;,
                &#39;hyperparam_option&#39;: &#39;strong-sparse&#39;,
                &#39;delay&#39;: 0},
            STRNAMES.FILTERING: {
                &#39;x_value_option&#39;:  &#39;loess&#39;,
                &#39;tune&#39;: (int(self.BURNIN/2), 50),
                &#39;a0&#39;: self.NEGBIN_A0,
                &#39;a1&#39;: self.NEGBIN_A1,
                &#39;v1&#39;: 1e-4,
                &#39;v2&#39;: 1e-4,
                &#39;proposal_init_scale&#39;:.001,
                &#39;intermediate_interpolation&#39;: &#39;linear-interpolation&#39;,
                &#39;intermediate_step&#39;: None, #(&#39;step&#39;, (1, None)), 
                &#39;essential_timepoints&#39;: &#39;union&#39;,
                &#39;delay&#39;: 1,
                &#39;window&#39;: 6,
                &#39;target_acceptance_rate&#39;: 0.44},
            STRNAMES.ZERO_INFLATION: {
                &#39;value_option&#39;: None,
                &#39;delay&#39;: 0},
            STRNAMES.CONCENTRATION: {
                &#39;value_option&#39;: &#39;prior-mean&#39;,
                &#39;hyperparam_option&#39;: &#39;diffuse&#39;,
                &#39;delay&#39;: 0, &#39;n_iter&#39;: 20},
            STRNAMES.CLUSTERING: {
                &#39;value_option&#39;: &#39;spearman&#39;, #&#39;fixed-clustering&#39;,
                &#39;delay&#39;: 2,
                &#39;n_clusters&#39;: 30,
                &#39;run_every_n_iterations&#39;: 4},
            STRNAMES.GLV_PARAMETERS: {
                &#39;update_jointly_pert_inter&#39;: True},
            STRNAMES.PROCESSVAR: {
                &#39;dof_option&#39;: &#39;diffuse&#39;, # &#39;half&#39;, 
                &#39;scale_option&#39;: &#39;med&#39;,
                &#39;value_option&#39;: &#39;prior-mean&#39;,
                &#39;delay&#39;: 0}
        }

        self.INITIALIZATION_ORDER = [
            STRNAMES.FILTERING,
            STRNAMES.ZERO_INFLATION,
            STRNAMES.CONCENTRATION,
            STRNAMES.CLUSTERING,
            STRNAMES.PROCESSVAR,
            STRNAMES.PRIOR_MEAN_GROWTH,
            STRNAMES.PRIOR_VAR_GROWTH,
            STRNAMES.GROWTH_VALUE,
            STRNAMES.PRIOR_MEAN_SELF_INTERACTIONS,
            STRNAMES.PRIOR_VAR_SELF_INTERACTIONS,
            STRNAMES.SELF_INTERACTION_VALUE,
            STRNAMES.PRIOR_MEAN_INTERACTIONS,
            STRNAMES.PRIOR_VAR_INTERACTIONS,
            STRNAMES.CLUSTER_INTERACTION_VALUE,
            STRNAMES.CLUSTER_INTERACTION_INDICATOR,
            STRNAMES.CLUSTER_INTERACTION_INDICATOR_PROB,
            STRNAMES.PRIOR_MEAN_PERT,
            STRNAMES.PRIOR_VAR_PERT,
            STRNAMES.PERT_INDICATOR,
                        STRNAMES.PERT_VALUE,
            STRNAMES.PERT_INDICATOR_PROB,
            STRNAMES.GLV_PARAMETERS,
            STRNAMES.QPCR_SCALES,
            STRNAMES.QPCR_DOFS,
            STRNAMES.QPCR_VARIANCES]

    def suffix(self):
        &#39;&#39;&#39;Create a suffix with the parameters
        &#39;&#39;&#39;
        s = &#39;s{}_b{}_ns{}_lo{}_mo{}&#39;.format(
            self.SEED, self.BURNIN, self.N_SAMPLES, self.LEAVE_OUT,
            self.MAX_N_TAXA)
        return s

    def cv_suffix(self):
        &#39;&#39;&#39;Create a master suffix with the parameters
        &#39;&#39;&#39;
        s = &#39;s{}_b{}_ns{}_mo{}&#39;.format(
            self.SEED, self.BURNIN, self.N_SAMPLES,
            self.MAX_N_TAXA)
        return s

    def cv_single_suffix(self):
        &#39;&#39;&#39;Create a suffix for a single cv round
        &#39;&#39;&#39;
        return &#39;leave_out{}&#39;.format(self.LEAVE_OUT)

    def make_metadata_file(self, fname):
        &#39;&#39;&#39;Make a metadata file that does an overview of the parameters in this class
        &#39;&#39;&#39;

        mystr = &#39;Global parameters\n&#39; \
            &#39;-----------------\n&#39; \
            &#39;Random seed: {seed}\n&#39; \
            &#39;Total number of Gibb steps: {n_samples}\n&#39; \
            &#39;Number of Gibb steps for burn-in: {burnin}\n&#39; \
            &#39;Saved location: {model_path}\n\n&#39; \
            &#39;Negative binomial dispersion parameters\n&#39; \
            &#39;---------------------------------------\n&#39; \
            &#39;a0: {a0:.4E}\n&#39; \
            &#39;a1: {a1:.4E}\n\n&#39; \
            &#39;Parameters learned and their order\n&#39; \
            &#39;----------------------------------\n&#39; \
            &#39;{params_learned}\n\n&#39; \
            &#39;Selected Initialization choices\n&#39; \
            &#39;-------------------------------\n&#39; \
            &#39;Cluster interaction probability prior: {clus_ind_prior}\n&#39; \
            &#39;Perturbation probability prior: {pert_ind_prior}\n&#39; \
            &#39;Filtering initialization: {filt}\n&#39; \
            &#39;Cluster initialization: {clus_init}\n&#39;
        # params learned
        # --------------
        i = 0
        params_learned = &#39;&#39;
        for pname in self.INFERENCE_ORDER:
            if self.LEARN[pname]:
                params_learned += &#39;{}: {}\n&#39;.format(i, pname)
                i += 1
        
        # init choices
        # ------------
        if self.INITIALIZATION_KWARGS[STRNAMES.CLUSTERING][&#39;value_option&#39;] == &#39;spearman&#39;:
            clus_init = &#39;Spearman Correlation, {} clusters&#39;.format(
                self.INITIALIZATION_KWARGS[STRNAMES.CLUSTERING][&#39;n_clusters&#39;])
        elif self.INITIALIZATION_KWARGS[STRNAMES.CLUSTERING][&#39;value_option&#39;] == &#39;no-clusters&#39;:
            clus_init = &#39;No clusters, everything in its own cluster&#39;
        elif self.INITIALIZATION_KWARGS[STRNAMES.CLUSTERING][&#39;value_option&#39;] == &#39;fixed-clustering&#39;:
            clus_init = &#39;Same topology as {}&#39;.format(
                self.INITIALIZATION_KWARGS[STRNAMES.CLUSTERING][&#39;value&#39;])
        elif self.INITIALIZATION_KWARGS[STRNAMES.CLUSTERING][&#39;value_option&#39;] == &#39;manual&#39;:
            clus_init = &#39;Manually with assignments {}&#39;.format(
                self.INITIALIZATION_KWARGS[STRNAMES.CLUSTERING][&#39;value&#39;])
        elif self.INITIALIZATION_KWARGS[STRNAMES.CLUSTERING][&#39;value_option&#39;] == &#39;random&#39;:
            clus_init = &#39;Randomly, with {} clusters&#39;.format(
                self.INITIALIZATION_KWARGS[STRNAMES.CLUSTERING][&#39;n_clusters&#39;])
        elif self.INITIALIZATION_KWARGS[STRNAMES.CLUSTERING][&#39;value_option&#39;] == &#39;taxonomy&#39;:
            clus_init = &#39;By taxonomic similarity, with {} clusters&#39;.format(
                self.INITIALIZATION_KWARGS[STRNAMES.CLUSTERING][&#39;n_clusters&#39;])
        else:
            clus_init = &#39;Something slse&#39;


        f = open(fname, &#39;w&#39;)
        f.write(mystr.format(
            seed=self.SEED, n_samples=self.N_SAMPLES,
            burnin=self.BURNIN, model_path=self.MODEL_PATH,
            a0=self.NEGBIN_A0, a1=self.NEGBIN_A1,
            params_learned=params_learned,
            clus_ind_prior=self.INITIALIZATION_KWARGS[STRNAMES.CLUSTER_INTERACTION_INDICATOR_PROB][&#39;hyperparam_option&#39;],
            pert_ind_prior=self.INITIALIZATION_KWARGS[STRNAMES.PERT_INDICATOR_PROB][&#39;hyperparam_option&#39;],
            filt=self.INITIALIZATION_KWARGS[STRNAMES.FILTERING][&#39;x_value_option&#39;],
            clus_init=clus_init))
        f.close()</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>mdsine2.config._BaseModelConfig</li>
<li><a title="mdsine2.pylab.base.Saveable" href="pylab/base.html#mdsine2.pylab.base.Saveable">Saveable</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="mdsine2.config.MDSINE2ModelConfig.cv_single_suffix"><code class="name flex">
<span>def <span class="ident">cv_single_suffix</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Create a suffix for a single cv round</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cv_single_suffix(self):
    &#39;&#39;&#39;Create a suffix for a single cv round
    &#39;&#39;&#39;
    return &#39;leave_out{}&#39;.format(self.LEAVE_OUT)</code></pre>
</details>
</dd>
<dt id="mdsine2.config.MDSINE2ModelConfig.cv_suffix"><code class="name flex">
<span>def <span class="ident">cv_suffix</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Create a master suffix with the parameters</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cv_suffix(self):
    &#39;&#39;&#39;Create a master suffix with the parameters
    &#39;&#39;&#39;
    s = &#39;s{}_b{}_ns{}_mo{}&#39;.format(
        self.SEED, self.BURNIN, self.N_SAMPLES,
        self.MAX_N_TAXA)
    return s</code></pre>
</details>
</dd>
<dt id="mdsine2.config.MDSINE2ModelConfig.make_metadata_file"><code class="name flex">
<span>def <span class="ident">make_metadata_file</span></span>(<span>self, fname)</span>
</code></dt>
<dd>
<div class="desc"><p>Make a metadata file that does an overview of the parameters in this class</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def make_metadata_file(self, fname):
    &#39;&#39;&#39;Make a metadata file that does an overview of the parameters in this class
    &#39;&#39;&#39;

    mystr = &#39;Global parameters\n&#39; \
        &#39;-----------------\n&#39; \
        &#39;Random seed: {seed}\n&#39; \
        &#39;Total number of Gibb steps: {n_samples}\n&#39; \
        &#39;Number of Gibb steps for burn-in: {burnin}\n&#39; \
        &#39;Saved location: {model_path}\n\n&#39; \
        &#39;Negative binomial dispersion parameters\n&#39; \
        &#39;---------------------------------------\n&#39; \
        &#39;a0: {a0:.4E}\n&#39; \
        &#39;a1: {a1:.4E}\n\n&#39; \
        &#39;Parameters learned and their order\n&#39; \
        &#39;----------------------------------\n&#39; \
        &#39;{params_learned}\n\n&#39; \
        &#39;Selected Initialization choices\n&#39; \
        &#39;-------------------------------\n&#39; \
        &#39;Cluster interaction probability prior: {clus_ind_prior}\n&#39; \
        &#39;Perturbation probability prior: {pert_ind_prior}\n&#39; \
        &#39;Filtering initialization: {filt}\n&#39; \
        &#39;Cluster initialization: {clus_init}\n&#39;
    # params learned
    # --------------
    i = 0
    params_learned = &#39;&#39;
    for pname in self.INFERENCE_ORDER:
        if self.LEARN[pname]:
            params_learned += &#39;{}: {}\n&#39;.format(i, pname)
            i += 1
    
    # init choices
    # ------------
    if self.INITIALIZATION_KWARGS[STRNAMES.CLUSTERING][&#39;value_option&#39;] == &#39;spearman&#39;:
        clus_init = &#39;Spearman Correlation, {} clusters&#39;.format(
            self.INITIALIZATION_KWARGS[STRNAMES.CLUSTERING][&#39;n_clusters&#39;])
    elif self.INITIALIZATION_KWARGS[STRNAMES.CLUSTERING][&#39;value_option&#39;] == &#39;no-clusters&#39;:
        clus_init = &#39;No clusters, everything in its own cluster&#39;
    elif self.INITIALIZATION_KWARGS[STRNAMES.CLUSTERING][&#39;value_option&#39;] == &#39;fixed-clustering&#39;:
        clus_init = &#39;Same topology as {}&#39;.format(
            self.INITIALIZATION_KWARGS[STRNAMES.CLUSTERING][&#39;value&#39;])
    elif self.INITIALIZATION_KWARGS[STRNAMES.CLUSTERING][&#39;value_option&#39;] == &#39;manual&#39;:
        clus_init = &#39;Manually with assignments {}&#39;.format(
            self.INITIALIZATION_KWARGS[STRNAMES.CLUSTERING][&#39;value&#39;])
    elif self.INITIALIZATION_KWARGS[STRNAMES.CLUSTERING][&#39;value_option&#39;] == &#39;random&#39;:
        clus_init = &#39;Randomly, with {} clusters&#39;.format(
            self.INITIALIZATION_KWARGS[STRNAMES.CLUSTERING][&#39;n_clusters&#39;])
    elif self.INITIALIZATION_KWARGS[STRNAMES.CLUSTERING][&#39;value_option&#39;] == &#39;taxonomy&#39;:
        clus_init = &#39;By taxonomic similarity, with {} clusters&#39;.format(
            self.INITIALIZATION_KWARGS[STRNAMES.CLUSTERING][&#39;n_clusters&#39;])
    else:
        clus_init = &#39;Something slse&#39;


    f = open(fname, &#39;w&#39;)
    f.write(mystr.format(
        seed=self.SEED, n_samples=self.N_SAMPLES,
        burnin=self.BURNIN, model_path=self.MODEL_PATH,
        a0=self.NEGBIN_A0, a1=self.NEGBIN_A1,
        params_learned=params_learned,
        clus_ind_prior=self.INITIALIZATION_KWARGS[STRNAMES.CLUSTER_INTERACTION_INDICATOR_PROB][&#39;hyperparam_option&#39;],
        pert_ind_prior=self.INITIALIZATION_KWARGS[STRNAMES.PERT_INDICATOR_PROB][&#39;hyperparam_option&#39;],
        filt=self.INITIALIZATION_KWARGS[STRNAMES.FILTERING][&#39;x_value_option&#39;],
        clus_init=clus_init))
    f.close()</code></pre>
</details>
</dd>
<dt id="mdsine2.config.MDSINE2ModelConfig.suffix"><code class="name flex">
<span>def <span class="ident">suffix</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Create a suffix with the parameters</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def suffix(self):
    &#39;&#39;&#39;Create a suffix with the parameters
    &#39;&#39;&#39;
    s = &#39;s{}_b{}_ns{}_lo{}_mo{}&#39;.format(
        self.SEED, self.BURNIN, self.N_SAMPLES, self.LEAVE_OUT,
        self.MAX_N_TAXA)
    return s</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="mdsine2.pylab.base.Saveable" href="pylab/base.html#mdsine2.pylab.base.Saveable">Saveable</a></b></code>:
<ul class="hlist">
<li><code><a title="mdsine2.pylab.base.Saveable.load" href="pylab/base.html#mdsine2.pylab.base.Saveable.load">load</a></code></li>
<li><code><a title="mdsine2.pylab.base.Saveable.save" href="pylab/base.html#mdsine2.pylab.base.Saveable.save">save</a></code></li>
<li><code><a title="mdsine2.pylab.base.Saveable.set_save_location" href="pylab/base.html#mdsine2.pylab.base.Saveable.set_save_location">set_save_location</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="mdsine2.config.NegBinConfig"><code class="flex name class">
<span>class <span class="ident">NegBinConfig</span></span>
<span>(</span><span>seed:Â int, burnin:Â int, n_samples:Â int, checkpoint:Â int, basepath:Â str)</span>
</code></dt>
<dd>
<div class="desc"><p>Configuration class for learning the negative binomial dispersion
parameters. Note that these parameters are learned offline.</p>
<h2 id="system-initialization">System Initialization</h2>
<ul>
<li>OUTPUT_BASEPATH : str<ul>
<li>Path to save the model</li>
</ul>
</li>
<li>SEED : int<ul>
<li>Seed to initialize inference with</li>
</ul>
</li>
<li>BURNIN : int<ul>
<li>Number of initial Gibb steps to throw away</li>
</ul>
</li>
<li>N_SAMPLES : int<ul>
<li>Total number of Gibb steps</li>
</ul>
</li>
<li>CHECKPOINT : int<ul>
<li>How often to write to disk</li>
</ul>
</li>
<li>MP_FILTERING : str<ul>
<li>How to do multiprocessing for filtering</li>
</ul>
</li>
<li>LEARN : Dict[str, bool]<ul>
<li>These are the dictionary of parameters which we are learning in the model.</li>
<li>If the name maps to True, then we learn it during inference. If it maps to
false, then we do not update its value duting inference.</li>
</ul>
</li>
<li>INFERENCE_ORDER : list<ul>
<li>This is the order to update the parameters during MCMC inference</li>
</ul>
</li>
<li>INITIALIZATION_KWARGS : Dict[str, Dict[str, Any]]<ul>
<li>These are the parameters to send into the <code>initialize</code> function for each variable
that we are learning</li>
</ul>
</li>
<li>INITIALIZATION_ORDER : list<ul>
<li>This is the order to initialize the variables</li>
</ul>
</li>
</ul>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>seed</code></strong> :&ensp;<code>int</code></dt>
<dd>Seed to start the inderence</dd>
<dt><strong><code>burnin</code></strong>, <strong><code>n_samples</code></strong> :&ensp;<code>int</code></dt>
<dd>How many iterations for burn-in and total samples, respectively.</dd>
<dt><strong><code>checkpoint</code></strong> :&ensp;<code>int</code></dt>
<dd>How often to write the trace in RAM to disk. Note that this must be
a multiple of both <code>burnin</code> and <code>n_samples</code></dd>
<dt><strong><code>basepath</code></strong> :&ensp;<code>str</code></dt>
<dd>This is the basepath to save the graph. A separate folder within
<code>basepath</code> will be created for the specific graph.</dd>
<dt><strong><code>synth</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, run with the synthetic data, where the parameters needed
to learn are <code>SYNTHETIC_A0</code> AND <code>SYNTHETIC_A1</code>.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class NegBinConfig(_BaseModelConfig):
    &#39;&#39;&#39;Configuration class for learning the negative binomial dispersion
    parameters. Note that these parameters are learned offline.

    System initialization
    ---------------------
    - OUTPUT_BASEPATH : str
        - Path to save the model
    - SEED : int
        - Seed to initialize inference with
    - BURNIN : int
        - Number of initial Gibb steps to throw away
    - N_SAMPLES : int
        - Total number of Gibb steps
    - CHECKPOINT : int
        - How often to write to disk
    - MP_FILTERING : str
        - How to do multiprocessing for filtering
    - LEARN : Dict[str, bool]
        - These are the dictionary of parameters which we are learning in the model.
        - If the name maps to True, then we learn it during inference. If it maps to 
          false, then we do not update its value duting inference.
    - INFERENCE_ORDER : list
        - This is the order to update the parameters during MCMC inference
    - INITIALIZATION_KWARGS : Dict[str, Dict[str, Any]]
        - These are the parameters to send into the `initialize` function for each variable
          that we are learning
    - INITIALIZATION_ORDER : list
        - This is the order to initialize the variables

    Parameters
    ----------
    seed : int
        Seed to start the inderence
    burnin, n_samples : int
        How many iterations for burn-in and total samples, respectively.
    checkpoint : int
        How often to write the trace in RAM to disk. Note that this must be
        a multiple of both `burnin` and `n_samples`
    basepath : str
        This is the basepath to save the graph. A separate folder within
        `basepath` will be created for the specific graph.
    synth : bool
        If True, run with the synthetic data, where the parameters needed
        to learn are `SYNTHETIC_A0` AND `SYNTHETIC_A1`.
    &#39;&#39;&#39;

    def __init__(self, seed: int, burnin: int, n_samples: int, checkpoint: int, 
        basepath: str):
        if basepath[-1] != &#39;/&#39;:
            basepath += &#39;/&#39;

        self.SEED = seed
        self.OUTPUT_BASEPATH = os.path.abspath(basepath)
        self.MODEL_PATH = self.OUTPUT_BASEPATH
        self.BURNIN = burnin
        self.N_SAMPLES = n_samples
        self.CHECKPOINT = checkpoint
        self.MP_FILTERING = &#39;debug&#39;

        self.INFERENCE_ORDER = [
            STRNAMES.NEGBIN_A0,
            STRNAMES.NEGBIN_A1,
            STRNAMES.FILTERING]

        self.LEARN = {
            STRNAMES.NEGBIN_A0: True,
            STRNAMES.NEGBIN_A1: True,
            STRNAMES.FILTERING: True}

        self.INITIALIZATION_ORDER = [
            STRNAMES.FILTERING,
            STRNAMES.NEGBIN_A0,
            STRNAMES.NEGBIN_A1]

        self.INITIALIZATION_KWARGS = {
            STRNAMES.NEGBIN_A0: {
                &#39;value&#39;: 1e-10,
                &#39;truncation_settings&#39;: (0, 1e5),
                &#39;tune&#39;: 50,
                &#39;end_tune&#39;: int(self.BURNIN/2),
                &#39;target_acceptance_rate&#39;: &#39;optimal&#39;, 
                &#39;proposal_option&#39;: &#39;auto&#39;,
                &#39;delay&#39;: 0},
            STRNAMES.NEGBIN_A1: {
                &#39;value&#39;: 0.1,
                &#39;tune&#39;: 50,
                &#39;truncation_settings&#39;: (0, 1e5),
                &#39;end_tune&#39;: int(self.BURNIN/2),
                &#39;target_acceptance_rate&#39;: &#39;optimal&#39;, 
                &#39;proposal_option&#39;: &#39;auto&#39;,
                &#39;delay&#39;: 0},
            STRNAMES.FILTERING: {
                &#39;tune&#39;: 50,
                &#39;end_tune&#39;: int(self.BURNIN/2),
                &#39;target_acceptance_rate&#39;: &#39;optimal&#39;, 
                &#39;qpcr_variance_inflation&#39;: 100,
                &#39;delay&#39;: 100}}

    def suffix(self):
        return &#39;seed{}_nb{}_ns{}&#39;.format(self.SEED, self.BURNIN, self.N_SAMPLES)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>mdsine2.config._BaseModelConfig</li>
<li><a title="mdsine2.pylab.base.Saveable" href="pylab/base.html#mdsine2.pylab.base.Saveable">Saveable</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="mdsine2.config.NegBinConfig.suffix"><code class="name flex">
<span>def <span class="ident">suffix</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def suffix(self):
    return &#39;seed{}_nb{}_ns{}&#39;.format(self.SEED, self.BURNIN, self.N_SAMPLES)</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="mdsine2.pylab.base.Saveable" href="pylab/base.html#mdsine2.pylab.base.Saveable">Saveable</a></b></code>:
<ul class="hlist">
<li><code><a title="mdsine2.pylab.base.Saveable.load" href="pylab/base.html#mdsine2.pylab.base.Saveable.load">load</a></code></li>
<li><code><a title="mdsine2.pylab.base.Saveable.save" href="pylab/base.html#mdsine2.pylab.base.Saveable.save">save</a></code></li>
<li><code><a title="mdsine2.pylab.base.Saveable.set_save_location" href="pylab/base.html#mdsine2.pylab.base.Saveable.set_save_location">set_save_location</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="mdsine2" href="index.html">mdsine2</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="mdsine2.config.isModelConfig" href="#mdsine2.config.isModelConfig">isModelConfig</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="mdsine2.config.FilteringConfig" href="#mdsine2.config.FilteringConfig">FilteringConfig</a></code></h4>
<ul class="">
<li><code><a title="mdsine2.config.FilteringConfig.suffix" href="#mdsine2.config.FilteringConfig.suffix">suffix</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="mdsine2.config.LoggingConfig" href="#mdsine2.config.LoggingConfig">LoggingConfig</a></code></h4>
</li>
<li>
<h4><code><a title="mdsine2.config.MDSINE2ModelConfig" href="#mdsine2.config.MDSINE2ModelConfig">MDSINE2ModelConfig</a></code></h4>
<ul class="">
<li><code><a title="mdsine2.config.MDSINE2ModelConfig.cv_single_suffix" href="#mdsine2.config.MDSINE2ModelConfig.cv_single_suffix">cv_single_suffix</a></code></li>
<li><code><a title="mdsine2.config.MDSINE2ModelConfig.cv_suffix" href="#mdsine2.config.MDSINE2ModelConfig.cv_suffix">cv_suffix</a></code></li>
<li><code><a title="mdsine2.config.MDSINE2ModelConfig.make_metadata_file" href="#mdsine2.config.MDSINE2ModelConfig.make_metadata_file">make_metadata_file</a></code></li>
<li><code><a title="mdsine2.config.MDSINE2ModelConfig.suffix" href="#mdsine2.config.MDSINE2ModelConfig.suffix">suffix</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="mdsine2.config.NegBinConfig" href="#mdsine2.config.NegBinConfig">NegBinConfig</a></code></h4>
<ul class="">
<li><code><a title="mdsine2.config.NegBinConfig.suffix" href="#mdsine2.config.NegBinConfig.suffix">suffix</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.1</a>.</p>
</footer>
</body>
</html>