<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.1" />
<title>mdsine2.synthetic API documentation</title>
<meta name="description" content="Generates synthetic systems" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>mdsine2.synthetic</code></h1>
</header>
<section id="section-intro">
<p>Generates synthetic systems</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#39;&#39;&#39;Generates synthetic systems
&#39;&#39;&#39;

import numpy as np
import logging
import os
import random

from typing import Union, Dict, Iterator, Tuple, List, Any, IO, Callable

from . import model as plmodel
from . import pylab as pl
from .names import STRNAMES
from .pylab import variables, Study, BaseMCMC

class Synthetic(pl.Saveable):
    &#39;&#39;&#39;Generate synthetic and semi synthetic datasets for testing MDSINE2.

    Parameters
    ----------
    name : str
        Name of the dataset and the name of the graph
    seed : int
        Seed to initialize the system
    &#39;&#39;&#39;

    def __init__(self, name: str, seed: int=None):
        self.G = pl.Graph(name=name, seed=seed)
        self.model = plmodel.gLVDynamicsSingleClustering(growth=None, interactions=None)
        self.taxa = None # mdsine2.pylab.base.TaxaSet
        self.subjs = None # list[str]
        self._data = None # list[np.ndarray]

    @property
    def perturbations(self):
        return self.G.perturbations

    def icml_dynamics(self, n_taxa: int=13):
        &#39;&#39;&#39;Recreate the dynamical system used in the ICML paper [1]. If you use the 
        default parameters you will get the same interaction matrix that was used in [1].

        We rescale the self-interactions and the interactions (and potentially growth) by 150
        so that the time-scales of the trajectories happen over days instead of minutes

        Parameters
        ----------
        n_taxa : int
            - These are how many taxa to include in the system. We will always have 3 clusters and
              the proportion of taxa in each cluster is as follows:
                - cluster 1 - 5/13
                - cluster 2 - 6/13
                - cluster 3 - 2/13
            - We assign each taxoon to each cluster the best we can, any extra taxa we put into cluster 3

        References
        ----------
        [1] Robust and Scalable Models of Microbiome Dynamics, TE Gibson, GK Gerber (2018)
        &#39;&#39;&#39;
        if not pl.isint(n_taxa):
            raise TypeError(&#39;`n_taxa` ({}) must be an int&#39;.format(type(n_taxa)))
        if n_taxa &lt; 3:
            raise ValueError(&#39;`n_taxa` ({}) must be &gt;= 3&#39;.format(n_taxa))
        
        # Make the TaxaSet
        taxa = pl.TaxaSet()
        for aidx in range(n_taxa):
            seq = &#39;&#39;.join(random.choice([&#39;A&#39;, &#39;T&#39;, &#39;G&#39;, &#39;C&#39;]) for _ in range(250))
            taxa.add_taxon(name=&#39;TAXA_{}&#39;.format(aidx+1), sequence=seq)
        self.taxa = taxa

        # Generate the cluster assignments
        c0size = int(5*n_taxa/13)
        c1size = int(6*n_taxa/13)
        c2size = int(n_taxa - c0size - c1size)

        clusters = np.asarray(([0]*c0size) + ([1]*c1size) + ([2]*c2size), dtype=int)
        clustering = pl.Clustering(items=taxa, clusters=clusters, name=STRNAMES.CLUSTERING_OBJ, G=self.G)

        # Generate the interactions
        interactions = pl.Interactions(clustering=clustering, use_indicators=True, G=self.G, 
            name=STRNAMES.INTERACTIONS_OBJ)
        c0 = clustering.order[0]
        c1 = clustering.order[1]
        c2 = clustering.order[2]

        frac = 150*n_taxa/13
        for interaction in interactions:
            if interaction.target_cid == c0 and interaction.source_cid == c1:
                interaction.value = 3/frac
                interaction.indicator = True
            elif interaction.target_cid == c0 and interaction.source_cid == c2:
                interaction.value = -1/frac
                interaction.indicator = True
            elif interaction.target_cid == c2 and interaction.source_cid == c0:
                interaction.value = 2/frac
                interaction.indicator = True
            elif interaction.target_cid == c2 and interaction.source_cid == c1:
                interaction.value = -4/frac
                interaction.indicator = True
            else:
                interaction.value = 0
                interaction.indicator = False
        self_interactions = np.ones(n_taxa, dtype=float) * 5 / 150
        self_interactions = pl.Variable(value=self_interactions, shape=(n_taxa, ),
            name=STRNAMES.SELF_INTERACTION_VALUE, G=self.G)

        A = interactions.get_datalevel_value_matrix()
        for i in range(A.shape[0]):
            A[i,i] = -self_interactions.value[i]
        
        self.model.interactions = A #

        # Generate growth
        self.model.growth = pl.random.uniform.sample(low=0.1, high=0.12, size=n_taxa)

    def set_timepoints(self, times: np.ndarray):
        &#39;&#39;&#39;Set the timepoints of the trajectory

        Parameters
        ----------
        times : np.ndarray
        &#39;&#39;&#39;
        if not pl.isarray(times):
            raise TypeError(&#39;`times` ({}) must be an array&#39;.format(times))
        times = np.sort(np.array(times))
        self.times = times

    def set_subjects(self, subjs: List[str]):
        &#39;&#39;&#39;Set the names of the subjects

        Parameters
        ----------
        subjs : list(str)
            A list of all the names of the subjects
        &#39;&#39;&#39;
        self.subjs = subjs

    def generate_trajectories(self, dt: float, init_dist: variables.Variable, 
        processvar: plmodel.MultiplicativeGlobal=None, seed: int=None):
        &#39;&#39;&#39;Forward simulate trajectories given the dynamics

        Parameters
        ----------
        dt : float
            Step size of the forward simulation
        init_dist : mdsine2.variables.RandomVariable
            This is the random distribution to intiialize the trajectories at
        processvar : mdsine2.model.MultiplicativeGlobal
            This is the process variance to simulate with.
            If this is not given then there will be no processvariance during simulation
        seed : int
            This is the seed to initialize at. If this is not given then the seed is no reset.
        &#39;&#39;&#39;
        if seed is not None:
            pl.random.seed(seed)
        
        self._data = {}
        for iii, subj in enumerate(self.subjs):
            logging.info(&#39;Forward simulating {}/{}&#39;.format(iii, len(self.subjs)))
            init_abund = init_dist.sample(size=len(self.taxa)).reshape(-1,1)

            pert_start = None
            pert_end = None
            pert_eff = None

            if self.perturbations is not None:
                pert_start = []
                pert_end = []
                pert_eff = []
                for perturbation in self.perturbations:
                    if subj in perturbation.starts:
                        pert_start.append(perturbation.starts[subj])
                        pert_end.append(perturbation.ends[subj])
                        pert_eff.append(perturbation.item_array())

            self.model.perturbation_ends = pert_end
            self.model.perturbation_starts = pert_start
            self.model.perturbations = pert_eff

            d = pl.integrate(dynamics=self.model, initial_conditions=init_abund, 
                dt=dt, n_days=self.times[-1]+dt, processvar=processvar, 
                subsample=True, times=self.times)
            self._data[subj] = d[&#39;X&#39;]
        
    def simulateMeasurementNoise(self, a0: float, a1: float, qpcr_noise_scale: float, 
        approx_read_depth: int, name: str=&#39;unnamed-study&#39;) -&gt; Study:
        &#39;&#39;&#39;This function converts the synthetic trajectories into &#34;real&#34; data
        by simulating read counts and qPCR measurements.

        Simulating qPCR measurements
        ----------------------------
        We simulate the qPCR measurements with a lognormal distribution that was
        fitted using the data from `subjset`. We use this parameterization to sample
        a qPCR measurement with mean from the total biomass of the simulated data.

        Simulating count data
        ---------------------
        First, we get the approximate read depth `r_k` with `approx_read_depth`. We then use `r_k`, 
        `a_0`, and `a_1` with the relative abundances to sample from a negative binomial 
        distirbution. We then use the relative abundances from this sample as the concentrations
        for a multinomial distribution with read depth `r_k`.

        Parameters
        ----------
        a0, a1 : numeric
            These are the negative binomial dispersion parameters that we are using to 
            simulate the data
        qpcr_noise_scale : numeric
            This is the parameter to scale the `s` parameter learned by the lognormal
            distribution.
        approx_read_depth : int
            This is the read depth to simulate to
        name : str
            Name of the study

        Returns
        -------
        mdsine2.Study
        &#39;&#39;&#39;
        if self.times is None or self._data is None or self.subjs is None:
            raise ValueError(&#39;Need to fully initialize the system before&#39;)

        logging.info(&#39;Fitting real data&#39;)

        # Make the study object
        study = pl.Study(taxa=self.taxa, name=name)
        for subjname in self.subjs:
            study.add_subject(name=subjname)

        # Add times for each subject
        for subj in study:
            subj.times = self.times
        
        # Make the qPCR measurements
        for subj in study:
            total_abund = np.sum(self._data[subj.name], axis=0)

            for tidx, t in enumerate(self.times):
                # Get the total abundance

                triplicates = np.exp(np.log(total_abund[tidx]) + \
                    qpcr_noise_scale * pl.random.normal.sample(size=3))
                subj.qpcr[t] = pl.qPCRdata(cfus=triplicates, mass=1., dilution_factor=1.)
        
        # Make the reads
        for subj in study:
            for tidx, t in enumerate(self.times):

                total_mass = np.sum(self._data[subj.name][:, tidx])
                rel_abund = (self._data[subj.name][:, tidx] + 1e-20) / total_mass

                phi = approx_read_depth * rel_abund
                eps = a0/rel_abund + a1

                # print(phi)
                # print(eps)

                reads = pl.random.negative_binomial.sample(mean=phi, dispersion=eps)
                subj.reads[t] = reads

        # Make the perturbations:
        if self.perturbations is not None and len(self.perturbations) &gt; 0:
            study.perturbations = self.G.perturbations

        return study
            
def make_semisynthetic(chain: BaseMCMC, min_bayes_factor: Union[float, int], name: str=None, 
    set_times: bool=True) -&gt; Synthetic:
    &#39;&#39;&#39;Make a semi synthetic system. We assume that the chain that we pass in was 
    run with FIXED CLUSTERING. We assume this because we need to set the cluster-cluster
    interactions and the cluster perturbations.
    
    How the synthetic system is set
    -------------------------------
    - n_taxa: Set to the number in chain.
    - clustering: The clusters assignments are set to the value of the Clustering class
    - interactions: Set to the expected value of the posterior. We only include interactions
      whose bayes factor is greater than `min_bayes_factor`.
    - perturbations: The number of perturbations is set to be the same as what is in the
      chain. The topology and values of the perturbations are set to the expected value
      of the posterior. We only include perturbation effects whose bayes factor is
      greater than `min_bayes_factor`
    - growth and self-interactions: These are set to the learned values for each of the taxa.
    - init_dist: The distirbution of the initial timepoints are set by fitting a log normal
      ditribution to the `init_dist_timepoint`th timepoint

    Parameters
    ----------
    chain : str, pylab.inference.BaseMCMC
        This is chain or the file location of the chain
    min_bayes_factor : numeric
        This is the minimum bayes factor needed for a perturbation/interaction
        to be used in the synthetic dataset
    name : str
        Name of the synthetic object
    set_times : bool
        If True, we set the times of the subject to be be a union of all of the subjects
        in chain

    Returns
    -------
    synthetic.Synthetic
    &#39;&#39;&#39;
    from .util import condense_fixed_clustering_interaction_matrix
    from .util import generate_interation_bayes_factors_posthoc
    from .util import condense_fixed_clustering_perturbation
    from .util import generate_perturbation_bayes_factors_posthoc

    if pl.isstr(chain):
        chain = pl.BaseMCMC.load(chain)
    if not pl.isMCMC(chain):
        raise TypeError(&#39;`chain` ({}) is not a mdsine2.BaseMCMC object&#39;.format(type(chain)))

    if not pl.isnumeric(min_bayes_factor):
        raise TypeError(&#39;`min_bayes_factor` ({}) nmust be a numeric&#39;.format(
            type(min_bayes_factor)))
    if min_bayes_factor &lt; 0:
        raise ValueError(&#39;`min_bayes_factor` ({}) must be &gt;= 0&#39;.format(min_bayes_factor))
    if not pl.isbool(set_times):
        raise TypeError(&#39;`set_times` ({}) must be a bool&#39;.format(type(set_times)))

    if name is None:
        name = chain.graph.data.subjects.name + &#39;-synthetic&#39;
    syn = Synthetic(name=name)
    syn.set_subjects(subjs=[subj.name for subj in chain.graph.data.subjects])
    syn.taxa = chain.graph.data.taxa

    if set_times:
        syn.times = chain.graph.data.subjects.times(&#39;union&#39;)
    else:
        syn.times = None

    # Set the clustering
    # ------------------
    cluster_assignments = chain.graph[STRNAMES.CLUSTERING_OBJ].toarray()
    clustering = pl.Clustering(cluster_assignments, G=syn.G, items=syn.taxa, 
        name=STRNAMES.CLUSTERING_OBJ)

    # Set the interactions
    # --------------------
    self_interactions = pl.summary(chain.graph[STRNAMES.SELF_INTERACTION_VALUE])[&#39;mean&#39;]
    A = chain.graph[STRNAMES.INTERACTIONS_OBJ].get_datalevel_value_matrix()
    A_cluster = condense_fixed_clustering_interaction_matrix(A, 
        clustering=chain.graph[STRNAMES.CLUSTERING_OBJ])

    bf = generate_interation_bayes_factors_posthoc(mcmc=chain)
    bf_cluster = condense_fixed_clustering_interaction_matrix(bf, 
        clustering=chain.graph[STRNAMES.CLUSTERING_OBJ])

    interactions = pl.Interactions(clustering=clustering, use_indicators=True, 
        name=STRNAMES.INTERACTIONS_OBJ, G=syn.G)

    for interaction in interactions:
        target_cid = interaction.target_cid
        source_cid = interaction.source_cid

        tcidx = clustering.cid2cidx[target_cid]
        scidx = clustering.cid2cidx[source_cid]

        if bf_cluster[tcidx, scidx] &gt;= min_bayes_factor:
            interaction.value = A_cluster[tcidx, scidx]
            interaction.indicator = True
        else:
            interaction.value = 0
            interaction.indicator = False

    A = interactions.get_datalevel_value_matrix()
    for i in range(A.shape[0]):
        A[i,i] = -self_interactions[i]

    syn.model.interactions = A

    # Set perturbations (if necessary)
    # --------------------------------
    if chain.graph.perturbations is not None:
        for perturbation_master in chain.graph.perturbations:
            perturbation = pl.ClusterPerturbationEffect(
                starts=perturbation_master.starts,
                ends=perturbation_master.ends,
                name=perturbation_master.name,
                G=syn.G, clustering=clustering)

            # Get the values and the bayes factors
            bf = generate_perturbation_bayes_factors_posthoc(chain, 
                perturbation=perturbation_master)
            bf_cluster = condense_fixed_clustering_perturbation(bf, 
                clustering=chain.graph[STRNAMES.CLUSTERING_OBJ])
            
            M = pl.summary(perturbation_master, set_nan_to_0=True)[&#39;mean&#39;]
            M_cluster = condense_fixed_clustering_perturbation(M, 
                clustering=chain.graph[STRNAMES.CLUSTERING_OBJ])

            for cidx, cluster in enumerate(clustering):
                if bf_cluster[cidx] &gt;= min_bayes_factor:
                    perturbation.magnitude.value[cluster.id] = M_cluster[cidx]
                    perturbation.indicator.value[cluster.id] = True
                else:
                    perturbation.magnitude.value[cluster.id] = 0
                    perturbation.indicator.value[cluster.id] = False

    # Set the growth rates
    # --------------------
    syn.model.growth = pl.summary(chain.graph[STRNAMES.GROWTH_VALUE])[&#39;mean&#39;]
    return syn

def subsample_timepoints(times: np.ndarray, N: int, required: np.ndarray=None) -&gt; np.ndarray:
    &#39;&#39;&#39;Subsample the timepoints `times` so that it has `N` timepoints.

    If required is not None, it is a list of timepoints that must be
    included (start/ends of perturbations, etc.) in the return times.

    Parameters
    ----------
    times : np.ndarray
        An array of timepoints
    N : int
        Total number of timepoints to be remaining
    required : None, np.ndarray
        An array of timepoints that need to be included in the return array
    &#39;&#39;&#39;
    if not pl.isarray(times):
        raise TypeError(&#39;`times` ({}) must be an array&#39;.format(type(times)))
    times = np.sort(np.array(times))

    if not pl.isint(N):
        raise TypeError(&#39;`N` ({}) must be an int&#39;.format(type(N)))
    if N &lt;= 1:
        raise ValueError(&#39;`N` ({}) must be &gt; 1&#39;.format(N))
    if required is not None:
        if not pl.isarray(required):
            raise TypeError(&#39;`required` ({}) must be an array&#39;.format(type(required)))
        for ele in required:
            if ele not in times:
                raise ValueError(&#39;({}) in `required` is not in times: ({})&#39;.format(ele,times))

        if N - len(required) &lt;= 0:
            raise ValueError(&#39;The number of required points ({}) is more than the total number &#39; \
                &#39;of points that need to remain ({})&#39;.format(len(required), N))

        add_at_end = []
        for ele in required:
            # get index at 
            idx = np.searchsorted(times, ele)
            add_at_end.append(ele)
            times = np.delete(times, [idx])
        N -= len(add_at_end)

    l = len(times)
    if N &gt;= l/2:
        # These are the indices to take away
        idxs = np.arange(0, l, step=l/(l-N), dtype=int)
        a = np.delete(times, idxs)

    else:
        # These are the indicates to keep
        idxs = np.arange(0, l, step=l/N, dtype=int)
        a = times[idxs]

    if required is not None:
        a = np.sort(np.append(a, add_at_end))
    return a

    </code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="mdsine2.synthetic.make_semisynthetic"><code class="name flex">
<span>def <span class="ident">make_semisynthetic</span></span>(<span>chain: <a title="mdsine2.pylab.inference.BaseMCMC" href="pylab/inference.html#mdsine2.pylab.inference.BaseMCMC">BaseMCMC</a>, min_bayes_factor: Union[float, int], name: str = None, set_times: bool = True) ‑> <a title="mdsine2.synthetic.Synthetic" href="#mdsine2.synthetic.Synthetic">Synthetic</a></span>
</code></dt>
<dd>
<div class="desc"><p>Make a semi synthetic system. We assume that the chain that we pass in was
run with FIXED CLUSTERING. We assume this because we need to set the cluster-cluster
interactions and the cluster perturbations.</p>
<h2 id="how-the-synthetic-system-is-set">How The Synthetic System Is Set</h2>
<ul>
<li>n_taxa: Set to the number in chain.</li>
<li>clustering: The clusters assignments are set to the value of the Clustering class</li>
<li>interactions: Set to the expected value of the posterior. We only include interactions
whose bayes factor is greater than <code>min_bayes_factor</code>.</li>
<li>perturbations: The number of perturbations is set to be the same as what is in the
chain. The topology and values of the perturbations are set to the expected value
of the posterior. We only include perturbation effects whose bayes factor is
greater than <code>min_bayes_factor</code></li>
<li>growth and self-interactions: These are set to the learned values for each of the taxa.</li>
<li>init_dist: The distirbution of the initial timepoints are set by fitting a log normal
ditribution to the <code>init_dist_timepoint</code>th timepoint</li>
</ul>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>chain</code></strong> :&ensp;<code>str, pylab.inference.BaseMCMC</code></dt>
<dd>This is chain or the file location of the chain</dd>
<dt><strong><code>min_bayes_factor</code></strong> :&ensp;<code>numeric</code></dt>
<dd>This is the minimum bayes factor needed for a perturbation/interaction
to be used in the synthetic dataset</dd>
<dt><strong><code>name</code></strong> :&ensp;<code>str</code></dt>
<dd>Name of the synthetic object</dd>
<dt><strong><code>set_times</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, we set the times of the subject to be be a union of all of the subjects
in chain</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>synthetic.Synthetic</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def make_semisynthetic(chain: BaseMCMC, min_bayes_factor: Union[float, int], name: str=None, 
    set_times: bool=True) -&gt; Synthetic:
    &#39;&#39;&#39;Make a semi synthetic system. We assume that the chain that we pass in was 
    run with FIXED CLUSTERING. We assume this because we need to set the cluster-cluster
    interactions and the cluster perturbations.
    
    How the synthetic system is set
    -------------------------------
    - n_taxa: Set to the number in chain.
    - clustering: The clusters assignments are set to the value of the Clustering class
    - interactions: Set to the expected value of the posterior. We only include interactions
      whose bayes factor is greater than `min_bayes_factor`.
    - perturbations: The number of perturbations is set to be the same as what is in the
      chain. The topology and values of the perturbations are set to the expected value
      of the posterior. We only include perturbation effects whose bayes factor is
      greater than `min_bayes_factor`
    - growth and self-interactions: These are set to the learned values for each of the taxa.
    - init_dist: The distirbution of the initial timepoints are set by fitting a log normal
      ditribution to the `init_dist_timepoint`th timepoint

    Parameters
    ----------
    chain : str, pylab.inference.BaseMCMC
        This is chain or the file location of the chain
    min_bayes_factor : numeric
        This is the minimum bayes factor needed for a perturbation/interaction
        to be used in the synthetic dataset
    name : str
        Name of the synthetic object
    set_times : bool
        If True, we set the times of the subject to be be a union of all of the subjects
        in chain

    Returns
    -------
    synthetic.Synthetic
    &#39;&#39;&#39;
    from .util import condense_fixed_clustering_interaction_matrix
    from .util import generate_interation_bayes_factors_posthoc
    from .util import condense_fixed_clustering_perturbation
    from .util import generate_perturbation_bayes_factors_posthoc

    if pl.isstr(chain):
        chain = pl.BaseMCMC.load(chain)
    if not pl.isMCMC(chain):
        raise TypeError(&#39;`chain` ({}) is not a mdsine2.BaseMCMC object&#39;.format(type(chain)))

    if not pl.isnumeric(min_bayes_factor):
        raise TypeError(&#39;`min_bayes_factor` ({}) nmust be a numeric&#39;.format(
            type(min_bayes_factor)))
    if min_bayes_factor &lt; 0:
        raise ValueError(&#39;`min_bayes_factor` ({}) must be &gt;= 0&#39;.format(min_bayes_factor))
    if not pl.isbool(set_times):
        raise TypeError(&#39;`set_times` ({}) must be a bool&#39;.format(type(set_times)))

    if name is None:
        name = chain.graph.data.subjects.name + &#39;-synthetic&#39;
    syn = Synthetic(name=name)
    syn.set_subjects(subjs=[subj.name for subj in chain.graph.data.subjects])
    syn.taxa = chain.graph.data.taxa

    if set_times:
        syn.times = chain.graph.data.subjects.times(&#39;union&#39;)
    else:
        syn.times = None

    # Set the clustering
    # ------------------
    cluster_assignments = chain.graph[STRNAMES.CLUSTERING_OBJ].toarray()
    clustering = pl.Clustering(cluster_assignments, G=syn.G, items=syn.taxa, 
        name=STRNAMES.CLUSTERING_OBJ)

    # Set the interactions
    # --------------------
    self_interactions = pl.summary(chain.graph[STRNAMES.SELF_INTERACTION_VALUE])[&#39;mean&#39;]
    A = chain.graph[STRNAMES.INTERACTIONS_OBJ].get_datalevel_value_matrix()
    A_cluster = condense_fixed_clustering_interaction_matrix(A, 
        clustering=chain.graph[STRNAMES.CLUSTERING_OBJ])

    bf = generate_interation_bayes_factors_posthoc(mcmc=chain)
    bf_cluster = condense_fixed_clustering_interaction_matrix(bf, 
        clustering=chain.graph[STRNAMES.CLUSTERING_OBJ])

    interactions = pl.Interactions(clustering=clustering, use_indicators=True, 
        name=STRNAMES.INTERACTIONS_OBJ, G=syn.G)

    for interaction in interactions:
        target_cid = interaction.target_cid
        source_cid = interaction.source_cid

        tcidx = clustering.cid2cidx[target_cid]
        scidx = clustering.cid2cidx[source_cid]

        if bf_cluster[tcidx, scidx] &gt;= min_bayes_factor:
            interaction.value = A_cluster[tcidx, scidx]
            interaction.indicator = True
        else:
            interaction.value = 0
            interaction.indicator = False

    A = interactions.get_datalevel_value_matrix()
    for i in range(A.shape[0]):
        A[i,i] = -self_interactions[i]

    syn.model.interactions = A

    # Set perturbations (if necessary)
    # --------------------------------
    if chain.graph.perturbations is not None:
        for perturbation_master in chain.graph.perturbations:
            perturbation = pl.ClusterPerturbationEffect(
                starts=perturbation_master.starts,
                ends=perturbation_master.ends,
                name=perturbation_master.name,
                G=syn.G, clustering=clustering)

            # Get the values and the bayes factors
            bf = generate_perturbation_bayes_factors_posthoc(chain, 
                perturbation=perturbation_master)
            bf_cluster = condense_fixed_clustering_perturbation(bf, 
                clustering=chain.graph[STRNAMES.CLUSTERING_OBJ])
            
            M = pl.summary(perturbation_master, set_nan_to_0=True)[&#39;mean&#39;]
            M_cluster = condense_fixed_clustering_perturbation(M, 
                clustering=chain.graph[STRNAMES.CLUSTERING_OBJ])

            for cidx, cluster in enumerate(clustering):
                if bf_cluster[cidx] &gt;= min_bayes_factor:
                    perturbation.magnitude.value[cluster.id] = M_cluster[cidx]
                    perturbation.indicator.value[cluster.id] = True
                else:
                    perturbation.magnitude.value[cluster.id] = 0
                    perturbation.indicator.value[cluster.id] = False

    # Set the growth rates
    # --------------------
    syn.model.growth = pl.summary(chain.graph[STRNAMES.GROWTH_VALUE])[&#39;mean&#39;]
    return syn</code></pre>
</details>
</dd>
<dt id="mdsine2.synthetic.subsample_timepoints"><code class="name flex">
<span>def <span class="ident">subsample_timepoints</span></span>(<span>times: numpy.ndarray, N: int, required: numpy.ndarray = None) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Subsample the timepoints <code>times</code> so that it has <code>N</code> timepoints.</p>
<p>If required is not None, it is a list of timepoints that must be
included (start/ends of perturbations, etc.) in the return times.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>times</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>An array of timepoints</dd>
<dt><strong><code>N</code></strong> :&ensp;<code>int</code></dt>
<dd>Total number of timepoints to be remaining</dd>
<dt><strong><code>required</code></strong> :&ensp;<code>None, np.ndarray</code></dt>
<dd>An array of timepoints that need to be included in the return array</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def subsample_timepoints(times: np.ndarray, N: int, required: np.ndarray=None) -&gt; np.ndarray:
    &#39;&#39;&#39;Subsample the timepoints `times` so that it has `N` timepoints.

    If required is not None, it is a list of timepoints that must be
    included (start/ends of perturbations, etc.) in the return times.

    Parameters
    ----------
    times : np.ndarray
        An array of timepoints
    N : int
        Total number of timepoints to be remaining
    required : None, np.ndarray
        An array of timepoints that need to be included in the return array
    &#39;&#39;&#39;
    if not pl.isarray(times):
        raise TypeError(&#39;`times` ({}) must be an array&#39;.format(type(times)))
    times = np.sort(np.array(times))

    if not pl.isint(N):
        raise TypeError(&#39;`N` ({}) must be an int&#39;.format(type(N)))
    if N &lt;= 1:
        raise ValueError(&#39;`N` ({}) must be &gt; 1&#39;.format(N))
    if required is not None:
        if not pl.isarray(required):
            raise TypeError(&#39;`required` ({}) must be an array&#39;.format(type(required)))
        for ele in required:
            if ele not in times:
                raise ValueError(&#39;({}) in `required` is not in times: ({})&#39;.format(ele,times))

        if N - len(required) &lt;= 0:
            raise ValueError(&#39;The number of required points ({}) is more than the total number &#39; \
                &#39;of points that need to remain ({})&#39;.format(len(required), N))

        add_at_end = []
        for ele in required:
            # get index at 
            idx = np.searchsorted(times, ele)
            add_at_end.append(ele)
            times = np.delete(times, [idx])
        N -= len(add_at_end)

    l = len(times)
    if N &gt;= l/2:
        # These are the indices to take away
        idxs = np.arange(0, l, step=l/(l-N), dtype=int)
        a = np.delete(times, idxs)

    else:
        # These are the indicates to keep
        idxs = np.arange(0, l, step=l/N, dtype=int)
        a = times[idxs]

    if required is not None:
        a = np.sort(np.append(a, add_at_end))
    return a</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="mdsine2.synthetic.Synthetic"><code class="flex name class">
<span>class <span class="ident">Synthetic</span></span>
<span>(</span><span>name: str, seed: int = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Generate synthetic and semi synthetic datasets for testing MDSINE2.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>name</code></strong> :&ensp;<code>str</code></dt>
<dd>Name of the dataset and the name of the graph</dd>
<dt><strong><code>seed</code></strong> :&ensp;<code>int</code></dt>
<dd>Seed to initialize the system</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Synthetic(pl.Saveable):
    &#39;&#39;&#39;Generate synthetic and semi synthetic datasets for testing MDSINE2.

    Parameters
    ----------
    name : str
        Name of the dataset and the name of the graph
    seed : int
        Seed to initialize the system
    &#39;&#39;&#39;

    def __init__(self, name: str, seed: int=None):
        self.G = pl.Graph(name=name, seed=seed)
        self.model = plmodel.gLVDynamicsSingleClustering(growth=None, interactions=None)
        self.taxa = None # mdsine2.pylab.base.TaxaSet
        self.subjs = None # list[str]
        self._data = None # list[np.ndarray]

    @property
    def perturbations(self):
        return self.G.perturbations

    def icml_dynamics(self, n_taxa: int=13):
        &#39;&#39;&#39;Recreate the dynamical system used in the ICML paper [1]. If you use the 
        default parameters you will get the same interaction matrix that was used in [1].

        We rescale the self-interactions and the interactions (and potentially growth) by 150
        so that the time-scales of the trajectories happen over days instead of minutes

        Parameters
        ----------
        n_taxa : int
            - These are how many taxa to include in the system. We will always have 3 clusters and
              the proportion of taxa in each cluster is as follows:
                - cluster 1 - 5/13
                - cluster 2 - 6/13
                - cluster 3 - 2/13
            - We assign each taxoon to each cluster the best we can, any extra taxa we put into cluster 3

        References
        ----------
        [1] Robust and Scalable Models of Microbiome Dynamics, TE Gibson, GK Gerber (2018)
        &#39;&#39;&#39;
        if not pl.isint(n_taxa):
            raise TypeError(&#39;`n_taxa` ({}) must be an int&#39;.format(type(n_taxa)))
        if n_taxa &lt; 3:
            raise ValueError(&#39;`n_taxa` ({}) must be &gt;= 3&#39;.format(n_taxa))
        
        # Make the TaxaSet
        taxa = pl.TaxaSet()
        for aidx in range(n_taxa):
            seq = &#39;&#39;.join(random.choice([&#39;A&#39;, &#39;T&#39;, &#39;G&#39;, &#39;C&#39;]) for _ in range(250))
            taxa.add_taxon(name=&#39;TAXA_{}&#39;.format(aidx+1), sequence=seq)
        self.taxa = taxa

        # Generate the cluster assignments
        c0size = int(5*n_taxa/13)
        c1size = int(6*n_taxa/13)
        c2size = int(n_taxa - c0size - c1size)

        clusters = np.asarray(([0]*c0size) + ([1]*c1size) + ([2]*c2size), dtype=int)
        clustering = pl.Clustering(items=taxa, clusters=clusters, name=STRNAMES.CLUSTERING_OBJ, G=self.G)

        # Generate the interactions
        interactions = pl.Interactions(clustering=clustering, use_indicators=True, G=self.G, 
            name=STRNAMES.INTERACTIONS_OBJ)
        c0 = clustering.order[0]
        c1 = clustering.order[1]
        c2 = clustering.order[2]

        frac = 150*n_taxa/13
        for interaction in interactions:
            if interaction.target_cid == c0 and interaction.source_cid == c1:
                interaction.value = 3/frac
                interaction.indicator = True
            elif interaction.target_cid == c0 and interaction.source_cid == c2:
                interaction.value = -1/frac
                interaction.indicator = True
            elif interaction.target_cid == c2 and interaction.source_cid == c0:
                interaction.value = 2/frac
                interaction.indicator = True
            elif interaction.target_cid == c2 and interaction.source_cid == c1:
                interaction.value = -4/frac
                interaction.indicator = True
            else:
                interaction.value = 0
                interaction.indicator = False
        self_interactions = np.ones(n_taxa, dtype=float) * 5 / 150
        self_interactions = pl.Variable(value=self_interactions, shape=(n_taxa, ),
            name=STRNAMES.SELF_INTERACTION_VALUE, G=self.G)

        A = interactions.get_datalevel_value_matrix()
        for i in range(A.shape[0]):
            A[i,i] = -self_interactions.value[i]
        
        self.model.interactions = A #

        # Generate growth
        self.model.growth = pl.random.uniform.sample(low=0.1, high=0.12, size=n_taxa)

    def set_timepoints(self, times: np.ndarray):
        &#39;&#39;&#39;Set the timepoints of the trajectory

        Parameters
        ----------
        times : np.ndarray
        &#39;&#39;&#39;
        if not pl.isarray(times):
            raise TypeError(&#39;`times` ({}) must be an array&#39;.format(times))
        times = np.sort(np.array(times))
        self.times = times

    def set_subjects(self, subjs: List[str]):
        &#39;&#39;&#39;Set the names of the subjects

        Parameters
        ----------
        subjs : list(str)
            A list of all the names of the subjects
        &#39;&#39;&#39;
        self.subjs = subjs

    def generate_trajectories(self, dt: float, init_dist: variables.Variable, 
        processvar: plmodel.MultiplicativeGlobal=None, seed: int=None):
        &#39;&#39;&#39;Forward simulate trajectories given the dynamics

        Parameters
        ----------
        dt : float
            Step size of the forward simulation
        init_dist : mdsine2.variables.RandomVariable
            This is the random distribution to intiialize the trajectories at
        processvar : mdsine2.model.MultiplicativeGlobal
            This is the process variance to simulate with.
            If this is not given then there will be no processvariance during simulation
        seed : int
            This is the seed to initialize at. If this is not given then the seed is no reset.
        &#39;&#39;&#39;
        if seed is not None:
            pl.random.seed(seed)
        
        self._data = {}
        for iii, subj in enumerate(self.subjs):
            logging.info(&#39;Forward simulating {}/{}&#39;.format(iii, len(self.subjs)))
            init_abund = init_dist.sample(size=len(self.taxa)).reshape(-1,1)

            pert_start = None
            pert_end = None
            pert_eff = None

            if self.perturbations is not None:
                pert_start = []
                pert_end = []
                pert_eff = []
                for perturbation in self.perturbations:
                    if subj in perturbation.starts:
                        pert_start.append(perturbation.starts[subj])
                        pert_end.append(perturbation.ends[subj])
                        pert_eff.append(perturbation.item_array())

            self.model.perturbation_ends = pert_end
            self.model.perturbation_starts = pert_start
            self.model.perturbations = pert_eff

            d = pl.integrate(dynamics=self.model, initial_conditions=init_abund, 
                dt=dt, n_days=self.times[-1]+dt, processvar=processvar, 
                subsample=True, times=self.times)
            self._data[subj] = d[&#39;X&#39;]
        
    def simulateMeasurementNoise(self, a0: float, a1: float, qpcr_noise_scale: float, 
        approx_read_depth: int, name: str=&#39;unnamed-study&#39;) -&gt; Study:
        &#39;&#39;&#39;This function converts the synthetic trajectories into &#34;real&#34; data
        by simulating read counts and qPCR measurements.

        Simulating qPCR measurements
        ----------------------------
        We simulate the qPCR measurements with a lognormal distribution that was
        fitted using the data from `subjset`. We use this parameterization to sample
        a qPCR measurement with mean from the total biomass of the simulated data.

        Simulating count data
        ---------------------
        First, we get the approximate read depth `r_k` with `approx_read_depth`. We then use `r_k`, 
        `a_0`, and `a_1` with the relative abundances to sample from a negative binomial 
        distirbution. We then use the relative abundances from this sample as the concentrations
        for a multinomial distribution with read depth `r_k`.

        Parameters
        ----------
        a0, a1 : numeric
            These are the negative binomial dispersion parameters that we are using to 
            simulate the data
        qpcr_noise_scale : numeric
            This is the parameter to scale the `s` parameter learned by the lognormal
            distribution.
        approx_read_depth : int
            This is the read depth to simulate to
        name : str
            Name of the study

        Returns
        -------
        mdsine2.Study
        &#39;&#39;&#39;
        if self.times is None or self._data is None or self.subjs is None:
            raise ValueError(&#39;Need to fully initialize the system before&#39;)

        logging.info(&#39;Fitting real data&#39;)

        # Make the study object
        study = pl.Study(taxa=self.taxa, name=name)
        for subjname in self.subjs:
            study.add_subject(name=subjname)

        # Add times for each subject
        for subj in study:
            subj.times = self.times
        
        # Make the qPCR measurements
        for subj in study:
            total_abund = np.sum(self._data[subj.name], axis=0)

            for tidx, t in enumerate(self.times):
                # Get the total abundance

                triplicates = np.exp(np.log(total_abund[tidx]) + \
                    qpcr_noise_scale * pl.random.normal.sample(size=3))
                subj.qpcr[t] = pl.qPCRdata(cfus=triplicates, mass=1., dilution_factor=1.)
        
        # Make the reads
        for subj in study:
            for tidx, t in enumerate(self.times):

                total_mass = np.sum(self._data[subj.name][:, tidx])
                rel_abund = (self._data[subj.name][:, tidx] + 1e-20) / total_mass

                phi = approx_read_depth * rel_abund
                eps = a0/rel_abund + a1

                # print(phi)
                # print(eps)

                reads = pl.random.negative_binomial.sample(mean=phi, dispersion=eps)
                subj.reads[t] = reads

        # Make the perturbations:
        if self.perturbations is not None and len(self.perturbations) &gt; 0:
            study.perturbations = self.G.perturbations

        return study</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="mdsine2.pylab.base.Saveable" href="pylab/base.html#mdsine2.pylab.base.Saveable">Saveable</a></li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="mdsine2.synthetic.Synthetic.perturbations"><code class="name">var <span class="ident">perturbations</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def perturbations(self):
    return self.G.perturbations</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="mdsine2.synthetic.Synthetic.generate_trajectories"><code class="name flex">
<span>def <span class="ident">generate_trajectories</span></span>(<span>self, dt: float, init_dist: <a title="mdsine2.pylab.variables.Variable" href="pylab/variables.html#mdsine2.pylab.variables.Variable">Variable</a>, processvar: <a title="mdsine2.model.MultiplicativeGlobal" href="model.html#mdsine2.model.MultiplicativeGlobal">MultiplicativeGlobal</a> = None, seed: int = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Forward simulate trajectories given the dynamics</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>dt</code></strong> :&ensp;<code>float</code></dt>
<dd>Step size of the forward simulation</dd>
<dt><strong><code>init_dist</code></strong> :&ensp;<code>mdsine2.variables.RandomVariable</code></dt>
<dd>This is the random distribution to intiialize the trajectories at</dd>
<dt><strong><code>processvar</code></strong> :&ensp;<code><a title="mdsine2.model.MultiplicativeGlobal" href="model.html#mdsine2.model.MultiplicativeGlobal">MultiplicativeGlobal</a></code></dt>
<dd>This is the process variance to simulate with.
If this is not given then there will be no processvariance during simulation</dd>
<dt><strong><code>seed</code></strong> :&ensp;<code>int</code></dt>
<dd>This is the seed to initialize at. If this is not given then the seed is no reset.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate_trajectories(self, dt: float, init_dist: variables.Variable, 
    processvar: plmodel.MultiplicativeGlobal=None, seed: int=None):
    &#39;&#39;&#39;Forward simulate trajectories given the dynamics

    Parameters
    ----------
    dt : float
        Step size of the forward simulation
    init_dist : mdsine2.variables.RandomVariable
        This is the random distribution to intiialize the trajectories at
    processvar : mdsine2.model.MultiplicativeGlobal
        This is the process variance to simulate with.
        If this is not given then there will be no processvariance during simulation
    seed : int
        This is the seed to initialize at. If this is not given then the seed is no reset.
    &#39;&#39;&#39;
    if seed is not None:
        pl.random.seed(seed)
    
    self._data = {}
    for iii, subj in enumerate(self.subjs):
        logging.info(&#39;Forward simulating {}/{}&#39;.format(iii, len(self.subjs)))
        init_abund = init_dist.sample(size=len(self.taxa)).reshape(-1,1)

        pert_start = None
        pert_end = None
        pert_eff = None

        if self.perturbations is not None:
            pert_start = []
            pert_end = []
            pert_eff = []
            for perturbation in self.perturbations:
                if subj in perturbation.starts:
                    pert_start.append(perturbation.starts[subj])
                    pert_end.append(perturbation.ends[subj])
                    pert_eff.append(perturbation.item_array())

        self.model.perturbation_ends = pert_end
        self.model.perturbation_starts = pert_start
        self.model.perturbations = pert_eff

        d = pl.integrate(dynamics=self.model, initial_conditions=init_abund, 
            dt=dt, n_days=self.times[-1]+dt, processvar=processvar, 
            subsample=True, times=self.times)
        self._data[subj] = d[&#39;X&#39;]</code></pre>
</details>
</dd>
<dt id="mdsine2.synthetic.Synthetic.icml_dynamics"><code class="name flex">
<span>def <span class="ident">icml_dynamics</span></span>(<span>self, n_taxa: int = 13)</span>
</code></dt>
<dd>
<div class="desc"><p>Recreate the dynamical system used in the ICML paper [1]. If you use the
default parameters you will get the same interaction matrix that was used in [1].</p>
<p>We rescale the self-interactions and the interactions (and potentially growth) by 150
so that the time-scales of the trajectories happen over days instead of minutes</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>n_taxa</code></strong> :&ensp;<code>int</code></dt>
<dd>
<ul>
<li>These are how many taxa to include in the system. We will always have 3 clusters and
the proportion of taxa in each cluster is as follows:<ul>
<li>cluster 1 - 5/13</li>
<li>cluster 2 - 6/13</li>
<li>cluster 3 - 2/13</li>
</ul>
</li>
<li>We assign each taxoon to each cluster the best we can, any extra taxa we put into cluster 3</li>
</ul>
</dd>
</dl>
<h2 id="references">References</h2>
<p>[1] Robust and Scalable Models of Microbiome Dynamics, TE Gibson, GK Gerber (2018)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def icml_dynamics(self, n_taxa: int=13):
    &#39;&#39;&#39;Recreate the dynamical system used in the ICML paper [1]. If you use the 
    default parameters you will get the same interaction matrix that was used in [1].

    We rescale the self-interactions and the interactions (and potentially growth) by 150
    so that the time-scales of the trajectories happen over days instead of minutes

    Parameters
    ----------
    n_taxa : int
        - These are how many taxa to include in the system. We will always have 3 clusters and
          the proportion of taxa in each cluster is as follows:
            - cluster 1 - 5/13
            - cluster 2 - 6/13
            - cluster 3 - 2/13
        - We assign each taxoon to each cluster the best we can, any extra taxa we put into cluster 3

    References
    ----------
    [1] Robust and Scalable Models of Microbiome Dynamics, TE Gibson, GK Gerber (2018)
    &#39;&#39;&#39;
    if not pl.isint(n_taxa):
        raise TypeError(&#39;`n_taxa` ({}) must be an int&#39;.format(type(n_taxa)))
    if n_taxa &lt; 3:
        raise ValueError(&#39;`n_taxa` ({}) must be &gt;= 3&#39;.format(n_taxa))
    
    # Make the TaxaSet
    taxa = pl.TaxaSet()
    for aidx in range(n_taxa):
        seq = &#39;&#39;.join(random.choice([&#39;A&#39;, &#39;T&#39;, &#39;G&#39;, &#39;C&#39;]) for _ in range(250))
        taxa.add_taxon(name=&#39;TAXA_{}&#39;.format(aidx+1), sequence=seq)
    self.taxa = taxa

    # Generate the cluster assignments
    c0size = int(5*n_taxa/13)
    c1size = int(6*n_taxa/13)
    c2size = int(n_taxa - c0size - c1size)

    clusters = np.asarray(([0]*c0size) + ([1]*c1size) + ([2]*c2size), dtype=int)
    clustering = pl.Clustering(items=taxa, clusters=clusters, name=STRNAMES.CLUSTERING_OBJ, G=self.G)

    # Generate the interactions
    interactions = pl.Interactions(clustering=clustering, use_indicators=True, G=self.G, 
        name=STRNAMES.INTERACTIONS_OBJ)
    c0 = clustering.order[0]
    c1 = clustering.order[1]
    c2 = clustering.order[2]

    frac = 150*n_taxa/13
    for interaction in interactions:
        if interaction.target_cid == c0 and interaction.source_cid == c1:
            interaction.value = 3/frac
            interaction.indicator = True
        elif interaction.target_cid == c0 and interaction.source_cid == c2:
            interaction.value = -1/frac
            interaction.indicator = True
        elif interaction.target_cid == c2 and interaction.source_cid == c0:
            interaction.value = 2/frac
            interaction.indicator = True
        elif interaction.target_cid == c2 and interaction.source_cid == c1:
            interaction.value = -4/frac
            interaction.indicator = True
        else:
            interaction.value = 0
            interaction.indicator = False
    self_interactions = np.ones(n_taxa, dtype=float) * 5 / 150
    self_interactions = pl.Variable(value=self_interactions, shape=(n_taxa, ),
        name=STRNAMES.SELF_INTERACTION_VALUE, G=self.G)

    A = interactions.get_datalevel_value_matrix()
    for i in range(A.shape[0]):
        A[i,i] = -self_interactions.value[i]
    
    self.model.interactions = A #

    # Generate growth
    self.model.growth = pl.random.uniform.sample(low=0.1, high=0.12, size=n_taxa)</code></pre>
</details>
</dd>
<dt id="mdsine2.synthetic.Synthetic.set_subjects"><code class="name flex">
<span>def <span class="ident">set_subjects</span></span>(<span>self, subjs: List[str])</span>
</code></dt>
<dd>
<div class="desc"><p>Set the names of the subjects</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>subjs</code></strong> :&ensp;<code>list(str)</code></dt>
<dd>A list of all the names of the subjects</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_subjects(self, subjs: List[str]):
    &#39;&#39;&#39;Set the names of the subjects

    Parameters
    ----------
    subjs : list(str)
        A list of all the names of the subjects
    &#39;&#39;&#39;
    self.subjs = subjs</code></pre>
</details>
</dd>
<dt id="mdsine2.synthetic.Synthetic.set_timepoints"><code class="name flex">
<span>def <span class="ident">set_timepoints</span></span>(<span>self, times: numpy.ndarray)</span>
</code></dt>
<dd>
<div class="desc"><p>Set the timepoints of the trajectory</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>times</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_timepoints(self, times: np.ndarray):
    &#39;&#39;&#39;Set the timepoints of the trajectory

    Parameters
    ----------
    times : np.ndarray
    &#39;&#39;&#39;
    if not pl.isarray(times):
        raise TypeError(&#39;`times` ({}) must be an array&#39;.format(times))
    times = np.sort(np.array(times))
    self.times = times</code></pre>
</details>
</dd>
<dt id="mdsine2.synthetic.Synthetic.simulateMeasurementNoise"><code class="name flex">
<span>def <span class="ident">simulateMeasurementNoise</span></span>(<span>self, a0: float, a1: float, qpcr_noise_scale: float, approx_read_depth: int, name: str = 'unnamed-study') ‑> <a title="mdsine2.pylab.base.Study" href="pylab/base.html#mdsine2.pylab.base.Study">Study</a></span>
</code></dt>
<dd>
<div class="desc"><p>This function converts the synthetic trajectories into "real" data
by simulating read counts and qPCR measurements.</p>
<h2 id="simulating-qpcr-measurements">Simulating Qpcr Measurements</h2>
<p>We simulate the qPCR measurements with a lognormal distribution that was
fitted using the data from <code>subjset</code>. We use this parameterization to sample
a qPCR measurement with mean from the total biomass of the simulated data.</p>
<h2 id="simulating-count-data">Simulating Count Data</h2>
<p>First, we get the approximate read depth <code>r_k</code> with <code>approx_read_depth</code>. We then use <code>r_k</code>,
<code>a_0</code>, and <code>a_1</code> with the relative abundances to sample from a negative binomial
distirbution. We then use the relative abundances from this sample as the concentrations
for a multinomial distribution with read depth <code>r_k</code>.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>a0</code></strong>, <strong><code>a1</code></strong> :&ensp;<code>numeric</code></dt>
<dd>These are the negative binomial dispersion parameters that we are using to
simulate the data</dd>
<dt><strong><code>qpcr_noise_scale</code></strong> :&ensp;<code>numeric</code></dt>
<dd>This is the parameter to scale the <code>s</code> parameter learned by the lognormal
distribution.</dd>
<dt><strong><code>approx_read_depth</code></strong> :&ensp;<code>int</code></dt>
<dd>This is the read depth to simulate to</dd>
<dt><strong><code>name</code></strong> :&ensp;<code>str</code></dt>
<dd>Name of the study</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>mdsine2.Study</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def simulateMeasurementNoise(self, a0: float, a1: float, qpcr_noise_scale: float, 
    approx_read_depth: int, name: str=&#39;unnamed-study&#39;) -&gt; Study:
    &#39;&#39;&#39;This function converts the synthetic trajectories into &#34;real&#34; data
    by simulating read counts and qPCR measurements.

    Simulating qPCR measurements
    ----------------------------
    We simulate the qPCR measurements with a lognormal distribution that was
    fitted using the data from `subjset`. We use this parameterization to sample
    a qPCR measurement with mean from the total biomass of the simulated data.

    Simulating count data
    ---------------------
    First, we get the approximate read depth `r_k` with `approx_read_depth`. We then use `r_k`, 
    `a_0`, and `a_1` with the relative abundances to sample from a negative binomial 
    distirbution. We then use the relative abundances from this sample as the concentrations
    for a multinomial distribution with read depth `r_k`.

    Parameters
    ----------
    a0, a1 : numeric
        These are the negative binomial dispersion parameters that we are using to 
        simulate the data
    qpcr_noise_scale : numeric
        This is the parameter to scale the `s` parameter learned by the lognormal
        distribution.
    approx_read_depth : int
        This is the read depth to simulate to
    name : str
        Name of the study

    Returns
    -------
    mdsine2.Study
    &#39;&#39;&#39;
    if self.times is None or self._data is None or self.subjs is None:
        raise ValueError(&#39;Need to fully initialize the system before&#39;)

    logging.info(&#39;Fitting real data&#39;)

    # Make the study object
    study = pl.Study(taxa=self.taxa, name=name)
    for subjname in self.subjs:
        study.add_subject(name=subjname)

    # Add times for each subject
    for subj in study:
        subj.times = self.times
    
    # Make the qPCR measurements
    for subj in study:
        total_abund = np.sum(self._data[subj.name], axis=0)

        for tidx, t in enumerate(self.times):
            # Get the total abundance

            triplicates = np.exp(np.log(total_abund[tidx]) + \
                qpcr_noise_scale * pl.random.normal.sample(size=3))
            subj.qpcr[t] = pl.qPCRdata(cfus=triplicates, mass=1., dilution_factor=1.)
    
    # Make the reads
    for subj in study:
        for tidx, t in enumerate(self.times):

            total_mass = np.sum(self._data[subj.name][:, tidx])
            rel_abund = (self._data[subj.name][:, tidx] + 1e-20) / total_mass

            phi = approx_read_depth * rel_abund
            eps = a0/rel_abund + a1

            # print(phi)
            # print(eps)

            reads = pl.random.negative_binomial.sample(mean=phi, dispersion=eps)
            subj.reads[t] = reads

    # Make the perturbations:
    if self.perturbations is not None and len(self.perturbations) &gt; 0:
        study.perturbations = self.G.perturbations

    return study</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="mdsine2.pylab.base.Saveable" href="pylab/base.html#mdsine2.pylab.base.Saveable">Saveable</a></b></code>:
<ul class="hlist">
<li><code><a title="mdsine2.pylab.base.Saveable.load" href="pylab/base.html#mdsine2.pylab.base.Saveable.load">load</a></code></li>
<li><code><a title="mdsine2.pylab.base.Saveable.save" href="pylab/base.html#mdsine2.pylab.base.Saveable.save">save</a></code></li>
<li><code><a title="mdsine2.pylab.base.Saveable.set_save_location" href="pylab/base.html#mdsine2.pylab.base.Saveable.set_save_location">set_save_location</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="mdsine2" href="index.html">mdsine2</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="mdsine2.synthetic.make_semisynthetic" href="#mdsine2.synthetic.make_semisynthetic">make_semisynthetic</a></code></li>
<li><code><a title="mdsine2.synthetic.subsample_timepoints" href="#mdsine2.synthetic.subsample_timepoints">subsample_timepoints</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="mdsine2.synthetic.Synthetic" href="#mdsine2.synthetic.Synthetic">Synthetic</a></code></h4>
<ul class="">
<li><code><a title="mdsine2.synthetic.Synthetic.generate_trajectories" href="#mdsine2.synthetic.Synthetic.generate_trajectories">generate_trajectories</a></code></li>
<li><code><a title="mdsine2.synthetic.Synthetic.icml_dynamics" href="#mdsine2.synthetic.Synthetic.icml_dynamics">icml_dynamics</a></code></li>
<li><code><a title="mdsine2.synthetic.Synthetic.perturbations" href="#mdsine2.synthetic.Synthetic.perturbations">perturbations</a></code></li>
<li><code><a title="mdsine2.synthetic.Synthetic.set_subjects" href="#mdsine2.synthetic.Synthetic.set_subjects">set_subjects</a></code></li>
<li><code><a title="mdsine2.synthetic.Synthetic.set_timepoints" href="#mdsine2.synthetic.Synthetic.set_timepoints">set_timepoints</a></code></li>
<li><code><a title="mdsine2.synthetic.Synthetic.simulateMeasurementNoise" href="#mdsine2.synthetic.Synthetic.simulateMeasurementNoise">simulateMeasurementNoise</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.1</a>.</p>
</footer>
</body>
</html>